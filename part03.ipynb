{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import random\n",
    "import kagglehub\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as notebook_tqdm # Needed for tqdm in Jupyter Notebook (Certain cell outputs will complain if this is not included)\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torchvision.models import ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EXPERIMENT_NAME = \"fire-smoke-detection-resnet-tuning\"\n",
    "SEED = 42\n",
    "NUM_EPOCHS = 10\n",
    "NUM_TRIALS = 5\n",
    "BATCH_SIZE_OPTIONS = [16, 32, 64, 128]\n",
    "LEARNING_RATE_OPTIONS = [1e-4, 1e-3, 1e-2]\n",
    "WEIGHT_DECAY_OPTIONS = [1e-6, 1e-5, 1e-4]\n",
    "EARLY_STOP_PATIENCE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = kagglehub.dataset_download(\"sayedgamal99/smoke-fire-detection-yolo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation Options\n",
    "Input images are expected to be 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "EVAL_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Custom Dataset\n",
    "The original dataset is structure as such:\n",
    "- []  = 'No Smoke and No Fire'\n",
    "- 0 = 'Smoke Only'\n",
    "- 1 = 'Fire and Smoke'\n",
    "\n",
    "The custom dataset modifies this as such:\n",
    "- 0 = 'No Smoke and No Fire'\n",
    "- 1 = 'Smoke Only'\n",
    "- 2 = 'Fire and Smoke'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_content = f.read().strip()\n",
    "\n",
    "        # 0: none, 1: smoke, 2: fire\n",
    "        if not label_content:\n",
    "            label = 0\n",
    "        else:\n",
    "            first_number = int(label_content.split()[0])\n",
    "            label = 1 if first_number == 0 else 2\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code\n",
    "The training parameters are provided by the Optuna Trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_params(params: dict, train_dataset, val_dataset, test_dataset) -> float:\n",
    "    \"\"\"\n",
    "    Train the model using the provided parameters and datasets.\n",
    "    Returns the best validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = params[\"batch_size\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    early_stop_patience = params[\"early_stop_patience\"]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = DEVICE\n",
    "    model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 3)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1)\n",
    "\n",
    "    run_name = f\"bs{batch_size}_lr{learning_rate:.0e}_wd{weight_decay:.0e}\"\n",
    "    with mlflow.start_run(nested=True, run_name=run_name):\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "        mlflow.log_param(\"early_stop_patience\", early_stop_patience)\n",
    "        mlflow.log_param(\"optimizer\", optimizer.__class__.__name__)\n",
    "        mlflow.log_param(\"scheduler\", scheduler.__class__.__name__)\n",
    "        mlflow.log_param(\"platform\", platform.platform())\n",
    "        mlflow.log_param(\"python_version\", platform.python_version())\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        best_epoch = -1\n",
    "        epochs_no_improve = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch_idx, (inputs, labels) in enumerate(train_loader, 1):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "                mlflow.log_metric(\"batch_training_loss\", loss.item(), step=epoch * len(train_loader) + batch_idx)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    v_loss = criterion(val_outputs, val_labels)\n",
    "                    val_loss += v_loss.item()\n",
    "                    _, val_predicted = torch.max(val_outputs, 1)\n",
    "                    val_correct += (val_predicted == val_labels).sum().item()\n",
    "                    val_total += val_labels.size(0)\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "            mlflow.log_metric(\"training_loss\", avg_train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"validation_loss\", avg_val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"validation_accuracy\", val_accuracy, step=epoch)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "            }\n",
    "            checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            mlflow.log_artifact(checkpoint_path)\n",
    "            os.remove(checkpoint_path)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = model.state_dict()\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_epoch = epoch\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= early_stop_patience:\n",
    "                    break\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "            best_val_accuracy = max(val_losses)\n",
    "            mlflow.log_metric(\"best_val_accuracy\", best_val_accuracy)\n",
    "            mlflow.log_metric(\"learning_rate\", optimizer.param_groups[0]['lr'], step=epoch)\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            best_checkpoint = {\n",
    "                \"epoch\": best_epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "            }\n",
    "            torch.save(best_checkpoint, \"best_model.pth\")\n",
    "            mlflow.log_artifact(\"best_model.pth\")\n",
    "            os.remove(\"best_model.pth\")\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_labels in test_loader:\n",
    "                test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "                outputs = model(test_inputs)\n",
    "                loss = criterion(outputs, test_labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_correct += (predicted == test_labels).sum().item()\n",
    "                test_total += test_labels.size(0)\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = test_correct / test_total if test_total > 0 else 0\n",
    "        mlflow.log_metric(\"test_loss\", avg_test_loss)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        # Plot the confusion matrix\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_labels in test_loader:\n",
    "                test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "                outputs = model(test_inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true.extend(test_labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(\"Test Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"test_confusion_matrix.png\")\n",
    "        mlflow.log_artifact(\"test_confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        os.remove(\"test_confusion_matrix.png\")\n",
    "\n",
    "        # Calculate average training loss per epoch\n",
    "        num_batches_per_epoch = len(train_loader)\n",
    "        train_loss_per_epoch = [\n",
    "            np.mean(train_losses[i * num_batches_per_epoch : (i + 1) * num_batches_per_epoch])\n",
    "            for i in range(len(val_losses))\n",
    "        ]\n",
    "\n",
    "        # Plot training loss per epoch\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, len(train_loss_per_epoch) + 1), train_loss_per_epoch, marker=\"o\", label=\"Training Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"training_loss_per_epoch.png\")\n",
    "        mlflow.log_artifact(\"training_loss_per_epoch.png\")\n",
    "        plt.close()\n",
    "        os.remove(\"training_loss_per_epoch.png\")\n",
    "\n",
    "        # Plot validation loss per epoch\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, marker=\"o\", color=\"orange\", label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Validation Loss per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"validation_loss_per_epoch.png\")\n",
    "        mlflow.log_artifact(\"validation_loss_per_epoch.png\")\n",
    "        plt.close()\n",
    "        os.remove(\"validation_loss_per_epoch.png\")\n",
    "\n",
    "        return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and Trails Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_dataset, val_dataset, test_dataset):\n",
    "    params = {\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", BATCH_SIZE_OPTIONS),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", LEARNING_RATE_OPTIONS[0], LEARNING_RATE_OPTIONS[-1]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", WEIGHT_DECAY_OPTIONS[0], WEIGHT_DECAY_OPTIONS[-1], log=True),\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "    }\n",
    "    return train_with_params(params, train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "def start_experiment():\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    # Load training dataset\n",
    "    train_dataset = CustomDataset(\n",
    "        images_dir= os.path.join(DATASET_PATH, \"data/train/images\"),\n",
    "        labels_dir= os.path.join(DATASET_PATH, \"data/train/labels\"),\n",
    "        transform=TRAIN_TRANSFORM\n",
    "    )\n",
    "\n",
    "    # Load validation dataset\n",
    "    val_dataset = CustomDataset(\n",
    "        images_dir= os.path.join(DATASET_PATH, \"data/val/images\"),\n",
    "        labels_dir= os.path.join(DATASET_PATH, \"data/val/labels\"),\n",
    "        transform=EVAL_TRANSFORM\n",
    "    )\n",
    "\n",
    "    # Load test dataset\n",
    "    test_dataset = CustomDataset(\n",
    "        images_dir= os.path.join(DATASET_PATH, \"data/test/images\"),\n",
    "        labels_dir= os.path.join(DATASET_PATH, \"data/test/labels\"),\n",
    "        transform=EVAL_TRANSFORM\n",
    "    )\n",
    "\n",
    "    # Create study\n",
    "    study = optuna.create_study(direction=\"minimize\", study_name=EXPERIMENT_NAME)\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, train_dataset, val_dataset, test_dataset),\n",
    "        n_trials=NUM_TRIALS\n",
    "    )\n",
    "\n",
    "    # Print best trial\n",
    "    print(\"Best trial:\")\n",
    "    print(f\"  Value (best validation loss): {study.best_trial.value}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    # Log best trial info with MLflow\n",
    "    mlflow.log_metric(\"best_val_loss\", study.best_trial.value)\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        mlflow.log_param(f\"best_{key}\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-27 19:21:15,696] A new study created in memory with name: fire-smoke-detection-resnet-tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/111, Training Loss: 1.2017\n",
      "Epoch 1/10, Batch 2/111, Training Loss: 0.8627\n",
      "Epoch 1/10, Batch 3/111, Training Loss: 0.8118\n",
      "Epoch 1/10, Batch 4/111, Training Loss: 0.8121\n",
      "Epoch 1/10, Batch 5/111, Training Loss: 0.7322\n",
      "Epoch 1/10, Batch 6/111, Training Loss: 0.5535\n",
      "Epoch 1/10, Batch 7/111, Training Loss: 0.6809\n",
      "Epoch 1/10, Batch 8/111, Training Loss: 0.5731\n",
      "Epoch 1/10, Batch 9/111, Training Loss: 0.5743\n",
      "Epoch 1/10, Batch 10/111, Training Loss: 0.5422\n",
      "Epoch 1/10, Batch 11/111, Training Loss: 0.6057\n",
      "Epoch 1/10, Batch 12/111, Training Loss: 0.6286\n",
      "Epoch 1/10, Batch 13/111, Training Loss: 0.6150\n",
      "Epoch 1/10, Batch 14/111, Training Loss: 0.6555\n",
      "Epoch 1/10, Batch 15/111, Training Loss: 0.6015\n",
      "Epoch 1/10, Batch 16/111, Training Loss: 0.4681\n",
      "Epoch 1/10, Batch 17/111, Training Loss: 0.6879\n",
      "Epoch 1/10, Batch 18/111, Training Loss: 0.5191\n",
      "Epoch 1/10, Batch 19/111, Training Loss: 0.5891\n",
      "Epoch 1/10, Batch 20/111, Training Loss: 0.3996\n",
      "Epoch 1/10, Batch 21/111, Training Loss: 0.6512\n",
      "Epoch 1/10, Batch 22/111, Training Loss: 0.3615\n",
      "Epoch 1/10, Batch 23/111, Training Loss: 0.4965\n",
      "Epoch 1/10, Batch 24/111, Training Loss: 0.4663\n",
      "Epoch 1/10, Batch 25/111, Training Loss: 0.4839\n",
      "Epoch 1/10, Batch 26/111, Training Loss: 0.5440\n",
      "Epoch 1/10, Batch 27/111, Training Loss: 0.3634\n",
      "Epoch 1/10, Batch 28/111, Training Loss: 0.5123\n",
      "Epoch 1/10, Batch 29/111, Training Loss: 0.3869\n",
      "Epoch 1/10, Batch 30/111, Training Loss: 0.5010\n",
      "Epoch 1/10, Batch 31/111, Training Loss: 0.3897\n",
      "Epoch 1/10, Batch 32/111, Training Loss: 0.3755\n",
      "Epoch 1/10, Batch 33/111, Training Loss: 0.5564\n",
      "Epoch 1/10, Batch 34/111, Training Loss: 0.3908\n",
      "Epoch 1/10, Batch 35/111, Training Loss: 0.6738\n",
      "Epoch 1/10, Batch 36/111, Training Loss: 0.3861\n",
      "Epoch 1/10, Batch 37/111, Training Loss: 0.6428\n",
      "Epoch 1/10, Batch 38/111, Training Loss: 0.4343\n",
      "Epoch 1/10, Batch 39/111, Training Loss: 0.4197\n",
      "Epoch 1/10, Batch 40/111, Training Loss: 0.6556\n",
      "Epoch 1/10, Batch 41/111, Training Loss: 0.4404\n",
      "Epoch 1/10, Batch 42/111, Training Loss: 0.3500\n",
      "Epoch 1/10, Batch 43/111, Training Loss: 0.3940\n",
      "Epoch 1/10, Batch 44/111, Training Loss: 0.4672\n",
      "Epoch 1/10, Batch 45/111, Training Loss: 0.4839\n",
      "Epoch 1/10, Batch 46/111, Training Loss: 0.4095\n",
      "Epoch 1/10, Batch 47/111, Training Loss: 0.4524\n",
      "Epoch 1/10, Batch 48/111, Training Loss: 0.3956\n",
      "Epoch 1/10, Batch 49/111, Training Loss: 0.3423\n",
      "Epoch 1/10, Batch 50/111, Training Loss: 0.4005\n",
      "Epoch 1/10, Batch 51/111, Training Loss: 0.3692\n",
      "Epoch 1/10, Batch 52/111, Training Loss: 0.3914\n",
      "Epoch 1/10, Batch 53/111, Training Loss: 0.3523\n",
      "Epoch 1/10, Batch 54/111, Training Loss: 0.4379\n",
      "Epoch 1/10, Batch 55/111, Training Loss: 0.4090\n",
      "Epoch 1/10, Batch 56/111, Training Loss: 0.5111\n",
      "Epoch 1/10, Batch 57/111, Training Loss: 0.4100\n",
      "Epoch 1/10, Batch 58/111, Training Loss: 0.3614\n",
      "Epoch 1/10, Batch 59/111, Training Loss: 0.4457\n",
      "Epoch 1/10, Batch 60/111, Training Loss: 0.4005\n",
      "Epoch 1/10, Batch 61/111, Training Loss: 0.3183\n",
      "Epoch 1/10, Batch 62/111, Training Loss: 0.4674\n",
      "Epoch 1/10, Batch 63/111, Training Loss: 0.4323\n",
      "Epoch 1/10, Batch 64/111, Training Loss: 0.5229\n",
      "Epoch 1/10, Batch 65/111, Training Loss: 0.3897\n",
      "Epoch 1/10, Batch 66/111, Training Loss: 0.2452\n",
      "Epoch 1/10, Batch 67/111, Training Loss: 0.3825\n",
      "Epoch 1/10, Batch 68/111, Training Loss: 0.3799\n",
      "Epoch 1/10, Batch 69/111, Training Loss: 0.2588\n",
      "Epoch 1/10, Batch 70/111, Training Loss: 0.3773\n",
      "Epoch 1/10, Batch 71/111, Training Loss: 0.3180\n",
      "Epoch 1/10, Batch 72/111, Training Loss: 0.3289\n",
      "Epoch 1/10, Batch 73/111, Training Loss: 0.3643\n",
      "Epoch 1/10, Batch 74/111, Training Loss: 0.4292\n",
      "Epoch 1/10, Batch 75/111, Training Loss: 0.4024\n",
      "Epoch 1/10, Batch 76/111, Training Loss: 0.4330\n",
      "Epoch 1/10, Batch 77/111, Training Loss: 0.3224\n",
      "Epoch 1/10, Batch 78/111, Training Loss: 0.3958\n",
      "Epoch 1/10, Batch 79/111, Training Loss: 0.3099\n",
      "Epoch 1/10, Batch 80/111, Training Loss: 0.3321\n",
      "Epoch 1/10, Batch 81/111, Training Loss: 0.4588\n",
      "Epoch 1/10, Batch 82/111, Training Loss: 0.4108\n",
      "Epoch 1/10, Batch 83/111, Training Loss: 0.3044\n",
      "Epoch 1/10, Batch 84/111, Training Loss: 0.3698\n",
      "Epoch 1/10, Batch 85/111, Training Loss: 0.2832\n",
      "Epoch 1/10, Batch 86/111, Training Loss: 0.3930\n",
      "Epoch 1/10, Batch 87/111, Training Loss: 0.4793\n",
      "Epoch 1/10, Batch 88/111, Training Loss: 0.3421\n",
      "Epoch 1/10, Batch 89/111, Training Loss: 0.5755\n",
      "Epoch 1/10, Batch 90/111, Training Loss: 0.3339\n",
      "Epoch 1/10, Batch 91/111, Training Loss: 0.3920\n",
      "Epoch 1/10, Batch 92/111, Training Loss: 0.3350\n",
      "Epoch 1/10, Batch 93/111, Training Loss: 0.3148\n",
      "Epoch 1/10, Batch 94/111, Training Loss: 0.3350\n",
      "Epoch 1/10, Batch 95/111, Training Loss: 0.3001\n",
      "Epoch 1/10, Batch 96/111, Training Loss: 0.3046\n",
      "Epoch 1/10, Batch 97/111, Training Loss: 0.3340\n",
      "Epoch 1/10, Batch 98/111, Training Loss: 0.3630\n",
      "Epoch 1/10, Batch 99/111, Training Loss: 0.3847\n",
      "Epoch 1/10, Batch 100/111, Training Loss: 0.2819\n",
      "Epoch 1/10, Batch 101/111, Training Loss: 0.2317\n",
      "Epoch 1/10, Batch 102/111, Training Loss: 0.4920\n",
      "Epoch 1/10, Batch 103/111, Training Loss: 0.4503\n",
      "Epoch 1/10, Batch 104/111, Training Loss: 0.2937\n",
      "Epoch 1/10, Batch 105/111, Training Loss: 0.2815\n",
      "Epoch 1/10, Batch 106/111, Training Loss: 0.3691\n",
      "Epoch 1/10, Batch 107/111, Training Loss: 0.3592\n",
      "Epoch 1/10, Batch 108/111, Training Loss: 0.3324\n",
      "Epoch 1/10, Batch 109/111, Training Loss: 0.2996\n",
      "Epoch 1/10, Batch 110/111, Training Loss: 0.2822\n",
      "Epoch 1/10, Batch 111/111, Training Loss: 0.3221\n",
      "Epoch 1/10, Training Loss: 0.4455, Validation Loss: 0.4898, Validation Accuracy: 0.8145\n",
      "Epoch 2/10, Batch 1/111, Training Loss: 0.2949\n",
      "Epoch 2/10, Batch 2/111, Training Loss: 0.2470\n",
      "Epoch 2/10, Batch 3/111, Training Loss: 0.2596\n",
      "Epoch 2/10, Batch 4/111, Training Loss: 0.2496\n",
      "Epoch 2/10, Batch 5/111, Training Loss: 0.2504\n",
      "Epoch 2/10, Batch 6/111, Training Loss: 0.2674\n",
      "Epoch 2/10, Batch 7/111, Training Loss: 0.3215\n",
      "Epoch 2/10, Batch 8/111, Training Loss: 0.2741\n",
      "Epoch 2/10, Batch 9/111, Training Loss: 0.2502\n",
      "Epoch 2/10, Batch 10/111, Training Loss: 0.2327\n",
      "Epoch 2/10, Batch 11/111, Training Loss: 0.2075\n",
      "Epoch 2/10, Batch 12/111, Training Loss: 0.3382\n",
      "Epoch 2/10, Batch 13/111, Training Loss: 0.2148\n",
      "Epoch 2/10, Batch 14/111, Training Loss: 0.3156\n",
      "Epoch 2/10, Batch 15/111, Training Loss: 0.3372\n",
      "Epoch 2/10, Batch 16/111, Training Loss: 0.3367\n",
      "Epoch 2/10, Batch 17/111, Training Loss: 0.2861\n",
      "Epoch 2/10, Batch 18/111, Training Loss: 0.2285\n",
      "Epoch 2/10, Batch 19/111, Training Loss: 0.2114\n",
      "Epoch 2/10, Batch 20/111, Training Loss: 0.2879\n",
      "Epoch 2/10, Batch 21/111, Training Loss: 0.3184\n",
      "Epoch 2/10, Batch 22/111, Training Loss: 0.3237\n",
      "Epoch 2/10, Batch 23/111, Training Loss: 0.3074\n",
      "Epoch 2/10, Batch 24/111, Training Loss: 0.2350\n",
      "Epoch 2/10, Batch 25/111, Training Loss: 0.3397\n",
      "Epoch 2/10, Batch 26/111, Training Loss: 0.3064\n",
      "Epoch 2/10, Batch 27/111, Training Loss: 0.2917\n",
      "Epoch 2/10, Batch 28/111, Training Loss: 0.2539\n",
      "Epoch 2/10, Batch 29/111, Training Loss: 0.2139\n",
      "Epoch 2/10, Batch 30/111, Training Loss: 0.2389\n",
      "Epoch 2/10, Batch 31/111, Training Loss: 0.1948\n",
      "Epoch 2/10, Batch 32/111, Training Loss: 0.2443\n",
      "Epoch 2/10, Batch 33/111, Training Loss: 0.2410\n",
      "Epoch 2/10, Batch 34/111, Training Loss: 0.1842\n",
      "Epoch 2/10, Batch 35/111, Training Loss: 0.2243\n",
      "Epoch 2/10, Batch 36/111, Training Loss: 0.4080\n",
      "Epoch 2/10, Batch 37/111, Training Loss: 0.2789\n",
      "Epoch 2/10, Batch 38/111, Training Loss: 0.3611\n",
      "Epoch 2/10, Batch 39/111, Training Loss: 0.4648\n",
      "Epoch 2/10, Batch 40/111, Training Loss: 0.3512\n",
      "Epoch 2/10, Batch 41/111, Training Loss: 0.1392\n",
      "Epoch 2/10, Batch 42/111, Training Loss: 0.2546\n",
      "Epoch 2/10, Batch 43/111, Training Loss: 0.2976\n",
      "Epoch 2/10, Batch 44/111, Training Loss: 0.2591\n",
      "Epoch 2/10, Batch 45/111, Training Loss: 0.3100\n",
      "Epoch 2/10, Batch 46/111, Training Loss: 0.2024\n",
      "Epoch 2/10, Batch 47/111, Training Loss: 0.3066\n",
      "Epoch 2/10, Batch 48/111, Training Loss: 0.2302\n",
      "Epoch 2/10, Batch 49/111, Training Loss: 0.1688\n",
      "Epoch 2/10, Batch 50/111, Training Loss: 0.3079\n",
      "Epoch 2/10, Batch 51/111, Training Loss: 0.2567\n",
      "Epoch 2/10, Batch 52/111, Training Loss: 0.2982\n",
      "Epoch 2/10, Batch 53/111, Training Loss: 0.2705\n",
      "Epoch 2/10, Batch 54/111, Training Loss: 0.3215\n",
      "Epoch 2/10, Batch 55/111, Training Loss: 0.2054\n",
      "Epoch 2/10, Batch 56/111, Training Loss: 0.3395\n",
      "Epoch 2/10, Batch 57/111, Training Loss: 0.2307\n",
      "Epoch 2/10, Batch 58/111, Training Loss: 0.2584\n",
      "Epoch 2/10, Batch 59/111, Training Loss: 0.2895\n",
      "Epoch 2/10, Batch 60/111, Training Loss: 0.2742\n",
      "Epoch 2/10, Batch 61/111, Training Loss: 0.2649\n",
      "Epoch 2/10, Batch 62/111, Training Loss: 0.3732\n",
      "Epoch 2/10, Batch 63/111, Training Loss: 0.3456\n",
      "Epoch 2/10, Batch 64/111, Training Loss: 0.2607\n",
      "Epoch 2/10, Batch 65/111, Training Loss: 0.3312\n",
      "Epoch 2/10, Batch 66/111, Training Loss: 0.2947\n",
      "Epoch 2/10, Batch 67/111, Training Loss: 0.2000\n",
      "Epoch 2/10, Batch 68/111, Training Loss: 0.3913\n",
      "Epoch 2/10, Batch 69/111, Training Loss: 0.2846\n",
      "Epoch 2/10, Batch 70/111, Training Loss: 0.3729\n",
      "Epoch 2/10, Batch 71/111, Training Loss: 0.2952\n",
      "Epoch 2/10, Batch 72/111, Training Loss: 0.3195\n",
      "Epoch 2/10, Batch 73/111, Training Loss: 0.2633\n",
      "Epoch 2/10, Batch 74/111, Training Loss: 0.3480\n",
      "Epoch 2/10, Batch 75/111, Training Loss: 0.3586\n",
      "Epoch 2/10, Batch 76/111, Training Loss: 0.2177\n",
      "Epoch 2/10, Batch 77/111, Training Loss: 0.2985\n",
      "Epoch 2/10, Batch 78/111, Training Loss: 0.3969\n",
      "Epoch 2/10, Batch 79/111, Training Loss: 0.1781\n",
      "Epoch 2/10, Batch 80/111, Training Loss: 0.4170\n",
      "Epoch 2/10, Batch 81/111, Training Loss: 0.4080\n",
      "Epoch 2/10, Batch 82/111, Training Loss: 0.1772\n",
      "Epoch 2/10, Batch 83/111, Training Loss: 0.2119\n",
      "Epoch 2/10, Batch 84/111, Training Loss: 0.2885\n",
      "Epoch 2/10, Batch 85/111, Training Loss: 0.2024\n",
      "Epoch 2/10, Batch 86/111, Training Loss: 0.2495\n",
      "Epoch 2/10, Batch 87/111, Training Loss: 0.3316\n",
      "Epoch 2/10, Batch 88/111, Training Loss: 0.2844\n",
      "Epoch 2/10, Batch 89/111, Training Loss: 0.2194\n",
      "Epoch 2/10, Batch 90/111, Training Loss: 0.1991\n",
      "Epoch 2/10, Batch 91/111, Training Loss: 0.2369\n",
      "Epoch 2/10, Batch 92/111, Training Loss: 0.2458\n",
      "Epoch 2/10, Batch 93/111, Training Loss: 0.2943\n",
      "Epoch 2/10, Batch 94/111, Training Loss: 0.2803\n",
      "Epoch 2/10, Batch 95/111, Training Loss: 0.3162\n",
      "Epoch 2/10, Batch 96/111, Training Loss: 0.3394\n",
      "Epoch 2/10, Batch 97/111, Training Loss: 0.3220\n",
      "Epoch 2/10, Batch 98/111, Training Loss: 0.2694\n",
      "Epoch 2/10, Batch 99/111, Training Loss: 0.2341\n",
      "Epoch 2/10, Batch 100/111, Training Loss: 0.2523\n",
      "Epoch 2/10, Batch 101/111, Training Loss: 0.3265\n",
      "Epoch 2/10, Batch 102/111, Training Loss: 0.3149\n",
      "Epoch 2/10, Batch 103/111, Training Loss: 0.2486\n",
      "Epoch 2/10, Batch 104/111, Training Loss: 0.2894\n",
      "Epoch 2/10, Batch 105/111, Training Loss: 0.1835\n",
      "Epoch 2/10, Batch 106/111, Training Loss: 0.3024\n",
      "Epoch 2/10, Batch 107/111, Training Loss: 0.1930\n",
      "Epoch 2/10, Batch 108/111, Training Loss: 0.2805\n",
      "Epoch 2/10, Batch 109/111, Training Loss: 0.3260\n",
      "Epoch 2/10, Batch 110/111, Training Loss: 0.2266\n",
      "Epoch 2/10, Batch 111/111, Training Loss: 0.1805\n",
      "Epoch 2/10, Training Loss: 0.2780, Validation Loss: 0.5498, Validation Accuracy: 0.7980\n",
      "Epoch 3/10, Batch 1/111, Training Loss: 0.1115\n",
      "Epoch 3/10, Batch 2/111, Training Loss: 0.2208\n",
      "Epoch 3/10, Batch 3/111, Training Loss: 0.2354\n",
      "Epoch 3/10, Batch 4/111, Training Loss: 0.3274\n",
      "Epoch 3/10, Batch 5/111, Training Loss: 0.1873\n",
      "Epoch 3/10, Batch 6/111, Training Loss: 0.2527\n",
      "Epoch 3/10, Batch 7/111, Training Loss: 0.1371\n",
      "Epoch 3/10, Batch 8/111, Training Loss: 0.1474\n",
      "Epoch 3/10, Batch 9/111, Training Loss: 0.1498\n",
      "Epoch 3/10, Batch 10/111, Training Loss: 0.2116\n",
      "Epoch 3/10, Batch 11/111, Training Loss: 0.3081\n",
      "Epoch 3/10, Batch 12/111, Training Loss: 0.2037\n",
      "Epoch 3/10, Batch 13/111, Training Loss: 0.2246\n",
      "Epoch 3/10, Batch 14/111, Training Loss: 0.2822\n",
      "Epoch 3/10, Batch 15/111, Training Loss: 0.1688\n",
      "Epoch 3/10, Batch 16/111, Training Loss: 0.1172\n",
      "Epoch 3/10, Batch 17/111, Training Loss: 0.2116\n",
      "Epoch 3/10, Batch 18/111, Training Loss: 0.1977\n",
      "Epoch 3/10, Batch 19/111, Training Loss: 0.1476\n",
      "Epoch 3/10, Batch 20/111, Training Loss: 0.2815\n",
      "Epoch 3/10, Batch 21/111, Training Loss: 0.1294\n",
      "Epoch 3/10, Batch 22/111, Training Loss: 0.1606\n",
      "Epoch 3/10, Batch 23/111, Training Loss: 0.3213\n",
      "Epoch 3/10, Batch 24/111, Training Loss: 0.1331\n",
      "Epoch 3/10, Batch 25/111, Training Loss: 0.2195\n",
      "Epoch 3/10, Batch 26/111, Training Loss: 0.2462\n",
      "Epoch 3/10, Batch 27/111, Training Loss: 0.2955\n",
      "Epoch 3/10, Batch 28/111, Training Loss: 0.2063\n",
      "Epoch 3/10, Batch 29/111, Training Loss: 0.2222\n",
      "Epoch 3/10, Batch 30/111, Training Loss: 0.1699\n",
      "Epoch 3/10, Batch 31/111, Training Loss: 0.1891\n",
      "Epoch 3/10, Batch 32/111, Training Loss: 0.2109\n",
      "Epoch 3/10, Batch 33/111, Training Loss: 0.1729\n",
      "Epoch 3/10, Batch 34/111, Training Loss: 0.2209\n",
      "Epoch 3/10, Batch 35/111, Training Loss: 0.1566\n",
      "Epoch 3/10, Batch 36/111, Training Loss: 0.2373\n",
      "Epoch 3/10, Batch 37/111, Training Loss: 0.2100\n",
      "Epoch 3/10, Batch 38/111, Training Loss: 0.2012\n",
      "Epoch 3/10, Batch 39/111, Training Loss: 0.1657\n",
      "Epoch 3/10, Batch 40/111, Training Loss: 0.1817\n",
      "Epoch 3/10, Batch 41/111, Training Loss: 0.2761\n",
      "Epoch 3/10, Batch 42/111, Training Loss: 0.3069\n",
      "Epoch 3/10, Batch 43/111, Training Loss: 0.2188\n",
      "Epoch 3/10, Batch 44/111, Training Loss: 0.1782\n",
      "Epoch 3/10, Batch 45/111, Training Loss: 0.1346\n",
      "Epoch 3/10, Batch 46/111, Training Loss: 0.1896\n",
      "Epoch 3/10, Batch 47/111, Training Loss: 0.2221\n",
      "Epoch 3/10, Batch 48/111, Training Loss: 0.1995\n",
      "Epoch 3/10, Batch 49/111, Training Loss: 0.1879\n",
      "Epoch 3/10, Batch 50/111, Training Loss: 0.2093\n",
      "Epoch 3/10, Batch 51/111, Training Loss: 0.2547\n",
      "Epoch 3/10, Batch 52/111, Training Loss: 0.1424\n",
      "Epoch 3/10, Batch 53/111, Training Loss: 0.2147\n",
      "Epoch 3/10, Batch 54/111, Training Loss: 0.2916\n",
      "Epoch 3/10, Batch 55/111, Training Loss: 0.2219\n",
      "Epoch 3/10, Batch 56/111, Training Loss: 0.1698\n",
      "Epoch 3/10, Batch 57/111, Training Loss: 0.3068\n",
      "Epoch 3/10, Batch 58/111, Training Loss: 0.1838\n",
      "Epoch 3/10, Batch 59/111, Training Loss: 0.1937\n",
      "Epoch 3/10, Batch 60/111, Training Loss: 0.1994\n",
      "Epoch 3/10, Batch 61/111, Training Loss: 0.2814\n",
      "Epoch 3/10, Batch 62/111, Training Loss: 0.1703\n",
      "Epoch 3/10, Batch 63/111, Training Loss: 0.2258\n",
      "Epoch 3/10, Batch 64/111, Training Loss: 0.1952\n",
      "Epoch 3/10, Batch 65/111, Training Loss: 0.2021\n",
      "Epoch 3/10, Batch 66/111, Training Loss: 0.1580\n",
      "Epoch 3/10, Batch 67/111, Training Loss: 0.2588\n",
      "Epoch 3/10, Batch 68/111, Training Loss: 0.1913\n",
      "Epoch 3/10, Batch 69/111, Training Loss: 0.2352\n",
      "Epoch 3/10, Batch 70/111, Training Loss: 0.2008\n",
      "Epoch 3/10, Batch 71/111, Training Loss: 0.2366\n",
      "Epoch 3/10, Batch 72/111, Training Loss: 0.2043\n",
      "Epoch 3/10, Batch 73/111, Training Loss: 0.2086\n",
      "Epoch 3/10, Batch 74/111, Training Loss: 0.1395\n",
      "Epoch 3/10, Batch 75/111, Training Loss: 0.2094\n",
      "Epoch 3/10, Batch 76/111, Training Loss: 0.2374\n",
      "Epoch 3/10, Batch 77/111, Training Loss: 0.1640\n",
      "Epoch 3/10, Batch 78/111, Training Loss: 0.1240\n",
      "Epoch 3/10, Batch 79/111, Training Loss: 0.1978\n",
      "Epoch 3/10, Batch 80/111, Training Loss: 0.2286\n",
      "Epoch 3/10, Batch 81/111, Training Loss: 0.1939\n",
      "Epoch 3/10, Batch 82/111, Training Loss: 0.3275\n",
      "Epoch 3/10, Batch 83/111, Training Loss: 0.2361\n",
      "Epoch 3/10, Batch 84/111, Training Loss: 0.2511\n",
      "Epoch 3/10, Batch 85/111, Training Loss: 0.2348\n",
      "Epoch 3/10, Batch 86/111, Training Loss: 0.3136\n",
      "Epoch 3/10, Batch 87/111, Training Loss: 0.2228\n",
      "Epoch 3/10, Batch 88/111, Training Loss: 0.2456\n",
      "Epoch 3/10, Batch 89/111, Training Loss: 0.1220\n",
      "Epoch 3/10, Batch 90/111, Training Loss: 0.3383\n",
      "Epoch 3/10, Batch 91/111, Training Loss: 0.1711\n",
      "Epoch 3/10, Batch 92/111, Training Loss: 0.1765\n",
      "Epoch 3/10, Batch 93/111, Training Loss: 0.3056\n",
      "Epoch 3/10, Batch 94/111, Training Loss: 0.1473\n",
      "Epoch 3/10, Batch 95/111, Training Loss: 0.3232\n",
      "Epoch 3/10, Batch 96/111, Training Loss: 0.3230\n",
      "Epoch 3/10, Batch 97/111, Training Loss: 0.1952\n",
      "Epoch 3/10, Batch 98/111, Training Loss: 0.2772\n",
      "Epoch 3/10, Batch 99/111, Training Loss: 0.1815\n",
      "Epoch 3/10, Batch 100/111, Training Loss: 0.3003\n",
      "Epoch 3/10, Batch 101/111, Training Loss: 0.2113\n",
      "Epoch 3/10, Batch 102/111, Training Loss: 0.2271\n",
      "Epoch 3/10, Batch 103/111, Training Loss: 0.2095\n",
      "Epoch 3/10, Batch 104/111, Training Loss: 0.2585\n",
      "Epoch 3/10, Batch 105/111, Training Loss: 0.2265\n",
      "Epoch 3/10, Batch 106/111, Training Loss: 0.1850\n",
      "Epoch 3/10, Batch 107/111, Training Loss: 0.2997\n",
      "Epoch 3/10, Batch 108/111, Training Loss: 0.2052\n",
      "Epoch 3/10, Batch 109/111, Training Loss: 0.2867\n",
      "Epoch 3/10, Batch 110/111, Training Loss: 0.3108\n",
      "Epoch 3/10, Batch 111/111, Training Loss: 0.2819\n",
      "Epoch 3/10, Training Loss: 0.2174, Validation Loss: 0.3628, Validation Accuracy: 0.8548\n",
      "Epoch 4/10, Batch 1/111, Training Loss: 0.2772\n",
      "Epoch 4/10, Batch 2/111, Training Loss: 0.2196\n",
      "Epoch 4/10, Batch 3/111, Training Loss: 0.1717\n",
      "Epoch 4/10, Batch 4/111, Training Loss: 0.2448\n",
      "Epoch 4/10, Batch 5/111, Training Loss: 0.3001\n",
      "Epoch 4/10, Batch 6/111, Training Loss: 0.1547\n",
      "Epoch 4/10, Batch 7/111, Training Loss: 0.3030\n",
      "Epoch 4/10, Batch 8/111, Training Loss: 0.1839\n",
      "Epoch 4/10, Batch 9/111, Training Loss: 0.2502\n",
      "Epoch 4/10, Batch 10/111, Training Loss: 0.2099\n",
      "Epoch 4/10, Batch 11/111, Training Loss: 0.1893\n",
      "Epoch 4/10, Batch 12/111, Training Loss: 0.2112\n",
      "Epoch 4/10, Batch 13/111, Training Loss: 0.2068\n",
      "Epoch 4/10, Batch 14/111, Training Loss: 0.1519\n",
      "Epoch 4/10, Batch 15/111, Training Loss: 0.1926\n",
      "Epoch 4/10, Batch 16/111, Training Loss: 0.1975\n",
      "Epoch 4/10, Batch 17/111, Training Loss: 0.2152\n",
      "Epoch 4/10, Batch 18/111, Training Loss: 0.2483\n",
      "Epoch 4/10, Batch 19/111, Training Loss: 0.1825\n",
      "Epoch 4/10, Batch 20/111, Training Loss: 0.1581\n",
      "Epoch 4/10, Batch 21/111, Training Loss: 0.1881\n",
      "Epoch 4/10, Batch 22/111, Training Loss: 0.1554\n",
      "Epoch 4/10, Batch 23/111, Training Loss: 0.1797\n",
      "Epoch 4/10, Batch 24/111, Training Loss: 0.2431\n",
      "Epoch 4/10, Batch 25/111, Training Loss: 0.1850\n",
      "Epoch 4/10, Batch 26/111, Training Loss: 0.2188\n",
      "Epoch 4/10, Batch 27/111, Training Loss: 0.2356\n",
      "Epoch 4/10, Batch 28/111, Training Loss: 0.1437\n",
      "Epoch 4/10, Batch 29/111, Training Loss: 0.2565\n",
      "Epoch 4/10, Batch 30/111, Training Loss: 0.1280\n",
      "Epoch 4/10, Batch 31/111, Training Loss: 0.1475\n",
      "Epoch 4/10, Batch 32/111, Training Loss: 0.1371\n",
      "Epoch 4/10, Batch 33/111, Training Loss: 0.2457\n",
      "Epoch 4/10, Batch 34/111, Training Loss: 0.1415\n",
      "Epoch 4/10, Batch 35/111, Training Loss: 0.0879\n",
      "Epoch 4/10, Batch 36/111, Training Loss: 0.1947\n",
      "Epoch 4/10, Batch 37/111, Training Loss: 0.2409\n",
      "Epoch 4/10, Batch 38/111, Training Loss: 0.1305\n",
      "Epoch 4/10, Batch 39/111, Training Loss: 0.1879\n",
      "Epoch 4/10, Batch 40/111, Training Loss: 0.1827\n",
      "Epoch 4/10, Batch 41/111, Training Loss: 0.1525\n",
      "Epoch 4/10, Batch 42/111, Training Loss: 0.1635\n",
      "Epoch 4/10, Batch 43/111, Training Loss: 0.1827\n",
      "Epoch 4/10, Batch 44/111, Training Loss: 0.2632\n",
      "Epoch 4/10, Batch 45/111, Training Loss: 0.1912\n",
      "Epoch 4/10, Batch 46/111, Training Loss: 0.3197\n",
      "Epoch 4/10, Batch 47/111, Training Loss: 0.2065\n",
      "Epoch 4/10, Batch 48/111, Training Loss: 0.2077\n",
      "Epoch 4/10, Batch 49/111, Training Loss: 0.2289\n",
      "Epoch 4/10, Batch 50/111, Training Loss: 0.1687\n",
      "Epoch 4/10, Batch 51/111, Training Loss: 0.1559\n",
      "Epoch 4/10, Batch 52/111, Training Loss: 0.2065\n",
      "Epoch 4/10, Batch 53/111, Training Loss: 0.1874\n",
      "Epoch 4/10, Batch 54/111, Training Loss: 0.2708\n",
      "Epoch 4/10, Batch 55/111, Training Loss: 0.0742\n",
      "Epoch 4/10, Batch 56/111, Training Loss: 0.1188\n",
      "Epoch 4/10, Batch 57/111, Training Loss: 0.1506\n",
      "Epoch 4/10, Batch 58/111, Training Loss: 0.1715\n",
      "Epoch 4/10, Batch 59/111, Training Loss: 0.3521\n",
      "Epoch 4/10, Batch 60/111, Training Loss: 0.1709\n",
      "Epoch 4/10, Batch 61/111, Training Loss: 0.1805\n",
      "Epoch 4/10, Batch 62/111, Training Loss: 0.1945\n",
      "Epoch 4/10, Batch 63/111, Training Loss: 0.2070\n",
      "Epoch 4/10, Batch 64/111, Training Loss: 0.2067\n",
      "Epoch 4/10, Batch 65/111, Training Loss: 0.2053\n",
      "Epoch 4/10, Batch 66/111, Training Loss: 0.1857\n",
      "Epoch 4/10, Batch 67/111, Training Loss: 0.2499\n",
      "Epoch 4/10, Batch 68/111, Training Loss: 0.1784\n",
      "Epoch 4/10, Batch 69/111, Training Loss: 0.2136\n",
      "Epoch 4/10, Batch 70/111, Training Loss: 0.1214\n",
      "Epoch 4/10, Batch 71/111, Training Loss: 0.2032\n",
      "Epoch 4/10, Batch 72/111, Training Loss: 0.2778\n",
      "Epoch 4/10, Batch 73/111, Training Loss: 0.2276\n",
      "Epoch 4/10, Batch 74/111, Training Loss: 0.2290\n",
      "Epoch 4/10, Batch 75/111, Training Loss: 0.1916\n",
      "Epoch 4/10, Batch 76/111, Training Loss: 0.2151\n",
      "Epoch 4/10, Batch 77/111, Training Loss: 0.1073\n",
      "Epoch 4/10, Batch 78/111, Training Loss: 0.1743\n",
      "Epoch 4/10, Batch 79/111, Training Loss: 0.1268\n",
      "Epoch 4/10, Batch 80/111, Training Loss: 0.2046\n",
      "Epoch 4/10, Batch 81/111, Training Loss: 0.3224\n",
      "Epoch 4/10, Batch 82/111, Training Loss: 0.1950\n",
      "Epoch 4/10, Batch 83/111, Training Loss: 0.1644\n",
      "Epoch 4/10, Batch 84/111, Training Loss: 0.1625\n",
      "Epoch 4/10, Batch 85/111, Training Loss: 0.2423\n",
      "Epoch 4/10, Batch 86/111, Training Loss: 0.1929\n",
      "Epoch 4/10, Batch 87/111, Training Loss: 0.2059\n",
      "Epoch 4/10, Batch 88/111, Training Loss: 0.1754\n",
      "Epoch 4/10, Batch 89/111, Training Loss: 0.2221\n",
      "Epoch 4/10, Batch 90/111, Training Loss: 0.1396\n",
      "Epoch 4/10, Batch 91/111, Training Loss: 0.1423\n",
      "Epoch 4/10, Batch 92/111, Training Loss: 0.1832\n",
      "Epoch 4/10, Batch 93/111, Training Loss: 0.2981\n",
      "Epoch 4/10, Batch 94/111, Training Loss: 0.2746\n",
      "Epoch 4/10, Batch 95/111, Training Loss: 0.1831\n",
      "Epoch 4/10, Batch 96/111, Training Loss: 0.2211\n",
      "Epoch 4/10, Batch 97/111, Training Loss: 0.2119\n",
      "Epoch 4/10, Batch 98/111, Training Loss: 0.1865\n",
      "Epoch 4/10, Batch 99/111, Training Loss: 0.1492\n",
      "Epoch 4/10, Batch 100/111, Training Loss: 0.1297\n",
      "Epoch 4/10, Batch 101/111, Training Loss: 0.2430\n",
      "Epoch 4/10, Batch 102/111, Training Loss: 0.1685\n",
      "Epoch 4/10, Batch 103/111, Training Loss: 0.1441\n",
      "Epoch 4/10, Batch 104/111, Training Loss: 0.1657\n",
      "Epoch 4/10, Batch 105/111, Training Loss: 0.2143\n",
      "Epoch 4/10, Batch 106/111, Training Loss: 0.1489\n",
      "Epoch 4/10, Batch 107/111, Training Loss: 0.1943\n",
      "Epoch 4/10, Batch 108/111, Training Loss: 0.1009\n",
      "Epoch 4/10, Batch 109/111, Training Loss: 0.2770\n",
      "Epoch 4/10, Batch 110/111, Training Loss: 0.2356\n",
      "Epoch 4/10, Batch 111/111, Training Loss: 0.1236\n",
      "Epoch 4/10, Training Loss: 0.1960, Validation Loss: 0.3247, Validation Accuracy: 0.8861\n",
      "Epoch 5/10, Batch 1/111, Training Loss: 0.1354\n",
      "Epoch 5/10, Batch 2/111, Training Loss: 0.1713\n",
      "Epoch 5/10, Batch 3/111, Training Loss: 0.1438\n",
      "Epoch 5/10, Batch 4/111, Training Loss: 0.1771\n",
      "Epoch 5/10, Batch 5/111, Training Loss: 0.1104\n",
      "Epoch 5/10, Batch 6/111, Training Loss: 0.1568\n",
      "Epoch 5/10, Batch 7/111, Training Loss: 0.1413\n",
      "Epoch 5/10, Batch 8/111, Training Loss: 0.1815\n",
      "Epoch 5/10, Batch 9/111, Training Loss: 0.1261\n",
      "Epoch 5/10, Batch 10/111, Training Loss: 0.0961\n",
      "Epoch 5/10, Batch 11/111, Training Loss: 0.1635\n",
      "Epoch 5/10, Batch 12/111, Training Loss: 0.1340\n",
      "Epoch 5/10, Batch 13/111, Training Loss: 0.1200\n",
      "Epoch 5/10, Batch 14/111, Training Loss: 0.0896\n",
      "Epoch 5/10, Batch 15/111, Training Loss: 0.1976\n",
      "Epoch 5/10, Batch 16/111, Training Loss: 0.2378\n",
      "Epoch 5/10, Batch 17/111, Training Loss: 0.1759\n",
      "Epoch 5/10, Batch 18/111, Training Loss: 0.1390\n",
      "Epoch 5/10, Batch 19/111, Training Loss: 0.1338\n",
      "Epoch 5/10, Batch 20/111, Training Loss: 0.1677\n",
      "Epoch 5/10, Batch 21/111, Training Loss: 0.2190\n",
      "Epoch 5/10, Batch 22/111, Training Loss: 0.1829\n",
      "Epoch 5/10, Batch 23/111, Training Loss: 0.1117\n",
      "Epoch 5/10, Batch 24/111, Training Loss: 0.1818\n",
      "Epoch 5/10, Batch 25/111, Training Loss: 0.2235\n",
      "Epoch 5/10, Batch 26/111, Training Loss: 0.1340\n",
      "Epoch 5/10, Batch 27/111, Training Loss: 0.2358\n",
      "Epoch 5/10, Batch 28/111, Training Loss: 0.1862\n",
      "Epoch 5/10, Batch 29/111, Training Loss: 0.1966\n",
      "Epoch 5/10, Batch 30/111, Training Loss: 0.1871\n",
      "Epoch 5/10, Batch 31/111, Training Loss: 0.1861\n",
      "Epoch 5/10, Batch 32/111, Training Loss: 0.1499\n",
      "Epoch 5/10, Batch 33/111, Training Loss: 0.1167\n",
      "Epoch 5/10, Batch 34/111, Training Loss: 0.1419\n",
      "Epoch 5/10, Batch 35/111, Training Loss: 0.0843\n",
      "Epoch 5/10, Batch 36/111, Training Loss: 0.1649\n",
      "Epoch 5/10, Batch 37/111, Training Loss: 0.1208\n",
      "Epoch 5/10, Batch 38/111, Training Loss: 0.2138\n",
      "Epoch 5/10, Batch 39/111, Training Loss: 0.0805\n",
      "Epoch 5/10, Batch 40/111, Training Loss: 0.2577\n",
      "Epoch 5/10, Batch 41/111, Training Loss: 0.2390\n",
      "Epoch 5/10, Batch 42/111, Training Loss: 0.0803\n",
      "Epoch 5/10, Batch 43/111, Training Loss: 0.1223\n",
      "Epoch 5/10, Batch 44/111, Training Loss: 0.1459\n",
      "Epoch 5/10, Batch 45/111, Training Loss: 0.1535\n",
      "Epoch 5/10, Batch 46/111, Training Loss: 0.1818\n",
      "Epoch 5/10, Batch 47/111, Training Loss: 0.1781\n",
      "Epoch 5/10, Batch 48/111, Training Loss: 0.1179\n",
      "Epoch 5/10, Batch 49/111, Training Loss: 0.1710\n",
      "Epoch 5/10, Batch 50/111, Training Loss: 0.1590\n",
      "Epoch 5/10, Batch 51/111, Training Loss: 0.1280\n",
      "Epoch 5/10, Batch 52/111, Training Loss: 0.1381\n",
      "Epoch 5/10, Batch 53/111, Training Loss: 0.1951\n",
      "Epoch 5/10, Batch 54/111, Training Loss: 0.1731\n",
      "Epoch 5/10, Batch 55/111, Training Loss: 0.1319\n",
      "Epoch 5/10, Batch 56/111, Training Loss: 0.3622\n",
      "Epoch 5/10, Batch 57/111, Training Loss: 0.1496\n",
      "Epoch 5/10, Batch 58/111, Training Loss: 0.1306\n",
      "Epoch 5/10, Batch 59/111, Training Loss: 0.1797\n",
      "Epoch 5/10, Batch 60/111, Training Loss: 0.1795\n",
      "Epoch 5/10, Batch 61/111, Training Loss: 0.2395\n",
      "Epoch 5/10, Batch 62/111, Training Loss: 0.3324\n",
      "Epoch 5/10, Batch 63/111, Training Loss: 0.0883\n",
      "Epoch 5/10, Batch 64/111, Training Loss: 0.1073\n",
      "Epoch 5/10, Batch 65/111, Training Loss: 0.1818\n",
      "Epoch 5/10, Batch 66/111, Training Loss: 0.1239\n",
      "Epoch 5/10, Batch 67/111, Training Loss: 0.1062\n",
      "Epoch 5/10, Batch 68/111, Training Loss: 0.2704\n",
      "Epoch 5/10, Batch 69/111, Training Loss: 0.1489\n",
      "Epoch 5/10, Batch 70/111, Training Loss: 0.1537\n",
      "Epoch 5/10, Batch 71/111, Training Loss: 0.1502\n",
      "Epoch 5/10, Batch 72/111, Training Loss: 0.1947\n",
      "Epoch 5/10, Batch 73/111, Training Loss: 0.2393\n",
      "Epoch 5/10, Batch 74/111, Training Loss: 0.1783\n",
      "Epoch 5/10, Batch 75/111, Training Loss: 0.1857\n",
      "Epoch 5/10, Batch 76/111, Training Loss: 0.1639\n",
      "Epoch 5/10, Batch 77/111, Training Loss: 0.1526\n",
      "Epoch 5/10, Batch 78/111, Training Loss: 0.1839\n",
      "Epoch 5/10, Batch 79/111, Training Loss: 0.1087\n",
      "Epoch 5/10, Batch 80/111, Training Loss: 0.1974\n",
      "Epoch 5/10, Batch 81/111, Training Loss: 0.2261\n",
      "Epoch 5/10, Batch 82/111, Training Loss: 0.1608\n",
      "Epoch 5/10, Batch 83/111, Training Loss: 0.1169\n",
      "Epoch 5/10, Batch 84/111, Training Loss: 0.0829\n",
      "Epoch 5/10, Batch 85/111, Training Loss: 0.1889\n",
      "Epoch 5/10, Batch 86/111, Training Loss: 0.1339\n",
      "Epoch 5/10, Batch 87/111, Training Loss: 0.1673\n",
      "Epoch 5/10, Batch 88/111, Training Loss: 0.1374\n",
      "Epoch 5/10, Batch 89/111, Training Loss: 0.1112\n",
      "Epoch 5/10, Batch 90/111, Training Loss: 0.1720\n",
      "Epoch 5/10, Batch 91/111, Training Loss: 0.1571\n",
      "Epoch 5/10, Batch 92/111, Training Loss: 0.1849\n",
      "Epoch 5/10, Batch 93/111, Training Loss: 0.2235\n",
      "Epoch 5/10, Batch 94/111, Training Loss: 0.2258\n",
      "Epoch 5/10, Batch 95/111, Training Loss: 0.2058\n",
      "Epoch 5/10, Batch 96/111, Training Loss: 0.1775\n",
      "Epoch 5/10, Batch 97/111, Training Loss: 0.2159\n",
      "Epoch 5/10, Batch 98/111, Training Loss: 0.1136\n",
      "Epoch 5/10, Batch 99/111, Training Loss: 0.1717\n",
      "Epoch 5/10, Batch 100/111, Training Loss: 0.2197\n",
      "Epoch 5/10, Batch 101/111, Training Loss: 0.1187\n",
      "Epoch 5/10, Batch 102/111, Training Loss: 0.1342\n",
      "Epoch 5/10, Batch 103/111, Training Loss: 0.1646\n",
      "Epoch 5/10, Batch 104/111, Training Loss: 0.1875\n",
      "Epoch 5/10, Batch 105/111, Training Loss: 0.1824\n",
      "Epoch 5/10, Batch 106/111, Training Loss: 0.2419\n",
      "Epoch 5/10, Batch 107/111, Training Loss: 0.2538\n",
      "Epoch 5/10, Batch 108/111, Training Loss: 0.2630\n",
      "Epoch 5/10, Batch 109/111, Training Loss: 0.1729\n",
      "Epoch 5/10, Batch 110/111, Training Loss: 0.1813\n",
      "Epoch 5/10, Batch 111/111, Training Loss: 0.2491\n",
      "Epoch 5/10, Training Loss: 0.1688, Validation Loss: 0.3499, Validation Accuracy: 0.8706\n",
      "Epoch 6/10, Batch 1/111, Training Loss: 0.1340\n",
      "Epoch 6/10, Batch 2/111, Training Loss: 0.1736\n",
      "Epoch 6/10, Batch 3/111, Training Loss: 0.1136\n",
      "Epoch 6/10, Batch 4/111, Training Loss: 0.1569\n",
      "Epoch 6/10, Batch 5/111, Training Loss: 0.0874\n",
      "Epoch 6/10, Batch 6/111, Training Loss: 0.2171\n",
      "Epoch 6/10, Batch 7/111, Training Loss: 0.1379\n",
      "Epoch 6/10, Batch 8/111, Training Loss: 0.1497\n",
      "Epoch 6/10, Batch 9/111, Training Loss: 0.1289\n",
      "Epoch 6/10, Batch 10/111, Training Loss: 0.1072\n",
      "Epoch 6/10, Batch 11/111, Training Loss: 0.0941\n",
      "Epoch 6/10, Batch 12/111, Training Loss: 0.0926\n",
      "Epoch 6/10, Batch 13/111, Training Loss: 0.1566\n",
      "Epoch 6/10, Batch 14/111, Training Loss: 0.1262\n",
      "Epoch 6/10, Batch 15/111, Training Loss: 0.0911\n",
      "Epoch 6/10, Batch 16/111, Training Loss: 0.1370\n",
      "Epoch 6/10, Batch 17/111, Training Loss: 0.2119\n",
      "Epoch 6/10, Batch 18/111, Training Loss: 0.1938\n",
      "Epoch 6/10, Batch 19/111, Training Loss: 0.1868\n",
      "Epoch 6/10, Batch 20/111, Training Loss: 0.1466\n",
      "Epoch 6/10, Batch 21/111, Training Loss: 0.1411\n",
      "Epoch 6/10, Batch 22/111, Training Loss: 0.1912\n",
      "Epoch 6/10, Batch 23/111, Training Loss: 0.1151\n",
      "Epoch 6/10, Batch 24/111, Training Loss: 0.1167\n",
      "Epoch 6/10, Batch 25/111, Training Loss: 0.1601\n",
      "Epoch 6/10, Batch 26/111, Training Loss: 0.1550\n",
      "Epoch 6/10, Batch 27/111, Training Loss: 0.1139\n",
      "Epoch 6/10, Batch 28/111, Training Loss: 0.1695\n",
      "Epoch 6/10, Batch 29/111, Training Loss: 0.1046\n",
      "Epoch 6/10, Batch 30/111, Training Loss: 0.0990\n",
      "Epoch 6/10, Batch 31/111, Training Loss: 0.1478\n",
      "Epoch 6/10, Batch 32/111, Training Loss: 0.1504\n",
      "Epoch 6/10, Batch 33/111, Training Loss: 0.1175\n",
      "Epoch 6/10, Batch 34/111, Training Loss: 0.0896\n",
      "Epoch 6/10, Batch 35/111, Training Loss: 0.0772\n",
      "Epoch 6/10, Batch 36/111, Training Loss: 0.1265\n",
      "Epoch 6/10, Batch 37/111, Training Loss: 0.2545\n",
      "Epoch 6/10, Batch 38/111, Training Loss: 0.1295\n",
      "Epoch 6/10, Batch 39/111, Training Loss: 0.1789\n",
      "Epoch 6/10, Batch 40/111, Training Loss: 0.1197\n",
      "Epoch 6/10, Batch 41/111, Training Loss: 0.1016\n",
      "Epoch 6/10, Batch 42/111, Training Loss: 0.1220\n",
      "Epoch 6/10, Batch 43/111, Training Loss: 0.1939\n",
      "Epoch 6/10, Batch 44/111, Training Loss: 0.1904\n",
      "Epoch 6/10, Batch 45/111, Training Loss: 0.1566\n",
      "Epoch 6/10, Batch 46/111, Training Loss: 0.1637\n",
      "Epoch 6/10, Batch 47/111, Training Loss: 0.1858\n",
      "Epoch 6/10, Batch 48/111, Training Loss: 0.1721\n",
      "Epoch 6/10, Batch 49/111, Training Loss: 0.1478\n",
      "Epoch 6/10, Batch 50/111, Training Loss: 0.0937\n",
      "Epoch 6/10, Batch 51/111, Training Loss: 0.1627\n",
      "Epoch 6/10, Batch 52/111, Training Loss: 0.1093\n",
      "Epoch 6/10, Batch 53/111, Training Loss: 0.1431\n",
      "Epoch 6/10, Batch 54/111, Training Loss: 0.1363\n",
      "Epoch 6/10, Batch 55/111, Training Loss: 0.1349\n",
      "Epoch 6/10, Batch 56/111, Training Loss: 0.1248\n",
      "Epoch 6/10, Batch 57/111, Training Loss: 0.1837\n",
      "Epoch 6/10, Batch 58/111, Training Loss: 0.2511\n",
      "Epoch 6/10, Batch 59/111, Training Loss: 0.0592\n",
      "Epoch 6/10, Batch 60/111, Training Loss: 0.1503\n",
      "Epoch 6/10, Batch 61/111, Training Loss: 0.1456\n",
      "Epoch 6/10, Batch 62/111, Training Loss: 0.1271\n",
      "Epoch 6/10, Batch 63/111, Training Loss: 0.0906\n",
      "Epoch 6/10, Batch 64/111, Training Loss: 0.2128\n",
      "Epoch 6/10, Batch 65/111, Training Loss: 0.2016\n",
      "Epoch 6/10, Batch 66/111, Training Loss: 0.1737\n",
      "Epoch 6/10, Batch 67/111, Training Loss: 0.1454\n",
      "Epoch 6/10, Batch 68/111, Training Loss: 0.1397\n",
      "Epoch 6/10, Batch 69/111, Training Loss: 0.1305\n",
      "Epoch 6/10, Batch 70/111, Training Loss: 0.1747\n",
      "Epoch 6/10, Batch 71/111, Training Loss: 0.1880\n",
      "Epoch 6/10, Batch 72/111, Training Loss: 0.1241\n",
      "Epoch 6/10, Batch 73/111, Training Loss: 0.1807\n",
      "Epoch 6/10, Batch 74/111, Training Loss: 0.1317\n",
      "Epoch 6/10, Batch 75/111, Training Loss: 0.1871\n",
      "Epoch 6/10, Batch 76/111, Training Loss: 0.1758\n",
      "Epoch 6/10, Batch 77/111, Training Loss: 0.1538\n",
      "Epoch 6/10, Batch 78/111, Training Loss: 0.2994\n",
      "Epoch 6/10, Batch 79/111, Training Loss: 0.0773\n",
      "Epoch 6/10, Batch 80/111, Training Loss: 0.1275\n",
      "Epoch 6/10, Batch 81/111, Training Loss: 0.1008\n",
      "Epoch 6/10, Batch 82/111, Training Loss: 0.1544\n",
      "Epoch 6/10, Batch 83/111, Training Loss: 0.3111\n",
      "Epoch 6/10, Batch 84/111, Training Loss: 0.1488\n",
      "Epoch 6/10, Batch 85/111, Training Loss: 0.1772\n",
      "Epoch 6/10, Batch 86/111, Training Loss: 0.2705\n",
      "Epoch 6/10, Batch 87/111, Training Loss: 0.0776\n",
      "Epoch 6/10, Batch 88/111, Training Loss: 0.1542\n",
      "Epoch 6/10, Batch 89/111, Training Loss: 0.1510\n",
      "Epoch 6/10, Batch 90/111, Training Loss: 0.0800\n",
      "Epoch 6/10, Batch 91/111, Training Loss: 0.1142\n",
      "Epoch 6/10, Batch 92/111, Training Loss: 0.2377\n",
      "Epoch 6/10, Batch 93/111, Training Loss: 0.2184\n",
      "Epoch 6/10, Batch 94/111, Training Loss: 0.1569\n",
      "Epoch 6/10, Batch 95/111, Training Loss: 0.0865\n",
      "Epoch 6/10, Batch 96/111, Training Loss: 0.2340\n",
      "Epoch 6/10, Batch 97/111, Training Loss: 0.2061\n",
      "Epoch 6/10, Batch 98/111, Training Loss: 0.1720\n",
      "Epoch 6/10, Batch 99/111, Training Loss: 0.1793\n",
      "Epoch 6/10, Batch 100/111, Training Loss: 0.1826\n",
      "Epoch 6/10, Batch 101/111, Training Loss: 0.2345\n",
      "Epoch 6/10, Batch 102/111, Training Loss: 0.1274\n",
      "Epoch 6/10, Batch 103/111, Training Loss: 0.1407\n",
      "Epoch 6/10, Batch 104/111, Training Loss: 0.1089\n",
      "Epoch 6/10, Batch 105/111, Training Loss: 0.1516\n",
      "Epoch 6/10, Batch 106/111, Training Loss: 0.1973\n",
      "Epoch 6/10, Batch 107/111, Training Loss: 0.1350\n",
      "Epoch 6/10, Batch 108/111, Training Loss: 0.1293\n",
      "Epoch 6/10, Batch 109/111, Training Loss: 0.1354\n",
      "Epoch 6/10, Batch 110/111, Training Loss: 0.1475\n",
      "Epoch 6/10, Batch 111/111, Training Loss: 0.1773\n",
      "Epoch 6/10, Training Loss: 0.1520, Validation Loss: 0.4720, Validation Accuracy: 0.8293\n",
      "Epoch 7/10, Batch 1/111, Training Loss: 0.0710\n",
      "Epoch 7/10, Batch 2/111, Training Loss: 0.0760\n",
      "Epoch 7/10, Batch 3/111, Training Loss: 0.1256\n",
      "Epoch 7/10, Batch 4/111, Training Loss: 0.1533\n",
      "Epoch 7/10, Batch 5/111, Training Loss: 0.1143\n",
      "Epoch 7/10, Batch 6/111, Training Loss: 0.1185\n",
      "Epoch 7/10, Batch 7/111, Training Loss: 0.1247\n",
      "Epoch 7/10, Batch 8/111, Training Loss: 0.1718\n",
      "Epoch 7/10, Batch 9/111, Training Loss: 0.1298\n",
      "Epoch 7/10, Batch 10/111, Training Loss: 0.1736\n",
      "Epoch 7/10, Batch 11/111, Training Loss: 0.0941\n",
      "Epoch 7/10, Batch 12/111, Training Loss: 0.0802\n",
      "Epoch 7/10, Batch 13/111, Training Loss: 0.1050\n",
      "Epoch 7/10, Batch 14/111, Training Loss: 0.1256\n",
      "Epoch 7/10, Batch 15/111, Training Loss: 0.0906\n",
      "Epoch 7/10, Batch 16/111, Training Loss: 0.1496\n",
      "Epoch 7/10, Batch 17/111, Training Loss: 0.1184\n",
      "Epoch 7/10, Batch 18/111, Training Loss: 0.0786\n",
      "Epoch 7/10, Batch 19/111, Training Loss: 0.1276\n",
      "Epoch 7/10, Batch 20/111, Training Loss: 0.0707\n",
      "Epoch 7/10, Batch 21/111, Training Loss: 0.1187\n",
      "Epoch 7/10, Batch 22/111, Training Loss: 0.1404\n",
      "Epoch 7/10, Batch 23/111, Training Loss: 0.0798\n",
      "Epoch 7/10, Batch 24/111, Training Loss: 0.1123\n",
      "Epoch 7/10, Batch 25/111, Training Loss: 0.0982\n",
      "Epoch 7/10, Batch 26/111, Training Loss: 0.1233\n",
      "Epoch 7/10, Batch 27/111, Training Loss: 0.0932\n",
      "Epoch 7/10, Batch 28/111, Training Loss: 0.1022\n",
      "Epoch 7/10, Batch 29/111, Training Loss: 0.1191\n",
      "Epoch 7/10, Batch 30/111, Training Loss: 0.0650\n",
      "Epoch 7/10, Batch 31/111, Training Loss: 0.1274\n",
      "Epoch 7/10, Batch 32/111, Training Loss: 0.1198\n",
      "Epoch 7/10, Batch 33/111, Training Loss: 0.1141\n",
      "Epoch 7/10, Batch 34/111, Training Loss: 0.1134\n",
      "Epoch 7/10, Batch 35/111, Training Loss: 0.0971\n",
      "Epoch 7/10, Batch 36/111, Training Loss: 0.1056\n",
      "Epoch 7/10, Batch 37/111, Training Loss: 0.0907\n",
      "Epoch 7/10, Batch 38/111, Training Loss: 0.1033\n",
      "Epoch 7/10, Batch 39/111, Training Loss: 0.1398\n",
      "Epoch 7/10, Batch 40/111, Training Loss: 0.0764\n",
      "Epoch 7/10, Batch 41/111, Training Loss: 0.0825\n",
      "Epoch 7/10, Batch 42/111, Training Loss: 0.0749\n",
      "Epoch 7/10, Batch 43/111, Training Loss: 0.1304\n",
      "Epoch 7/10, Batch 44/111, Training Loss: 0.0794\n",
      "Epoch 7/10, Batch 45/111, Training Loss: 0.0486\n",
      "Epoch 7/10, Batch 46/111, Training Loss: 0.1026\n",
      "Epoch 7/10, Batch 47/111, Training Loss: 0.0890\n",
      "Epoch 7/10, Batch 48/111, Training Loss: 0.0957\n",
      "Epoch 7/10, Batch 49/111, Training Loss: 0.1308\n",
      "Epoch 7/10, Batch 50/111, Training Loss: 0.1089\n",
      "Epoch 7/10, Batch 51/111, Training Loss: 0.0734\n",
      "Epoch 7/10, Batch 52/111, Training Loss: 0.0401\n",
      "Epoch 7/10, Batch 53/111, Training Loss: 0.0809\n",
      "Epoch 7/10, Batch 54/111, Training Loss: 0.1549\n",
      "Epoch 7/10, Batch 55/111, Training Loss: 0.1197\n",
      "Epoch 7/10, Batch 56/111, Training Loss: 0.0881\n",
      "Epoch 7/10, Batch 57/111, Training Loss: 0.0819\n",
      "Epoch 7/10, Batch 58/111, Training Loss: 0.0747\n",
      "Epoch 7/10, Batch 59/111, Training Loss: 0.1156\n",
      "Epoch 7/10, Batch 60/111, Training Loss: 0.0878\n",
      "Epoch 7/10, Batch 61/111, Training Loss: 0.1111\n",
      "Epoch 7/10, Batch 62/111, Training Loss: 0.1079\n",
      "Epoch 7/10, Batch 63/111, Training Loss: 0.0668\n",
      "Epoch 7/10, Batch 64/111, Training Loss: 0.0578\n",
      "Epoch 7/10, Batch 65/111, Training Loss: 0.0677\n",
      "Epoch 7/10, Batch 66/111, Training Loss: 0.0803\n",
      "Epoch 7/10, Batch 67/111, Training Loss: 0.0670\n",
      "Epoch 7/10, Batch 68/111, Training Loss: 0.0586\n",
      "Epoch 7/10, Batch 69/111, Training Loss: 0.0901\n",
      "Epoch 7/10, Batch 70/111, Training Loss: 0.0772\n",
      "Epoch 7/10, Batch 71/111, Training Loss: 0.0688\n",
      "Epoch 7/10, Batch 72/111, Training Loss: 0.0971\n",
      "Epoch 7/10, Batch 73/111, Training Loss: 0.0758\n",
      "Epoch 7/10, Batch 74/111, Training Loss: 0.1285\n",
      "Epoch 7/10, Batch 75/111, Training Loss: 0.1033\n",
      "Epoch 7/10, Batch 76/111, Training Loss: 0.1043\n",
      "Epoch 7/10, Batch 77/111, Training Loss: 0.0747\n",
      "Epoch 7/10, Batch 78/111, Training Loss: 0.0905\n",
      "Epoch 7/10, Batch 79/111, Training Loss: 0.2111\n",
      "Epoch 7/10, Batch 80/111, Training Loss: 0.0798\n",
      "Epoch 7/10, Batch 81/111, Training Loss: 0.0775\n",
      "Epoch 7/10, Batch 82/111, Training Loss: 0.0763\n",
      "Epoch 7/10, Batch 83/111, Training Loss: 0.0727\n",
      "Epoch 7/10, Batch 84/111, Training Loss: 0.0641\n",
      "Epoch 7/10, Batch 85/111, Training Loss: 0.0773\n",
      "Epoch 7/10, Batch 86/111, Training Loss: 0.0797\n",
      "Epoch 7/10, Batch 87/111, Training Loss: 0.0929\n",
      "Epoch 7/10, Batch 88/111, Training Loss: 0.1360\n",
      "Epoch 7/10, Batch 89/111, Training Loss: 0.0495\n",
      "Epoch 7/10, Batch 90/111, Training Loss: 0.1064\n",
      "Epoch 7/10, Batch 91/111, Training Loss: 0.1179\n",
      "Epoch 7/10, Batch 92/111, Training Loss: 0.0203\n",
      "Epoch 7/10, Batch 93/111, Training Loss: 0.0505\n",
      "Epoch 7/10, Batch 94/111, Training Loss: 0.0921\n",
      "Epoch 7/10, Batch 95/111, Training Loss: 0.0906\n",
      "Epoch 7/10, Batch 96/111, Training Loss: 0.1047\n",
      "Epoch 7/10, Batch 97/111, Training Loss: 0.0710\n",
      "Epoch 7/10, Batch 98/111, Training Loss: 0.0731\n",
      "Epoch 7/10, Batch 99/111, Training Loss: 0.0926\n",
      "Epoch 7/10, Batch 100/111, Training Loss: 0.0503\n",
      "Epoch 7/10, Batch 101/111, Training Loss: 0.0978\n",
      "Epoch 7/10, Batch 102/111, Training Loss: 0.0819\n",
      "Epoch 7/10, Batch 103/111, Training Loss: 0.0570\n",
      "Epoch 7/10, Batch 104/111, Training Loss: 0.0455\n",
      "Epoch 7/10, Batch 105/111, Training Loss: 0.0652\n",
      "Epoch 7/10, Batch 106/111, Training Loss: 0.0504\n",
      "Epoch 7/10, Batch 107/111, Training Loss: 0.0756\n",
      "Epoch 7/10, Batch 108/111, Training Loss: 0.1028\n",
      "Epoch 7/10, Batch 109/111, Training Loss: 0.0596\n",
      "Epoch 7/10, Batch 110/111, Training Loss: 0.0857\n",
      "Epoch 7/10, Batch 111/111, Training Loss: 0.0633\n",
      "Epoch 7/10, Training Loss: 0.0946, Validation Loss: 0.2197, Validation Accuracy: 0.9135\n",
      "Epoch 8/10, Batch 1/111, Training Loss: 0.0701\n",
      "Epoch 8/10, Batch 2/111, Training Loss: 0.0615\n",
      "Epoch 8/10, Batch 3/111, Training Loss: 0.0446\n",
      "Epoch 8/10, Batch 4/111, Training Loss: 0.0656\n",
      "Epoch 8/10, Batch 5/111, Training Loss: 0.0535\n",
      "Epoch 8/10, Batch 6/111, Training Loss: 0.0846\n",
      "Epoch 8/10, Batch 7/111, Training Loss: 0.0611\n",
      "Epoch 8/10, Batch 8/111, Training Loss: 0.0875\n",
      "Epoch 8/10, Batch 9/111, Training Loss: 0.0404\n",
      "Epoch 8/10, Batch 10/111, Training Loss: 0.0826\n",
      "Epoch 8/10, Batch 11/111, Training Loss: 0.0512\n",
      "Epoch 8/10, Batch 12/111, Training Loss: 0.0699\n",
      "Epoch 8/10, Batch 13/111, Training Loss: 0.0380\n",
      "Epoch 8/10, Batch 14/111, Training Loss: 0.0750\n",
      "Epoch 8/10, Batch 15/111, Training Loss: 0.0621\n",
      "Epoch 8/10, Batch 16/111, Training Loss: 0.0693\n",
      "Epoch 8/10, Batch 17/111, Training Loss: 0.0595\n",
      "Epoch 8/10, Batch 18/111, Training Loss: 0.0709\n",
      "Epoch 8/10, Batch 19/111, Training Loss: 0.0539\n",
      "Epoch 8/10, Batch 20/111, Training Loss: 0.0585\n",
      "Epoch 8/10, Batch 21/111, Training Loss: 0.0406\n",
      "Epoch 8/10, Batch 22/111, Training Loss: 0.0585\n",
      "Epoch 8/10, Batch 23/111, Training Loss: 0.0849\n",
      "Epoch 8/10, Batch 24/111, Training Loss: 0.1144\n",
      "Epoch 8/10, Batch 25/111, Training Loss: 0.0783\n",
      "Epoch 8/10, Batch 26/111, Training Loss: 0.0506\n",
      "Epoch 8/10, Batch 27/111, Training Loss: 0.0701\n",
      "Epoch 8/10, Batch 28/111, Training Loss: 0.0417\n",
      "Epoch 8/10, Batch 29/111, Training Loss: 0.0831\n",
      "Epoch 8/10, Batch 30/111, Training Loss: 0.0812\n",
      "Epoch 8/10, Batch 31/111, Training Loss: 0.0398\n",
      "Epoch 8/10, Batch 32/111, Training Loss: 0.1081\n",
      "Epoch 8/10, Batch 33/111, Training Loss: 0.0410\n",
      "Epoch 8/10, Batch 34/111, Training Loss: 0.1611\n",
      "Epoch 8/10, Batch 35/111, Training Loss: 0.0521\n",
      "Epoch 8/10, Batch 36/111, Training Loss: 0.0833\n",
      "Epoch 8/10, Batch 37/111, Training Loss: 0.0613\n",
      "Epoch 8/10, Batch 38/111, Training Loss: 0.0459\n",
      "Epoch 8/10, Batch 39/111, Training Loss: 0.0731\n",
      "Epoch 8/10, Batch 40/111, Training Loss: 0.0382\n",
      "Epoch 8/10, Batch 41/111, Training Loss: 0.0497\n",
      "Epoch 8/10, Batch 42/111, Training Loss: 0.0332\n",
      "Epoch 8/10, Batch 43/111, Training Loss: 0.0689\n",
      "Epoch 8/10, Batch 44/111, Training Loss: 0.0415\n",
      "Epoch 8/10, Batch 45/111, Training Loss: 0.0495\n",
      "Epoch 8/10, Batch 46/111, Training Loss: 0.0401\n",
      "Epoch 8/10, Batch 47/111, Training Loss: 0.0451\n",
      "Epoch 8/10, Batch 48/111, Training Loss: 0.0530\n",
      "Epoch 8/10, Batch 49/111, Training Loss: 0.0645\n",
      "Epoch 8/10, Batch 50/111, Training Loss: 0.0786\n",
      "Epoch 8/10, Batch 51/111, Training Loss: 0.1112\n",
      "Epoch 8/10, Batch 52/111, Training Loss: 0.0810\n",
      "Epoch 8/10, Batch 53/111, Training Loss: 0.0367\n",
      "Epoch 8/10, Batch 54/111, Training Loss: 0.0682\n",
      "Epoch 8/10, Batch 55/111, Training Loss: 0.0603\n",
      "Epoch 8/10, Batch 56/111, Training Loss: 0.0493\n",
      "Epoch 8/10, Batch 57/111, Training Loss: 0.0668\n",
      "Epoch 8/10, Batch 58/111, Training Loss: 0.1140\n",
      "Epoch 8/10, Batch 59/111, Training Loss: 0.0433\n",
      "Epoch 8/10, Batch 60/111, Training Loss: 0.0475\n",
      "Epoch 8/10, Batch 61/111, Training Loss: 0.0484\n",
      "Epoch 8/10, Batch 62/111, Training Loss: 0.0678\n",
      "Epoch 8/10, Batch 63/111, Training Loss: 0.0770\n",
      "Epoch 8/10, Batch 64/111, Training Loss: 0.0769\n",
      "Epoch 8/10, Batch 65/111, Training Loss: 0.0401\n",
      "Epoch 8/10, Batch 66/111, Training Loss: 0.0742\n",
      "Epoch 8/10, Batch 67/111, Training Loss: 0.0298\n",
      "Epoch 8/10, Batch 68/111, Training Loss: 0.0566\n",
      "Epoch 8/10, Batch 69/111, Training Loss: 0.1266\n",
      "Epoch 8/10, Batch 70/111, Training Loss: 0.0533\n",
      "Epoch 8/10, Batch 71/111, Training Loss: 0.0392\n",
      "Epoch 8/10, Batch 72/111, Training Loss: 0.1235\n",
      "Epoch 8/10, Batch 73/111, Training Loss: 0.0637\n",
      "Epoch 8/10, Batch 74/111, Training Loss: 0.0619\n",
      "Epoch 8/10, Batch 75/111, Training Loss: 0.0595\n",
      "Epoch 8/10, Batch 76/111, Training Loss: 0.1082\n",
      "Epoch 8/10, Batch 77/111, Training Loss: 0.0603\n",
      "Epoch 8/10, Batch 78/111, Training Loss: 0.0804\n",
      "Epoch 8/10, Batch 79/111, Training Loss: 0.0626\n",
      "Epoch 8/10, Batch 80/111, Training Loss: 0.0678\n",
      "Epoch 8/10, Batch 81/111, Training Loss: 0.0793\n",
      "Epoch 8/10, Batch 82/111, Training Loss: 0.0632\n",
      "Epoch 8/10, Batch 83/111, Training Loss: 0.0855\n",
      "Epoch 8/10, Batch 84/111, Training Loss: 0.0687\n",
      "Epoch 8/10, Batch 85/111, Training Loss: 0.0349\n",
      "Epoch 8/10, Batch 86/111, Training Loss: 0.0348\n",
      "Epoch 8/10, Batch 87/111, Training Loss: 0.0870\n",
      "Epoch 8/10, Batch 88/111, Training Loss: 0.0354\n",
      "Epoch 8/10, Batch 89/111, Training Loss: 0.0857\n",
      "Epoch 8/10, Batch 90/111, Training Loss: 0.0609\n",
      "Epoch 8/10, Batch 91/111, Training Loss: 0.0466\n",
      "Epoch 8/10, Batch 92/111, Training Loss: 0.0691\n",
      "Epoch 8/10, Batch 93/111, Training Loss: 0.0898\n",
      "Epoch 8/10, Batch 94/111, Training Loss: 0.0563\n",
      "Epoch 8/10, Batch 95/111, Training Loss: 0.0861\n",
      "Epoch 8/10, Batch 96/111, Training Loss: 0.0982\n",
      "Epoch 8/10, Batch 97/111, Training Loss: 0.0660\n",
      "Epoch 8/10, Batch 98/111, Training Loss: 0.0428\n",
      "Epoch 8/10, Batch 99/111, Training Loss: 0.0291\n",
      "Epoch 8/10, Batch 100/111, Training Loss: 0.0512\n",
      "Epoch 8/10, Batch 101/111, Training Loss: 0.0573\n",
      "Epoch 8/10, Batch 102/111, Training Loss: 0.0391\n",
      "Epoch 8/10, Batch 103/111, Training Loss: 0.0671\n",
      "Epoch 8/10, Batch 104/111, Training Loss: 0.0637\n",
      "Epoch 8/10, Batch 105/111, Training Loss: 0.0424\n",
      "Epoch 8/10, Batch 106/111, Training Loss: 0.0404\n",
      "Epoch 8/10, Batch 107/111, Training Loss: 0.0394\n",
      "Epoch 8/10, Batch 108/111, Training Loss: 0.0732\n",
      "Epoch 8/10, Batch 109/111, Training Loss: 0.0696\n",
      "Epoch 8/10, Batch 110/111, Training Loss: 0.0735\n",
      "Epoch 8/10, Batch 111/111, Training Loss: 0.0494\n",
      "Epoch 8/10, Training Loss: 0.0643, Validation Loss: 0.2411, Validation Accuracy: 0.9132\n",
      "Epoch 9/10, Batch 1/111, Training Loss: 0.0386\n",
      "Epoch 9/10, Batch 2/111, Training Loss: 0.0378\n",
      "Epoch 9/10, Batch 3/111, Training Loss: 0.0784\n",
      "Epoch 9/10, Batch 4/111, Training Loss: 0.0273\n",
      "Epoch 9/10, Batch 5/111, Training Loss: 0.0434\n",
      "Epoch 9/10, Batch 6/111, Training Loss: 0.0334\n",
      "Epoch 9/10, Batch 7/111, Training Loss: 0.0603\n",
      "Epoch 9/10, Batch 8/111, Training Loss: 0.0392\n",
      "Epoch 9/10, Batch 9/111, Training Loss: 0.0672\n",
      "Epoch 9/10, Batch 10/111, Training Loss: 0.0649\n",
      "Epoch 9/10, Batch 11/111, Training Loss: 0.0384\n",
      "Epoch 9/10, Batch 12/111, Training Loss: 0.0629\n",
      "Epoch 9/10, Batch 13/111, Training Loss: 0.0479\n",
      "Epoch 9/10, Batch 14/111, Training Loss: 0.0261\n",
      "Epoch 9/10, Batch 15/111, Training Loss: 0.0432\n",
      "Epoch 9/10, Batch 16/111, Training Loss: 0.0543\n",
      "Epoch 9/10, Batch 17/111, Training Loss: 0.0236\n",
      "Epoch 9/10, Batch 18/111, Training Loss: 0.0513\n",
      "Epoch 9/10, Batch 19/111, Training Loss: 0.0599\n",
      "Epoch 9/10, Batch 20/111, Training Loss: 0.0614\n",
      "Epoch 9/10, Batch 21/111, Training Loss: 0.0604\n",
      "Epoch 9/10, Batch 22/111, Training Loss: 0.0262\n",
      "Epoch 9/10, Batch 23/111, Training Loss: 0.0512\n",
      "Epoch 9/10, Batch 24/111, Training Loss: 0.0274\n",
      "Epoch 9/10, Batch 25/111, Training Loss: 0.0599\n",
      "Epoch 9/10, Batch 26/111, Training Loss: 0.0878\n",
      "Epoch 9/10, Batch 27/111, Training Loss: 0.0337\n",
      "Epoch 9/10, Batch 28/111, Training Loss: 0.0732\n",
      "Epoch 9/10, Batch 29/111, Training Loss: 0.1064\n",
      "Epoch 9/10, Batch 30/111, Training Loss: 0.0583\n",
      "Epoch 9/10, Batch 31/111, Training Loss: 0.0765\n",
      "Epoch 9/10, Batch 32/111, Training Loss: 0.1093\n",
      "Epoch 9/10, Batch 33/111, Training Loss: 0.0427\n",
      "Epoch 9/10, Batch 34/111, Training Loss: 0.0975\n",
      "Epoch 9/10, Batch 35/111, Training Loss: 0.0698\n",
      "Epoch 9/10, Batch 36/111, Training Loss: 0.0824\n",
      "Epoch 9/10, Batch 37/111, Training Loss: 0.0897\n",
      "Epoch 9/10, Batch 38/111, Training Loss: 0.0451\n",
      "Epoch 9/10, Batch 39/111, Training Loss: 0.0495\n",
      "Epoch 9/10, Batch 40/111, Training Loss: 0.0645\n",
      "Epoch 9/10, Batch 41/111, Training Loss: 0.0423\n",
      "Epoch 9/10, Batch 42/111, Training Loss: 0.1392\n",
      "Epoch 9/10, Batch 43/111, Training Loss: 0.0813\n",
      "Epoch 9/10, Batch 44/111, Training Loss: 0.0431\n",
      "Epoch 9/10, Batch 45/111, Training Loss: 0.0467\n",
      "Epoch 9/10, Batch 46/111, Training Loss: 0.0499\n",
      "Epoch 9/10, Batch 47/111, Training Loss: 0.0781\n",
      "Epoch 9/10, Batch 48/111, Training Loss: 0.0320\n",
      "Epoch 9/10, Batch 49/111, Training Loss: 0.0578\n",
      "Epoch 9/10, Batch 50/111, Training Loss: 0.0707\n",
      "Epoch 9/10, Batch 51/111, Training Loss: 0.0284\n",
      "Epoch 9/10, Batch 52/111, Training Loss: 0.0480\n",
      "Epoch 9/10, Batch 53/111, Training Loss: 0.0282\n",
      "Epoch 9/10, Batch 54/111, Training Loss: 0.0467\n",
      "Epoch 9/10, Batch 55/111, Training Loss: 0.0402\n",
      "Epoch 9/10, Batch 56/111, Training Loss: 0.0634\n",
      "Epoch 9/10, Batch 57/111, Training Loss: 0.0577\n",
      "Epoch 9/10, Batch 58/111, Training Loss: 0.0369\n",
      "Epoch 9/10, Batch 59/111, Training Loss: 0.0429\n",
      "Epoch 9/10, Batch 60/111, Training Loss: 0.0382\n",
      "Epoch 9/10, Batch 61/111, Training Loss: 0.0495\n",
      "Epoch 9/10, Batch 62/111, Training Loss: 0.0552\n",
      "Epoch 9/10, Batch 63/111, Training Loss: 0.0236\n",
      "Epoch 9/10, Batch 64/111, Training Loss: 0.0385\n",
      "Epoch 9/10, Batch 65/111, Training Loss: 0.0387\n",
      "Epoch 9/10, Batch 66/111, Training Loss: 0.0599\n",
      "Epoch 9/10, Batch 67/111, Training Loss: 0.0218\n",
      "Epoch 9/10, Batch 68/111, Training Loss: 0.1506\n",
      "Epoch 9/10, Batch 69/111, Training Loss: 0.0254\n",
      "Epoch 9/10, Batch 70/111, Training Loss: 0.0739\n",
      "Epoch 9/10, Batch 71/111, Training Loss: 0.1095\n",
      "Epoch 9/10, Batch 72/111, Training Loss: 0.0331\n",
      "Epoch 9/10, Batch 73/111, Training Loss: 0.0541\n",
      "Epoch 9/10, Batch 74/111, Training Loss: 0.0484\n",
      "Epoch 9/10, Batch 75/111, Training Loss: 0.0519\n",
      "Epoch 9/10, Batch 76/111, Training Loss: 0.0343\n",
      "Epoch 9/10, Batch 77/111, Training Loss: 0.0212\n",
      "Epoch 9/10, Batch 78/111, Training Loss: 0.0335\n",
      "Epoch 9/10, Batch 79/111, Training Loss: 0.0567\n",
      "Epoch 9/10, Batch 80/111, Training Loss: 0.0202\n",
      "Epoch 9/10, Batch 81/111, Training Loss: 0.0713\n",
      "Epoch 9/10, Batch 82/111, Training Loss: 0.0480\n",
      "Epoch 9/10, Batch 83/111, Training Loss: 0.0634\n",
      "Epoch 9/10, Batch 84/111, Training Loss: 0.0433\n",
      "Epoch 9/10, Batch 85/111, Training Loss: 0.0598\n",
      "Epoch 9/10, Batch 86/111, Training Loss: 0.0140\n",
      "Epoch 9/10, Batch 87/111, Training Loss: 0.0422\n",
      "Epoch 9/10, Batch 88/111, Training Loss: 0.0546\n",
      "Epoch 9/10, Batch 89/111, Training Loss: 0.0475\n",
      "Epoch 9/10, Batch 90/111, Training Loss: 0.0238\n",
      "Epoch 9/10, Batch 91/111, Training Loss: 0.0900\n",
      "Epoch 9/10, Batch 92/111, Training Loss: 0.0552\n",
      "Epoch 9/10, Batch 93/111, Training Loss: 0.0361\n",
      "Epoch 9/10, Batch 94/111, Training Loss: 0.0230\n",
      "Epoch 9/10, Batch 95/111, Training Loss: 0.0459\n",
      "Epoch 9/10, Batch 96/111, Training Loss: 0.0378\n",
      "Epoch 9/10, Batch 97/111, Training Loss: 0.0674\n",
      "Epoch 9/10, Batch 98/111, Training Loss: 0.0251\n",
      "Epoch 9/10, Batch 99/111, Training Loss: 0.0334\n",
      "Epoch 9/10, Batch 100/111, Training Loss: 0.0583\n",
      "Epoch 9/10, Batch 101/111, Training Loss: 0.0607\n",
      "Epoch 9/10, Batch 102/111, Training Loss: 0.0392\n",
      "Epoch 9/10, Batch 103/111, Training Loss: 0.0639\n",
      "Epoch 9/10, Batch 104/111, Training Loss: 0.0419\n",
      "Epoch 9/10, Batch 105/111, Training Loss: 0.0309\n",
      "Epoch 9/10, Batch 106/111, Training Loss: 0.0756\n",
      "Epoch 9/10, Batch 107/111, Training Loss: 0.0505\n",
      "Epoch 9/10, Batch 108/111, Training Loss: 0.0685\n",
      "Epoch 9/10, Batch 109/111, Training Loss: 0.0592\n",
      "Epoch 9/10, Batch 110/111, Training Loss: 0.0317\n",
      "Epoch 9/10, Batch 111/111, Training Loss: 0.1068\n",
      "Epoch 9/10, Training Loss: 0.0533, Validation Loss: 0.2603, Validation Accuracy: 0.9100\n",
      "Epoch 10/10, Batch 1/111, Training Loss: 0.0660\n",
      "Epoch 10/10, Batch 2/111, Training Loss: 0.0207\n",
      "Epoch 10/10, Batch 3/111, Training Loss: 0.0353\n",
      "Epoch 10/10, Batch 4/111, Training Loss: 0.0387\n",
      "Epoch 10/10, Batch 5/111, Training Loss: 0.0175\n",
      "Epoch 10/10, Batch 6/111, Training Loss: 0.0212\n",
      "Epoch 10/10, Batch 7/111, Training Loss: 0.0247\n",
      "Epoch 10/10, Batch 8/111, Training Loss: 0.0251\n",
      "Epoch 10/10, Batch 9/111, Training Loss: 0.0265\n",
      "Epoch 10/10, Batch 10/111, Training Loss: 0.0311\n",
      "Epoch 10/10, Batch 11/111, Training Loss: 0.0298\n",
      "Epoch 10/10, Batch 12/111, Training Loss: 0.0388\n",
      "Epoch 10/10, Batch 13/111, Training Loss: 0.0842\n",
      "Epoch 10/10, Batch 14/111, Training Loss: 0.0740\n",
      "Epoch 10/10, Batch 15/111, Training Loss: 0.0509\n",
      "Epoch 10/10, Batch 16/111, Training Loss: 0.0201\n",
      "Epoch 10/10, Batch 17/111, Training Loss: 0.0392\n",
      "Epoch 10/10, Batch 18/111, Training Loss: 0.0432\n",
      "Epoch 10/10, Batch 19/111, Training Loss: 0.0321\n",
      "Epoch 10/10, Batch 20/111, Training Loss: 0.0356\n",
      "Epoch 10/10, Batch 21/111, Training Loss: 0.0437\n",
      "Epoch 10/10, Batch 22/111, Training Loss: 0.0331\n",
      "Epoch 10/10, Batch 23/111, Training Loss: 0.0285\n",
      "Epoch 10/10, Batch 24/111, Training Loss: 0.0175\n",
      "Epoch 10/10, Batch 25/111, Training Loss: 0.0535\n",
      "Epoch 10/10, Batch 26/111, Training Loss: 0.0792\n",
      "Epoch 10/10, Batch 27/111, Training Loss: 0.0503\n",
      "Epoch 10/10, Batch 28/111, Training Loss: 0.0658\n",
      "Epoch 10/10, Batch 29/111, Training Loss: 0.0277\n",
      "Epoch 10/10, Batch 30/111, Training Loss: 0.0856\n",
      "Epoch 10/10, Batch 31/111, Training Loss: 0.0266\n",
      "Epoch 10/10, Batch 32/111, Training Loss: 0.0411\n",
      "Epoch 10/10, Batch 33/111, Training Loss: 0.0399\n",
      "Epoch 10/10, Batch 34/111, Training Loss: 0.0192\n",
      "Epoch 10/10, Batch 35/111, Training Loss: 0.0455\n",
      "Epoch 10/10, Batch 36/111, Training Loss: 0.0577\n",
      "Epoch 10/10, Batch 37/111, Training Loss: 0.0629\n",
      "Epoch 10/10, Batch 38/111, Training Loss: 0.0235\n",
      "Epoch 10/10, Batch 39/111, Training Loss: 0.0152\n",
      "Epoch 10/10, Batch 40/111, Training Loss: 0.0471\n",
      "Epoch 10/10, Batch 41/111, Training Loss: 0.0372\n",
      "Epoch 10/10, Batch 42/111, Training Loss: 0.0271\n",
      "Epoch 10/10, Batch 43/111, Training Loss: 0.0158\n",
      "Epoch 10/10, Batch 44/111, Training Loss: 0.0550\n",
      "Epoch 10/10, Batch 45/111, Training Loss: 0.0139\n",
      "Epoch 10/10, Batch 46/111, Training Loss: 0.0251\n",
      "Epoch 10/10, Batch 47/111, Training Loss: 0.0500\n",
      "Epoch 10/10, Batch 48/111, Training Loss: 0.0628\n",
      "Epoch 10/10, Batch 49/111, Training Loss: 0.0515\n",
      "Epoch 10/10, Batch 50/111, Training Loss: 0.0462\n",
      "Epoch 10/10, Batch 51/111, Training Loss: 0.0406\n",
      "Epoch 10/10, Batch 52/111, Training Loss: 0.0720\n",
      "Epoch 10/10, Batch 53/111, Training Loss: 0.0143\n",
      "Epoch 10/10, Batch 54/111, Training Loss: 0.0261\n",
      "Epoch 10/10, Batch 55/111, Training Loss: 0.0740\n",
      "Epoch 10/10, Batch 56/111, Training Loss: 0.0204\n",
      "Epoch 10/10, Batch 57/111, Training Loss: 0.0290\n",
      "Epoch 10/10, Batch 58/111, Training Loss: 0.0398\n",
      "Epoch 10/10, Batch 59/111, Training Loss: 0.0437\n",
      "Epoch 10/10, Batch 60/111, Training Loss: 0.0473\n",
      "Epoch 10/10, Batch 61/111, Training Loss: 0.0234\n",
      "Epoch 10/10, Batch 62/111, Training Loss: 0.0428\n",
      "Epoch 10/10, Batch 63/111, Training Loss: 0.0375\n",
      "Epoch 10/10, Batch 64/111, Training Loss: 0.0353\n",
      "Epoch 10/10, Batch 65/111, Training Loss: 0.0450\n",
      "Epoch 10/10, Batch 66/111, Training Loss: 0.0136\n",
      "Epoch 10/10, Batch 67/111, Training Loss: 0.0387\n",
      "Epoch 10/10, Batch 68/111, Training Loss: 0.0251\n",
      "Epoch 10/10, Batch 69/111, Training Loss: 0.0616\n",
      "Epoch 10/10, Batch 70/111, Training Loss: 0.0240\n",
      "Epoch 10/10, Batch 71/111, Training Loss: 0.0377\n",
      "Epoch 10/10, Batch 72/111, Training Loss: 0.0305\n",
      "Epoch 10/10, Batch 73/111, Training Loss: 0.0362\n",
      "Epoch 10/10, Batch 74/111, Training Loss: 0.0391\n",
      "Epoch 10/10, Batch 75/111, Training Loss: 0.0656\n",
      "Epoch 10/10, Batch 76/111, Training Loss: 0.0345\n",
      "Epoch 10/10, Batch 77/111, Training Loss: 0.0646\n",
      "Epoch 10/10, Batch 78/111, Training Loss: 0.0275\n",
      "Epoch 10/10, Batch 79/111, Training Loss: 0.0368\n",
      "Epoch 10/10, Batch 80/111, Training Loss: 0.0437\n",
      "Epoch 10/10, Batch 81/111, Training Loss: 0.0711\n",
      "Epoch 10/10, Batch 82/111, Training Loss: 0.0233\n",
      "Epoch 10/10, Batch 83/111, Training Loss: 0.0421\n",
      "Epoch 10/10, Batch 84/111, Training Loss: 0.0400\n",
      "Epoch 10/10, Batch 85/111, Training Loss: 0.0236\n",
      "Epoch 10/10, Batch 86/111, Training Loss: 0.0439\n",
      "Epoch 10/10, Batch 87/111, Training Loss: 0.0182\n",
      "Epoch 10/10, Batch 88/111, Training Loss: 0.0214\n",
      "Epoch 10/10, Batch 89/111, Training Loss: 0.0348\n",
      "Epoch 10/10, Batch 90/111, Training Loss: 0.0532\n",
      "Epoch 10/10, Batch 91/111, Training Loss: 0.0513\n",
      "Epoch 10/10, Batch 92/111, Training Loss: 0.0282\n",
      "Epoch 10/10, Batch 93/111, Training Loss: 0.0278\n",
      "Epoch 10/10, Batch 94/111, Training Loss: 0.0349\n",
      "Epoch 10/10, Batch 95/111, Training Loss: 0.0278\n",
      "Epoch 10/10, Batch 96/111, Training Loss: 0.0223\n",
      "Epoch 10/10, Batch 97/111, Training Loss: 0.0310\n",
      "Epoch 10/10, Batch 98/111, Training Loss: 0.0561\n",
      "Epoch 10/10, Batch 99/111, Training Loss: 0.0359\n",
      "Epoch 10/10, Batch 100/111, Training Loss: 0.0321\n",
      "Epoch 10/10, Batch 101/111, Training Loss: 0.0358\n",
      "Epoch 10/10, Batch 102/111, Training Loss: 0.0926\n",
      "Epoch 10/10, Batch 103/111, Training Loss: 0.0403\n",
      "Epoch 10/10, Batch 104/111, Training Loss: 0.0525\n",
      "Epoch 10/10, Batch 105/111, Training Loss: 0.0595\n",
      "Epoch 10/10, Batch 106/111, Training Loss: 0.0177\n",
      "Epoch 10/10, Batch 107/111, Training Loss: 0.0720\n",
      "Epoch 10/10, Batch 108/111, Training Loss: 0.0332\n",
      "Epoch 10/10, Batch 109/111, Training Loss: 0.0233\n",
      "Epoch 10/10, Batch 110/111, Training Loss: 0.0224\n",
      "Epoch 10/10, Batch 111/111, Training Loss: 0.0136\n",
      "Epoch 10/10, Training Loss: 0.0392, Validation Loss: 0.2635, Validation Accuracy: 0.9106\n",
      "Test Loss: 0.2514, Test Accuracy: 0.9222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-27 19:42:16,688] Trial 0 finished with value: 0.2196991708036512 and parameters: {'batch_size': 128, 'learning_rate': 0.0005523846634426587, 'weight_decay': 7.827086112748106e-05}. Best is trial 0 with value: 0.2196991708036512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/883, Training Loss: 1.4911\n",
      "Epoch 1/10, Batch 2/883, Training Loss: 1.9218\n",
      "Epoch 1/10, Batch 3/883, Training Loss: 2.5364\n",
      "Epoch 1/10, Batch 4/883, Training Loss: 4.6537\n",
      "Epoch 1/10, Batch 5/883, Training Loss: 4.2209\n",
      "Epoch 1/10, Batch 6/883, Training Loss: 1.5251\n",
      "Epoch 1/10, Batch 7/883, Training Loss: 1.8556\n",
      "Epoch 1/10, Batch 8/883, Training Loss: 1.3658\n",
      "Epoch 1/10, Batch 9/883, Training Loss: 1.2086\n",
      "Epoch 1/10, Batch 10/883, Training Loss: 1.0472\n",
      "Epoch 1/10, Batch 11/883, Training Loss: 1.4641\n",
      "Epoch 1/10, Batch 12/883, Training Loss: 2.1578\n",
      "Epoch 1/10, Batch 13/883, Training Loss: 0.8838\n",
      "Epoch 1/10, Batch 14/883, Training Loss: 0.8370\n",
      "Epoch 1/10, Batch 15/883, Training Loss: 1.1974\n",
      "Epoch 1/10, Batch 16/883, Training Loss: 1.2286\n",
      "Epoch 1/10, Batch 17/883, Training Loss: 0.9854\n",
      "Epoch 1/10, Batch 18/883, Training Loss: 0.9684\n",
      "Epoch 1/10, Batch 19/883, Training Loss: 1.0462\n",
      "Epoch 1/10, Batch 20/883, Training Loss: 1.2593\n",
      "Epoch 1/10, Batch 21/883, Training Loss: 0.9387\n",
      "Epoch 1/10, Batch 22/883, Training Loss: 0.9811\n",
      "Epoch 1/10, Batch 23/883, Training Loss: 1.0795\n",
      "Epoch 1/10, Batch 24/883, Training Loss: 0.9933\n",
      "Epoch 1/10, Batch 25/883, Training Loss: 0.9121\n",
      "Epoch 1/10, Batch 26/883, Training Loss: 1.0691\n",
      "Epoch 1/10, Batch 27/883, Training Loss: 0.8153\n",
      "Epoch 1/10, Batch 28/883, Training Loss: 0.8892\n",
      "Epoch 1/10, Batch 29/883, Training Loss: 0.7912\n",
      "Epoch 1/10, Batch 30/883, Training Loss: 1.1994\n",
      "Epoch 1/10, Batch 31/883, Training Loss: 1.0549\n",
      "Epoch 1/10, Batch 32/883, Training Loss: 0.8003\n",
      "Epoch 1/10, Batch 33/883, Training Loss: 1.4279\n",
      "Epoch 1/10, Batch 34/883, Training Loss: 0.9606\n",
      "Epoch 1/10, Batch 35/883, Training Loss: 0.7750\n",
      "Epoch 1/10, Batch 36/883, Training Loss: 1.0603\n",
      "Epoch 1/10, Batch 37/883, Training Loss: 1.3187\n",
      "Epoch 1/10, Batch 38/883, Training Loss: 0.9677\n",
      "Epoch 1/10, Batch 39/883, Training Loss: 1.0418\n",
      "Epoch 1/10, Batch 40/883, Training Loss: 1.1745\n",
      "Epoch 1/10, Batch 41/883, Training Loss: 1.0267\n",
      "Epoch 1/10, Batch 42/883, Training Loss: 1.3509\n",
      "Epoch 1/10, Batch 43/883, Training Loss: 1.0122\n",
      "Epoch 1/10, Batch 44/883, Training Loss: 1.1615\n",
      "Epoch 1/10, Batch 45/883, Training Loss: 0.8560\n",
      "Epoch 1/10, Batch 46/883, Training Loss: 1.0018\n",
      "Epoch 1/10, Batch 47/883, Training Loss: 0.8052\n",
      "Epoch 1/10, Batch 48/883, Training Loss: 0.9586\n",
      "Epoch 1/10, Batch 49/883, Training Loss: 1.0508\n",
      "Epoch 1/10, Batch 50/883, Training Loss: 1.0163\n",
      "Epoch 1/10, Batch 51/883, Training Loss: 0.8085\n",
      "Epoch 1/10, Batch 52/883, Training Loss: 0.9489\n",
      "Epoch 1/10, Batch 53/883, Training Loss: 1.1504\n",
      "Epoch 1/10, Batch 54/883, Training Loss: 0.9264\n",
      "Epoch 1/10, Batch 55/883, Training Loss: 0.8304\n",
      "Epoch 1/10, Batch 56/883, Training Loss: 0.9944\n",
      "Epoch 1/10, Batch 57/883, Training Loss: 1.0597\n",
      "Epoch 1/10, Batch 58/883, Training Loss: 0.9949\n",
      "Epoch 1/10, Batch 59/883, Training Loss: 0.9248\n",
      "Epoch 1/10, Batch 60/883, Training Loss: 1.0622\n",
      "Epoch 1/10, Batch 61/883, Training Loss: 0.9715\n",
      "Epoch 1/10, Batch 62/883, Training Loss: 0.9764\n",
      "Epoch 1/10, Batch 63/883, Training Loss: 0.8865\n",
      "Epoch 1/10, Batch 64/883, Training Loss: 0.9988\n",
      "Epoch 1/10, Batch 65/883, Training Loss: 1.6719\n",
      "Epoch 1/10, Batch 66/883, Training Loss: 0.9966\n",
      "Epoch 1/10, Batch 67/883, Training Loss: 0.9038\n",
      "Epoch 1/10, Batch 68/883, Training Loss: 0.9422\n",
      "Epoch 1/10, Batch 69/883, Training Loss: 0.9291\n",
      "Epoch 1/10, Batch 70/883, Training Loss: 1.0069\n",
      "Epoch 1/10, Batch 71/883, Training Loss: 1.0526\n",
      "Epoch 1/10, Batch 72/883, Training Loss: 1.1735\n",
      "Epoch 1/10, Batch 73/883, Training Loss: 1.2479\n",
      "Epoch 1/10, Batch 74/883, Training Loss: 0.8301\n",
      "Epoch 1/10, Batch 75/883, Training Loss: 0.8657\n",
      "Epoch 1/10, Batch 76/883, Training Loss: 0.8584\n",
      "Epoch 1/10, Batch 77/883, Training Loss: 1.0460\n",
      "Epoch 1/10, Batch 78/883, Training Loss: 0.9064\n",
      "Epoch 1/10, Batch 79/883, Training Loss: 0.9522\n",
      "Epoch 1/10, Batch 80/883, Training Loss: 0.8102\n",
      "Epoch 1/10, Batch 81/883, Training Loss: 0.8354\n",
      "Epoch 1/10, Batch 82/883, Training Loss: 1.0225\n",
      "Epoch 1/10, Batch 83/883, Training Loss: 0.9582\n",
      "Epoch 1/10, Batch 84/883, Training Loss: 1.3401\n",
      "Epoch 1/10, Batch 85/883, Training Loss: 1.3261\n",
      "Epoch 1/10, Batch 86/883, Training Loss: 0.9105\n",
      "Epoch 1/10, Batch 87/883, Training Loss: 0.8224\n",
      "Epoch 1/10, Batch 88/883, Training Loss: 1.0484\n",
      "Epoch 1/10, Batch 89/883, Training Loss: 1.3304\n",
      "Epoch 1/10, Batch 90/883, Training Loss: 0.9130\n",
      "Epoch 1/10, Batch 91/883, Training Loss: 1.1680\n",
      "Epoch 1/10, Batch 92/883, Training Loss: 1.1109\n",
      "Epoch 1/10, Batch 93/883, Training Loss: 1.0745\n",
      "Epoch 1/10, Batch 94/883, Training Loss: 1.0735\n",
      "Epoch 1/10, Batch 95/883, Training Loss: 1.4046\n",
      "Epoch 1/10, Batch 96/883, Training Loss: 1.4212\n",
      "Epoch 1/10, Batch 97/883, Training Loss: 0.8880\n",
      "Epoch 1/10, Batch 98/883, Training Loss: 1.2779\n",
      "Epoch 1/10, Batch 99/883, Training Loss: 0.6840\n",
      "Epoch 1/10, Batch 100/883, Training Loss: 0.8741\n",
      "Epoch 1/10, Batch 101/883, Training Loss: 1.1780\n",
      "Epoch 1/10, Batch 102/883, Training Loss: 1.0231\n",
      "Epoch 1/10, Batch 103/883, Training Loss: 1.4717\n",
      "Epoch 1/10, Batch 104/883, Training Loss: 0.7707\n",
      "Epoch 1/10, Batch 105/883, Training Loss: 1.3129\n",
      "Epoch 1/10, Batch 106/883, Training Loss: 0.8469\n",
      "Epoch 1/10, Batch 107/883, Training Loss: 1.0261\n",
      "Epoch 1/10, Batch 108/883, Training Loss: 0.7777\n",
      "Epoch 1/10, Batch 109/883, Training Loss: 0.8512\n",
      "Epoch 1/10, Batch 110/883, Training Loss: 0.6437\n",
      "Epoch 1/10, Batch 111/883, Training Loss: 1.0002\n",
      "Epoch 1/10, Batch 112/883, Training Loss: 0.6073\n",
      "Epoch 1/10, Batch 113/883, Training Loss: 1.1362\n",
      "Epoch 1/10, Batch 114/883, Training Loss: 1.0531\n",
      "Epoch 1/10, Batch 115/883, Training Loss: 1.5993\n",
      "Epoch 1/10, Batch 116/883, Training Loss: 0.9214\n",
      "Epoch 1/10, Batch 117/883, Training Loss: 0.7600\n",
      "Epoch 1/10, Batch 118/883, Training Loss: 0.7817\n",
      "Epoch 1/10, Batch 119/883, Training Loss: 1.1158\n",
      "Epoch 1/10, Batch 120/883, Training Loss: 0.8880\n",
      "Epoch 1/10, Batch 121/883, Training Loss: 0.8643\n",
      "Epoch 1/10, Batch 122/883, Training Loss: 0.8939\n",
      "Epoch 1/10, Batch 123/883, Training Loss: 1.2174\n",
      "Epoch 1/10, Batch 124/883, Training Loss: 1.2478\n",
      "Epoch 1/10, Batch 125/883, Training Loss: 0.9578\n",
      "Epoch 1/10, Batch 126/883, Training Loss: 1.1312\n",
      "Epoch 1/10, Batch 127/883, Training Loss: 1.0162\n",
      "Epoch 1/10, Batch 128/883, Training Loss: 1.3768\n",
      "Epoch 1/10, Batch 129/883, Training Loss: 1.7682\n",
      "Epoch 1/10, Batch 130/883, Training Loss: 0.8861\n",
      "Epoch 1/10, Batch 131/883, Training Loss: 1.0571\n",
      "Epoch 1/10, Batch 132/883, Training Loss: 0.7689\n",
      "Epoch 1/10, Batch 133/883, Training Loss: 1.1295\n",
      "Epoch 1/10, Batch 134/883, Training Loss: 0.9077\n",
      "Epoch 1/10, Batch 135/883, Training Loss: 1.2462\n",
      "Epoch 1/10, Batch 136/883, Training Loss: 1.2888\n",
      "Epoch 1/10, Batch 137/883, Training Loss: 1.2379\n",
      "Epoch 1/10, Batch 138/883, Training Loss: 1.2872\n",
      "Epoch 1/10, Batch 139/883, Training Loss: 1.2780\n",
      "Epoch 1/10, Batch 140/883, Training Loss: 0.7308\n",
      "Epoch 1/10, Batch 141/883, Training Loss: 1.0374\n",
      "Epoch 1/10, Batch 142/883, Training Loss: 1.0228\n",
      "Epoch 1/10, Batch 143/883, Training Loss: 0.9845\n",
      "Epoch 1/10, Batch 144/883, Training Loss: 1.0620\n",
      "Epoch 1/10, Batch 145/883, Training Loss: 0.9394\n",
      "Epoch 1/10, Batch 146/883, Training Loss: 0.8505\n",
      "Epoch 1/10, Batch 147/883, Training Loss: 0.8461\n",
      "Epoch 1/10, Batch 148/883, Training Loss: 1.1259\n",
      "Epoch 1/10, Batch 149/883, Training Loss: 1.2569\n",
      "Epoch 1/10, Batch 150/883, Training Loss: 0.9353\n",
      "Epoch 1/10, Batch 151/883, Training Loss: 1.0665\n",
      "Epoch 1/10, Batch 152/883, Training Loss: 1.0890\n",
      "Epoch 1/10, Batch 153/883, Training Loss: 1.0194\n",
      "Epoch 1/10, Batch 154/883, Training Loss: 1.1068\n",
      "Epoch 1/10, Batch 155/883, Training Loss: 1.0163\n",
      "Epoch 1/10, Batch 156/883, Training Loss: 0.9004\n",
      "Epoch 1/10, Batch 157/883, Training Loss: 0.8607\n",
      "Epoch 1/10, Batch 158/883, Training Loss: 0.8335\n",
      "Epoch 1/10, Batch 159/883, Training Loss: 1.0768\n",
      "Epoch 1/10, Batch 160/883, Training Loss: 0.8101\n",
      "Epoch 1/10, Batch 161/883, Training Loss: 0.9102\n",
      "Epoch 1/10, Batch 162/883, Training Loss: 0.8446\n",
      "Epoch 1/10, Batch 163/883, Training Loss: 1.1543\n",
      "Epoch 1/10, Batch 164/883, Training Loss: 0.9065\n",
      "Epoch 1/10, Batch 165/883, Training Loss: 0.9241\n",
      "Epoch 1/10, Batch 166/883, Training Loss: 0.7157\n",
      "Epoch 1/10, Batch 167/883, Training Loss: 1.4563\n",
      "Epoch 1/10, Batch 168/883, Training Loss: 1.0310\n",
      "Epoch 1/10, Batch 169/883, Training Loss: 0.6666\n",
      "Epoch 1/10, Batch 170/883, Training Loss: 0.7293\n",
      "Epoch 1/10, Batch 171/883, Training Loss: 0.7450\n",
      "Epoch 1/10, Batch 172/883, Training Loss: 0.8615\n",
      "Epoch 1/10, Batch 173/883, Training Loss: 0.9953\n",
      "Epoch 1/10, Batch 174/883, Training Loss: 1.0832\n",
      "Epoch 1/10, Batch 175/883, Training Loss: 1.0849\n",
      "Epoch 1/10, Batch 176/883, Training Loss: 0.8293\n",
      "Epoch 1/10, Batch 177/883, Training Loss: 0.8547\n",
      "Epoch 1/10, Batch 178/883, Training Loss: 0.8797\n",
      "Epoch 1/10, Batch 179/883, Training Loss: 0.8334\n",
      "Epoch 1/10, Batch 180/883, Training Loss: 1.0851\n",
      "Epoch 1/10, Batch 181/883, Training Loss: 0.9177\n",
      "Epoch 1/10, Batch 182/883, Training Loss: 1.0690\n",
      "Epoch 1/10, Batch 183/883, Training Loss: 1.1613\n",
      "Epoch 1/10, Batch 184/883, Training Loss: 1.0088\n",
      "Epoch 1/10, Batch 185/883, Training Loss: 0.7139\n",
      "Epoch 1/10, Batch 186/883, Training Loss: 1.2024\n",
      "Epoch 1/10, Batch 187/883, Training Loss: 0.9700\n",
      "Epoch 1/10, Batch 188/883, Training Loss: 0.9059\n",
      "Epoch 1/10, Batch 189/883, Training Loss: 1.3335\n",
      "Epoch 1/10, Batch 190/883, Training Loss: 1.0308\n",
      "Epoch 1/10, Batch 191/883, Training Loss: 0.8307\n",
      "Epoch 1/10, Batch 192/883, Training Loss: 0.7579\n",
      "Epoch 1/10, Batch 193/883, Training Loss: 0.9812\n",
      "Epoch 1/10, Batch 194/883, Training Loss: 0.9052\n",
      "Epoch 1/10, Batch 195/883, Training Loss: 0.9280\n",
      "Epoch 1/10, Batch 196/883, Training Loss: 1.0330\n",
      "Epoch 1/10, Batch 197/883, Training Loss: 0.8574\n",
      "Epoch 1/10, Batch 198/883, Training Loss: 0.7078\n",
      "Epoch 1/10, Batch 199/883, Training Loss: 0.7870\n",
      "Epoch 1/10, Batch 200/883, Training Loss: 0.9151\n",
      "Epoch 1/10, Batch 201/883, Training Loss: 0.9422\n",
      "Epoch 1/10, Batch 202/883, Training Loss: 1.1083\n",
      "Epoch 1/10, Batch 203/883, Training Loss: 1.2371\n",
      "Epoch 1/10, Batch 204/883, Training Loss: 0.9539\n",
      "Epoch 1/10, Batch 205/883, Training Loss: 0.6852\n",
      "Epoch 1/10, Batch 206/883, Training Loss: 0.9511\n",
      "Epoch 1/10, Batch 207/883, Training Loss: 0.7150\n",
      "Epoch 1/10, Batch 208/883, Training Loss: 0.9927\n",
      "Epoch 1/10, Batch 209/883, Training Loss: 1.0748\n",
      "Epoch 1/10, Batch 210/883, Training Loss: 0.7480\n",
      "Epoch 1/10, Batch 211/883, Training Loss: 0.8044\n",
      "Epoch 1/10, Batch 212/883, Training Loss: 0.8488\n",
      "Epoch 1/10, Batch 213/883, Training Loss: 1.1940\n",
      "Epoch 1/10, Batch 214/883, Training Loss: 1.0434\n",
      "Epoch 1/10, Batch 215/883, Training Loss: 0.8394\n",
      "Epoch 1/10, Batch 216/883, Training Loss: 1.2802\n",
      "Epoch 1/10, Batch 217/883, Training Loss: 0.8526\n",
      "Epoch 1/10, Batch 218/883, Training Loss: 0.7861\n",
      "Epoch 1/10, Batch 219/883, Training Loss: 1.1677\n",
      "Epoch 1/10, Batch 220/883, Training Loss: 0.8261\n",
      "Epoch 1/10, Batch 221/883, Training Loss: 0.5922\n",
      "Epoch 1/10, Batch 222/883, Training Loss: 0.8288\n",
      "Epoch 1/10, Batch 223/883, Training Loss: 0.8032\n",
      "Epoch 1/10, Batch 224/883, Training Loss: 0.7211\n",
      "Epoch 1/10, Batch 225/883, Training Loss: 1.1874\n",
      "Epoch 1/10, Batch 226/883, Training Loss: 0.6587\n",
      "Epoch 1/10, Batch 227/883, Training Loss: 1.0820\n",
      "Epoch 1/10, Batch 228/883, Training Loss: 0.6423\n",
      "Epoch 1/10, Batch 229/883, Training Loss: 1.0158\n",
      "Epoch 1/10, Batch 230/883, Training Loss: 0.9179\n",
      "Epoch 1/10, Batch 231/883, Training Loss: 0.7260\n",
      "Epoch 1/10, Batch 232/883, Training Loss: 1.0806\n",
      "Epoch 1/10, Batch 233/883, Training Loss: 1.1695\n",
      "Epoch 1/10, Batch 234/883, Training Loss: 1.1365\n",
      "Epoch 1/10, Batch 235/883, Training Loss: 1.0810\n",
      "Epoch 1/10, Batch 236/883, Training Loss: 0.8432\n",
      "Epoch 1/10, Batch 237/883, Training Loss: 0.8753\n",
      "Epoch 1/10, Batch 238/883, Training Loss: 0.8265\n",
      "Epoch 1/10, Batch 239/883, Training Loss: 1.4434\n",
      "Epoch 1/10, Batch 240/883, Training Loss: 1.0503\n",
      "Epoch 1/10, Batch 241/883, Training Loss: 1.1960\n",
      "Epoch 1/10, Batch 242/883, Training Loss: 1.0313\n",
      "Epoch 1/10, Batch 243/883, Training Loss: 1.0682\n",
      "Epoch 1/10, Batch 244/883, Training Loss: 0.9550\n",
      "Epoch 1/10, Batch 245/883, Training Loss: 0.9566\n",
      "Epoch 1/10, Batch 246/883, Training Loss: 0.7315\n",
      "Epoch 1/10, Batch 247/883, Training Loss: 0.7971\n",
      "Epoch 1/10, Batch 248/883, Training Loss: 0.7552\n",
      "Epoch 1/10, Batch 249/883, Training Loss: 0.8826\n",
      "Epoch 1/10, Batch 250/883, Training Loss: 0.8725\n",
      "Epoch 1/10, Batch 251/883, Training Loss: 0.7708\n",
      "Epoch 1/10, Batch 252/883, Training Loss: 0.8319\n",
      "Epoch 1/10, Batch 253/883, Training Loss: 0.8860\n",
      "Epoch 1/10, Batch 254/883, Training Loss: 0.7721\n",
      "Epoch 1/10, Batch 255/883, Training Loss: 0.8800\n",
      "Epoch 1/10, Batch 256/883, Training Loss: 0.8905\n",
      "Epoch 1/10, Batch 257/883, Training Loss: 1.2776\n",
      "Epoch 1/10, Batch 258/883, Training Loss: 1.1060\n",
      "Epoch 1/10, Batch 259/883, Training Loss: 0.7989\n",
      "Epoch 1/10, Batch 260/883, Training Loss: 0.9079\n",
      "Epoch 1/10, Batch 261/883, Training Loss: 0.7475\n",
      "Epoch 1/10, Batch 262/883, Training Loss: 1.1594\n",
      "Epoch 1/10, Batch 263/883, Training Loss: 0.9415\n",
      "Epoch 1/10, Batch 264/883, Training Loss: 0.8766\n",
      "Epoch 1/10, Batch 265/883, Training Loss: 0.7986\n",
      "Epoch 1/10, Batch 266/883, Training Loss: 0.9032\n",
      "Epoch 1/10, Batch 267/883, Training Loss: 0.9320\n",
      "Epoch 1/10, Batch 268/883, Training Loss: 1.0569\n",
      "Epoch 1/10, Batch 269/883, Training Loss: 0.8247\n",
      "Epoch 1/10, Batch 270/883, Training Loss: 1.1823\n",
      "Epoch 1/10, Batch 271/883, Training Loss: 0.9605\n",
      "Epoch 1/10, Batch 272/883, Training Loss: 0.8243\n",
      "Epoch 1/10, Batch 273/883, Training Loss: 0.8668\n",
      "Epoch 1/10, Batch 274/883, Training Loss: 0.7618\n",
      "Epoch 1/10, Batch 275/883, Training Loss: 0.9031\n",
      "Epoch 1/10, Batch 276/883, Training Loss: 1.0908\n",
      "Epoch 1/10, Batch 277/883, Training Loss: 0.8605\n",
      "Epoch 1/10, Batch 278/883, Training Loss: 0.8031\n",
      "Epoch 1/10, Batch 279/883, Training Loss: 1.1018\n",
      "Epoch 1/10, Batch 280/883, Training Loss: 0.8190\n",
      "Epoch 1/10, Batch 281/883, Training Loss: 1.0682\n",
      "Epoch 1/10, Batch 282/883, Training Loss: 1.1493\n",
      "Epoch 1/10, Batch 283/883, Training Loss: 0.7886\n",
      "Epoch 1/10, Batch 284/883, Training Loss: 0.9957\n",
      "Epoch 1/10, Batch 285/883, Training Loss: 0.9663\n",
      "Epoch 1/10, Batch 286/883, Training Loss: 1.0785\n",
      "Epoch 1/10, Batch 287/883, Training Loss: 0.8830\n",
      "Epoch 1/10, Batch 288/883, Training Loss: 0.9635\n",
      "Epoch 1/10, Batch 289/883, Training Loss: 1.0319\n",
      "Epoch 1/10, Batch 290/883, Training Loss: 1.1062\n",
      "Epoch 1/10, Batch 291/883, Training Loss: 0.8648\n",
      "Epoch 1/10, Batch 292/883, Training Loss: 1.0942\n",
      "Epoch 1/10, Batch 293/883, Training Loss: 1.0498\n",
      "Epoch 1/10, Batch 294/883, Training Loss: 0.7984\n",
      "Epoch 1/10, Batch 295/883, Training Loss: 0.7678\n",
      "Epoch 1/10, Batch 296/883, Training Loss: 0.8245\n",
      "Epoch 1/10, Batch 297/883, Training Loss: 0.8271\n",
      "Epoch 1/10, Batch 298/883, Training Loss: 0.9115\n",
      "Epoch 1/10, Batch 299/883, Training Loss: 0.8991\n",
      "Epoch 1/10, Batch 300/883, Training Loss: 0.9697\n",
      "Epoch 1/10, Batch 301/883, Training Loss: 1.2204\n",
      "Epoch 1/10, Batch 302/883, Training Loss: 0.9142\n",
      "Epoch 1/10, Batch 303/883, Training Loss: 1.1656\n",
      "Epoch 1/10, Batch 304/883, Training Loss: 0.6623\n",
      "Epoch 1/10, Batch 305/883, Training Loss: 0.7292\n",
      "Epoch 1/10, Batch 306/883, Training Loss: 1.2368\n",
      "Epoch 1/10, Batch 307/883, Training Loss: 1.1265\n",
      "Epoch 1/10, Batch 308/883, Training Loss: 0.7071\n",
      "Epoch 1/10, Batch 309/883, Training Loss: 0.8615\n",
      "Epoch 1/10, Batch 310/883, Training Loss: 1.0083\n",
      "Epoch 1/10, Batch 311/883, Training Loss: 0.9584\n",
      "Epoch 1/10, Batch 312/883, Training Loss: 1.1341\n",
      "Epoch 1/10, Batch 313/883, Training Loss: 1.0073\n",
      "Epoch 1/10, Batch 314/883, Training Loss: 0.8886\n",
      "Epoch 1/10, Batch 315/883, Training Loss: 0.9039\n",
      "Epoch 1/10, Batch 316/883, Training Loss: 1.0160\n",
      "Epoch 1/10, Batch 317/883, Training Loss: 0.9525\n",
      "Epoch 1/10, Batch 318/883, Training Loss: 0.7719\n",
      "Epoch 1/10, Batch 319/883, Training Loss: 0.9099\n",
      "Epoch 1/10, Batch 320/883, Training Loss: 0.8650\n",
      "Epoch 1/10, Batch 321/883, Training Loss: 1.0081\n",
      "Epoch 1/10, Batch 322/883, Training Loss: 1.1034\n",
      "Epoch 1/10, Batch 323/883, Training Loss: 0.6848\n",
      "Epoch 1/10, Batch 324/883, Training Loss: 0.8626\n",
      "Epoch 1/10, Batch 325/883, Training Loss: 0.8456\n",
      "Epoch 1/10, Batch 326/883, Training Loss: 0.9698\n",
      "Epoch 1/10, Batch 327/883, Training Loss: 0.7804\n",
      "Epoch 1/10, Batch 328/883, Training Loss: 1.0176\n",
      "Epoch 1/10, Batch 329/883, Training Loss: 0.7475\n",
      "Epoch 1/10, Batch 330/883, Training Loss: 1.3917\n",
      "Epoch 1/10, Batch 331/883, Training Loss: 0.8571\n",
      "Epoch 1/10, Batch 332/883, Training Loss: 0.7435\n",
      "Epoch 1/10, Batch 333/883, Training Loss: 0.9491\n",
      "Epoch 1/10, Batch 334/883, Training Loss: 0.7327\n",
      "Epoch 1/10, Batch 335/883, Training Loss: 0.8725\n",
      "Epoch 1/10, Batch 336/883, Training Loss: 1.0990\n",
      "Epoch 1/10, Batch 337/883, Training Loss: 1.1128\n",
      "Epoch 1/10, Batch 338/883, Training Loss: 1.3142\n",
      "Epoch 1/10, Batch 339/883, Training Loss: 0.9225\n",
      "Epoch 1/10, Batch 340/883, Training Loss: 0.8265\n",
      "Epoch 1/10, Batch 341/883, Training Loss: 0.9020\n",
      "Epoch 1/10, Batch 342/883, Training Loss: 1.1480\n",
      "Epoch 1/10, Batch 343/883, Training Loss: 0.7061\n",
      "Epoch 1/10, Batch 344/883, Training Loss: 1.0516\n",
      "Epoch 1/10, Batch 345/883, Training Loss: 1.2486\n",
      "Epoch 1/10, Batch 346/883, Training Loss: 1.1404\n",
      "Epoch 1/10, Batch 347/883, Training Loss: 0.8229\n",
      "Epoch 1/10, Batch 348/883, Training Loss: 1.4779\n",
      "Epoch 1/10, Batch 349/883, Training Loss: 1.4876\n",
      "Epoch 1/10, Batch 350/883, Training Loss: 1.1879\n",
      "Epoch 1/10, Batch 351/883, Training Loss: 1.0209\n",
      "Epoch 1/10, Batch 352/883, Training Loss: 0.9807\n",
      "Epoch 1/10, Batch 353/883, Training Loss: 0.8733\n",
      "Epoch 1/10, Batch 354/883, Training Loss: 0.9617\n",
      "Epoch 1/10, Batch 355/883, Training Loss: 1.0383\n",
      "Epoch 1/10, Batch 356/883, Training Loss: 1.0439\n",
      "Epoch 1/10, Batch 357/883, Training Loss: 1.2813\n",
      "Epoch 1/10, Batch 358/883, Training Loss: 1.1100\n",
      "Epoch 1/10, Batch 359/883, Training Loss: 0.9782\n",
      "Epoch 1/10, Batch 360/883, Training Loss: 1.0092\n",
      "Epoch 1/10, Batch 361/883, Training Loss: 0.8512\n",
      "Epoch 1/10, Batch 362/883, Training Loss: 0.8625\n",
      "Epoch 1/10, Batch 363/883, Training Loss: 0.9175\n",
      "Epoch 1/10, Batch 364/883, Training Loss: 0.8316\n",
      "Epoch 1/10, Batch 365/883, Training Loss: 1.2456\n",
      "Epoch 1/10, Batch 366/883, Training Loss: 1.2276\n",
      "Epoch 1/10, Batch 367/883, Training Loss: 0.8037\n",
      "Epoch 1/10, Batch 368/883, Training Loss: 0.8152\n",
      "Epoch 1/10, Batch 369/883, Training Loss: 0.8563\n",
      "Epoch 1/10, Batch 370/883, Training Loss: 0.8303\n",
      "Epoch 1/10, Batch 371/883, Training Loss: 0.7845\n",
      "Epoch 1/10, Batch 372/883, Training Loss: 0.8005\n",
      "Epoch 1/10, Batch 373/883, Training Loss: 1.1291\n",
      "Epoch 1/10, Batch 374/883, Training Loss: 0.9671\n",
      "Epoch 1/10, Batch 375/883, Training Loss: 0.6733\n",
      "Epoch 1/10, Batch 376/883, Training Loss: 0.7759\n",
      "Epoch 1/10, Batch 377/883, Training Loss: 1.0625\n",
      "Epoch 1/10, Batch 378/883, Training Loss: 0.7550\n",
      "Epoch 1/10, Batch 379/883, Training Loss: 0.6554\n",
      "Epoch 1/10, Batch 380/883, Training Loss: 1.0844\n",
      "Epoch 1/10, Batch 381/883, Training Loss: 0.8025\n",
      "Epoch 1/10, Batch 382/883, Training Loss: 0.9011\n",
      "Epoch 1/10, Batch 383/883, Training Loss: 0.9112\n",
      "Epoch 1/10, Batch 384/883, Training Loss: 1.0304\n",
      "Epoch 1/10, Batch 385/883, Training Loss: 0.6287\n",
      "Epoch 1/10, Batch 386/883, Training Loss: 1.1149\n",
      "Epoch 1/10, Batch 387/883, Training Loss: 0.9755\n",
      "Epoch 1/10, Batch 388/883, Training Loss: 0.6184\n",
      "Epoch 1/10, Batch 389/883, Training Loss: 1.1763\n",
      "Epoch 1/10, Batch 390/883, Training Loss: 0.7294\n",
      "Epoch 1/10, Batch 391/883, Training Loss: 0.6300\n",
      "Epoch 1/10, Batch 392/883, Training Loss: 0.7400\n",
      "Epoch 1/10, Batch 393/883, Training Loss: 1.0829\n",
      "Epoch 1/10, Batch 394/883, Training Loss: 0.9470\n",
      "Epoch 1/10, Batch 395/883, Training Loss: 1.0191\n",
      "Epoch 1/10, Batch 396/883, Training Loss: 0.9155\n",
      "Epoch 1/10, Batch 397/883, Training Loss: 0.9585\n",
      "Epoch 1/10, Batch 398/883, Training Loss: 0.8554\n",
      "Epoch 1/10, Batch 399/883, Training Loss: 0.8427\n",
      "Epoch 1/10, Batch 400/883, Training Loss: 0.8381\n",
      "Epoch 1/10, Batch 401/883, Training Loss: 1.0530\n",
      "Epoch 1/10, Batch 402/883, Training Loss: 0.9892\n",
      "Epoch 1/10, Batch 403/883, Training Loss: 0.6305\n",
      "Epoch 1/10, Batch 404/883, Training Loss: 1.0172\n",
      "Epoch 1/10, Batch 405/883, Training Loss: 0.8480\n",
      "Epoch 1/10, Batch 406/883, Training Loss: 0.8029\n",
      "Epoch 1/10, Batch 407/883, Training Loss: 1.0914\n",
      "Epoch 1/10, Batch 408/883, Training Loss: 0.7970\n",
      "Epoch 1/10, Batch 409/883, Training Loss: 1.1856\n",
      "Epoch 1/10, Batch 410/883, Training Loss: 0.8575\n",
      "Epoch 1/10, Batch 411/883, Training Loss: 1.0611\n",
      "Epoch 1/10, Batch 412/883, Training Loss: 1.2010\n",
      "Epoch 1/10, Batch 413/883, Training Loss: 0.9962\n",
      "Epoch 1/10, Batch 414/883, Training Loss: 0.9811\n",
      "Epoch 1/10, Batch 415/883, Training Loss: 0.9819\n",
      "Epoch 1/10, Batch 416/883, Training Loss: 0.8098\n",
      "Epoch 1/10, Batch 417/883, Training Loss: 0.9745\n",
      "Epoch 1/10, Batch 418/883, Training Loss: 0.8295\n",
      "Epoch 1/10, Batch 419/883, Training Loss: 1.1205\n",
      "Epoch 1/10, Batch 420/883, Training Loss: 0.9536\n",
      "Epoch 1/10, Batch 421/883, Training Loss: 1.2010\n",
      "Epoch 1/10, Batch 422/883, Training Loss: 0.8053\n",
      "Epoch 1/10, Batch 423/883, Training Loss: 0.8416\n",
      "Epoch 1/10, Batch 424/883, Training Loss: 0.8962\n",
      "Epoch 1/10, Batch 425/883, Training Loss: 1.0206\n",
      "Epoch 1/10, Batch 426/883, Training Loss: 0.7781\n",
      "Epoch 1/10, Batch 427/883, Training Loss: 0.9424\n",
      "Epoch 1/10, Batch 428/883, Training Loss: 0.8009\n",
      "Epoch 1/10, Batch 429/883, Training Loss: 0.8128\n",
      "Epoch 1/10, Batch 430/883, Training Loss: 0.9002\n",
      "Epoch 1/10, Batch 431/883, Training Loss: 0.8543\n",
      "Epoch 1/10, Batch 432/883, Training Loss: 1.0387\n",
      "Epoch 1/10, Batch 433/883, Training Loss: 0.9528\n",
      "Epoch 1/10, Batch 434/883, Training Loss: 0.7435\n",
      "Epoch 1/10, Batch 435/883, Training Loss: 1.0879\n",
      "Epoch 1/10, Batch 436/883, Training Loss: 0.8278\n",
      "Epoch 1/10, Batch 437/883, Training Loss: 0.8362\n",
      "Epoch 1/10, Batch 438/883, Training Loss: 0.7025\n",
      "Epoch 1/10, Batch 439/883, Training Loss: 0.8530\n",
      "Epoch 1/10, Batch 440/883, Training Loss: 0.8031\n",
      "Epoch 1/10, Batch 441/883, Training Loss: 0.6795\n",
      "Epoch 1/10, Batch 442/883, Training Loss: 0.9323\n",
      "Epoch 1/10, Batch 443/883, Training Loss: 0.7693\n",
      "Epoch 1/10, Batch 444/883, Training Loss: 0.8685\n",
      "Epoch 1/10, Batch 445/883, Training Loss: 0.8682\n",
      "Epoch 1/10, Batch 446/883, Training Loss: 0.8719\n",
      "Epoch 1/10, Batch 447/883, Training Loss: 0.9903\n",
      "Epoch 1/10, Batch 448/883, Training Loss: 0.7089\n",
      "Epoch 1/10, Batch 449/883, Training Loss: 1.1408\n",
      "Epoch 1/10, Batch 450/883, Training Loss: 0.8819\n",
      "Epoch 1/10, Batch 451/883, Training Loss: 0.7346\n",
      "Epoch 1/10, Batch 452/883, Training Loss: 1.4193\n",
      "Epoch 1/10, Batch 453/883, Training Loss: 1.0419\n",
      "Epoch 1/10, Batch 454/883, Training Loss: 0.8556\n",
      "Epoch 1/10, Batch 455/883, Training Loss: 0.9514\n",
      "Epoch 1/10, Batch 456/883, Training Loss: 0.9165\n",
      "Epoch 1/10, Batch 457/883, Training Loss: 0.8100\n",
      "Epoch 1/10, Batch 458/883, Training Loss: 1.0568\n",
      "Epoch 1/10, Batch 459/883, Training Loss: 0.8177\n",
      "Epoch 1/10, Batch 460/883, Training Loss: 0.8507\n",
      "Epoch 1/10, Batch 461/883, Training Loss: 0.7774\n",
      "Epoch 1/10, Batch 462/883, Training Loss: 0.7939\n",
      "Epoch 1/10, Batch 463/883, Training Loss: 0.7837\n",
      "Epoch 1/10, Batch 464/883, Training Loss: 0.7543\n",
      "Epoch 1/10, Batch 465/883, Training Loss: 0.7877\n",
      "Epoch 1/10, Batch 466/883, Training Loss: 0.8737\n",
      "Epoch 1/10, Batch 467/883, Training Loss: 0.6533\n",
      "Epoch 1/10, Batch 468/883, Training Loss: 0.6686\n",
      "Epoch 1/10, Batch 469/883, Training Loss: 0.9751\n",
      "Epoch 1/10, Batch 470/883, Training Loss: 0.8877\n",
      "Epoch 1/10, Batch 471/883, Training Loss: 0.8024\n",
      "Epoch 1/10, Batch 472/883, Training Loss: 1.1434\n",
      "Epoch 1/10, Batch 473/883, Training Loss: 0.7697\n",
      "Epoch 1/10, Batch 474/883, Training Loss: 0.9849\n",
      "Epoch 1/10, Batch 475/883, Training Loss: 0.7877\n",
      "Epoch 1/10, Batch 476/883, Training Loss: 0.7717\n",
      "Epoch 1/10, Batch 477/883, Training Loss: 1.0720\n",
      "Epoch 1/10, Batch 478/883, Training Loss: 0.8363\n",
      "Epoch 1/10, Batch 479/883, Training Loss: 0.9225\n",
      "Epoch 1/10, Batch 480/883, Training Loss: 0.8751\n",
      "Epoch 1/10, Batch 481/883, Training Loss: 0.9826\n",
      "Epoch 1/10, Batch 482/883, Training Loss: 0.9443\n",
      "Epoch 1/10, Batch 483/883, Training Loss: 0.9361\n",
      "Epoch 1/10, Batch 484/883, Training Loss: 0.7186\n",
      "Epoch 1/10, Batch 485/883, Training Loss: 1.4734\n",
      "Epoch 1/10, Batch 486/883, Training Loss: 1.1612\n",
      "Epoch 1/10, Batch 487/883, Training Loss: 0.9148\n",
      "Epoch 1/10, Batch 488/883, Training Loss: 0.9175\n",
      "Epoch 1/10, Batch 489/883, Training Loss: 0.8023\n",
      "Epoch 1/10, Batch 490/883, Training Loss: 0.8972\n",
      "Epoch 1/10, Batch 491/883, Training Loss: 0.8486\n",
      "Epoch 1/10, Batch 492/883, Training Loss: 0.8490\n",
      "Epoch 1/10, Batch 493/883, Training Loss: 0.6696\n",
      "Epoch 1/10, Batch 494/883, Training Loss: 0.7889\n",
      "Epoch 1/10, Batch 495/883, Training Loss: 0.8506\n",
      "Epoch 1/10, Batch 496/883, Training Loss: 0.7221\n",
      "Epoch 1/10, Batch 497/883, Training Loss: 0.7449\n",
      "Epoch 1/10, Batch 498/883, Training Loss: 0.8085\n",
      "Epoch 1/10, Batch 499/883, Training Loss: 0.6918\n",
      "Epoch 1/10, Batch 500/883, Training Loss: 1.2142\n",
      "Epoch 1/10, Batch 501/883, Training Loss: 0.7754\n",
      "Epoch 1/10, Batch 502/883, Training Loss: 0.8334\n",
      "Epoch 1/10, Batch 503/883, Training Loss: 0.6893\n",
      "Epoch 1/10, Batch 504/883, Training Loss: 1.0093\n",
      "Epoch 1/10, Batch 505/883, Training Loss: 0.7450\n",
      "Epoch 1/10, Batch 506/883, Training Loss: 0.6801\n",
      "Epoch 1/10, Batch 507/883, Training Loss: 0.6833\n",
      "Epoch 1/10, Batch 508/883, Training Loss: 0.9163\n",
      "Epoch 1/10, Batch 509/883, Training Loss: 1.0452\n",
      "Epoch 1/10, Batch 510/883, Training Loss: 0.6910\n",
      "Epoch 1/10, Batch 511/883, Training Loss: 0.9065\n",
      "Epoch 1/10, Batch 512/883, Training Loss: 1.0047\n",
      "Epoch 1/10, Batch 513/883, Training Loss: 0.8247\n",
      "Epoch 1/10, Batch 514/883, Training Loss: 1.1834\n",
      "Epoch 1/10, Batch 515/883, Training Loss: 0.7128\n",
      "Epoch 1/10, Batch 516/883, Training Loss: 0.7585\n",
      "Epoch 1/10, Batch 517/883, Training Loss: 0.9733\n",
      "Epoch 1/10, Batch 518/883, Training Loss: 0.7503\n",
      "Epoch 1/10, Batch 519/883, Training Loss: 0.6102\n",
      "Epoch 1/10, Batch 520/883, Training Loss: 1.4744\n",
      "Epoch 1/10, Batch 521/883, Training Loss: 0.9290\n",
      "Epoch 1/10, Batch 522/883, Training Loss: 1.0313\n",
      "Epoch 1/10, Batch 523/883, Training Loss: 1.0212\n",
      "Epoch 1/10, Batch 524/883, Training Loss: 0.8038\n",
      "Epoch 1/10, Batch 525/883, Training Loss: 0.6404\n",
      "Epoch 1/10, Batch 526/883, Training Loss: 0.6186\n",
      "Epoch 1/10, Batch 527/883, Training Loss: 0.5778\n",
      "Epoch 1/10, Batch 528/883, Training Loss: 0.9407\n",
      "Epoch 1/10, Batch 529/883, Training Loss: 0.7371\n",
      "Epoch 1/10, Batch 530/883, Training Loss: 0.9552\n",
      "Epoch 1/10, Batch 531/883, Training Loss: 1.5110\n",
      "Epoch 1/10, Batch 532/883, Training Loss: 1.0930\n",
      "Epoch 1/10, Batch 533/883, Training Loss: 1.0887\n",
      "Epoch 1/10, Batch 534/883, Training Loss: 0.8069\n",
      "Epoch 1/10, Batch 535/883, Training Loss: 0.7079\n",
      "Epoch 1/10, Batch 536/883, Training Loss: 0.8274\n",
      "Epoch 1/10, Batch 537/883, Training Loss: 0.9861\n",
      "Epoch 1/10, Batch 538/883, Training Loss: 0.6851\n",
      "Epoch 1/10, Batch 539/883, Training Loss: 1.0274\n",
      "Epoch 1/10, Batch 540/883, Training Loss: 0.8024\n",
      "Epoch 1/10, Batch 541/883, Training Loss: 0.9772\n",
      "Epoch 1/10, Batch 542/883, Training Loss: 0.9563\n",
      "Epoch 1/10, Batch 543/883, Training Loss: 0.7519\n",
      "Epoch 1/10, Batch 544/883, Training Loss: 0.8643\n",
      "Epoch 1/10, Batch 545/883, Training Loss: 0.8666\n",
      "Epoch 1/10, Batch 546/883, Training Loss: 0.9173\n",
      "Epoch 1/10, Batch 547/883, Training Loss: 1.0108\n",
      "Epoch 1/10, Batch 548/883, Training Loss: 0.7738\n",
      "Epoch 1/10, Batch 549/883, Training Loss: 0.7721\n",
      "Epoch 1/10, Batch 550/883, Training Loss: 0.8095\n",
      "Epoch 1/10, Batch 551/883, Training Loss: 0.8383\n",
      "Epoch 1/10, Batch 552/883, Training Loss: 0.9451\n",
      "Epoch 1/10, Batch 553/883, Training Loss: 0.8876\n",
      "Epoch 1/10, Batch 554/883, Training Loss: 0.8248\n",
      "Epoch 1/10, Batch 555/883, Training Loss: 0.7264\n",
      "Epoch 1/10, Batch 556/883, Training Loss: 0.7864\n",
      "Epoch 1/10, Batch 557/883, Training Loss: 0.7779\n",
      "Epoch 1/10, Batch 558/883, Training Loss: 1.2178\n",
      "Epoch 1/10, Batch 559/883, Training Loss: 0.9630\n",
      "Epoch 1/10, Batch 560/883, Training Loss: 1.0060\n",
      "Epoch 1/10, Batch 561/883, Training Loss: 0.9119\n",
      "Epoch 1/10, Batch 562/883, Training Loss: 0.8841\n",
      "Epoch 1/10, Batch 563/883, Training Loss: 1.1207\n",
      "Epoch 1/10, Batch 564/883, Training Loss: 0.7878\n",
      "Epoch 1/10, Batch 565/883, Training Loss: 0.9081\n",
      "Epoch 1/10, Batch 566/883, Training Loss: 1.0581\n",
      "Epoch 1/10, Batch 567/883, Training Loss: 1.1717\n",
      "Epoch 1/10, Batch 568/883, Training Loss: 1.0560\n",
      "Epoch 1/10, Batch 569/883, Training Loss: 1.0040\n",
      "Epoch 1/10, Batch 570/883, Training Loss: 0.7504\n",
      "Epoch 1/10, Batch 571/883, Training Loss: 0.7575\n",
      "Epoch 1/10, Batch 572/883, Training Loss: 0.7534\n",
      "Epoch 1/10, Batch 573/883, Training Loss: 0.8516\n",
      "Epoch 1/10, Batch 574/883, Training Loss: 1.2205\n",
      "Epoch 1/10, Batch 575/883, Training Loss: 1.0711\n",
      "Epoch 1/10, Batch 576/883, Training Loss: 0.6991\n",
      "Epoch 1/10, Batch 577/883, Training Loss: 0.7982\n",
      "Epoch 1/10, Batch 578/883, Training Loss: 0.7949\n",
      "Epoch 1/10, Batch 579/883, Training Loss: 0.8871\n",
      "Epoch 1/10, Batch 580/883, Training Loss: 0.8392\n",
      "Epoch 1/10, Batch 581/883, Training Loss: 0.9670\n",
      "Epoch 1/10, Batch 582/883, Training Loss: 0.7472\n",
      "Epoch 1/10, Batch 583/883, Training Loss: 1.5008\n",
      "Epoch 1/10, Batch 584/883, Training Loss: 0.9915\n",
      "Epoch 1/10, Batch 585/883, Training Loss: 1.1280\n",
      "Epoch 1/10, Batch 586/883, Training Loss: 0.9813\n",
      "Epoch 1/10, Batch 587/883, Training Loss: 0.8758\n",
      "Epoch 1/10, Batch 588/883, Training Loss: 0.9830\n",
      "Epoch 1/10, Batch 589/883, Training Loss: 0.8729\n",
      "Epoch 1/10, Batch 590/883, Training Loss: 1.0458\n",
      "Epoch 1/10, Batch 591/883, Training Loss: 1.0648\n",
      "Epoch 1/10, Batch 592/883, Training Loss: 0.9709\n",
      "Epoch 1/10, Batch 593/883, Training Loss: 0.8476\n",
      "Epoch 1/10, Batch 594/883, Training Loss: 0.9285\n",
      "Epoch 1/10, Batch 595/883, Training Loss: 0.8524\n",
      "Epoch 1/10, Batch 596/883, Training Loss: 0.9895\n",
      "Epoch 1/10, Batch 597/883, Training Loss: 0.8884\n",
      "Epoch 1/10, Batch 598/883, Training Loss: 0.9629\n",
      "Epoch 1/10, Batch 599/883, Training Loss: 0.7721\n",
      "Epoch 1/10, Batch 600/883, Training Loss: 0.7610\n",
      "Epoch 1/10, Batch 601/883, Training Loss: 0.8774\n",
      "Epoch 1/10, Batch 602/883, Training Loss: 0.8974\n",
      "Epoch 1/10, Batch 603/883, Training Loss: 0.7898\n",
      "Epoch 1/10, Batch 604/883, Training Loss: 0.9455\n",
      "Epoch 1/10, Batch 605/883, Training Loss: 1.0998\n",
      "Epoch 1/10, Batch 606/883, Training Loss: 1.1058\n",
      "Epoch 1/10, Batch 607/883, Training Loss: 0.8642\n",
      "Epoch 1/10, Batch 608/883, Training Loss: 0.8171\n",
      "Epoch 1/10, Batch 609/883, Training Loss: 1.0668\n",
      "Epoch 1/10, Batch 610/883, Training Loss: 1.2227\n",
      "Epoch 1/10, Batch 611/883, Training Loss: 0.9986\n",
      "Epoch 1/10, Batch 612/883, Training Loss: 0.9413\n",
      "Epoch 1/10, Batch 613/883, Training Loss: 0.8511\n",
      "Epoch 1/10, Batch 614/883, Training Loss: 1.0307\n",
      "Epoch 1/10, Batch 615/883, Training Loss: 0.8495\n",
      "Epoch 1/10, Batch 616/883, Training Loss: 0.8221\n",
      "Epoch 1/10, Batch 617/883, Training Loss: 1.2421\n",
      "Epoch 1/10, Batch 618/883, Training Loss: 1.1344\n",
      "Epoch 1/10, Batch 619/883, Training Loss: 0.7323\n",
      "Epoch 1/10, Batch 620/883, Training Loss: 0.9510\n",
      "Epoch 1/10, Batch 621/883, Training Loss: 0.8265\n",
      "Epoch 1/10, Batch 622/883, Training Loss: 0.7695\n",
      "Epoch 1/10, Batch 623/883, Training Loss: 0.6389\n",
      "Epoch 1/10, Batch 624/883, Training Loss: 0.9832\n",
      "Epoch 1/10, Batch 625/883, Training Loss: 1.2863\n",
      "Epoch 1/10, Batch 626/883, Training Loss: 0.9281\n",
      "Epoch 1/10, Batch 627/883, Training Loss: 1.2131\n",
      "Epoch 1/10, Batch 628/883, Training Loss: 0.8155\n",
      "Epoch 1/10, Batch 629/883, Training Loss: 1.1368\n",
      "Epoch 1/10, Batch 630/883, Training Loss: 1.0116\n",
      "Epoch 1/10, Batch 631/883, Training Loss: 0.8680\n",
      "Epoch 1/10, Batch 632/883, Training Loss: 0.8977\n",
      "Epoch 1/10, Batch 633/883, Training Loss: 0.9346\n",
      "Epoch 1/10, Batch 634/883, Training Loss: 0.8279\n",
      "Epoch 1/10, Batch 635/883, Training Loss: 0.8746\n",
      "Epoch 1/10, Batch 636/883, Training Loss: 0.9414\n",
      "Epoch 1/10, Batch 637/883, Training Loss: 0.9127\n",
      "Epoch 1/10, Batch 638/883, Training Loss: 0.7999\n",
      "Epoch 1/10, Batch 639/883, Training Loss: 0.9925\n",
      "Epoch 1/10, Batch 640/883, Training Loss: 0.7787\n",
      "Epoch 1/10, Batch 641/883, Training Loss: 0.6963\n",
      "Epoch 1/10, Batch 642/883, Training Loss: 0.9084\n",
      "Epoch 1/10, Batch 643/883, Training Loss: 0.7663\n",
      "Epoch 1/10, Batch 644/883, Training Loss: 0.6931\n",
      "Epoch 1/10, Batch 645/883, Training Loss: 0.9206\n",
      "Epoch 1/10, Batch 646/883, Training Loss: 0.8896\n",
      "Epoch 1/10, Batch 647/883, Training Loss: 0.9095\n",
      "Epoch 1/10, Batch 648/883, Training Loss: 0.9274\n",
      "Epoch 1/10, Batch 649/883, Training Loss: 0.9009\n",
      "Epoch 1/10, Batch 650/883, Training Loss: 1.0959\n",
      "Epoch 1/10, Batch 651/883, Training Loss: 0.8793\n",
      "Epoch 1/10, Batch 652/883, Training Loss: 1.1841\n",
      "Epoch 1/10, Batch 653/883, Training Loss: 0.8588\n",
      "Epoch 1/10, Batch 654/883, Training Loss: 0.9088\n",
      "Epoch 1/10, Batch 655/883, Training Loss: 0.7819\n",
      "Epoch 1/10, Batch 656/883, Training Loss: 0.7862\n",
      "Epoch 1/10, Batch 657/883, Training Loss: 0.7807\n",
      "Epoch 1/10, Batch 658/883, Training Loss: 0.8085\n",
      "Epoch 1/10, Batch 659/883, Training Loss: 0.6935\n",
      "Epoch 1/10, Batch 660/883, Training Loss: 0.9935\n",
      "Epoch 1/10, Batch 661/883, Training Loss: 1.1385\n",
      "Epoch 1/10, Batch 662/883, Training Loss: 0.9253\n",
      "Epoch 1/10, Batch 663/883, Training Loss: 0.7577\n",
      "Epoch 1/10, Batch 664/883, Training Loss: 0.6930\n",
      "Epoch 1/10, Batch 665/883, Training Loss: 0.8756\n",
      "Epoch 1/10, Batch 666/883, Training Loss: 1.1061\n",
      "Epoch 1/10, Batch 667/883, Training Loss: 0.8162\n",
      "Epoch 1/10, Batch 668/883, Training Loss: 0.6474\n",
      "Epoch 1/10, Batch 669/883, Training Loss: 0.7192\n",
      "Epoch 1/10, Batch 670/883, Training Loss: 0.9832\n",
      "Epoch 1/10, Batch 671/883, Training Loss: 0.6634\n",
      "Epoch 1/10, Batch 672/883, Training Loss: 0.7616\n",
      "Epoch 1/10, Batch 673/883, Training Loss: 1.0163\n",
      "Epoch 1/10, Batch 674/883, Training Loss: 1.0156\n",
      "Epoch 1/10, Batch 675/883, Training Loss: 0.9566\n",
      "Epoch 1/10, Batch 676/883, Training Loss: 0.7417\n",
      "Epoch 1/10, Batch 677/883, Training Loss: 0.7322\n",
      "Epoch 1/10, Batch 678/883, Training Loss: 0.9151\n",
      "Epoch 1/10, Batch 679/883, Training Loss: 0.6991\n",
      "Epoch 1/10, Batch 680/883, Training Loss: 0.8079\n",
      "Epoch 1/10, Batch 681/883, Training Loss: 0.8500\n",
      "Epoch 1/10, Batch 682/883, Training Loss: 0.7854\n",
      "Epoch 1/10, Batch 683/883, Training Loss: 0.9298\n",
      "Epoch 1/10, Batch 684/883, Training Loss: 0.7601\n",
      "Epoch 1/10, Batch 685/883, Training Loss: 1.3324\n",
      "Epoch 1/10, Batch 686/883, Training Loss: 0.9599\n",
      "Epoch 1/10, Batch 687/883, Training Loss: 1.0506\n",
      "Epoch 1/10, Batch 688/883, Training Loss: 0.9715\n",
      "Epoch 1/10, Batch 689/883, Training Loss: 0.9359\n",
      "Epoch 1/10, Batch 690/883, Training Loss: 0.9169\n",
      "Epoch 1/10, Batch 691/883, Training Loss: 0.8126\n",
      "Epoch 1/10, Batch 692/883, Training Loss: 0.7747\n",
      "Epoch 1/10, Batch 693/883, Training Loss: 0.8219\n",
      "Epoch 1/10, Batch 694/883, Training Loss: 1.0556\n",
      "Epoch 1/10, Batch 695/883, Training Loss: 1.1272\n",
      "Epoch 1/10, Batch 696/883, Training Loss: 0.8484\n",
      "Epoch 1/10, Batch 697/883, Training Loss: 0.9932\n",
      "Epoch 1/10, Batch 698/883, Training Loss: 0.6640\n",
      "Epoch 1/10, Batch 699/883, Training Loss: 0.9980\n",
      "Epoch 1/10, Batch 700/883, Training Loss: 0.7147\n",
      "Epoch 1/10, Batch 701/883, Training Loss: 1.0330\n",
      "Epoch 1/10, Batch 702/883, Training Loss: 1.2114\n",
      "Epoch 1/10, Batch 703/883, Training Loss: 0.7631\n",
      "Epoch 1/10, Batch 704/883, Training Loss: 0.7280\n",
      "Epoch 1/10, Batch 705/883, Training Loss: 0.8487\n",
      "Epoch 1/10, Batch 706/883, Training Loss: 0.9553\n",
      "Epoch 1/10, Batch 707/883, Training Loss: 0.8974\n",
      "Epoch 1/10, Batch 708/883, Training Loss: 0.9045\n",
      "Epoch 1/10, Batch 709/883, Training Loss: 0.8207\n",
      "Epoch 1/10, Batch 710/883, Training Loss: 0.8018\n",
      "Epoch 1/10, Batch 711/883, Training Loss: 0.8067\n",
      "Epoch 1/10, Batch 712/883, Training Loss: 1.0201\n",
      "Epoch 1/10, Batch 713/883, Training Loss: 0.9528\n",
      "Epoch 1/10, Batch 714/883, Training Loss: 0.8812\n",
      "Epoch 1/10, Batch 715/883, Training Loss: 0.8073\n",
      "Epoch 1/10, Batch 716/883, Training Loss: 0.7886\n",
      "Epoch 1/10, Batch 717/883, Training Loss: 0.8859\n",
      "Epoch 1/10, Batch 718/883, Training Loss: 1.2225\n",
      "Epoch 1/10, Batch 719/883, Training Loss: 1.0962\n",
      "Epoch 1/10, Batch 720/883, Training Loss: 0.7909\n",
      "Epoch 1/10, Batch 721/883, Training Loss: 1.0825\n",
      "Epoch 1/10, Batch 722/883, Training Loss: 1.1264\n",
      "Epoch 1/10, Batch 723/883, Training Loss: 0.7531\n",
      "Epoch 1/10, Batch 724/883, Training Loss: 1.1025\n",
      "Epoch 1/10, Batch 725/883, Training Loss: 0.8059\n",
      "Epoch 1/10, Batch 726/883, Training Loss: 1.1639\n",
      "Epoch 1/10, Batch 727/883, Training Loss: 1.0385\n",
      "Epoch 1/10, Batch 728/883, Training Loss: 1.1976\n",
      "Epoch 1/10, Batch 729/883, Training Loss: 0.9020\n",
      "Epoch 1/10, Batch 730/883, Training Loss: 0.9001\n",
      "Epoch 1/10, Batch 731/883, Training Loss: 0.9797\n",
      "Epoch 1/10, Batch 732/883, Training Loss: 0.8135\n",
      "Epoch 1/10, Batch 733/883, Training Loss: 0.9319\n",
      "Epoch 1/10, Batch 734/883, Training Loss: 0.9206\n",
      "Epoch 1/10, Batch 735/883, Training Loss: 0.9823\n",
      "Epoch 1/10, Batch 736/883, Training Loss: 0.8129\n",
      "Epoch 1/10, Batch 737/883, Training Loss: 0.7743\n",
      "Epoch 1/10, Batch 738/883, Training Loss: 1.1950\n",
      "Epoch 1/10, Batch 739/883, Training Loss: 0.7928\n",
      "Epoch 1/10, Batch 740/883, Training Loss: 0.9419\n",
      "Epoch 1/10, Batch 741/883, Training Loss: 0.7352\n",
      "Epoch 1/10, Batch 742/883, Training Loss: 0.8419\n",
      "Epoch 1/10, Batch 743/883, Training Loss: 0.7536\n",
      "Epoch 1/10, Batch 744/883, Training Loss: 0.8055\n",
      "Epoch 1/10, Batch 745/883, Training Loss: 0.9462\n",
      "Epoch 1/10, Batch 746/883, Training Loss: 0.6561\n",
      "Epoch 1/10, Batch 747/883, Training Loss: 0.9005\n",
      "Epoch 1/10, Batch 748/883, Training Loss: 0.6389\n",
      "Epoch 1/10, Batch 749/883, Training Loss: 0.8509\n",
      "Epoch 1/10, Batch 750/883, Training Loss: 0.8272\n",
      "Epoch 1/10, Batch 751/883, Training Loss: 0.7967\n",
      "Epoch 1/10, Batch 752/883, Training Loss: 1.0229\n",
      "Epoch 1/10, Batch 753/883, Training Loss: 0.7689\n",
      "Epoch 1/10, Batch 754/883, Training Loss: 0.7393\n",
      "Epoch 1/10, Batch 755/883, Training Loss: 0.9627\n",
      "Epoch 1/10, Batch 756/883, Training Loss: 0.8902\n",
      "Epoch 1/10, Batch 757/883, Training Loss: 0.9252\n",
      "Epoch 1/10, Batch 758/883, Training Loss: 1.2061\n",
      "Epoch 1/10, Batch 759/883, Training Loss: 0.7367\n",
      "Epoch 1/10, Batch 760/883, Training Loss: 0.8563\n",
      "Epoch 1/10, Batch 761/883, Training Loss: 0.6760\n",
      "Epoch 1/10, Batch 762/883, Training Loss: 0.9184\n",
      "Epoch 1/10, Batch 763/883, Training Loss: 0.9555\n",
      "Epoch 1/10, Batch 764/883, Training Loss: 0.9029\n",
      "Epoch 1/10, Batch 765/883, Training Loss: 1.2289\n",
      "Epoch 1/10, Batch 766/883, Training Loss: 0.6598\n",
      "Epoch 1/10, Batch 767/883, Training Loss: 0.7829\n",
      "Epoch 1/10, Batch 768/883, Training Loss: 1.0474\n",
      "Epoch 1/10, Batch 769/883, Training Loss: 0.9588\n",
      "Epoch 1/10, Batch 770/883, Training Loss: 0.7426\n",
      "Epoch 1/10, Batch 771/883, Training Loss: 0.9640\n",
      "Epoch 1/10, Batch 772/883, Training Loss: 0.9081\n",
      "Epoch 1/10, Batch 773/883, Training Loss: 0.7403\n",
      "Epoch 1/10, Batch 774/883, Training Loss: 0.7536\n",
      "Epoch 1/10, Batch 775/883, Training Loss: 1.2123\n",
      "Epoch 1/10, Batch 776/883, Training Loss: 0.7412\n",
      "Epoch 1/10, Batch 777/883, Training Loss: 0.9266\n",
      "Epoch 1/10, Batch 778/883, Training Loss: 0.9690\n",
      "Epoch 1/10, Batch 779/883, Training Loss: 1.5494\n",
      "Epoch 1/10, Batch 780/883, Training Loss: 0.8904\n",
      "Epoch 1/10, Batch 781/883, Training Loss: 0.8018\n",
      "Epoch 1/10, Batch 782/883, Training Loss: 0.6002\n",
      "Epoch 1/10, Batch 783/883, Training Loss: 0.8722\n",
      "Epoch 1/10, Batch 784/883, Training Loss: 0.9890\n",
      "Epoch 1/10, Batch 785/883, Training Loss: 0.7694\n",
      "Epoch 1/10, Batch 786/883, Training Loss: 0.8674\n",
      "Epoch 1/10, Batch 787/883, Training Loss: 0.9715\n",
      "Epoch 1/10, Batch 788/883, Training Loss: 1.1811\n",
      "Epoch 1/10, Batch 789/883, Training Loss: 0.9470\n",
      "Epoch 1/10, Batch 790/883, Training Loss: 1.0147\n",
      "Epoch 1/10, Batch 791/883, Training Loss: 0.9833\n",
      "Epoch 1/10, Batch 792/883, Training Loss: 0.7329\n",
      "Epoch 1/10, Batch 793/883, Training Loss: 1.0472\n",
      "Epoch 1/10, Batch 794/883, Training Loss: 0.9082\n",
      "Epoch 1/10, Batch 795/883, Training Loss: 1.0253\n",
      "Epoch 1/10, Batch 796/883, Training Loss: 0.8268\n",
      "Epoch 1/10, Batch 797/883, Training Loss: 0.9734\n",
      "Epoch 1/10, Batch 798/883, Training Loss: 0.8024\n",
      "Epoch 1/10, Batch 799/883, Training Loss: 0.8460\n",
      "Epoch 1/10, Batch 800/883, Training Loss: 0.9065\n",
      "Epoch 1/10, Batch 801/883, Training Loss: 0.7376\n",
      "Epoch 1/10, Batch 802/883, Training Loss: 0.9518\n",
      "Epoch 1/10, Batch 803/883, Training Loss: 0.9548\n",
      "Epoch 1/10, Batch 804/883, Training Loss: 0.7135\n",
      "Epoch 1/10, Batch 805/883, Training Loss: 0.8340\n",
      "Epoch 1/10, Batch 806/883, Training Loss: 0.8917\n",
      "Epoch 1/10, Batch 807/883, Training Loss: 0.7867\n",
      "Epoch 1/10, Batch 808/883, Training Loss: 0.9297\n",
      "Epoch 1/10, Batch 809/883, Training Loss: 0.6615\n",
      "Epoch 1/10, Batch 810/883, Training Loss: 0.7042\n",
      "Epoch 1/10, Batch 811/883, Training Loss: 0.7798\n",
      "Epoch 1/10, Batch 812/883, Training Loss: 1.1615\n",
      "Epoch 1/10, Batch 813/883, Training Loss: 0.5500\n",
      "Epoch 1/10, Batch 814/883, Training Loss: 0.8594\n",
      "Epoch 1/10, Batch 815/883, Training Loss: 0.7843\n",
      "Epoch 1/10, Batch 816/883, Training Loss: 0.7963\n",
      "Epoch 1/10, Batch 817/883, Training Loss: 0.5889\n",
      "Epoch 1/10, Batch 818/883, Training Loss: 0.6683\n",
      "Epoch 1/10, Batch 819/883, Training Loss: 0.9262\n",
      "Epoch 1/10, Batch 820/883, Training Loss: 0.7229\n",
      "Epoch 1/10, Batch 821/883, Training Loss: 1.1290\n",
      "Epoch 1/10, Batch 822/883, Training Loss: 0.8996\n",
      "Epoch 1/10, Batch 823/883, Training Loss: 0.7064\n",
      "Epoch 1/10, Batch 824/883, Training Loss: 1.0338\n",
      "Epoch 1/10, Batch 825/883, Training Loss: 0.7351\n",
      "Epoch 1/10, Batch 826/883, Training Loss: 0.9632\n",
      "Epoch 1/10, Batch 827/883, Training Loss: 0.8568\n",
      "Epoch 1/10, Batch 828/883, Training Loss: 0.9230\n",
      "Epoch 1/10, Batch 829/883, Training Loss: 0.9049\n",
      "Epoch 1/10, Batch 830/883, Training Loss: 0.7083\n",
      "Epoch 1/10, Batch 831/883, Training Loss: 0.8419\n",
      "Epoch 1/10, Batch 832/883, Training Loss: 0.8713\n",
      "Epoch 1/10, Batch 833/883, Training Loss: 1.2013\n",
      "Epoch 1/10, Batch 834/883, Training Loss: 0.5909\n",
      "Epoch 1/10, Batch 835/883, Training Loss: 0.8131\n",
      "Epoch 1/10, Batch 836/883, Training Loss: 0.8979\n",
      "Epoch 1/10, Batch 837/883, Training Loss: 1.1228\n",
      "Epoch 1/10, Batch 838/883, Training Loss: 0.9237\n",
      "Epoch 1/10, Batch 839/883, Training Loss: 0.8997\n",
      "Epoch 1/10, Batch 840/883, Training Loss: 1.1565\n",
      "Epoch 1/10, Batch 841/883, Training Loss: 0.8920\n",
      "Epoch 1/10, Batch 842/883, Training Loss: 0.9012\n",
      "Epoch 1/10, Batch 843/883, Training Loss: 0.9666\n",
      "Epoch 1/10, Batch 844/883, Training Loss: 0.8625\n",
      "Epoch 1/10, Batch 845/883, Training Loss: 0.9822\n",
      "Epoch 1/10, Batch 846/883, Training Loss: 0.9539\n",
      "Epoch 1/10, Batch 847/883, Training Loss: 1.1246\n",
      "Epoch 1/10, Batch 848/883, Training Loss: 0.8948\n",
      "Epoch 1/10, Batch 849/883, Training Loss: 0.9493\n",
      "Epoch 1/10, Batch 850/883, Training Loss: 0.8363\n",
      "Epoch 1/10, Batch 851/883, Training Loss: 0.9397\n",
      "Epoch 1/10, Batch 852/883, Training Loss: 0.8783\n",
      "Epoch 1/10, Batch 853/883, Training Loss: 0.8581\n",
      "Epoch 1/10, Batch 854/883, Training Loss: 0.7386\n",
      "Epoch 1/10, Batch 855/883, Training Loss: 0.8680\n",
      "Epoch 1/10, Batch 856/883, Training Loss: 0.9891\n",
      "Epoch 1/10, Batch 857/883, Training Loss: 0.8706\n",
      "Epoch 1/10, Batch 858/883, Training Loss: 0.8591\n",
      "Epoch 1/10, Batch 859/883, Training Loss: 0.6952\n",
      "Epoch 1/10, Batch 860/883, Training Loss: 0.6745\n",
      "Epoch 1/10, Batch 861/883, Training Loss: 0.6765\n",
      "Epoch 1/10, Batch 862/883, Training Loss: 0.6931\n",
      "Epoch 1/10, Batch 863/883, Training Loss: 0.6775\n",
      "Epoch 1/10, Batch 864/883, Training Loss: 0.8571\n",
      "Epoch 1/10, Batch 865/883, Training Loss: 0.9661\n",
      "Epoch 1/10, Batch 866/883, Training Loss: 0.7117\n",
      "Epoch 1/10, Batch 867/883, Training Loss: 0.9105\n",
      "Epoch 1/10, Batch 868/883, Training Loss: 1.0683\n",
      "Epoch 1/10, Batch 869/883, Training Loss: 0.8258\n",
      "Epoch 1/10, Batch 870/883, Training Loss: 0.9643\n",
      "Epoch 1/10, Batch 871/883, Training Loss: 0.8782\n",
      "Epoch 1/10, Batch 872/883, Training Loss: 0.8077\n",
      "Epoch 1/10, Batch 873/883, Training Loss: 0.9264\n",
      "Epoch 1/10, Batch 874/883, Training Loss: 0.8104\n",
      "Epoch 1/10, Batch 875/883, Training Loss: 1.1200\n",
      "Epoch 1/10, Batch 876/883, Training Loss: 1.1373\n",
      "Epoch 1/10, Batch 877/883, Training Loss: 0.9868\n",
      "Epoch 1/10, Batch 878/883, Training Loss: 0.9717\n",
      "Epoch 1/10, Batch 879/883, Training Loss: 0.9303\n",
      "Epoch 1/10, Batch 880/883, Training Loss: 0.6256\n",
      "Epoch 1/10, Batch 881/883, Training Loss: 0.6330\n",
      "Epoch 1/10, Batch 882/883, Training Loss: 0.7908\n",
      "Epoch 1/10, Batch 883/883, Training Loss: 0.6081\n",
      "Epoch 1/10, Training Loss: 0.9472, Validation Loss: 0.8312, Validation Accuracy: 0.5976\n",
      "Epoch 2/10, Batch 1/883, Training Loss: 0.8899\n",
      "Epoch 2/10, Batch 2/883, Training Loss: 0.9091\n",
      "Epoch 2/10, Batch 3/883, Training Loss: 1.0008\n",
      "Epoch 2/10, Batch 4/883, Training Loss: 0.6707\n",
      "Epoch 2/10, Batch 5/883, Training Loss: 0.9081\n",
      "Epoch 2/10, Batch 6/883, Training Loss: 0.9750\n",
      "Epoch 2/10, Batch 7/883, Training Loss: 1.3256\n",
      "Epoch 2/10, Batch 8/883, Training Loss: 1.1821\n",
      "Epoch 2/10, Batch 9/883, Training Loss: 1.1256\n",
      "Epoch 2/10, Batch 10/883, Training Loss: 0.8438\n",
      "Epoch 2/10, Batch 11/883, Training Loss: 0.8383\n",
      "Epoch 2/10, Batch 12/883, Training Loss: 1.0314\n",
      "Epoch 2/10, Batch 13/883, Training Loss: 0.7474\n",
      "Epoch 2/10, Batch 14/883, Training Loss: 0.9533\n",
      "Epoch 2/10, Batch 15/883, Training Loss: 0.9812\n",
      "Epoch 2/10, Batch 16/883, Training Loss: 0.7530\n",
      "Epoch 2/10, Batch 17/883, Training Loss: 0.9027\n",
      "Epoch 2/10, Batch 18/883, Training Loss: 1.1882\n",
      "Epoch 2/10, Batch 19/883, Training Loss: 0.9049\n",
      "Epoch 2/10, Batch 20/883, Training Loss: 0.9175\n",
      "Epoch 2/10, Batch 21/883, Training Loss: 0.8449\n",
      "Epoch 2/10, Batch 22/883, Training Loss: 0.9760\n",
      "Epoch 2/10, Batch 23/883, Training Loss: 1.0296\n",
      "Epoch 2/10, Batch 24/883, Training Loss: 0.8821\n",
      "Epoch 2/10, Batch 25/883, Training Loss: 0.9239\n",
      "Epoch 2/10, Batch 26/883, Training Loss: 0.9710\n",
      "Epoch 2/10, Batch 27/883, Training Loss: 0.7531\n",
      "Epoch 2/10, Batch 28/883, Training Loss: 0.8134\n",
      "Epoch 2/10, Batch 29/883, Training Loss: 0.8632\n",
      "Epoch 2/10, Batch 30/883, Training Loss: 0.8771\n",
      "Epoch 2/10, Batch 31/883, Training Loss: 0.9068\n",
      "Epoch 2/10, Batch 32/883, Training Loss: 1.1086\n",
      "Epoch 2/10, Batch 33/883, Training Loss: 1.0224\n",
      "Epoch 2/10, Batch 34/883, Training Loss: 0.7790\n",
      "Epoch 2/10, Batch 35/883, Training Loss: 0.8002\n",
      "Epoch 2/10, Batch 36/883, Training Loss: 0.8987\n",
      "Epoch 2/10, Batch 37/883, Training Loss: 0.7287\n",
      "Epoch 2/10, Batch 38/883, Training Loss: 0.8004\n",
      "Epoch 2/10, Batch 39/883, Training Loss: 0.8812\n",
      "Epoch 2/10, Batch 40/883, Training Loss: 1.0043\n",
      "Epoch 2/10, Batch 41/883, Training Loss: 0.9366\n",
      "Epoch 2/10, Batch 42/883, Training Loss: 0.7437\n",
      "Epoch 2/10, Batch 43/883, Training Loss: 0.8330\n",
      "Epoch 2/10, Batch 44/883, Training Loss: 1.0928\n",
      "Epoch 2/10, Batch 45/883, Training Loss: 0.9652\n",
      "Epoch 2/10, Batch 46/883, Training Loss: 0.9741\n",
      "Epoch 2/10, Batch 47/883, Training Loss: 0.8781\n",
      "Epoch 2/10, Batch 48/883, Training Loss: 0.9006\n",
      "Epoch 2/10, Batch 49/883, Training Loss: 0.8727\n",
      "Epoch 2/10, Batch 50/883, Training Loss: 1.1386\n",
      "Epoch 2/10, Batch 51/883, Training Loss: 0.9474\n",
      "Epoch 2/10, Batch 52/883, Training Loss: 1.0155\n",
      "Epoch 2/10, Batch 53/883, Training Loss: 1.0674\n",
      "Epoch 2/10, Batch 54/883, Training Loss: 1.0149\n",
      "Epoch 2/10, Batch 55/883, Training Loss: 0.8699\n",
      "Epoch 2/10, Batch 56/883, Training Loss: 0.8532\n",
      "Epoch 2/10, Batch 57/883, Training Loss: 0.7542\n",
      "Epoch 2/10, Batch 58/883, Training Loss: 0.9236\n",
      "Epoch 2/10, Batch 59/883, Training Loss: 0.8677\n",
      "Epoch 2/10, Batch 60/883, Training Loss: 0.8571\n",
      "Epoch 2/10, Batch 61/883, Training Loss: 0.7966\n",
      "Epoch 2/10, Batch 62/883, Training Loss: 0.8967\n",
      "Epoch 2/10, Batch 63/883, Training Loss: 0.7988\n",
      "Epoch 2/10, Batch 64/883, Training Loss: 1.1460\n",
      "Epoch 2/10, Batch 65/883, Training Loss: 0.8201\n",
      "Epoch 2/10, Batch 66/883, Training Loss: 1.0158\n",
      "Epoch 2/10, Batch 67/883, Training Loss: 1.0378\n",
      "Epoch 2/10, Batch 68/883, Training Loss: 0.8617\n",
      "Epoch 2/10, Batch 69/883, Training Loss: 0.7743\n",
      "Epoch 2/10, Batch 70/883, Training Loss: 0.8345\n",
      "Epoch 2/10, Batch 71/883, Training Loss: 0.7759\n",
      "Epoch 2/10, Batch 72/883, Training Loss: 0.8831\n",
      "Epoch 2/10, Batch 73/883, Training Loss: 0.9513\n",
      "Epoch 2/10, Batch 74/883, Training Loss: 0.8457\n",
      "Epoch 2/10, Batch 75/883, Training Loss: 0.8864\n",
      "Epoch 2/10, Batch 76/883, Training Loss: 0.8134\n",
      "Epoch 2/10, Batch 77/883, Training Loss: 0.7119\n",
      "Epoch 2/10, Batch 78/883, Training Loss: 0.9562\n",
      "Epoch 2/10, Batch 79/883, Training Loss: 0.7892\n",
      "Epoch 2/10, Batch 80/883, Training Loss: 0.9195\n",
      "Epoch 2/10, Batch 81/883, Training Loss: 0.7785\n",
      "Epoch 2/10, Batch 82/883, Training Loss: 0.8954\n",
      "Epoch 2/10, Batch 83/883, Training Loss: 0.6715\n",
      "Epoch 2/10, Batch 84/883, Training Loss: 0.8202\n",
      "Epoch 2/10, Batch 85/883, Training Loss: 0.8413\n",
      "Epoch 2/10, Batch 86/883, Training Loss: 0.6641\n",
      "Epoch 2/10, Batch 87/883, Training Loss: 0.6793\n",
      "Epoch 2/10, Batch 88/883, Training Loss: 0.6945\n",
      "Epoch 2/10, Batch 89/883, Training Loss: 1.4163\n",
      "Epoch 2/10, Batch 90/883, Training Loss: 0.7876\n",
      "Epoch 2/10, Batch 91/883, Training Loss: 0.9114\n",
      "Epoch 2/10, Batch 92/883, Training Loss: 0.9325\n",
      "Epoch 2/10, Batch 93/883, Training Loss: 1.1570\n",
      "Epoch 2/10, Batch 94/883, Training Loss: 0.9644\n",
      "Epoch 2/10, Batch 95/883, Training Loss: 0.7790\n",
      "Epoch 2/10, Batch 96/883, Training Loss: 0.8545\n",
      "Epoch 2/10, Batch 97/883, Training Loss: 0.8728\n",
      "Epoch 2/10, Batch 98/883, Training Loss: 0.7922\n",
      "Epoch 2/10, Batch 99/883, Training Loss: 0.7315\n",
      "Epoch 2/10, Batch 100/883, Training Loss: 0.7538\n",
      "Epoch 2/10, Batch 101/883, Training Loss: 0.8588\n",
      "Epoch 2/10, Batch 102/883, Training Loss: 0.8636\n",
      "Epoch 2/10, Batch 103/883, Training Loss: 0.8597\n",
      "Epoch 2/10, Batch 104/883, Training Loss: 0.8145\n",
      "Epoch 2/10, Batch 105/883, Training Loss: 0.9449\n",
      "Epoch 2/10, Batch 106/883, Training Loss: 0.6941\n",
      "Epoch 2/10, Batch 107/883, Training Loss: 0.6277\n",
      "Epoch 2/10, Batch 108/883, Training Loss: 1.0604\n",
      "Epoch 2/10, Batch 109/883, Training Loss: 0.7220\n",
      "Epoch 2/10, Batch 110/883, Training Loss: 0.7765\n",
      "Epoch 2/10, Batch 111/883, Training Loss: 0.5628\n",
      "Epoch 2/10, Batch 112/883, Training Loss: 0.9922\n",
      "Epoch 2/10, Batch 113/883, Training Loss: 0.8172\n",
      "Epoch 2/10, Batch 114/883, Training Loss: 0.6508\n",
      "Epoch 2/10, Batch 115/883, Training Loss: 0.7717\n",
      "Epoch 2/10, Batch 116/883, Training Loss: 0.7110\n",
      "Epoch 2/10, Batch 117/883, Training Loss: 0.7957\n",
      "Epoch 2/10, Batch 118/883, Training Loss: 0.8440\n",
      "Epoch 2/10, Batch 119/883, Training Loss: 0.8303\n",
      "Epoch 2/10, Batch 120/883, Training Loss: 0.9406\n",
      "Epoch 2/10, Batch 121/883, Training Loss: 0.8077\n",
      "Epoch 2/10, Batch 122/883, Training Loss: 0.6890\n",
      "Epoch 2/10, Batch 123/883, Training Loss: 0.7511\n",
      "Epoch 2/10, Batch 124/883, Training Loss: 0.8313\n",
      "Epoch 2/10, Batch 125/883, Training Loss: 0.9669\n",
      "Epoch 2/10, Batch 126/883, Training Loss: 0.8011\n",
      "Epoch 2/10, Batch 127/883, Training Loss: 0.6693\n",
      "Epoch 2/10, Batch 128/883, Training Loss: 0.8164\n",
      "Epoch 2/10, Batch 129/883, Training Loss: 1.0053\n",
      "Epoch 2/10, Batch 130/883, Training Loss: 0.8566\n",
      "Epoch 2/10, Batch 131/883, Training Loss: 0.8501\n",
      "Epoch 2/10, Batch 132/883, Training Loss: 1.0300\n",
      "Epoch 2/10, Batch 133/883, Training Loss: 0.7709\n",
      "Epoch 2/10, Batch 134/883, Training Loss: 0.6661\n",
      "Epoch 2/10, Batch 135/883, Training Loss: 0.9178\n",
      "Epoch 2/10, Batch 136/883, Training Loss: 0.8454\n",
      "Epoch 2/10, Batch 137/883, Training Loss: 1.1179\n",
      "Epoch 2/10, Batch 138/883, Training Loss: 1.0954\n",
      "Epoch 2/10, Batch 139/883, Training Loss: 0.6178\n",
      "Epoch 2/10, Batch 140/883, Training Loss: 0.9722\n",
      "Epoch 2/10, Batch 141/883, Training Loss: 0.9302\n",
      "Epoch 2/10, Batch 142/883, Training Loss: 0.6566\n",
      "Epoch 2/10, Batch 143/883, Training Loss: 0.6719\n",
      "Epoch 2/10, Batch 144/883, Training Loss: 0.7984\n",
      "Epoch 2/10, Batch 145/883, Training Loss: 0.8238\n",
      "Epoch 2/10, Batch 146/883, Training Loss: 1.1443\n",
      "Epoch 2/10, Batch 147/883, Training Loss: 1.0014\n",
      "Epoch 2/10, Batch 148/883, Training Loss: 0.8292\n",
      "Epoch 2/10, Batch 149/883, Training Loss: 0.8738\n",
      "Epoch 2/10, Batch 150/883, Training Loss: 0.7991\n",
      "Epoch 2/10, Batch 151/883, Training Loss: 0.5422\n",
      "Epoch 2/10, Batch 152/883, Training Loss: 0.6191\n",
      "Epoch 2/10, Batch 153/883, Training Loss: 0.9585\n",
      "Epoch 2/10, Batch 154/883, Training Loss: 1.1000\n",
      "Epoch 2/10, Batch 155/883, Training Loss: 0.7333\n",
      "Epoch 2/10, Batch 156/883, Training Loss: 0.7641\n",
      "Epoch 2/10, Batch 157/883, Training Loss: 0.5880\n",
      "Epoch 2/10, Batch 158/883, Training Loss: 0.7053\n",
      "Epoch 2/10, Batch 159/883, Training Loss: 1.0583\n",
      "Epoch 2/10, Batch 160/883, Training Loss: 0.9032\n",
      "Epoch 2/10, Batch 161/883, Training Loss: 0.6636\n",
      "Epoch 2/10, Batch 162/883, Training Loss: 0.9227\n",
      "Epoch 2/10, Batch 163/883, Training Loss: 0.8451\n",
      "Epoch 2/10, Batch 164/883, Training Loss: 0.5847\n",
      "Epoch 2/10, Batch 165/883, Training Loss: 0.8722\n",
      "Epoch 2/10, Batch 166/883, Training Loss: 0.6715\n",
      "Epoch 2/10, Batch 167/883, Training Loss: 0.9045\n",
      "Epoch 2/10, Batch 168/883, Training Loss: 0.9914\n",
      "Epoch 2/10, Batch 169/883, Training Loss: 0.9264\n",
      "Epoch 2/10, Batch 170/883, Training Loss: 0.8094\n",
      "Epoch 2/10, Batch 171/883, Training Loss: 0.7549\n",
      "Epoch 2/10, Batch 172/883, Training Loss: 0.9303\n",
      "Epoch 2/10, Batch 173/883, Training Loss: 0.6508\n",
      "Epoch 2/10, Batch 174/883, Training Loss: 0.7232\n",
      "Epoch 2/10, Batch 175/883, Training Loss: 0.9599\n",
      "Epoch 2/10, Batch 176/883, Training Loss: 0.7025\n",
      "Epoch 2/10, Batch 177/883, Training Loss: 0.8112\n",
      "Epoch 2/10, Batch 178/883, Training Loss: 0.9033\n",
      "Epoch 2/10, Batch 179/883, Training Loss: 0.5732\n",
      "Epoch 2/10, Batch 180/883, Training Loss: 0.8564\n",
      "Epoch 2/10, Batch 181/883, Training Loss: 0.4575\n",
      "Epoch 2/10, Batch 182/883, Training Loss: 0.8098\n",
      "Epoch 2/10, Batch 183/883, Training Loss: 0.8988\n",
      "Epoch 2/10, Batch 184/883, Training Loss: 1.4210\n",
      "Epoch 2/10, Batch 185/883, Training Loss: 0.8596\n",
      "Epoch 2/10, Batch 186/883, Training Loss: 0.7510\n",
      "Epoch 2/10, Batch 187/883, Training Loss: 0.7313\n",
      "Epoch 2/10, Batch 188/883, Training Loss: 0.8584\n",
      "Epoch 2/10, Batch 189/883, Training Loss: 0.7866\n",
      "Epoch 2/10, Batch 190/883, Training Loss: 0.7627\n",
      "Epoch 2/10, Batch 191/883, Training Loss: 0.6982\n",
      "Epoch 2/10, Batch 192/883, Training Loss: 0.8488\n",
      "Epoch 2/10, Batch 193/883, Training Loss: 0.7991\n",
      "Epoch 2/10, Batch 194/883, Training Loss: 0.9051\n",
      "Epoch 2/10, Batch 195/883, Training Loss: 0.9417\n",
      "Epoch 2/10, Batch 196/883, Training Loss: 0.9291\n",
      "Epoch 2/10, Batch 197/883, Training Loss: 0.9130\n",
      "Epoch 2/10, Batch 198/883, Training Loss: 0.7731\n",
      "Epoch 2/10, Batch 199/883, Training Loss: 0.7865\n",
      "Epoch 2/10, Batch 200/883, Training Loss: 0.9695\n",
      "Epoch 2/10, Batch 201/883, Training Loss: 0.7496\n",
      "Epoch 2/10, Batch 202/883, Training Loss: 0.7528\n",
      "Epoch 2/10, Batch 203/883, Training Loss: 0.8253\n",
      "Epoch 2/10, Batch 204/883, Training Loss: 0.6205\n",
      "Epoch 2/10, Batch 205/883, Training Loss: 1.1186\n",
      "Epoch 2/10, Batch 206/883, Training Loss: 0.9358\n",
      "Epoch 2/10, Batch 207/883, Training Loss: 0.7285\n",
      "Epoch 2/10, Batch 208/883, Training Loss: 0.7577\n",
      "Epoch 2/10, Batch 209/883, Training Loss: 0.7102\n",
      "Epoch 2/10, Batch 210/883, Training Loss: 0.7732\n",
      "Epoch 2/10, Batch 211/883, Training Loss: 0.9232\n",
      "Epoch 2/10, Batch 212/883, Training Loss: 1.3305\n",
      "Epoch 2/10, Batch 213/883, Training Loss: 0.8871\n",
      "Epoch 2/10, Batch 214/883, Training Loss: 0.7510\n",
      "Epoch 2/10, Batch 215/883, Training Loss: 0.8319\n",
      "Epoch 2/10, Batch 216/883, Training Loss: 0.6438\n",
      "Epoch 2/10, Batch 217/883, Training Loss: 0.9846\n",
      "Epoch 2/10, Batch 218/883, Training Loss: 0.8970\n",
      "Epoch 2/10, Batch 219/883, Training Loss: 0.8858\n",
      "Epoch 2/10, Batch 220/883, Training Loss: 1.0055\n",
      "Epoch 2/10, Batch 221/883, Training Loss: 0.7710\n",
      "Epoch 2/10, Batch 222/883, Training Loss: 1.0702\n",
      "Epoch 2/10, Batch 223/883, Training Loss: 0.8546\n",
      "Epoch 2/10, Batch 224/883, Training Loss: 0.7990\n",
      "Epoch 2/10, Batch 225/883, Training Loss: 0.7798\n",
      "Epoch 2/10, Batch 226/883, Training Loss: 0.7432\n",
      "Epoch 2/10, Batch 227/883, Training Loss: 0.7465\n",
      "Epoch 2/10, Batch 228/883, Training Loss: 0.9861\n",
      "Epoch 2/10, Batch 229/883, Training Loss: 0.7513\n",
      "Epoch 2/10, Batch 230/883, Training Loss: 0.7512\n",
      "Epoch 2/10, Batch 231/883, Training Loss: 0.8759\n",
      "Epoch 2/10, Batch 232/883, Training Loss: 0.6673\n",
      "Epoch 2/10, Batch 233/883, Training Loss: 0.7499\n",
      "Epoch 2/10, Batch 234/883, Training Loss: 1.1509\n",
      "Epoch 2/10, Batch 235/883, Training Loss: 0.8648\n",
      "Epoch 2/10, Batch 236/883, Training Loss: 0.6366\n",
      "Epoch 2/10, Batch 237/883, Training Loss: 0.8768\n",
      "Epoch 2/10, Batch 238/883, Training Loss: 0.9070\n",
      "Epoch 2/10, Batch 239/883, Training Loss: 0.7618\n",
      "Epoch 2/10, Batch 240/883, Training Loss: 0.8073\n",
      "Epoch 2/10, Batch 241/883, Training Loss: 0.7436\n",
      "Epoch 2/10, Batch 242/883, Training Loss: 0.7143\n",
      "Epoch 2/10, Batch 243/883, Training Loss: 0.8739\n",
      "Epoch 2/10, Batch 244/883, Training Loss: 0.5427\n",
      "Epoch 2/10, Batch 245/883, Training Loss: 0.7893\n",
      "Epoch 2/10, Batch 246/883, Training Loss: 1.3692\n",
      "Epoch 2/10, Batch 247/883, Training Loss: 0.9851\n",
      "Epoch 2/10, Batch 248/883, Training Loss: 0.8932\n",
      "Epoch 2/10, Batch 249/883, Training Loss: 0.9834\n",
      "Epoch 2/10, Batch 250/883, Training Loss: 0.6635\n",
      "Epoch 2/10, Batch 251/883, Training Loss: 0.7895\n",
      "Epoch 2/10, Batch 252/883, Training Loss: 0.7763\n",
      "Epoch 2/10, Batch 253/883, Training Loss: 0.7129\n",
      "Epoch 2/10, Batch 254/883, Training Loss: 0.7134\n",
      "Epoch 2/10, Batch 255/883, Training Loss: 1.2012\n",
      "Epoch 2/10, Batch 256/883, Training Loss: 0.9790\n",
      "Epoch 2/10, Batch 257/883, Training Loss: 1.0873\n",
      "Epoch 2/10, Batch 258/883, Training Loss: 0.7669\n",
      "Epoch 2/10, Batch 259/883, Training Loss: 0.6972\n",
      "Epoch 2/10, Batch 260/883, Training Loss: 0.9962\n",
      "Epoch 2/10, Batch 261/883, Training Loss: 0.9825\n",
      "Epoch 2/10, Batch 262/883, Training Loss: 0.8552\n",
      "Epoch 2/10, Batch 263/883, Training Loss: 0.8341\n",
      "Epoch 2/10, Batch 264/883, Training Loss: 0.8502\n",
      "Epoch 2/10, Batch 265/883, Training Loss: 0.9288\n",
      "Epoch 2/10, Batch 266/883, Training Loss: 0.9287\n",
      "Epoch 2/10, Batch 267/883, Training Loss: 0.8661\n",
      "Epoch 2/10, Batch 268/883, Training Loss: 0.8760\n",
      "Epoch 2/10, Batch 269/883, Training Loss: 0.7542\n",
      "Epoch 2/10, Batch 270/883, Training Loss: 0.9084\n",
      "Epoch 2/10, Batch 271/883, Training Loss: 0.9014\n",
      "Epoch 2/10, Batch 272/883, Training Loss: 0.8623\n",
      "Epoch 2/10, Batch 273/883, Training Loss: 0.9214\n",
      "Epoch 2/10, Batch 274/883, Training Loss: 0.7343\n",
      "Epoch 2/10, Batch 275/883, Training Loss: 0.7984\n",
      "Epoch 2/10, Batch 276/883, Training Loss: 0.7394\n",
      "Epoch 2/10, Batch 277/883, Training Loss: 0.8981\n",
      "Epoch 2/10, Batch 278/883, Training Loss: 0.9542\n",
      "Epoch 2/10, Batch 279/883, Training Loss: 0.5675\n",
      "Epoch 2/10, Batch 280/883, Training Loss: 0.6062\n",
      "Epoch 2/10, Batch 281/883, Training Loss: 1.0410\n",
      "Epoch 2/10, Batch 282/883, Training Loss: 0.9154\n",
      "Epoch 2/10, Batch 283/883, Training Loss: 0.7428\n",
      "Epoch 2/10, Batch 284/883, Training Loss: 0.7800\n",
      "Epoch 2/10, Batch 285/883, Training Loss: 0.7849\n",
      "Epoch 2/10, Batch 286/883, Training Loss: 0.8590\n",
      "Epoch 2/10, Batch 287/883, Training Loss: 0.8000\n",
      "Epoch 2/10, Batch 288/883, Training Loss: 0.6274\n",
      "Epoch 2/10, Batch 289/883, Training Loss: 0.6733\n",
      "Epoch 2/10, Batch 290/883, Training Loss: 0.9851\n",
      "Epoch 2/10, Batch 291/883, Training Loss: 1.0783\n",
      "Epoch 2/10, Batch 292/883, Training Loss: 0.9886\n",
      "Epoch 2/10, Batch 293/883, Training Loss: 0.9676\n",
      "Epoch 2/10, Batch 294/883, Training Loss: 1.0545\n",
      "Epoch 2/10, Batch 295/883, Training Loss: 0.7955\n",
      "Epoch 2/10, Batch 296/883, Training Loss: 0.7737\n",
      "Epoch 2/10, Batch 297/883, Training Loss: 0.8849\n",
      "Epoch 2/10, Batch 298/883, Training Loss: 0.9649\n",
      "Epoch 2/10, Batch 299/883, Training Loss: 0.8383\n",
      "Epoch 2/10, Batch 300/883, Training Loss: 0.9012\n",
      "Epoch 2/10, Batch 301/883, Training Loss: 0.7469\n",
      "Epoch 2/10, Batch 302/883, Training Loss: 0.8198\n",
      "Epoch 2/10, Batch 303/883, Training Loss: 0.8893\n",
      "Epoch 2/10, Batch 304/883, Training Loss: 0.9315\n",
      "Epoch 2/10, Batch 305/883, Training Loss: 0.7537\n",
      "Epoch 2/10, Batch 306/883, Training Loss: 1.1024\n",
      "Epoch 2/10, Batch 307/883, Training Loss: 0.8283\n",
      "Epoch 2/10, Batch 308/883, Training Loss: 0.8157\n",
      "Epoch 2/10, Batch 309/883, Training Loss: 0.7947\n",
      "Epoch 2/10, Batch 310/883, Training Loss: 0.8427\n",
      "Epoch 2/10, Batch 311/883, Training Loss: 0.7926\n",
      "Epoch 2/10, Batch 312/883, Training Loss: 0.9066\n",
      "Epoch 2/10, Batch 313/883, Training Loss: 0.9462\n",
      "Epoch 2/10, Batch 314/883, Training Loss: 0.9099\n",
      "Epoch 2/10, Batch 315/883, Training Loss: 0.7686\n",
      "Epoch 2/10, Batch 316/883, Training Loss: 0.6936\n",
      "Epoch 2/10, Batch 317/883, Training Loss: 0.8133\n",
      "Epoch 2/10, Batch 318/883, Training Loss: 0.8694\n",
      "Epoch 2/10, Batch 319/883, Training Loss: 0.7598\n",
      "Epoch 2/10, Batch 320/883, Training Loss: 0.7474\n",
      "Epoch 2/10, Batch 321/883, Training Loss: 0.7815\n",
      "Epoch 2/10, Batch 322/883, Training Loss: 0.7874\n",
      "Epoch 2/10, Batch 323/883, Training Loss: 0.9521\n",
      "Epoch 2/10, Batch 324/883, Training Loss: 0.9126\n",
      "Epoch 2/10, Batch 325/883, Training Loss: 1.6082\n",
      "Epoch 2/10, Batch 326/883, Training Loss: 1.1435\n",
      "Epoch 2/10, Batch 327/883, Training Loss: 0.8394\n",
      "Epoch 2/10, Batch 328/883, Training Loss: 0.6633\n",
      "Epoch 2/10, Batch 329/883, Training Loss: 0.7876\n",
      "Epoch 2/10, Batch 330/883, Training Loss: 0.7372\n",
      "Epoch 2/10, Batch 331/883, Training Loss: 0.9055\n",
      "Epoch 2/10, Batch 332/883, Training Loss: 0.6776\n",
      "Epoch 2/10, Batch 333/883, Training Loss: 0.7915\n",
      "Epoch 2/10, Batch 334/883, Training Loss: 0.8364\n",
      "Epoch 2/10, Batch 335/883, Training Loss: 0.7391\n",
      "Epoch 2/10, Batch 336/883, Training Loss: 0.8544\n",
      "Epoch 2/10, Batch 337/883, Training Loss: 0.8617\n",
      "Epoch 2/10, Batch 338/883, Training Loss: 0.6422\n",
      "Epoch 2/10, Batch 339/883, Training Loss: 0.9036\n",
      "Epoch 2/10, Batch 340/883, Training Loss: 0.7771\n",
      "Epoch 2/10, Batch 341/883, Training Loss: 0.8169\n",
      "Epoch 2/10, Batch 342/883, Training Loss: 0.8724\n",
      "Epoch 2/10, Batch 343/883, Training Loss: 0.5419\n",
      "Epoch 2/10, Batch 344/883, Training Loss: 0.8285\n",
      "Epoch 2/10, Batch 345/883, Training Loss: 1.0493\n",
      "Epoch 2/10, Batch 346/883, Training Loss: 0.8542\n",
      "Epoch 2/10, Batch 347/883, Training Loss: 1.1176\n",
      "Epoch 2/10, Batch 348/883, Training Loss: 0.9170\n",
      "Epoch 2/10, Batch 349/883, Training Loss: 0.7660\n",
      "Epoch 2/10, Batch 350/883, Training Loss: 0.7966\n",
      "Epoch 2/10, Batch 351/883, Training Loss: 0.8072\n",
      "Epoch 2/10, Batch 352/883, Training Loss: 0.8181\n",
      "Epoch 2/10, Batch 353/883, Training Loss: 0.9622\n",
      "Epoch 2/10, Batch 354/883, Training Loss: 1.0796\n",
      "Epoch 2/10, Batch 355/883, Training Loss: 0.9641\n",
      "Epoch 2/10, Batch 356/883, Training Loss: 0.8728\n",
      "Epoch 2/10, Batch 357/883, Training Loss: 0.7167\n",
      "Epoch 2/10, Batch 358/883, Training Loss: 0.8588\n",
      "Epoch 2/10, Batch 359/883, Training Loss: 0.8970\n",
      "Epoch 2/10, Batch 360/883, Training Loss: 0.7888\n",
      "Epoch 2/10, Batch 361/883, Training Loss: 0.6450\n",
      "Epoch 2/10, Batch 362/883, Training Loss: 0.9379\n",
      "Epoch 2/10, Batch 363/883, Training Loss: 0.6696\n",
      "Epoch 2/10, Batch 364/883, Training Loss: 0.7086\n",
      "Epoch 2/10, Batch 365/883, Training Loss: 0.6711\n",
      "Epoch 2/10, Batch 366/883, Training Loss: 0.9893\n",
      "Epoch 2/10, Batch 367/883, Training Loss: 1.0368\n",
      "Epoch 2/10, Batch 368/883, Training Loss: 0.6947\n",
      "Epoch 2/10, Batch 369/883, Training Loss: 1.0959\n",
      "Epoch 2/10, Batch 370/883, Training Loss: 1.0300\n",
      "Epoch 2/10, Batch 371/883, Training Loss: 1.0254\n",
      "Epoch 2/10, Batch 372/883, Training Loss: 0.8576\n",
      "Epoch 2/10, Batch 373/883, Training Loss: 0.9053\n",
      "Epoch 2/10, Batch 374/883, Training Loss: 1.0001\n",
      "Epoch 2/10, Batch 375/883, Training Loss: 0.7243\n",
      "Epoch 2/10, Batch 376/883, Training Loss: 0.8415\n",
      "Epoch 2/10, Batch 377/883, Training Loss: 0.9432\n",
      "Epoch 2/10, Batch 378/883, Training Loss: 1.0327\n",
      "Epoch 2/10, Batch 379/883, Training Loss: 0.7907\n",
      "Epoch 2/10, Batch 380/883, Training Loss: 0.8612\n",
      "Epoch 2/10, Batch 381/883, Training Loss: 0.7745\n",
      "Epoch 2/10, Batch 382/883, Training Loss: 0.9327\n",
      "Epoch 2/10, Batch 383/883, Training Loss: 0.8815\n",
      "Epoch 2/10, Batch 384/883, Training Loss: 1.0651\n",
      "Epoch 2/10, Batch 385/883, Training Loss: 0.6666\n",
      "Epoch 2/10, Batch 386/883, Training Loss: 0.8421\n",
      "Epoch 2/10, Batch 387/883, Training Loss: 0.9306\n",
      "Epoch 2/10, Batch 388/883, Training Loss: 0.7220\n",
      "Epoch 2/10, Batch 389/883, Training Loss: 0.7719\n",
      "Epoch 2/10, Batch 390/883, Training Loss: 0.8822\n",
      "Epoch 2/10, Batch 391/883, Training Loss: 0.7286\n",
      "Epoch 2/10, Batch 392/883, Training Loss: 0.7653\n",
      "Epoch 2/10, Batch 393/883, Training Loss: 0.7652\n",
      "Epoch 2/10, Batch 394/883, Training Loss: 0.9942\n",
      "Epoch 2/10, Batch 395/883, Training Loss: 0.8637\n",
      "Epoch 2/10, Batch 396/883, Training Loss: 0.6555\n",
      "Epoch 2/10, Batch 397/883, Training Loss: 0.7126\n",
      "Epoch 2/10, Batch 398/883, Training Loss: 0.7474\n",
      "Epoch 2/10, Batch 399/883, Training Loss: 0.7128\n",
      "Epoch 2/10, Batch 400/883, Training Loss: 0.8127\n",
      "Epoch 2/10, Batch 401/883, Training Loss: 0.7462\n",
      "Epoch 2/10, Batch 402/883, Training Loss: 1.2243\n",
      "Epoch 2/10, Batch 403/883, Training Loss: 1.1520\n",
      "Epoch 2/10, Batch 404/883, Training Loss: 1.0365\n",
      "Epoch 2/10, Batch 405/883, Training Loss: 0.7290\n",
      "Epoch 2/10, Batch 406/883, Training Loss: 0.9259\n",
      "Epoch 2/10, Batch 407/883, Training Loss: 0.7501\n",
      "Epoch 2/10, Batch 408/883, Training Loss: 0.7352\n",
      "Epoch 2/10, Batch 409/883, Training Loss: 0.7344\n",
      "Epoch 2/10, Batch 410/883, Training Loss: 0.8721\n",
      "Epoch 2/10, Batch 411/883, Training Loss: 0.6805\n",
      "Epoch 2/10, Batch 412/883, Training Loss: 0.7788\n",
      "Epoch 2/10, Batch 413/883, Training Loss: 0.9095\n",
      "Epoch 2/10, Batch 414/883, Training Loss: 0.7099\n",
      "Epoch 2/10, Batch 415/883, Training Loss: 0.7593\n",
      "Epoch 2/10, Batch 416/883, Training Loss: 0.8435\n",
      "Epoch 2/10, Batch 417/883, Training Loss: 0.7963\n",
      "Epoch 2/10, Batch 418/883, Training Loss: 0.6342\n",
      "Epoch 2/10, Batch 419/883, Training Loss: 0.6945\n",
      "Epoch 2/10, Batch 420/883, Training Loss: 0.8650\n",
      "Epoch 2/10, Batch 421/883, Training Loss: 0.7368\n",
      "Epoch 2/10, Batch 422/883, Training Loss: 1.0874\n",
      "Epoch 2/10, Batch 423/883, Training Loss: 0.7979\n",
      "Epoch 2/10, Batch 424/883, Training Loss: 0.6861\n",
      "Epoch 2/10, Batch 425/883, Training Loss: 0.8968\n",
      "Epoch 2/10, Batch 426/883, Training Loss: 0.6547\n",
      "Epoch 2/10, Batch 427/883, Training Loss: 0.6547\n",
      "Epoch 2/10, Batch 428/883, Training Loss: 0.8732\n",
      "Epoch 2/10, Batch 429/883, Training Loss: 0.8568\n",
      "Epoch 2/10, Batch 430/883, Training Loss: 0.8570\n",
      "Epoch 2/10, Batch 431/883, Training Loss: 0.8354\n",
      "Epoch 2/10, Batch 432/883, Training Loss: 1.1310\n",
      "Epoch 2/10, Batch 433/883, Training Loss: 1.2537\n",
      "Epoch 2/10, Batch 434/883, Training Loss: 0.6826\n",
      "Epoch 2/10, Batch 435/883, Training Loss: 0.9584\n",
      "Epoch 2/10, Batch 436/883, Training Loss: 0.9531\n",
      "Epoch 2/10, Batch 437/883, Training Loss: 0.8828\n",
      "Epoch 2/10, Batch 438/883, Training Loss: 0.6529\n",
      "Epoch 2/10, Batch 439/883, Training Loss: 0.6262\n",
      "Epoch 2/10, Batch 440/883, Training Loss: 0.8094\n",
      "Epoch 2/10, Batch 441/883, Training Loss: 0.6626\n",
      "Epoch 2/10, Batch 442/883, Training Loss: 0.7037\n",
      "Epoch 2/10, Batch 443/883, Training Loss: 0.6406\n",
      "Epoch 2/10, Batch 444/883, Training Loss: 0.6394\n",
      "Epoch 2/10, Batch 445/883, Training Loss: 0.9814\n",
      "Epoch 2/10, Batch 446/883, Training Loss: 0.6581\n",
      "Epoch 2/10, Batch 447/883, Training Loss: 0.6770\n",
      "Epoch 2/10, Batch 448/883, Training Loss: 0.8064\n",
      "Epoch 2/10, Batch 449/883, Training Loss: 0.8624\n",
      "Epoch 2/10, Batch 450/883, Training Loss: 0.6789\n",
      "Epoch 2/10, Batch 451/883, Training Loss: 0.9984\n",
      "Epoch 2/10, Batch 452/883, Training Loss: 1.0313\n",
      "Epoch 2/10, Batch 453/883, Training Loss: 0.6392\n",
      "Epoch 2/10, Batch 454/883, Training Loss: 0.6835\n",
      "Epoch 2/10, Batch 455/883, Training Loss: 0.9183\n",
      "Epoch 2/10, Batch 456/883, Training Loss: 0.7933\n",
      "Epoch 2/10, Batch 457/883, Training Loss: 0.8230\n",
      "Epoch 2/10, Batch 458/883, Training Loss: 1.1058\n",
      "Epoch 2/10, Batch 459/883, Training Loss: 0.9411\n",
      "Epoch 2/10, Batch 460/883, Training Loss: 0.8451\n",
      "Epoch 2/10, Batch 461/883, Training Loss: 1.0598\n",
      "Epoch 2/10, Batch 462/883, Training Loss: 0.9590\n",
      "Epoch 2/10, Batch 463/883, Training Loss: 0.8223\n",
      "Epoch 2/10, Batch 464/883, Training Loss: 0.9699\n",
      "Epoch 2/10, Batch 465/883, Training Loss: 0.7963\n",
      "Epoch 2/10, Batch 466/883, Training Loss: 1.0546\n",
      "Epoch 2/10, Batch 467/883, Training Loss: 0.8386\n",
      "Epoch 2/10, Batch 468/883, Training Loss: 0.9743\n",
      "Epoch 2/10, Batch 469/883, Training Loss: 0.8331\n",
      "Epoch 2/10, Batch 470/883, Training Loss: 0.8517\n",
      "Epoch 2/10, Batch 471/883, Training Loss: 1.0370\n",
      "Epoch 2/10, Batch 472/883, Training Loss: 0.8811\n",
      "Epoch 2/10, Batch 473/883, Training Loss: 0.9285\n",
      "Epoch 2/10, Batch 474/883, Training Loss: 0.7920\n",
      "Epoch 2/10, Batch 475/883, Training Loss: 0.8557\n",
      "Epoch 2/10, Batch 476/883, Training Loss: 0.8649\n",
      "Epoch 2/10, Batch 477/883, Training Loss: 0.7962\n",
      "Epoch 2/10, Batch 478/883, Training Loss: 0.7317\n",
      "Epoch 2/10, Batch 479/883, Training Loss: 1.0197\n",
      "Epoch 2/10, Batch 480/883, Training Loss: 0.9617\n",
      "Epoch 2/10, Batch 481/883, Training Loss: 0.6664\n",
      "Epoch 2/10, Batch 482/883, Training Loss: 0.7347\n",
      "Epoch 2/10, Batch 483/883, Training Loss: 0.6111\n",
      "Epoch 2/10, Batch 484/883, Training Loss: 1.1719\n",
      "Epoch 2/10, Batch 485/883, Training Loss: 0.6805\n",
      "Epoch 2/10, Batch 486/883, Training Loss: 0.8721\n",
      "Epoch 2/10, Batch 487/883, Training Loss: 0.7404\n",
      "Epoch 2/10, Batch 488/883, Training Loss: 0.7152\n",
      "Epoch 2/10, Batch 489/883, Training Loss: 0.8410\n",
      "Epoch 2/10, Batch 490/883, Training Loss: 0.6492\n",
      "Epoch 2/10, Batch 491/883, Training Loss: 0.9287\n",
      "Epoch 2/10, Batch 492/883, Training Loss: 1.1807\n",
      "Epoch 2/10, Batch 493/883, Training Loss: 0.8966\n",
      "Epoch 2/10, Batch 494/883, Training Loss: 0.7205\n",
      "Epoch 2/10, Batch 495/883, Training Loss: 0.5552\n",
      "Epoch 2/10, Batch 496/883, Training Loss: 0.7547\n",
      "Epoch 2/10, Batch 497/883, Training Loss: 0.7753\n",
      "Epoch 2/10, Batch 498/883, Training Loss: 0.8668\n",
      "Epoch 2/10, Batch 499/883, Training Loss: 0.5535\n",
      "Epoch 2/10, Batch 500/883, Training Loss: 1.0440\n",
      "Epoch 2/10, Batch 501/883, Training Loss: 0.7225\n",
      "Epoch 2/10, Batch 502/883, Training Loss: 0.9192\n",
      "Epoch 2/10, Batch 503/883, Training Loss: 0.8497\n",
      "Epoch 2/10, Batch 504/883, Training Loss: 0.7121\n",
      "Epoch 2/10, Batch 505/883, Training Loss: 0.5415\n",
      "Epoch 2/10, Batch 506/883, Training Loss: 1.0253\n",
      "Epoch 2/10, Batch 507/883, Training Loss: 1.0467\n",
      "Epoch 2/10, Batch 508/883, Training Loss: 1.3409\n",
      "Epoch 2/10, Batch 509/883, Training Loss: 0.7027\n",
      "Epoch 2/10, Batch 510/883, Training Loss: 0.9839\n",
      "Epoch 2/10, Batch 511/883, Training Loss: 0.7246\n",
      "Epoch 2/10, Batch 512/883, Training Loss: 0.8168\n",
      "Epoch 2/10, Batch 513/883, Training Loss: 0.8157\n",
      "Epoch 2/10, Batch 514/883, Training Loss: 1.0255\n",
      "Epoch 2/10, Batch 515/883, Training Loss: 0.9826\n",
      "Epoch 2/10, Batch 516/883, Training Loss: 0.7119\n",
      "Epoch 2/10, Batch 517/883, Training Loss: 0.7801\n",
      "Epoch 2/10, Batch 518/883, Training Loss: 0.5164\n",
      "Epoch 2/10, Batch 519/883, Training Loss: 0.8813\n",
      "Epoch 2/10, Batch 520/883, Training Loss: 1.0842\n",
      "Epoch 2/10, Batch 521/883, Training Loss: 0.7279\n",
      "Epoch 2/10, Batch 522/883, Training Loss: 0.9899\n",
      "Epoch 2/10, Batch 523/883, Training Loss: 0.7363\n",
      "Epoch 2/10, Batch 524/883, Training Loss: 0.6470\n",
      "Epoch 2/10, Batch 525/883, Training Loss: 1.1189\n",
      "Epoch 2/10, Batch 526/883, Training Loss: 0.5841\n",
      "Epoch 2/10, Batch 527/883, Training Loss: 0.7270\n",
      "Epoch 2/10, Batch 528/883, Training Loss: 1.2961\n",
      "Epoch 2/10, Batch 529/883, Training Loss: 1.1752\n",
      "Epoch 2/10, Batch 530/883, Training Loss: 0.7556\n",
      "Epoch 2/10, Batch 531/883, Training Loss: 0.7892\n",
      "Epoch 2/10, Batch 532/883, Training Loss: 0.9757\n",
      "Epoch 2/10, Batch 533/883, Training Loss: 0.7554\n",
      "Epoch 2/10, Batch 534/883, Training Loss: 0.8263\n",
      "Epoch 2/10, Batch 535/883, Training Loss: 1.0597\n",
      "Epoch 2/10, Batch 536/883, Training Loss: 0.9377\n",
      "Epoch 2/10, Batch 537/883, Training Loss: 0.8090\n",
      "Epoch 2/10, Batch 538/883, Training Loss: 0.8646\n",
      "Epoch 2/10, Batch 539/883, Training Loss: 0.6861\n",
      "Epoch 2/10, Batch 540/883, Training Loss: 1.0245\n",
      "Epoch 2/10, Batch 541/883, Training Loss: 0.8407\n",
      "Epoch 2/10, Batch 542/883, Training Loss: 0.6873\n",
      "Epoch 2/10, Batch 543/883, Training Loss: 0.7494\n",
      "Epoch 2/10, Batch 544/883, Training Loss: 0.6587\n",
      "Epoch 2/10, Batch 545/883, Training Loss: 0.8996\n",
      "Epoch 2/10, Batch 546/883, Training Loss: 1.1130\n",
      "Epoch 2/10, Batch 547/883, Training Loss: 0.6530\n",
      "Epoch 2/10, Batch 548/883, Training Loss: 0.7661\n",
      "Epoch 2/10, Batch 549/883, Training Loss: 0.5868\n",
      "Epoch 2/10, Batch 550/883, Training Loss: 0.4999\n",
      "Epoch 2/10, Batch 551/883, Training Loss: 0.7802\n",
      "Epoch 2/10, Batch 552/883, Training Loss: 0.9803\n",
      "Epoch 2/10, Batch 553/883, Training Loss: 1.1527\n",
      "Epoch 2/10, Batch 554/883, Training Loss: 0.7137\n",
      "Epoch 2/10, Batch 555/883, Training Loss: 0.7635\n",
      "Epoch 2/10, Batch 556/883, Training Loss: 0.7027\n",
      "Epoch 2/10, Batch 557/883, Training Loss: 0.8532\n",
      "Epoch 2/10, Batch 558/883, Training Loss: 0.7181\n",
      "Epoch 2/10, Batch 559/883, Training Loss: 0.8889\n",
      "Epoch 2/10, Batch 560/883, Training Loss: 0.7110\n",
      "Epoch 2/10, Batch 561/883, Training Loss: 0.9345\n",
      "Epoch 2/10, Batch 562/883, Training Loss: 0.8067\n",
      "Epoch 2/10, Batch 563/883, Training Loss: 1.1398\n",
      "Epoch 2/10, Batch 564/883, Training Loss: 0.7229\n",
      "Epoch 2/10, Batch 565/883, Training Loss: 0.7596\n",
      "Epoch 2/10, Batch 566/883, Training Loss: 0.8592\n",
      "Epoch 2/10, Batch 567/883, Training Loss: 0.7461\n",
      "Epoch 2/10, Batch 568/883, Training Loss: 0.7179\n",
      "Epoch 2/10, Batch 569/883, Training Loss: 0.8529\n",
      "Epoch 2/10, Batch 570/883, Training Loss: 0.8652\n",
      "Epoch 2/10, Batch 571/883, Training Loss: 0.8734\n",
      "Epoch 2/10, Batch 572/883, Training Loss: 0.6160\n",
      "Epoch 2/10, Batch 573/883, Training Loss: 0.6353\n",
      "Epoch 2/10, Batch 574/883, Training Loss: 0.7240\n",
      "Epoch 2/10, Batch 575/883, Training Loss: 0.6265\n",
      "Epoch 2/10, Batch 576/883, Training Loss: 0.9833\n",
      "Epoch 2/10, Batch 577/883, Training Loss: 0.8225\n",
      "Epoch 2/10, Batch 578/883, Training Loss: 1.1469\n",
      "Epoch 2/10, Batch 579/883, Training Loss: 0.5056\n",
      "Epoch 2/10, Batch 580/883, Training Loss: 0.9867\n",
      "Epoch 2/10, Batch 581/883, Training Loss: 0.8261\n",
      "Epoch 2/10, Batch 582/883, Training Loss: 0.5699\n",
      "Epoch 2/10, Batch 583/883, Training Loss: 1.1355\n",
      "Epoch 2/10, Batch 584/883, Training Loss: 1.2138\n",
      "Epoch 2/10, Batch 585/883, Training Loss: 0.9828\n",
      "Epoch 2/10, Batch 586/883, Training Loss: 0.6770\n",
      "Epoch 2/10, Batch 587/883, Training Loss: 0.8643\n",
      "Epoch 2/10, Batch 588/883, Training Loss: 0.8268\n",
      "Epoch 2/10, Batch 589/883, Training Loss: 0.8530\n",
      "Epoch 2/10, Batch 590/883, Training Loss: 0.7919\n",
      "Epoch 2/10, Batch 591/883, Training Loss: 1.1556\n",
      "Epoch 2/10, Batch 592/883, Training Loss: 0.7906\n",
      "Epoch 2/10, Batch 593/883, Training Loss: 0.8765\n",
      "Epoch 2/10, Batch 594/883, Training Loss: 0.9463\n",
      "Epoch 2/10, Batch 595/883, Training Loss: 0.7528\n",
      "Epoch 2/10, Batch 596/883, Training Loss: 0.8427\n",
      "Epoch 2/10, Batch 597/883, Training Loss: 0.8833\n",
      "Epoch 2/10, Batch 598/883, Training Loss: 0.7873\n",
      "Epoch 2/10, Batch 599/883, Training Loss: 0.8105\n",
      "Epoch 2/10, Batch 600/883, Training Loss: 0.9310\n",
      "Epoch 2/10, Batch 601/883, Training Loss: 0.8936\n",
      "Epoch 2/10, Batch 602/883, Training Loss: 0.8520\n",
      "Epoch 2/10, Batch 603/883, Training Loss: 0.8621\n",
      "Epoch 2/10, Batch 604/883, Training Loss: 0.7880\n",
      "Epoch 2/10, Batch 605/883, Training Loss: 0.8676\n",
      "Epoch 2/10, Batch 606/883, Training Loss: 0.9796\n",
      "Epoch 2/10, Batch 607/883, Training Loss: 0.7074\n",
      "Epoch 2/10, Batch 608/883, Training Loss: 0.5709\n",
      "Epoch 2/10, Batch 609/883, Training Loss: 0.8850\n",
      "Epoch 2/10, Batch 610/883, Training Loss: 0.8958\n",
      "Epoch 2/10, Batch 611/883, Training Loss: 1.0022\n",
      "Epoch 2/10, Batch 612/883, Training Loss: 0.9190\n",
      "Epoch 2/10, Batch 613/883, Training Loss: 0.6391\n",
      "Epoch 2/10, Batch 614/883, Training Loss: 0.7811\n",
      "Epoch 2/10, Batch 615/883, Training Loss: 0.9643\n",
      "Epoch 2/10, Batch 616/883, Training Loss: 0.9004\n",
      "Epoch 2/10, Batch 617/883, Training Loss: 0.8575\n",
      "Epoch 2/10, Batch 618/883, Training Loss: 1.0387\n",
      "Epoch 2/10, Batch 619/883, Training Loss: 0.7281\n",
      "Epoch 2/10, Batch 620/883, Training Loss: 0.8103\n",
      "Epoch 2/10, Batch 621/883, Training Loss: 0.6898\n",
      "Epoch 2/10, Batch 622/883, Training Loss: 0.7777\n",
      "Epoch 2/10, Batch 623/883, Training Loss: 1.1648\n",
      "Epoch 2/10, Batch 624/883, Training Loss: 0.8116\n",
      "Epoch 2/10, Batch 625/883, Training Loss: 0.8497\n",
      "Epoch 2/10, Batch 626/883, Training Loss: 1.0113\n",
      "Epoch 2/10, Batch 627/883, Training Loss: 0.4255\n",
      "Epoch 2/10, Batch 628/883, Training Loss: 0.7695\n",
      "Epoch 2/10, Batch 629/883, Training Loss: 0.5171\n",
      "Epoch 2/10, Batch 630/883, Training Loss: 0.6202\n",
      "Epoch 2/10, Batch 631/883, Training Loss: 0.7805\n",
      "Epoch 2/10, Batch 632/883, Training Loss: 0.8753\n",
      "Epoch 2/10, Batch 633/883, Training Loss: 0.9672\n",
      "Epoch 2/10, Batch 634/883, Training Loss: 0.8924\n",
      "Epoch 2/10, Batch 635/883, Training Loss: 0.6415\n",
      "Epoch 2/10, Batch 636/883, Training Loss: 0.9242\n",
      "Epoch 2/10, Batch 637/883, Training Loss: 0.6813\n",
      "Epoch 2/10, Batch 638/883, Training Loss: 0.7454\n",
      "Epoch 2/10, Batch 639/883, Training Loss: 0.6096\n",
      "Epoch 2/10, Batch 640/883, Training Loss: 0.9457\n",
      "Epoch 2/10, Batch 641/883, Training Loss: 0.7411\n",
      "Epoch 2/10, Batch 642/883, Training Loss: 0.7509\n",
      "Epoch 2/10, Batch 643/883, Training Loss: 0.9320\n",
      "Epoch 2/10, Batch 644/883, Training Loss: 0.8300\n",
      "Epoch 2/10, Batch 645/883, Training Loss: 0.9930\n",
      "Epoch 2/10, Batch 646/883, Training Loss: 0.6824\n",
      "Epoch 2/10, Batch 647/883, Training Loss: 0.9556\n",
      "Epoch 2/10, Batch 648/883, Training Loss: 1.0583\n",
      "Epoch 2/10, Batch 649/883, Training Loss: 0.8142\n",
      "Epoch 2/10, Batch 650/883, Training Loss: 0.7184\n",
      "Epoch 2/10, Batch 651/883, Training Loss: 0.9981\n",
      "Epoch 2/10, Batch 652/883, Training Loss: 0.7271\n",
      "Epoch 2/10, Batch 653/883, Training Loss: 0.6354\n",
      "Epoch 2/10, Batch 654/883, Training Loss: 1.3615\n",
      "Epoch 2/10, Batch 655/883, Training Loss: 0.7367\n",
      "Epoch 2/10, Batch 656/883, Training Loss: 0.8141\n",
      "Epoch 2/10, Batch 657/883, Training Loss: 1.1086\n",
      "Epoch 2/10, Batch 658/883, Training Loss: 0.6938\n",
      "Epoch 2/10, Batch 659/883, Training Loss: 0.6148\n",
      "Epoch 2/10, Batch 660/883, Training Loss: 0.7425\n",
      "Epoch 2/10, Batch 661/883, Training Loss: 0.7234\n",
      "Epoch 2/10, Batch 662/883, Training Loss: 0.6378\n",
      "Epoch 2/10, Batch 663/883, Training Loss: 0.7985\n",
      "Epoch 2/10, Batch 664/883, Training Loss: 0.8523\n",
      "Epoch 2/10, Batch 665/883, Training Loss: 0.8917\n",
      "Epoch 2/10, Batch 666/883, Training Loss: 0.6959\n",
      "Epoch 2/10, Batch 667/883, Training Loss: 0.8124\n",
      "Epoch 2/10, Batch 668/883, Training Loss: 0.7961\n",
      "Epoch 2/10, Batch 669/883, Training Loss: 0.7008\n",
      "Epoch 2/10, Batch 670/883, Training Loss: 0.9138\n",
      "Epoch 2/10, Batch 671/883, Training Loss: 0.7013\n",
      "Epoch 2/10, Batch 672/883, Training Loss: 0.7425\n",
      "Epoch 2/10, Batch 673/883, Training Loss: 0.5860\n",
      "Epoch 2/10, Batch 674/883, Training Loss: 0.7822\n",
      "Epoch 2/10, Batch 675/883, Training Loss: 0.5573\n",
      "Epoch 2/10, Batch 676/883, Training Loss: 0.6678\n",
      "Epoch 2/10, Batch 677/883, Training Loss: 0.9959\n",
      "Epoch 2/10, Batch 678/883, Training Loss: 1.0552\n",
      "Epoch 2/10, Batch 679/883, Training Loss: 0.9050\n",
      "Epoch 2/10, Batch 680/883, Training Loss: 0.6941\n",
      "Epoch 2/10, Batch 681/883, Training Loss: 0.9725\n",
      "Epoch 2/10, Batch 682/883, Training Loss: 0.6128\n",
      "Epoch 2/10, Batch 683/883, Training Loss: 0.7364\n",
      "Epoch 2/10, Batch 684/883, Training Loss: 0.6324\n",
      "Epoch 2/10, Batch 685/883, Training Loss: 0.6911\n",
      "Epoch 2/10, Batch 686/883, Training Loss: 0.6547\n",
      "Epoch 2/10, Batch 687/883, Training Loss: 0.7267\n",
      "Epoch 2/10, Batch 688/883, Training Loss: 0.8117\n",
      "Epoch 2/10, Batch 689/883, Training Loss: 0.6064\n",
      "Epoch 2/10, Batch 690/883, Training Loss: 0.7355\n",
      "Epoch 2/10, Batch 691/883, Training Loss: 0.7618\n",
      "Epoch 2/10, Batch 692/883, Training Loss: 0.7338\n",
      "Epoch 2/10, Batch 693/883, Training Loss: 0.8943\n",
      "Epoch 2/10, Batch 694/883, Training Loss: 0.5514\n",
      "Epoch 2/10, Batch 695/883, Training Loss: 0.7974\n",
      "Epoch 2/10, Batch 696/883, Training Loss: 0.7066\n",
      "Epoch 2/10, Batch 697/883, Training Loss: 0.6955\n",
      "Epoch 2/10, Batch 698/883, Training Loss: 1.4959\n",
      "Epoch 2/10, Batch 699/883, Training Loss: 0.6718\n",
      "Epoch 2/10, Batch 700/883, Training Loss: 0.6903\n",
      "Epoch 2/10, Batch 701/883, Training Loss: 0.9519\n",
      "Epoch 2/10, Batch 702/883, Training Loss: 0.8090\n",
      "Epoch 2/10, Batch 703/883, Training Loss: 0.7796\n",
      "Epoch 2/10, Batch 704/883, Training Loss: 0.6575\n",
      "Epoch 2/10, Batch 705/883, Training Loss: 0.6508\n",
      "Epoch 2/10, Batch 706/883, Training Loss: 0.9765\n",
      "Epoch 2/10, Batch 707/883, Training Loss: 0.8430\n",
      "Epoch 2/10, Batch 708/883, Training Loss: 0.7534\n",
      "Epoch 2/10, Batch 709/883, Training Loss: 0.7763\n",
      "Epoch 2/10, Batch 710/883, Training Loss: 0.7079\n",
      "Epoch 2/10, Batch 711/883, Training Loss: 0.8386\n",
      "Epoch 2/10, Batch 712/883, Training Loss: 0.9956\n",
      "Epoch 2/10, Batch 713/883, Training Loss: 0.6942\n",
      "Epoch 2/10, Batch 714/883, Training Loss: 0.9134\n",
      "Epoch 2/10, Batch 715/883, Training Loss: 1.1382\n",
      "Epoch 2/10, Batch 716/883, Training Loss: 0.9952\n",
      "Epoch 2/10, Batch 717/883, Training Loss: 0.8353\n",
      "Epoch 2/10, Batch 718/883, Training Loss: 0.7516\n",
      "Epoch 2/10, Batch 719/883, Training Loss: 0.9609\n",
      "Epoch 2/10, Batch 720/883, Training Loss: 0.7983\n",
      "Epoch 2/10, Batch 721/883, Training Loss: 1.0448\n",
      "Epoch 2/10, Batch 722/883, Training Loss: 1.3459\n",
      "Epoch 2/10, Batch 723/883, Training Loss: 0.7765\n",
      "Epoch 2/10, Batch 724/883, Training Loss: 0.8542\n",
      "Epoch 2/10, Batch 725/883, Training Loss: 1.0321\n",
      "Epoch 2/10, Batch 726/883, Training Loss: 0.8810\n",
      "Epoch 2/10, Batch 727/883, Training Loss: 0.8150\n",
      "Epoch 2/10, Batch 728/883, Training Loss: 0.9962\n",
      "Epoch 2/10, Batch 729/883, Training Loss: 0.8991\n",
      "Epoch 2/10, Batch 730/883, Training Loss: 0.8274\n",
      "Epoch 2/10, Batch 731/883, Training Loss: 0.8848\n",
      "Epoch 2/10, Batch 732/883, Training Loss: 0.7198\n",
      "Epoch 2/10, Batch 733/883, Training Loss: 0.7671\n",
      "Epoch 2/10, Batch 734/883, Training Loss: 0.8359\n",
      "Epoch 2/10, Batch 735/883, Training Loss: 0.7545\n",
      "Epoch 2/10, Batch 736/883, Training Loss: 0.7841\n",
      "Epoch 2/10, Batch 737/883, Training Loss: 0.9463\n",
      "Epoch 2/10, Batch 738/883, Training Loss: 0.8199\n",
      "Epoch 2/10, Batch 739/883, Training Loss: 0.7203\n",
      "Epoch 2/10, Batch 740/883, Training Loss: 0.7494\n",
      "Epoch 2/10, Batch 741/883, Training Loss: 0.7389\n",
      "Epoch 2/10, Batch 742/883, Training Loss: 0.6891\n",
      "Epoch 2/10, Batch 743/883, Training Loss: 0.6811\n",
      "Epoch 2/10, Batch 744/883, Training Loss: 0.6972\n",
      "Epoch 2/10, Batch 745/883, Training Loss: 0.8311\n",
      "Epoch 2/10, Batch 746/883, Training Loss: 0.7706\n",
      "Epoch 2/10, Batch 747/883, Training Loss: 0.8062\n",
      "Epoch 2/10, Batch 748/883, Training Loss: 0.6232\n",
      "Epoch 2/10, Batch 749/883, Training Loss: 1.1299\n",
      "Epoch 2/10, Batch 750/883, Training Loss: 0.7689\n",
      "Epoch 2/10, Batch 751/883, Training Loss: 1.1939\n",
      "Epoch 2/10, Batch 752/883, Training Loss: 0.6996\n",
      "Epoch 2/10, Batch 753/883, Training Loss: 0.7961\n",
      "Epoch 2/10, Batch 754/883, Training Loss: 0.9395\n",
      "Epoch 2/10, Batch 755/883, Training Loss: 0.7763\n",
      "Epoch 2/10, Batch 756/883, Training Loss: 0.7823\n",
      "Epoch 2/10, Batch 757/883, Training Loss: 1.1060\n",
      "Epoch 2/10, Batch 758/883, Training Loss: 0.6916\n",
      "Epoch 2/10, Batch 759/883, Training Loss: 1.6227\n",
      "Epoch 2/10, Batch 760/883, Training Loss: 0.7002\n",
      "Epoch 2/10, Batch 761/883, Training Loss: 0.9217\n",
      "Epoch 2/10, Batch 762/883, Training Loss: 1.0404\n",
      "Epoch 2/10, Batch 763/883, Training Loss: 0.7234\n",
      "Epoch 2/10, Batch 764/883, Training Loss: 0.9079\n",
      "Epoch 2/10, Batch 765/883, Training Loss: 0.9325\n",
      "Epoch 2/10, Batch 766/883, Training Loss: 0.7835\n",
      "Epoch 2/10, Batch 767/883, Training Loss: 0.7124\n",
      "Epoch 2/10, Batch 768/883, Training Loss: 0.7549\n",
      "Epoch 2/10, Batch 769/883, Training Loss: 0.7573\n",
      "Epoch 2/10, Batch 770/883, Training Loss: 0.8316\n",
      "Epoch 2/10, Batch 771/883, Training Loss: 0.7518\n",
      "Epoch 2/10, Batch 772/883, Training Loss: 0.9057\n",
      "Epoch 2/10, Batch 773/883, Training Loss: 0.8378\n",
      "Epoch 2/10, Batch 774/883, Training Loss: 0.5861\n",
      "Epoch 2/10, Batch 775/883, Training Loss: 0.5124\n",
      "Epoch 2/10, Batch 776/883, Training Loss: 1.0281\n",
      "Epoch 2/10, Batch 777/883, Training Loss: 0.5466\n",
      "Epoch 2/10, Batch 778/883, Training Loss: 0.9714\n",
      "Epoch 2/10, Batch 779/883, Training Loss: 0.7893\n",
      "Epoch 2/10, Batch 780/883, Training Loss: 0.7505\n",
      "Epoch 2/10, Batch 781/883, Training Loss: 0.6427\n",
      "Epoch 2/10, Batch 782/883, Training Loss: 0.8463\n",
      "Epoch 2/10, Batch 783/883, Training Loss: 0.8825\n",
      "Epoch 2/10, Batch 784/883, Training Loss: 0.7228\n",
      "Epoch 2/10, Batch 785/883, Training Loss: 0.9986\n",
      "Epoch 2/10, Batch 786/883, Training Loss: 0.6942\n",
      "Epoch 2/10, Batch 787/883, Training Loss: 0.7244\n",
      "Epoch 2/10, Batch 788/883, Training Loss: 0.7691\n",
      "Epoch 2/10, Batch 789/883, Training Loss: 0.7320\n",
      "Epoch 2/10, Batch 790/883, Training Loss: 1.0683\n",
      "Epoch 2/10, Batch 791/883, Training Loss: 0.7271\n",
      "Epoch 2/10, Batch 792/883, Training Loss: 0.8903\n",
      "Epoch 2/10, Batch 793/883, Training Loss: 0.9971\n",
      "Epoch 2/10, Batch 794/883, Training Loss: 0.8175\n",
      "Epoch 2/10, Batch 795/883, Training Loss: 0.7360\n",
      "Epoch 2/10, Batch 796/883, Training Loss: 0.8966\n",
      "Epoch 2/10, Batch 797/883, Training Loss: 0.8877\n",
      "Epoch 2/10, Batch 798/883, Training Loss: 0.8746\n",
      "Epoch 2/10, Batch 799/883, Training Loss: 0.8617\n",
      "Epoch 2/10, Batch 800/883, Training Loss: 0.8800\n",
      "Epoch 2/10, Batch 801/883, Training Loss: 0.8036\n",
      "Epoch 2/10, Batch 802/883, Training Loss: 0.9008\n",
      "Epoch 2/10, Batch 803/883, Training Loss: 0.7445\n",
      "Epoch 2/10, Batch 804/883, Training Loss: 1.0326\n",
      "Epoch 2/10, Batch 805/883, Training Loss: 0.7078\n",
      "Epoch 2/10, Batch 806/883, Training Loss: 0.7736\n",
      "Epoch 2/10, Batch 807/883, Training Loss: 0.8160\n",
      "Epoch 2/10, Batch 808/883, Training Loss: 0.7041\n",
      "Epoch 2/10, Batch 809/883, Training Loss: 0.6191\n",
      "Epoch 2/10, Batch 810/883, Training Loss: 0.8062\n",
      "Epoch 2/10, Batch 811/883, Training Loss: 0.6892\n",
      "Epoch 2/10, Batch 812/883, Training Loss: 0.8987\n",
      "Epoch 2/10, Batch 813/883, Training Loss: 0.7005\n",
      "Epoch 2/10, Batch 814/883, Training Loss: 1.0147\n",
      "Epoch 2/10, Batch 815/883, Training Loss: 0.8232\n",
      "Epoch 2/10, Batch 816/883, Training Loss: 0.8578\n",
      "Epoch 2/10, Batch 817/883, Training Loss: 0.9987\n",
      "Epoch 2/10, Batch 818/883, Training Loss: 0.6565\n",
      "Epoch 2/10, Batch 819/883, Training Loss: 0.7599\n",
      "Epoch 2/10, Batch 820/883, Training Loss: 0.9217\n",
      "Epoch 2/10, Batch 821/883, Training Loss: 0.9009\n",
      "Epoch 2/10, Batch 822/883, Training Loss: 0.6332\n",
      "Epoch 2/10, Batch 823/883, Training Loss: 0.7585\n",
      "Epoch 2/10, Batch 824/883, Training Loss: 0.8525\n",
      "Epoch 2/10, Batch 825/883, Training Loss: 0.8069\n",
      "Epoch 2/10, Batch 826/883, Training Loss: 0.7870\n",
      "Epoch 2/10, Batch 827/883, Training Loss: 0.8785\n",
      "Epoch 2/10, Batch 828/883, Training Loss: 0.8906\n",
      "Epoch 2/10, Batch 829/883, Training Loss: 0.7138\n",
      "Epoch 2/10, Batch 830/883, Training Loss: 0.6060\n",
      "Epoch 2/10, Batch 831/883, Training Loss: 0.5600\n",
      "Epoch 2/10, Batch 832/883, Training Loss: 0.8473\n",
      "Epoch 2/10, Batch 833/883, Training Loss: 0.7807\n",
      "Epoch 2/10, Batch 834/883, Training Loss: 0.9387\n",
      "Epoch 2/10, Batch 835/883, Training Loss: 0.7554\n",
      "Epoch 2/10, Batch 836/883, Training Loss: 0.7897\n",
      "Epoch 2/10, Batch 837/883, Training Loss: 0.9708\n",
      "Epoch 2/10, Batch 838/883, Training Loss: 0.9308\n",
      "Epoch 2/10, Batch 839/883, Training Loss: 0.8328\n",
      "Epoch 2/10, Batch 840/883, Training Loss: 0.9819\n",
      "Epoch 2/10, Batch 841/883, Training Loss: 0.5487\n",
      "Epoch 2/10, Batch 842/883, Training Loss: 0.7764\n",
      "Epoch 2/10, Batch 843/883, Training Loss: 0.6213\n",
      "Epoch 2/10, Batch 844/883, Training Loss: 0.8864\n",
      "Epoch 2/10, Batch 845/883, Training Loss: 0.8931\n",
      "Epoch 2/10, Batch 846/883, Training Loss: 0.6199\n",
      "Epoch 2/10, Batch 847/883, Training Loss: 0.5639\n",
      "Epoch 2/10, Batch 848/883, Training Loss: 0.7666\n",
      "Epoch 2/10, Batch 849/883, Training Loss: 0.6367\n",
      "Epoch 2/10, Batch 850/883, Training Loss: 0.7658\n",
      "Epoch 2/10, Batch 851/883, Training Loss: 0.5416\n",
      "Epoch 2/10, Batch 852/883, Training Loss: 0.8882\n",
      "Epoch 2/10, Batch 853/883, Training Loss: 0.6367\n",
      "Epoch 2/10, Batch 854/883, Training Loss: 0.7934\n",
      "Epoch 2/10, Batch 855/883, Training Loss: 0.5011\n",
      "Epoch 2/10, Batch 856/883, Training Loss: 0.9485\n",
      "Epoch 2/10, Batch 857/883, Training Loss: 1.0016\n",
      "Epoch 2/10, Batch 858/883, Training Loss: 0.7241\n",
      "Epoch 2/10, Batch 859/883, Training Loss: 0.9063\n",
      "Epoch 2/10, Batch 860/883, Training Loss: 0.7672\n",
      "Epoch 2/10, Batch 861/883, Training Loss: 0.7985\n",
      "Epoch 2/10, Batch 862/883, Training Loss: 1.0301\n",
      "Epoch 2/10, Batch 863/883, Training Loss: 0.9265\n",
      "Epoch 2/10, Batch 864/883, Training Loss: 0.6829\n",
      "Epoch 2/10, Batch 865/883, Training Loss: 0.8755\n",
      "Epoch 2/10, Batch 866/883, Training Loss: 0.7420\n",
      "Epoch 2/10, Batch 867/883, Training Loss: 0.7801\n",
      "Epoch 2/10, Batch 868/883, Training Loss: 0.8269\n",
      "Epoch 2/10, Batch 869/883, Training Loss: 0.7714\n",
      "Epoch 2/10, Batch 870/883, Training Loss: 0.7592\n",
      "Epoch 2/10, Batch 871/883, Training Loss: 0.6941\n",
      "Epoch 2/10, Batch 872/883, Training Loss: 0.6284\n",
      "Epoch 2/10, Batch 873/883, Training Loss: 0.8239\n",
      "Epoch 2/10, Batch 874/883, Training Loss: 0.7884\n",
      "Epoch 2/10, Batch 875/883, Training Loss: 1.0129\n",
      "Epoch 2/10, Batch 876/883, Training Loss: 0.8723\n",
      "Epoch 2/10, Batch 877/883, Training Loss: 0.6715\n",
      "Epoch 2/10, Batch 878/883, Training Loss: 0.8523\n",
      "Epoch 2/10, Batch 879/883, Training Loss: 0.6493\n",
      "Epoch 2/10, Batch 880/883, Training Loss: 0.8843\n",
      "Epoch 2/10, Batch 881/883, Training Loss: 0.8319\n",
      "Epoch 2/10, Batch 882/883, Training Loss: 0.8460\n",
      "Epoch 2/10, Batch 883/883, Training Loss: 0.8431\n",
      "Epoch 2/10, Training Loss: 0.8387, Validation Loss: 0.8155, Validation Accuracy: 0.5905\n",
      "Epoch 3/10, Batch 1/883, Training Loss: 1.0648\n",
      "Epoch 3/10, Batch 2/883, Training Loss: 0.6600\n",
      "Epoch 3/10, Batch 3/883, Training Loss: 0.6179\n",
      "Epoch 3/10, Batch 4/883, Training Loss: 0.6575\n",
      "Epoch 3/10, Batch 5/883, Training Loss: 0.8612\n",
      "Epoch 3/10, Batch 6/883, Training Loss: 0.8370\n",
      "Epoch 3/10, Batch 7/883, Training Loss: 0.8230\n",
      "Epoch 3/10, Batch 8/883, Training Loss: 0.5816\n",
      "Epoch 3/10, Batch 9/883, Training Loss: 0.6097\n",
      "Epoch 3/10, Batch 10/883, Training Loss: 1.0458\n",
      "Epoch 3/10, Batch 11/883, Training Loss: 0.8031\n",
      "Epoch 3/10, Batch 12/883, Training Loss: 0.8462\n",
      "Epoch 3/10, Batch 13/883, Training Loss: 0.9296\n",
      "Epoch 3/10, Batch 14/883, Training Loss: 0.7534\n",
      "Epoch 3/10, Batch 15/883, Training Loss: 0.5330\n",
      "Epoch 3/10, Batch 16/883, Training Loss: 0.6068\n",
      "Epoch 3/10, Batch 17/883, Training Loss: 1.0292\n",
      "Epoch 3/10, Batch 18/883, Training Loss: 0.7826\n",
      "Epoch 3/10, Batch 19/883, Training Loss: 0.7710\n",
      "Epoch 3/10, Batch 20/883, Training Loss: 0.7008\n",
      "Epoch 3/10, Batch 21/883, Training Loss: 0.6924\n",
      "Epoch 3/10, Batch 22/883, Training Loss: 0.6801\n",
      "Epoch 3/10, Batch 23/883, Training Loss: 0.7462\n",
      "Epoch 3/10, Batch 24/883, Training Loss: 1.1444\n",
      "Epoch 3/10, Batch 25/883, Training Loss: 0.6296\n",
      "Epoch 3/10, Batch 26/883, Training Loss: 0.7557\n",
      "Epoch 3/10, Batch 27/883, Training Loss: 0.9639\n",
      "Epoch 3/10, Batch 28/883, Training Loss: 0.9104\n",
      "Epoch 3/10, Batch 29/883, Training Loss: 0.6965\n",
      "Epoch 3/10, Batch 30/883, Training Loss: 0.7512\n",
      "Epoch 3/10, Batch 31/883, Training Loss: 0.8935\n",
      "Epoch 3/10, Batch 32/883, Training Loss: 0.6499\n",
      "Epoch 3/10, Batch 33/883, Training Loss: 0.8657\n",
      "Epoch 3/10, Batch 34/883, Training Loss: 0.8061\n",
      "Epoch 3/10, Batch 35/883, Training Loss: 0.7622\n",
      "Epoch 3/10, Batch 36/883, Training Loss: 0.6135\n",
      "Epoch 3/10, Batch 37/883, Training Loss: 0.7050\n",
      "Epoch 3/10, Batch 38/883, Training Loss: 0.6785\n",
      "Epoch 3/10, Batch 39/883, Training Loss: 0.7005\n",
      "Epoch 3/10, Batch 40/883, Training Loss: 0.9009\n",
      "Epoch 3/10, Batch 41/883, Training Loss: 0.4873\n",
      "Epoch 3/10, Batch 42/883, Training Loss: 0.9844\n",
      "Epoch 3/10, Batch 43/883, Training Loss: 1.2986\n",
      "Epoch 3/10, Batch 44/883, Training Loss: 0.8392\n",
      "Epoch 3/10, Batch 45/883, Training Loss: 0.8588\n",
      "Epoch 3/10, Batch 46/883, Training Loss: 0.9749\n",
      "Epoch 3/10, Batch 47/883, Training Loss: 0.7787\n",
      "Epoch 3/10, Batch 48/883, Training Loss: 0.6041\n",
      "Epoch 3/10, Batch 49/883, Training Loss: 0.8855\n",
      "Epoch 3/10, Batch 50/883, Training Loss: 0.8485\n",
      "Epoch 3/10, Batch 51/883, Training Loss: 0.8083\n",
      "Epoch 3/10, Batch 52/883, Training Loss: 0.7721\n",
      "Epoch 3/10, Batch 53/883, Training Loss: 0.6896\n",
      "Epoch 3/10, Batch 54/883, Training Loss: 0.5530\n",
      "Epoch 3/10, Batch 55/883, Training Loss: 0.8396\n",
      "Epoch 3/10, Batch 56/883, Training Loss: 0.7888\n",
      "Epoch 3/10, Batch 57/883, Training Loss: 0.9795\n",
      "Epoch 3/10, Batch 58/883, Training Loss: 0.8617\n",
      "Epoch 3/10, Batch 59/883, Training Loss: 0.6573\n",
      "Epoch 3/10, Batch 60/883, Training Loss: 0.7784\n",
      "Epoch 3/10, Batch 61/883, Training Loss: 1.0302\n",
      "Epoch 3/10, Batch 62/883, Training Loss: 0.7143\n",
      "Epoch 3/10, Batch 63/883, Training Loss: 0.8494\n",
      "Epoch 3/10, Batch 64/883, Training Loss: 0.7124\n",
      "Epoch 3/10, Batch 65/883, Training Loss: 0.9145\n",
      "Epoch 3/10, Batch 66/883, Training Loss: 0.7292\n",
      "Epoch 3/10, Batch 67/883, Training Loss: 1.0515\n",
      "Epoch 3/10, Batch 68/883, Training Loss: 0.7204\n",
      "Epoch 3/10, Batch 69/883, Training Loss: 0.8124\n",
      "Epoch 3/10, Batch 70/883, Training Loss: 0.9066\n",
      "Epoch 3/10, Batch 71/883, Training Loss: 0.8931\n",
      "Epoch 3/10, Batch 72/883, Training Loss: 0.7374\n",
      "Epoch 3/10, Batch 73/883, Training Loss: 0.7533\n",
      "Epoch 3/10, Batch 74/883, Training Loss: 0.6812\n",
      "Epoch 3/10, Batch 75/883, Training Loss: 0.6782\n",
      "Epoch 3/10, Batch 76/883, Training Loss: 0.6708\n",
      "Epoch 3/10, Batch 77/883, Training Loss: 0.7463\n",
      "Epoch 3/10, Batch 78/883, Training Loss: 0.5179\n",
      "Epoch 3/10, Batch 79/883, Training Loss: 0.8459\n",
      "Epoch 3/10, Batch 80/883, Training Loss: 0.6875\n",
      "Epoch 3/10, Batch 81/883, Training Loss: 0.6390\n",
      "Epoch 3/10, Batch 82/883, Training Loss: 0.9654\n",
      "Epoch 3/10, Batch 83/883, Training Loss: 0.5582\n",
      "Epoch 3/10, Batch 84/883, Training Loss: 0.7583\n",
      "Epoch 3/10, Batch 85/883, Training Loss: 0.9066\n",
      "Epoch 3/10, Batch 86/883, Training Loss: 0.5616\n",
      "Epoch 3/10, Batch 87/883, Training Loss: 0.7360\n",
      "Epoch 3/10, Batch 88/883, Training Loss: 1.0576\n",
      "Epoch 3/10, Batch 89/883, Training Loss: 1.0989\n",
      "Epoch 3/10, Batch 90/883, Training Loss: 0.7610\n",
      "Epoch 3/10, Batch 91/883, Training Loss: 0.7022\n",
      "Epoch 3/10, Batch 92/883, Training Loss: 0.8100\n",
      "Epoch 3/10, Batch 93/883, Training Loss: 0.8420\n",
      "Epoch 3/10, Batch 94/883, Training Loss: 0.6914\n",
      "Epoch 3/10, Batch 95/883, Training Loss: 0.7739\n",
      "Epoch 3/10, Batch 96/883, Training Loss: 0.6316\n",
      "Epoch 3/10, Batch 97/883, Training Loss: 0.8834\n",
      "Epoch 3/10, Batch 98/883, Training Loss: 0.6518\n",
      "Epoch 3/10, Batch 99/883, Training Loss: 0.6726\n",
      "Epoch 3/10, Batch 100/883, Training Loss: 0.7791\n",
      "Epoch 3/10, Batch 101/883, Training Loss: 0.7359\n",
      "Epoch 3/10, Batch 102/883, Training Loss: 0.6213\n",
      "Epoch 3/10, Batch 103/883, Training Loss: 0.7793\n",
      "Epoch 3/10, Batch 104/883, Training Loss: 0.6366\n",
      "Epoch 3/10, Batch 105/883, Training Loss: 0.7385\n",
      "Epoch 3/10, Batch 106/883, Training Loss: 0.5594\n",
      "Epoch 3/10, Batch 107/883, Training Loss: 0.6765\n",
      "Epoch 3/10, Batch 108/883, Training Loss: 0.9396\n",
      "Epoch 3/10, Batch 109/883, Training Loss: 0.8511\n",
      "Epoch 3/10, Batch 110/883, Training Loss: 0.8864\n",
      "Epoch 3/10, Batch 111/883, Training Loss: 1.0217\n",
      "Epoch 3/10, Batch 112/883, Training Loss: 0.9568\n",
      "Epoch 3/10, Batch 113/883, Training Loss: 0.7054\n",
      "Epoch 3/10, Batch 114/883, Training Loss: 1.1641\n",
      "Epoch 3/10, Batch 115/883, Training Loss: 0.8369\n",
      "Epoch 3/10, Batch 116/883, Training Loss: 1.0477\n",
      "Epoch 3/10, Batch 117/883, Training Loss: 0.7069\n",
      "Epoch 3/10, Batch 118/883, Training Loss: 0.7194\n",
      "Epoch 3/10, Batch 119/883, Training Loss: 0.8610\n",
      "Epoch 3/10, Batch 120/883, Training Loss: 0.7876\n",
      "Epoch 3/10, Batch 121/883, Training Loss: 0.8012\n",
      "Epoch 3/10, Batch 122/883, Training Loss: 0.9258\n",
      "Epoch 3/10, Batch 123/883, Training Loss: 0.9711\n",
      "Epoch 3/10, Batch 124/883, Training Loss: 0.7485\n",
      "Epoch 3/10, Batch 125/883, Training Loss: 0.5492\n",
      "Epoch 3/10, Batch 126/883, Training Loss: 0.5899\n",
      "Epoch 3/10, Batch 127/883, Training Loss: 0.7520\n",
      "Epoch 3/10, Batch 128/883, Training Loss: 0.7156\n",
      "Epoch 3/10, Batch 129/883, Training Loss: 0.6979\n",
      "Epoch 3/10, Batch 130/883, Training Loss: 0.8343\n",
      "Epoch 3/10, Batch 131/883, Training Loss: 1.0641\n",
      "Epoch 3/10, Batch 132/883, Training Loss: 1.1138\n",
      "Epoch 3/10, Batch 133/883, Training Loss: 0.5386\n",
      "Epoch 3/10, Batch 134/883, Training Loss: 0.5835\n",
      "Epoch 3/10, Batch 135/883, Training Loss: 0.9058\n",
      "Epoch 3/10, Batch 136/883, Training Loss: 0.8072\n",
      "Epoch 3/10, Batch 137/883, Training Loss: 0.7987\n",
      "Epoch 3/10, Batch 138/883, Training Loss: 0.9984\n",
      "Epoch 3/10, Batch 139/883, Training Loss: 0.8694\n",
      "Epoch 3/10, Batch 140/883, Training Loss: 0.9725\n",
      "Epoch 3/10, Batch 141/883, Training Loss: 0.9368\n",
      "Epoch 3/10, Batch 142/883, Training Loss: 0.6686\n",
      "Epoch 3/10, Batch 143/883, Training Loss: 0.6486\n",
      "Epoch 3/10, Batch 144/883, Training Loss: 1.0155\n",
      "Epoch 3/10, Batch 145/883, Training Loss: 0.7744\n",
      "Epoch 3/10, Batch 146/883, Training Loss: 0.8555\n",
      "Epoch 3/10, Batch 147/883, Training Loss: 0.8468\n",
      "Epoch 3/10, Batch 148/883, Training Loss: 0.8172\n",
      "Epoch 3/10, Batch 149/883, Training Loss: 0.6849\n",
      "Epoch 3/10, Batch 150/883, Training Loss: 0.9764\n",
      "Epoch 3/10, Batch 151/883, Training Loss: 0.8758\n",
      "Epoch 3/10, Batch 152/883, Training Loss: 0.7772\n",
      "Epoch 3/10, Batch 153/883, Training Loss: 0.5973\n",
      "Epoch 3/10, Batch 154/883, Training Loss: 0.8461\n",
      "Epoch 3/10, Batch 155/883, Training Loss: 1.0234\n",
      "Epoch 3/10, Batch 156/883, Training Loss: 0.6791\n",
      "Epoch 3/10, Batch 157/883, Training Loss: 0.9623\n",
      "Epoch 3/10, Batch 158/883, Training Loss: 0.6889\n",
      "Epoch 3/10, Batch 159/883, Training Loss: 0.5017\n",
      "Epoch 3/10, Batch 160/883, Training Loss: 0.8532\n",
      "Epoch 3/10, Batch 161/883, Training Loss: 0.6332\n",
      "Epoch 3/10, Batch 162/883, Training Loss: 0.7121\n",
      "Epoch 3/10, Batch 163/883, Training Loss: 0.9404\n",
      "Epoch 3/10, Batch 164/883, Training Loss: 0.7148\n",
      "Epoch 3/10, Batch 165/883, Training Loss: 0.8381\n",
      "Epoch 3/10, Batch 166/883, Training Loss: 0.7451\n",
      "Epoch 3/10, Batch 167/883, Training Loss: 1.1536\n",
      "Epoch 3/10, Batch 168/883, Training Loss: 0.7118\n",
      "Epoch 3/10, Batch 169/883, Training Loss: 1.2773\n",
      "Epoch 3/10, Batch 170/883, Training Loss: 0.8136\n",
      "Epoch 3/10, Batch 171/883, Training Loss: 1.0289\n",
      "Epoch 3/10, Batch 172/883, Training Loss: 0.8147\n",
      "Epoch 3/10, Batch 173/883, Training Loss: 0.6259\n",
      "Epoch 3/10, Batch 174/883, Training Loss: 0.7060\n",
      "Epoch 3/10, Batch 175/883, Training Loss: 0.8435\n",
      "Epoch 3/10, Batch 176/883, Training Loss: 0.8861\n",
      "Epoch 3/10, Batch 177/883, Training Loss: 0.6771\n",
      "Epoch 3/10, Batch 178/883, Training Loss: 0.8026\n",
      "Epoch 3/10, Batch 179/883, Training Loss: 0.8087\n",
      "Epoch 3/10, Batch 180/883, Training Loss: 0.7752\n",
      "Epoch 3/10, Batch 181/883, Training Loss: 0.7465\n",
      "Epoch 3/10, Batch 182/883, Training Loss: 0.8626\n",
      "Epoch 3/10, Batch 183/883, Training Loss: 0.7689\n",
      "Epoch 3/10, Batch 184/883, Training Loss: 0.6157\n",
      "Epoch 3/10, Batch 185/883, Training Loss: 0.6299\n",
      "Epoch 3/10, Batch 186/883, Training Loss: 0.9366\n",
      "Epoch 3/10, Batch 187/883, Training Loss: 0.8204\n",
      "Epoch 3/10, Batch 188/883, Training Loss: 1.0486\n",
      "Epoch 3/10, Batch 189/883, Training Loss: 0.8422\n",
      "Epoch 3/10, Batch 190/883, Training Loss: 0.6140\n",
      "Epoch 3/10, Batch 191/883, Training Loss: 1.0255\n",
      "Epoch 3/10, Batch 192/883, Training Loss: 0.7988\n",
      "Epoch 3/10, Batch 193/883, Training Loss: 0.9605\n",
      "Epoch 3/10, Batch 194/883, Training Loss: 0.9680\n",
      "Epoch 3/10, Batch 195/883, Training Loss: 1.0660\n",
      "Epoch 3/10, Batch 196/883, Training Loss: 0.7664\n",
      "Epoch 3/10, Batch 197/883, Training Loss: 0.9033\n",
      "Epoch 3/10, Batch 198/883, Training Loss: 0.7284\n",
      "Epoch 3/10, Batch 199/883, Training Loss: 0.9858\n",
      "Epoch 3/10, Batch 200/883, Training Loss: 0.7251\n",
      "Epoch 3/10, Batch 201/883, Training Loss: 0.8835\n",
      "Epoch 3/10, Batch 202/883, Training Loss: 0.6229\n",
      "Epoch 3/10, Batch 203/883, Training Loss: 0.7812\n",
      "Epoch 3/10, Batch 204/883, Training Loss: 0.7609\n",
      "Epoch 3/10, Batch 205/883, Training Loss: 0.6430\n",
      "Epoch 3/10, Batch 206/883, Training Loss: 0.6120\n",
      "Epoch 3/10, Batch 207/883, Training Loss: 0.7169\n",
      "Epoch 3/10, Batch 208/883, Training Loss: 0.8536\n",
      "Epoch 3/10, Batch 209/883, Training Loss: 0.8372\n",
      "Epoch 3/10, Batch 210/883, Training Loss: 0.6229\n",
      "Epoch 3/10, Batch 211/883, Training Loss: 0.9109\n",
      "Epoch 3/10, Batch 212/883, Training Loss: 0.6511\n",
      "Epoch 3/10, Batch 213/883, Training Loss: 1.1578\n",
      "Epoch 3/10, Batch 214/883, Training Loss: 0.5305\n",
      "Epoch 3/10, Batch 215/883, Training Loss: 0.7974\n",
      "Epoch 3/10, Batch 216/883, Training Loss: 0.7672\n",
      "Epoch 3/10, Batch 217/883, Training Loss: 1.1660\n",
      "Epoch 3/10, Batch 218/883, Training Loss: 0.8514\n",
      "Epoch 3/10, Batch 219/883, Training Loss: 0.9382\n",
      "Epoch 3/10, Batch 220/883, Training Loss: 0.6132\n",
      "Epoch 3/10, Batch 221/883, Training Loss: 0.7783\n",
      "Epoch 3/10, Batch 222/883, Training Loss: 0.9126\n",
      "Epoch 3/10, Batch 223/883, Training Loss: 0.8168\n",
      "Epoch 3/10, Batch 224/883, Training Loss: 0.8676\n",
      "Epoch 3/10, Batch 225/883, Training Loss: 0.8333\n",
      "Epoch 3/10, Batch 226/883, Training Loss: 0.8992\n",
      "Epoch 3/10, Batch 227/883, Training Loss: 0.8081\n",
      "Epoch 3/10, Batch 228/883, Training Loss: 0.9390\n",
      "Epoch 3/10, Batch 229/883, Training Loss: 0.7391\n",
      "Epoch 3/10, Batch 230/883, Training Loss: 0.9007\n",
      "Epoch 3/10, Batch 231/883, Training Loss: 0.9680\n",
      "Epoch 3/10, Batch 232/883, Training Loss: 0.6658\n",
      "Epoch 3/10, Batch 233/883, Training Loss: 0.9679\n",
      "Epoch 3/10, Batch 234/883, Training Loss: 0.7579\n",
      "Epoch 3/10, Batch 235/883, Training Loss: 0.8435\n",
      "Epoch 3/10, Batch 236/883, Training Loss: 0.7484\n",
      "Epoch 3/10, Batch 237/883, Training Loss: 0.6233\n",
      "Epoch 3/10, Batch 238/883, Training Loss: 0.6284\n",
      "Epoch 3/10, Batch 239/883, Training Loss: 1.1018\n",
      "Epoch 3/10, Batch 240/883, Training Loss: 0.8336\n",
      "Epoch 3/10, Batch 241/883, Training Loss: 0.6752\n",
      "Epoch 3/10, Batch 242/883, Training Loss: 0.8801\n",
      "Epoch 3/10, Batch 243/883, Training Loss: 0.6746\n",
      "Epoch 3/10, Batch 244/883, Training Loss: 1.1673\n",
      "Epoch 3/10, Batch 245/883, Training Loss: 0.9801\n",
      "Epoch 3/10, Batch 246/883, Training Loss: 0.5522\n",
      "Epoch 3/10, Batch 247/883, Training Loss: 0.7082\n",
      "Epoch 3/10, Batch 248/883, Training Loss: 0.6666\n",
      "Epoch 3/10, Batch 249/883, Training Loss: 0.6927\n",
      "Epoch 3/10, Batch 250/883, Training Loss: 0.7452\n",
      "Epoch 3/10, Batch 251/883, Training Loss: 0.7198\n",
      "Epoch 3/10, Batch 252/883, Training Loss: 0.8468\n",
      "Epoch 3/10, Batch 253/883, Training Loss: 0.9821\n",
      "Epoch 3/10, Batch 254/883, Training Loss: 0.8540\n",
      "Epoch 3/10, Batch 255/883, Training Loss: 0.7737\n",
      "Epoch 3/10, Batch 256/883, Training Loss: 0.6265\n",
      "Epoch 3/10, Batch 257/883, Training Loss: 0.6317\n",
      "Epoch 3/10, Batch 258/883, Training Loss: 1.1300\n",
      "Epoch 3/10, Batch 259/883, Training Loss: 0.6603\n",
      "Epoch 3/10, Batch 260/883, Training Loss: 0.9005\n",
      "Epoch 3/10, Batch 261/883, Training Loss: 0.8322\n",
      "Epoch 3/10, Batch 262/883, Training Loss: 0.6667\n",
      "Epoch 3/10, Batch 263/883, Training Loss: 0.6132\n",
      "Epoch 3/10, Batch 264/883, Training Loss: 0.6294\n",
      "Epoch 3/10, Batch 265/883, Training Loss: 0.8111\n",
      "Epoch 3/10, Batch 266/883, Training Loss: 0.9722\n",
      "Epoch 3/10, Batch 267/883, Training Loss: 1.0388\n",
      "Epoch 3/10, Batch 268/883, Training Loss: 0.8489\n",
      "Epoch 3/10, Batch 269/883, Training Loss: 0.8726\n",
      "Epoch 3/10, Batch 270/883, Training Loss: 0.7146\n",
      "Epoch 3/10, Batch 271/883, Training Loss: 0.9992\n",
      "Epoch 3/10, Batch 272/883, Training Loss: 0.6104\n",
      "Epoch 3/10, Batch 273/883, Training Loss: 0.7413\n",
      "Epoch 3/10, Batch 274/883, Training Loss: 1.0415\n",
      "Epoch 3/10, Batch 275/883, Training Loss: 0.6738\n",
      "Epoch 3/10, Batch 276/883, Training Loss: 0.7238\n",
      "Epoch 3/10, Batch 277/883, Training Loss: 0.9639\n",
      "Epoch 3/10, Batch 278/883, Training Loss: 0.7517\n",
      "Epoch 3/10, Batch 279/883, Training Loss: 0.7318\n",
      "Epoch 3/10, Batch 280/883, Training Loss: 0.8431\n",
      "Epoch 3/10, Batch 281/883, Training Loss: 0.6342\n",
      "Epoch 3/10, Batch 282/883, Training Loss: 0.8981\n",
      "Epoch 3/10, Batch 283/883, Training Loss: 0.9408\n",
      "Epoch 3/10, Batch 284/883, Training Loss: 0.9388\n",
      "Epoch 3/10, Batch 285/883, Training Loss: 0.7134\n",
      "Epoch 3/10, Batch 286/883, Training Loss: 0.8643\n",
      "Epoch 3/10, Batch 287/883, Training Loss: 0.6546\n",
      "Epoch 3/10, Batch 288/883, Training Loss: 1.0318\n",
      "Epoch 3/10, Batch 289/883, Training Loss: 0.7534\n",
      "Epoch 3/10, Batch 290/883, Training Loss: 0.8350\n",
      "Epoch 3/10, Batch 291/883, Training Loss: 0.7223\n",
      "Epoch 3/10, Batch 292/883, Training Loss: 0.6917\n",
      "Epoch 3/10, Batch 293/883, Training Loss: 0.7971\n",
      "Epoch 3/10, Batch 294/883, Training Loss: 0.6911\n",
      "Epoch 3/10, Batch 295/883, Training Loss: 0.7839\n",
      "Epoch 3/10, Batch 296/883, Training Loss: 0.7958\n",
      "Epoch 3/10, Batch 297/883, Training Loss: 0.5996\n",
      "Epoch 3/10, Batch 298/883, Training Loss: 0.4985\n",
      "Epoch 3/10, Batch 299/883, Training Loss: 0.6558\n",
      "Epoch 3/10, Batch 300/883, Training Loss: 0.8406\n",
      "Epoch 3/10, Batch 301/883, Training Loss: 0.7550\n",
      "Epoch 3/10, Batch 302/883, Training Loss: 0.5479\n",
      "Epoch 3/10, Batch 303/883, Training Loss: 0.5786\n",
      "Epoch 3/10, Batch 304/883, Training Loss: 0.7708\n",
      "Epoch 3/10, Batch 305/883, Training Loss: 0.7617\n",
      "Epoch 3/10, Batch 306/883, Training Loss: 0.7174\n",
      "Epoch 3/10, Batch 307/883, Training Loss: 0.6327\n",
      "Epoch 3/10, Batch 308/883, Training Loss: 1.0177\n",
      "Epoch 3/10, Batch 309/883, Training Loss: 0.8387\n",
      "Epoch 3/10, Batch 310/883, Training Loss: 0.9958\n",
      "Epoch 3/10, Batch 311/883, Training Loss: 1.0564\n",
      "Epoch 3/10, Batch 312/883, Training Loss: 0.7065\n",
      "Epoch 3/10, Batch 313/883, Training Loss: 0.8144\n",
      "Epoch 3/10, Batch 314/883, Training Loss: 0.7924\n",
      "Epoch 3/10, Batch 315/883, Training Loss: 0.8673\n",
      "Epoch 3/10, Batch 316/883, Training Loss: 0.7439\n",
      "Epoch 3/10, Batch 317/883, Training Loss: 0.8741\n",
      "Epoch 3/10, Batch 318/883, Training Loss: 0.6488\n",
      "Epoch 3/10, Batch 319/883, Training Loss: 0.6026\n",
      "Epoch 3/10, Batch 320/883, Training Loss: 0.8534\n",
      "Epoch 3/10, Batch 321/883, Training Loss: 1.3388\n",
      "Epoch 3/10, Batch 322/883, Training Loss: 1.0217\n",
      "Epoch 3/10, Batch 323/883, Training Loss: 0.6685\n",
      "Epoch 3/10, Batch 324/883, Training Loss: 0.7420\n",
      "Epoch 3/10, Batch 325/883, Training Loss: 0.7449\n",
      "Epoch 3/10, Batch 326/883, Training Loss: 0.7753\n",
      "Epoch 3/10, Batch 327/883, Training Loss: 1.1058\n",
      "Epoch 3/10, Batch 328/883, Training Loss: 0.5775\n",
      "Epoch 3/10, Batch 329/883, Training Loss: 0.7184\n",
      "Epoch 3/10, Batch 330/883, Training Loss: 0.8596\n",
      "Epoch 3/10, Batch 331/883, Training Loss: 1.0429\n",
      "Epoch 3/10, Batch 332/883, Training Loss: 0.8599\n",
      "Epoch 3/10, Batch 333/883, Training Loss: 0.7998\n",
      "Epoch 3/10, Batch 334/883, Training Loss: 0.7983\n",
      "Epoch 3/10, Batch 335/883, Training Loss: 0.7325\n",
      "Epoch 3/10, Batch 336/883, Training Loss: 0.8537\n",
      "Epoch 3/10, Batch 337/883, Training Loss: 0.8313\n",
      "Epoch 3/10, Batch 338/883, Training Loss: 0.6135\n",
      "Epoch 3/10, Batch 339/883, Training Loss: 1.2016\n",
      "Epoch 3/10, Batch 340/883, Training Loss: 0.8097\n",
      "Epoch 3/10, Batch 341/883, Training Loss: 0.8150\n",
      "Epoch 3/10, Batch 342/883, Training Loss: 0.6462\n",
      "Epoch 3/10, Batch 343/883, Training Loss: 0.8419\n",
      "Epoch 3/10, Batch 344/883, Training Loss: 0.8493\n",
      "Epoch 3/10, Batch 345/883, Training Loss: 0.8490\n",
      "Epoch 3/10, Batch 346/883, Training Loss: 0.8971\n",
      "Epoch 3/10, Batch 347/883, Training Loss: 0.6761\n",
      "Epoch 3/10, Batch 348/883, Training Loss: 0.8640\n",
      "Epoch 3/10, Batch 349/883, Training Loss: 0.9945\n",
      "Epoch 3/10, Batch 350/883, Training Loss: 0.6525\n",
      "Epoch 3/10, Batch 351/883, Training Loss: 0.6101\n",
      "Epoch 3/10, Batch 352/883, Training Loss: 0.6355\n",
      "Epoch 3/10, Batch 353/883, Training Loss: 0.5263\n",
      "Epoch 3/10, Batch 354/883, Training Loss: 0.8394\n",
      "Epoch 3/10, Batch 355/883, Training Loss: 0.6792\n",
      "Epoch 3/10, Batch 356/883, Training Loss: 0.9748\n",
      "Epoch 3/10, Batch 357/883, Training Loss: 0.9221\n",
      "Epoch 3/10, Batch 358/883, Training Loss: 0.7228\n",
      "Epoch 3/10, Batch 359/883, Training Loss: 0.9631\n",
      "Epoch 3/10, Batch 360/883, Training Loss: 0.9002\n",
      "Epoch 3/10, Batch 361/883, Training Loss: 0.7794\n",
      "Epoch 3/10, Batch 362/883, Training Loss: 0.8176\n",
      "Epoch 3/10, Batch 363/883, Training Loss: 0.9656\n",
      "Epoch 3/10, Batch 364/883, Training Loss: 0.6194\n",
      "Epoch 3/10, Batch 365/883, Training Loss: 0.6367\n",
      "Epoch 3/10, Batch 366/883, Training Loss: 0.9083\n",
      "Epoch 3/10, Batch 367/883, Training Loss: 0.6507\n",
      "Epoch 3/10, Batch 368/883, Training Loss: 0.7423\n",
      "Epoch 3/10, Batch 369/883, Training Loss: 0.5756\n",
      "Epoch 3/10, Batch 370/883, Training Loss: 0.5888\n",
      "Epoch 3/10, Batch 371/883, Training Loss: 1.1708\n",
      "Epoch 3/10, Batch 372/883, Training Loss: 1.2738\n",
      "Epoch 3/10, Batch 373/883, Training Loss: 0.5120\n",
      "Epoch 3/10, Batch 374/883, Training Loss: 0.6826\n",
      "Epoch 3/10, Batch 375/883, Training Loss: 1.1229\n",
      "Epoch 3/10, Batch 376/883, Training Loss: 0.6134\n",
      "Epoch 3/10, Batch 377/883, Training Loss: 0.8782\n",
      "Epoch 3/10, Batch 378/883, Training Loss: 0.7075\n",
      "Epoch 3/10, Batch 379/883, Training Loss: 0.7192\n",
      "Epoch 3/10, Batch 380/883, Training Loss: 1.0566\n",
      "Epoch 3/10, Batch 381/883, Training Loss: 0.6290\n",
      "Epoch 3/10, Batch 382/883, Training Loss: 0.6685\n",
      "Epoch 3/10, Batch 383/883, Training Loss: 0.6537\n",
      "Epoch 3/10, Batch 384/883, Training Loss: 0.8058\n",
      "Epoch 3/10, Batch 385/883, Training Loss: 0.9206\n",
      "Epoch 3/10, Batch 386/883, Training Loss: 0.7566\n",
      "Epoch 3/10, Batch 387/883, Training Loss: 0.5406\n",
      "Epoch 3/10, Batch 388/883, Training Loss: 0.9090\n",
      "Epoch 3/10, Batch 389/883, Training Loss: 0.6626\n",
      "Epoch 3/10, Batch 390/883, Training Loss: 0.7684\n",
      "Epoch 3/10, Batch 391/883, Training Loss: 0.7165\n",
      "Epoch 3/10, Batch 392/883, Training Loss: 0.7976\n",
      "Epoch 3/10, Batch 393/883, Training Loss: 0.8752\n",
      "Epoch 3/10, Batch 394/883, Training Loss: 0.8683\n",
      "Epoch 3/10, Batch 395/883, Training Loss: 0.7125\n",
      "Epoch 3/10, Batch 396/883, Training Loss: 0.6638\n",
      "Epoch 3/10, Batch 397/883, Training Loss: 1.0505\n",
      "Epoch 3/10, Batch 398/883, Training Loss: 0.9153\n",
      "Epoch 3/10, Batch 399/883, Training Loss: 0.6846\n",
      "Epoch 3/10, Batch 400/883, Training Loss: 0.6037\n",
      "Epoch 3/10, Batch 401/883, Training Loss: 1.0692\n",
      "Epoch 3/10, Batch 402/883, Training Loss: 0.6342\n",
      "Epoch 3/10, Batch 403/883, Training Loss: 0.6896\n",
      "Epoch 3/10, Batch 404/883, Training Loss: 0.7559\n",
      "Epoch 3/10, Batch 405/883, Training Loss: 0.7022\n",
      "Epoch 3/10, Batch 406/883, Training Loss: 1.0650\n",
      "Epoch 3/10, Batch 407/883, Training Loss: 0.8737\n",
      "Epoch 3/10, Batch 408/883, Training Loss: 0.8888\n",
      "Epoch 3/10, Batch 409/883, Training Loss: 0.6916\n",
      "Epoch 3/10, Batch 410/883, Training Loss: 0.7760\n",
      "Epoch 3/10, Batch 411/883, Training Loss: 1.0101\n",
      "Epoch 3/10, Batch 412/883, Training Loss: 0.9460\n",
      "Epoch 3/10, Batch 413/883, Training Loss: 0.6756\n",
      "Epoch 3/10, Batch 414/883, Training Loss: 0.6808\n",
      "Epoch 3/10, Batch 415/883, Training Loss: 0.9443\n",
      "Epoch 3/10, Batch 416/883, Training Loss: 0.9414\n",
      "Epoch 3/10, Batch 417/883, Training Loss: 0.6691\n",
      "Epoch 3/10, Batch 418/883, Training Loss: 0.6766\n",
      "Epoch 3/10, Batch 419/883, Training Loss: 0.7601\n",
      "Epoch 3/10, Batch 420/883, Training Loss: 0.6792\n",
      "Epoch 3/10, Batch 421/883, Training Loss: 0.8867\n",
      "Epoch 3/10, Batch 422/883, Training Loss: 0.9526\n",
      "Epoch 3/10, Batch 423/883, Training Loss: 0.9165\n",
      "Epoch 3/10, Batch 424/883, Training Loss: 0.7035\n",
      "Epoch 3/10, Batch 425/883, Training Loss: 0.5978\n",
      "Epoch 3/10, Batch 426/883, Training Loss: 0.6678\n",
      "Epoch 3/10, Batch 427/883, Training Loss: 0.9906\n",
      "Epoch 3/10, Batch 428/883, Training Loss: 0.7741\n",
      "Epoch 3/10, Batch 429/883, Training Loss: 1.1942\n",
      "Epoch 3/10, Batch 430/883, Training Loss: 0.6750\n",
      "Epoch 3/10, Batch 431/883, Training Loss: 0.9395\n",
      "Epoch 3/10, Batch 432/883, Training Loss: 0.9086\n",
      "Epoch 3/10, Batch 433/883, Training Loss: 0.8288\n",
      "Epoch 3/10, Batch 434/883, Training Loss: 0.7806\n",
      "Epoch 3/10, Batch 435/883, Training Loss: 0.9436\n",
      "Epoch 3/10, Batch 436/883, Training Loss: 1.1008\n",
      "Epoch 3/10, Batch 437/883, Training Loss: 0.9937\n",
      "Epoch 3/10, Batch 438/883, Training Loss: 1.1924\n",
      "Epoch 3/10, Batch 439/883, Training Loss: 1.2591\n",
      "Epoch 3/10, Batch 440/883, Training Loss: 1.0745\n",
      "Epoch 3/10, Batch 441/883, Training Loss: 0.7573\n",
      "Epoch 3/10, Batch 442/883, Training Loss: 0.8098\n",
      "Epoch 3/10, Batch 443/883, Training Loss: 0.8731\n",
      "Epoch 3/10, Batch 444/883, Training Loss: 0.6356\n",
      "Epoch 3/10, Batch 445/883, Training Loss: 0.6769\n",
      "Epoch 3/10, Batch 446/883, Training Loss: 0.7126\n",
      "Epoch 3/10, Batch 447/883, Training Loss: 0.8913\n",
      "Epoch 3/10, Batch 448/883, Training Loss: 0.7121\n",
      "Epoch 3/10, Batch 449/883, Training Loss: 0.6269\n",
      "Epoch 3/10, Batch 450/883, Training Loss: 0.8160\n",
      "Epoch 3/10, Batch 451/883, Training Loss: 0.6753\n",
      "Epoch 3/10, Batch 452/883, Training Loss: 1.0728\n",
      "Epoch 3/10, Batch 453/883, Training Loss: 0.9043\n",
      "Epoch 3/10, Batch 454/883, Training Loss: 0.8221\n",
      "Epoch 3/10, Batch 455/883, Training Loss: 1.0442\n",
      "Epoch 3/10, Batch 456/883, Training Loss: 0.9582\n",
      "Epoch 3/10, Batch 457/883, Training Loss: 0.6372\n",
      "Epoch 3/10, Batch 458/883, Training Loss: 0.7059\n",
      "Epoch 3/10, Batch 459/883, Training Loss: 0.9104\n",
      "Epoch 3/10, Batch 460/883, Training Loss: 0.6603\n",
      "Epoch 3/10, Batch 461/883, Training Loss: 0.7728\n",
      "Epoch 3/10, Batch 462/883, Training Loss: 1.1446\n",
      "Epoch 3/10, Batch 463/883, Training Loss: 0.6127\n",
      "Epoch 3/10, Batch 464/883, Training Loss: 0.8297\n",
      "Epoch 3/10, Batch 465/883, Training Loss: 0.8226\n",
      "Epoch 3/10, Batch 466/883, Training Loss: 0.5534\n",
      "Epoch 3/10, Batch 467/883, Training Loss: 0.6502\n",
      "Epoch 3/10, Batch 468/883, Training Loss: 0.9244\n",
      "Epoch 3/10, Batch 469/883, Training Loss: 1.3249\n",
      "Epoch 3/10, Batch 470/883, Training Loss: 0.7670\n",
      "Epoch 3/10, Batch 471/883, Training Loss: 1.0246\n",
      "Epoch 3/10, Batch 472/883, Training Loss: 0.6349\n",
      "Epoch 3/10, Batch 473/883, Training Loss: 0.7860\n",
      "Epoch 3/10, Batch 474/883, Training Loss: 0.6891\n",
      "Epoch 3/10, Batch 475/883, Training Loss: 0.8412\n",
      "Epoch 3/10, Batch 476/883, Training Loss: 0.6320\n",
      "Epoch 3/10, Batch 477/883, Training Loss: 0.6031\n",
      "Epoch 3/10, Batch 478/883, Training Loss: 0.8047\n",
      "Epoch 3/10, Batch 479/883, Training Loss: 1.0314\n",
      "Epoch 3/10, Batch 480/883, Training Loss: 0.8097\n",
      "Epoch 3/10, Batch 481/883, Training Loss: 0.6832\n",
      "Epoch 3/10, Batch 482/883, Training Loss: 0.9912\n",
      "Epoch 3/10, Batch 483/883, Training Loss: 0.7637\n",
      "Epoch 3/10, Batch 484/883, Training Loss: 0.6363\n",
      "Epoch 3/10, Batch 485/883, Training Loss: 0.8543\n",
      "Epoch 3/10, Batch 486/883, Training Loss: 0.7682\n",
      "Epoch 3/10, Batch 487/883, Training Loss: 0.7525\n",
      "Epoch 3/10, Batch 488/883, Training Loss: 0.6294\n",
      "Epoch 3/10, Batch 489/883, Training Loss: 0.8399\n",
      "Epoch 3/10, Batch 490/883, Training Loss: 0.6144\n",
      "Epoch 3/10, Batch 491/883, Training Loss: 0.9097\n",
      "Epoch 3/10, Batch 492/883, Training Loss: 0.8203\n",
      "Epoch 3/10, Batch 493/883, Training Loss: 0.7757\n",
      "Epoch 3/10, Batch 494/883, Training Loss: 0.8232\n",
      "Epoch 3/10, Batch 495/883, Training Loss: 0.7493\n",
      "Epoch 3/10, Batch 496/883, Training Loss: 0.9038\n",
      "Epoch 3/10, Batch 497/883, Training Loss: 0.9622\n",
      "Epoch 3/10, Batch 498/883, Training Loss: 0.8937\n",
      "Epoch 3/10, Batch 499/883, Training Loss: 0.7581\n",
      "Epoch 3/10, Batch 500/883, Training Loss: 0.6261\n",
      "Epoch 3/10, Batch 501/883, Training Loss: 0.7032\n",
      "Epoch 3/10, Batch 502/883, Training Loss: 0.7813\n",
      "Epoch 3/10, Batch 503/883, Training Loss: 0.7091\n",
      "Epoch 3/10, Batch 504/883, Training Loss: 0.7962\n",
      "Epoch 3/10, Batch 505/883, Training Loss: 0.7784\n",
      "Epoch 3/10, Batch 506/883, Training Loss: 0.7563\n",
      "Epoch 3/10, Batch 507/883, Training Loss: 0.6480\n",
      "Epoch 3/10, Batch 508/883, Training Loss: 0.6447\n",
      "Epoch 3/10, Batch 509/883, Training Loss: 0.9794\n",
      "Epoch 3/10, Batch 510/883, Training Loss: 0.6608\n",
      "Epoch 3/10, Batch 511/883, Training Loss: 0.5568\n",
      "Epoch 3/10, Batch 512/883, Training Loss: 0.9565\n",
      "Epoch 3/10, Batch 513/883, Training Loss: 1.0796\n",
      "Epoch 3/10, Batch 514/883, Training Loss: 0.5787\n",
      "Epoch 3/10, Batch 515/883, Training Loss: 0.9075\n",
      "Epoch 3/10, Batch 516/883, Training Loss: 0.8382\n",
      "Epoch 3/10, Batch 517/883, Training Loss: 0.6571\n",
      "Epoch 3/10, Batch 518/883, Training Loss: 0.7872\n",
      "Epoch 3/10, Batch 519/883, Training Loss: 0.5670\n",
      "Epoch 3/10, Batch 520/883, Training Loss: 1.0828\n",
      "Epoch 3/10, Batch 521/883, Training Loss: 0.6092\n",
      "Epoch 3/10, Batch 522/883, Training Loss: 0.5495\n",
      "Epoch 3/10, Batch 523/883, Training Loss: 0.7330\n",
      "Epoch 3/10, Batch 524/883, Training Loss: 0.8874\n",
      "Epoch 3/10, Batch 525/883, Training Loss: 0.9056\n",
      "Epoch 3/10, Batch 526/883, Training Loss: 0.9272\n",
      "Epoch 3/10, Batch 527/883, Training Loss: 0.6370\n",
      "Epoch 3/10, Batch 528/883, Training Loss: 0.8727\n",
      "Epoch 3/10, Batch 529/883, Training Loss: 0.5163\n",
      "Epoch 3/10, Batch 530/883, Training Loss: 1.1816\n",
      "Epoch 3/10, Batch 531/883, Training Loss: 0.6370\n",
      "Epoch 3/10, Batch 532/883, Training Loss: 1.0771\n",
      "Epoch 3/10, Batch 533/883, Training Loss: 0.6504\n",
      "Epoch 3/10, Batch 534/883, Training Loss: 0.8589\n",
      "Epoch 3/10, Batch 535/883, Training Loss: 0.6673\n",
      "Epoch 3/10, Batch 536/883, Training Loss: 0.7184\n",
      "Epoch 3/10, Batch 537/883, Training Loss: 0.6693\n",
      "Epoch 3/10, Batch 538/883, Training Loss: 0.7241\n",
      "Epoch 3/10, Batch 539/883, Training Loss: 1.1381\n",
      "Epoch 3/10, Batch 540/883, Training Loss: 0.9581\n",
      "Epoch 3/10, Batch 541/883, Training Loss: 0.5609\n",
      "Epoch 3/10, Batch 542/883, Training Loss: 0.7358\n",
      "Epoch 3/10, Batch 543/883, Training Loss: 0.6855\n",
      "Epoch 3/10, Batch 544/883, Training Loss: 0.6901\n",
      "Epoch 3/10, Batch 545/883, Training Loss: 0.8719\n",
      "Epoch 3/10, Batch 546/883, Training Loss: 0.5542\n",
      "Epoch 3/10, Batch 547/883, Training Loss: 0.8531\n",
      "Epoch 3/10, Batch 548/883, Training Loss: 0.5849\n",
      "Epoch 3/10, Batch 549/883, Training Loss: 0.8050\n",
      "Epoch 3/10, Batch 550/883, Training Loss: 0.6897\n",
      "Epoch 3/10, Batch 551/883, Training Loss: 1.2337\n",
      "Epoch 3/10, Batch 552/883, Training Loss: 1.0413\n",
      "Epoch 3/10, Batch 553/883, Training Loss: 0.4092\n",
      "Epoch 3/10, Batch 554/883, Training Loss: 0.8717\n",
      "Epoch 3/10, Batch 555/883, Training Loss: 0.9267\n",
      "Epoch 3/10, Batch 556/883, Training Loss: 0.6144\n",
      "Epoch 3/10, Batch 557/883, Training Loss: 0.7717\n",
      "Epoch 3/10, Batch 558/883, Training Loss: 1.0487\n",
      "Epoch 3/10, Batch 559/883, Training Loss: 0.7511\n",
      "Epoch 3/10, Batch 560/883, Training Loss: 0.8755\n",
      "Epoch 3/10, Batch 561/883, Training Loss: 0.8485\n",
      "Epoch 3/10, Batch 562/883, Training Loss: 0.8492\n",
      "Epoch 3/10, Batch 563/883, Training Loss: 0.7511\n",
      "Epoch 3/10, Batch 564/883, Training Loss: 0.9137\n",
      "Epoch 3/10, Batch 565/883, Training Loss: 0.6662\n",
      "Epoch 3/10, Batch 566/883, Training Loss: 0.6525\n",
      "Epoch 3/10, Batch 567/883, Training Loss: 0.8647\n",
      "Epoch 3/10, Batch 568/883, Training Loss: 0.8669\n",
      "Epoch 3/10, Batch 569/883, Training Loss: 0.9170\n",
      "Epoch 3/10, Batch 570/883, Training Loss: 1.0424\n",
      "Epoch 3/10, Batch 571/883, Training Loss: 0.8184\n",
      "Epoch 3/10, Batch 572/883, Training Loss: 0.8928\n",
      "Epoch 3/10, Batch 573/883, Training Loss: 0.7938\n",
      "Epoch 3/10, Batch 574/883, Training Loss: 0.9035\n",
      "Epoch 3/10, Batch 575/883, Training Loss: 0.9679\n",
      "Epoch 3/10, Batch 576/883, Training Loss: 0.7446\n",
      "Epoch 3/10, Batch 577/883, Training Loss: 0.7012\n",
      "Epoch 3/10, Batch 578/883, Training Loss: 0.7795\n",
      "Epoch 3/10, Batch 579/883, Training Loss: 0.8045\n",
      "Epoch 3/10, Batch 580/883, Training Loss: 0.6399\n",
      "Epoch 3/10, Batch 581/883, Training Loss: 0.7478\n",
      "Epoch 3/10, Batch 582/883, Training Loss: 0.6370\n",
      "Epoch 3/10, Batch 583/883, Training Loss: 0.6597\n",
      "Epoch 3/10, Batch 584/883, Training Loss: 0.7126\n",
      "Epoch 3/10, Batch 585/883, Training Loss: 0.9918\n",
      "Epoch 3/10, Batch 586/883, Training Loss: 0.7727\n",
      "Epoch 3/10, Batch 587/883, Training Loss: 0.6618\n",
      "Epoch 3/10, Batch 588/883, Training Loss: 0.8347\n",
      "Epoch 3/10, Batch 589/883, Training Loss: 0.8183\n",
      "Epoch 3/10, Batch 590/883, Training Loss: 0.8021\n",
      "Epoch 3/10, Batch 591/883, Training Loss: 0.9568\n",
      "Epoch 3/10, Batch 592/883, Training Loss: 1.0258\n",
      "Epoch 3/10, Batch 593/883, Training Loss: 0.8593\n",
      "Epoch 3/10, Batch 594/883, Training Loss: 0.8238\n",
      "Epoch 3/10, Batch 595/883, Training Loss: 0.5980\n",
      "Epoch 3/10, Batch 596/883, Training Loss: 0.7932\n",
      "Epoch 3/10, Batch 597/883, Training Loss: 0.5851\n",
      "Epoch 3/10, Batch 598/883, Training Loss: 0.7738\n",
      "Epoch 3/10, Batch 599/883, Training Loss: 0.9505\n",
      "Epoch 3/10, Batch 600/883, Training Loss: 0.7293\n",
      "Epoch 3/10, Batch 601/883, Training Loss: 0.7433\n",
      "Epoch 3/10, Batch 602/883, Training Loss: 0.8247\n",
      "Epoch 3/10, Batch 603/883, Training Loss: 0.6027\n",
      "Epoch 3/10, Batch 604/883, Training Loss: 0.9425\n",
      "Epoch 3/10, Batch 605/883, Training Loss: 0.8175\n",
      "Epoch 3/10, Batch 606/883, Training Loss: 0.8853\n",
      "Epoch 3/10, Batch 607/883, Training Loss: 0.7614\n",
      "Epoch 3/10, Batch 608/883, Training Loss: 0.9092\n",
      "Epoch 3/10, Batch 609/883, Training Loss: 0.5671\n",
      "Epoch 3/10, Batch 610/883, Training Loss: 0.7737\n",
      "Epoch 3/10, Batch 611/883, Training Loss: 0.6974\n",
      "Epoch 3/10, Batch 612/883, Training Loss: 0.8191\n",
      "Epoch 3/10, Batch 613/883, Training Loss: 0.6593\n",
      "Epoch 3/10, Batch 614/883, Training Loss: 1.0117\n",
      "Epoch 3/10, Batch 615/883, Training Loss: 0.9820\n",
      "Epoch 3/10, Batch 616/883, Training Loss: 0.7173\n",
      "Epoch 3/10, Batch 617/883, Training Loss: 0.6977\n",
      "Epoch 3/10, Batch 618/883, Training Loss: 0.8021\n",
      "Epoch 3/10, Batch 619/883, Training Loss: 0.6584\n",
      "Epoch 3/10, Batch 620/883, Training Loss: 0.8547\n",
      "Epoch 3/10, Batch 621/883, Training Loss: 0.7388\n",
      "Epoch 3/10, Batch 622/883, Training Loss: 0.6863\n",
      "Epoch 3/10, Batch 623/883, Training Loss: 0.7416\n",
      "Epoch 3/10, Batch 624/883, Training Loss: 0.6979\n",
      "Epoch 3/10, Batch 625/883, Training Loss: 0.8493\n",
      "Epoch 3/10, Batch 626/883, Training Loss: 0.8494\n",
      "Epoch 3/10, Batch 627/883, Training Loss: 0.7579\n",
      "Epoch 3/10, Batch 628/883, Training Loss: 0.7796\n",
      "Epoch 3/10, Batch 629/883, Training Loss: 0.8476\n",
      "Epoch 3/10, Batch 630/883, Training Loss: 0.8472\n",
      "Epoch 3/10, Batch 631/883, Training Loss: 0.5488\n",
      "Epoch 3/10, Batch 632/883, Training Loss: 0.8420\n",
      "Epoch 3/10, Batch 633/883, Training Loss: 0.7145\n",
      "Epoch 3/10, Batch 634/883, Training Loss: 0.9249\n",
      "Epoch 3/10, Batch 635/883, Training Loss: 0.9430\n",
      "Epoch 3/10, Batch 636/883, Training Loss: 0.9363\n",
      "Epoch 3/10, Batch 637/883, Training Loss: 0.6032\n",
      "Epoch 3/10, Batch 638/883, Training Loss: 1.0980\n",
      "Epoch 3/10, Batch 639/883, Training Loss: 0.6454\n",
      "Epoch 3/10, Batch 640/883, Training Loss: 0.7439\n",
      "Epoch 3/10, Batch 641/883, Training Loss: 0.7183\n",
      "Epoch 3/10, Batch 642/883, Training Loss: 0.9003\n",
      "Epoch 3/10, Batch 643/883, Training Loss: 0.7806\n",
      "Epoch 3/10, Batch 644/883, Training Loss: 0.8581\n",
      "Epoch 3/10, Batch 645/883, Training Loss: 0.8578\n",
      "Epoch 3/10, Batch 646/883, Training Loss: 0.7548\n",
      "Epoch 3/10, Batch 647/883, Training Loss: 0.8513\n",
      "Epoch 3/10, Batch 648/883, Training Loss: 0.7043\n",
      "Epoch 3/10, Batch 649/883, Training Loss: 0.6113\n",
      "Epoch 3/10, Batch 650/883, Training Loss: 0.6891\n",
      "Epoch 3/10, Batch 651/883, Training Loss: 0.6471\n",
      "Epoch 3/10, Batch 652/883, Training Loss: 0.7800\n",
      "Epoch 3/10, Batch 653/883, Training Loss: 0.7735\n",
      "Epoch 3/10, Batch 654/883, Training Loss: 0.9564\n",
      "Epoch 3/10, Batch 655/883, Training Loss: 1.2705\n",
      "Epoch 3/10, Batch 656/883, Training Loss: 0.6885\n",
      "Epoch 3/10, Batch 657/883, Training Loss: 0.7972\n",
      "Epoch 3/10, Batch 658/883, Training Loss: 0.6707\n",
      "Epoch 3/10, Batch 659/883, Training Loss: 0.9105\n",
      "Epoch 3/10, Batch 660/883, Training Loss: 0.7273\n",
      "Epoch 3/10, Batch 661/883, Training Loss: 0.7986\n",
      "Epoch 3/10, Batch 662/883, Training Loss: 0.6591\n",
      "Epoch 3/10, Batch 663/883, Training Loss: 0.7860\n",
      "Epoch 3/10, Batch 664/883, Training Loss: 0.7219\n",
      "Epoch 3/10, Batch 665/883, Training Loss: 0.6697\n",
      "Epoch 3/10, Batch 666/883, Training Loss: 0.7145\n",
      "Epoch 3/10, Batch 667/883, Training Loss: 0.8433\n",
      "Epoch 3/10, Batch 668/883, Training Loss: 0.8555\n",
      "Epoch 3/10, Batch 669/883, Training Loss: 0.8809\n",
      "Epoch 3/10, Batch 670/883, Training Loss: 0.9598\n",
      "Epoch 3/10, Batch 671/883, Training Loss: 0.8379\n",
      "Epoch 3/10, Batch 672/883, Training Loss: 0.7149\n",
      "Epoch 3/10, Batch 673/883, Training Loss: 0.8756\n",
      "Epoch 3/10, Batch 674/883, Training Loss: 0.6575\n",
      "Epoch 3/10, Batch 675/883, Training Loss: 0.6665\n",
      "Epoch 3/10, Batch 676/883, Training Loss: 0.5717\n",
      "Epoch 3/10, Batch 677/883, Training Loss: 0.7305\n",
      "Epoch 3/10, Batch 678/883, Training Loss: 0.7644\n",
      "Epoch 3/10, Batch 679/883, Training Loss: 0.6885\n",
      "Epoch 3/10, Batch 680/883, Training Loss: 0.6942\n",
      "Epoch 3/10, Batch 681/883, Training Loss: 1.1805\n",
      "Epoch 3/10, Batch 682/883, Training Loss: 0.5743\n",
      "Epoch 3/10, Batch 683/883, Training Loss: 0.5492\n",
      "Epoch 3/10, Batch 684/883, Training Loss: 0.5682\n",
      "Epoch 3/10, Batch 685/883, Training Loss: 0.5887\n",
      "Epoch 3/10, Batch 686/883, Training Loss: 0.7424\n",
      "Epoch 3/10, Batch 687/883, Training Loss: 0.7744\n",
      "Epoch 3/10, Batch 688/883, Training Loss: 0.5918\n",
      "Epoch 3/10, Batch 689/883, Training Loss: 0.7635\n",
      "Epoch 3/10, Batch 690/883, Training Loss: 0.6868\n",
      "Epoch 3/10, Batch 691/883, Training Loss: 1.0850\n",
      "Epoch 3/10, Batch 692/883, Training Loss: 0.5353\n",
      "Epoch 3/10, Batch 693/883, Training Loss: 0.6690\n",
      "Epoch 3/10, Batch 694/883, Training Loss: 0.7035\n",
      "Epoch 3/10, Batch 695/883, Training Loss: 0.6693\n",
      "Epoch 3/10, Batch 696/883, Training Loss: 0.9410\n",
      "Epoch 3/10, Batch 697/883, Training Loss: 1.0601\n",
      "Epoch 3/10, Batch 698/883, Training Loss: 0.5665\n",
      "Epoch 3/10, Batch 699/883, Training Loss: 1.0084\n",
      "Epoch 3/10, Batch 700/883, Training Loss: 0.5187\n",
      "Epoch 3/10, Batch 701/883, Training Loss: 1.0773\n",
      "Epoch 3/10, Batch 702/883, Training Loss: 0.4654\n",
      "Epoch 3/10, Batch 703/883, Training Loss: 1.1441\n",
      "Epoch 3/10, Batch 704/883, Training Loss: 0.7734\n",
      "Epoch 3/10, Batch 705/883, Training Loss: 0.7916\n",
      "Epoch 3/10, Batch 706/883, Training Loss: 0.7746\n",
      "Epoch 3/10, Batch 707/883, Training Loss: 1.0031\n",
      "Epoch 3/10, Batch 708/883, Training Loss: 0.7308\n",
      "Epoch 3/10, Batch 709/883, Training Loss: 0.6881\n",
      "Epoch 3/10, Batch 710/883, Training Loss: 0.5886\n",
      "Epoch 3/10, Batch 711/883, Training Loss: 0.7274\n",
      "Epoch 3/10, Batch 712/883, Training Loss: 0.9812\n",
      "Epoch 3/10, Batch 713/883, Training Loss: 0.6560\n",
      "Epoch 3/10, Batch 714/883, Training Loss: 0.7475\n",
      "Epoch 3/10, Batch 715/883, Training Loss: 0.6982\n",
      "Epoch 3/10, Batch 716/883, Training Loss: 0.8092\n",
      "Epoch 3/10, Batch 717/883, Training Loss: 0.7760\n",
      "Epoch 3/10, Batch 718/883, Training Loss: 0.9541\n",
      "Epoch 3/10, Batch 719/883, Training Loss: 0.7774\n",
      "Epoch 3/10, Batch 720/883, Training Loss: 0.6482\n",
      "Epoch 3/10, Batch 721/883, Training Loss: 0.8545\n",
      "Epoch 3/10, Batch 722/883, Training Loss: 0.8559\n",
      "Epoch 3/10, Batch 723/883, Training Loss: 0.6649\n",
      "Epoch 3/10, Batch 724/883, Training Loss: 0.9618\n",
      "Epoch 3/10, Batch 725/883, Training Loss: 1.1324\n",
      "Epoch 3/10, Batch 726/883, Training Loss: 0.8356\n",
      "Epoch 3/10, Batch 727/883, Training Loss: 1.2455\n",
      "Epoch 3/10, Batch 728/883, Training Loss: 0.8939\n",
      "Epoch 3/10, Batch 729/883, Training Loss: 0.7233\n",
      "Epoch 3/10, Batch 730/883, Training Loss: 0.7227\n",
      "Epoch 3/10, Batch 731/883, Training Loss: 0.8385\n",
      "Epoch 3/10, Batch 732/883, Training Loss: 0.8083\n",
      "Epoch 3/10, Batch 733/883, Training Loss: 0.7478\n",
      "Epoch 3/10, Batch 734/883, Training Loss: 0.6953\n",
      "Epoch 3/10, Batch 735/883, Training Loss: 0.7544\n",
      "Epoch 3/10, Batch 736/883, Training Loss: 0.8121\n",
      "Epoch 3/10, Batch 737/883, Training Loss: 0.8975\n",
      "Epoch 3/10, Batch 738/883, Training Loss: 0.7325\n",
      "Epoch 3/10, Batch 739/883, Training Loss: 1.0390\n",
      "Epoch 3/10, Batch 740/883, Training Loss: 0.5556\n",
      "Epoch 3/10, Batch 741/883, Training Loss: 0.9469\n",
      "Epoch 3/10, Batch 742/883, Training Loss: 0.8013\n",
      "Epoch 3/10, Batch 743/883, Training Loss: 0.8748\n",
      "Epoch 3/10, Batch 744/883, Training Loss: 0.7800\n",
      "Epoch 3/10, Batch 745/883, Training Loss: 0.4784\n",
      "Epoch 3/10, Batch 746/883, Training Loss: 0.6925\n",
      "Epoch 3/10, Batch 747/883, Training Loss: 0.6412\n",
      "Epoch 3/10, Batch 748/883, Training Loss: 0.8737\n",
      "Epoch 3/10, Batch 749/883, Training Loss: 0.6989\n",
      "Epoch 3/10, Batch 750/883, Training Loss: 0.6050\n",
      "Epoch 3/10, Batch 751/883, Training Loss: 0.6749\n",
      "Epoch 3/10, Batch 752/883, Training Loss: 0.7192\n",
      "Epoch 3/10, Batch 753/883, Training Loss: 0.6668\n",
      "Epoch 3/10, Batch 754/883, Training Loss: 0.6110\n",
      "Epoch 3/10, Batch 755/883, Training Loss: 0.8864\n",
      "Epoch 3/10, Batch 756/883, Training Loss: 0.7235\n",
      "Epoch 3/10, Batch 757/883, Training Loss: 1.1437\n",
      "Epoch 3/10, Batch 758/883, Training Loss: 0.8368\n",
      "Epoch 3/10, Batch 759/883, Training Loss: 0.5664\n",
      "Epoch 3/10, Batch 760/883, Training Loss: 1.1028\n",
      "Epoch 3/10, Batch 761/883, Training Loss: 1.4054\n",
      "Epoch 3/10, Batch 762/883, Training Loss: 0.6830\n",
      "Epoch 3/10, Batch 763/883, Training Loss: 0.6514\n",
      "Epoch 3/10, Batch 764/883, Training Loss: 0.7415\n",
      "Epoch 3/10, Batch 765/883, Training Loss: 0.9042\n",
      "Epoch 3/10, Batch 766/883, Training Loss: 0.6427\n",
      "Epoch 3/10, Batch 767/883, Training Loss: 0.5827\n",
      "Epoch 3/10, Batch 768/883, Training Loss: 0.6852\n",
      "Epoch 3/10, Batch 769/883, Training Loss: 0.8532\n",
      "Epoch 3/10, Batch 770/883, Training Loss: 0.8571\n",
      "Epoch 3/10, Batch 771/883, Training Loss: 0.9086\n",
      "Epoch 3/10, Batch 772/883, Training Loss: 1.0015\n",
      "Epoch 3/10, Batch 773/883, Training Loss: 0.9051\n",
      "Epoch 3/10, Batch 774/883, Training Loss: 0.9016\n",
      "Epoch 3/10, Batch 775/883, Training Loss: 0.9547\n",
      "Epoch 3/10, Batch 776/883, Training Loss: 1.2082\n",
      "Epoch 3/10, Batch 777/883, Training Loss: 0.9104\n",
      "Epoch 3/10, Batch 778/883, Training Loss: 0.7042\n",
      "Epoch 3/10, Batch 779/883, Training Loss: 0.9326\n",
      "Epoch 3/10, Batch 780/883, Training Loss: 0.8294\n",
      "Epoch 3/10, Batch 781/883, Training Loss: 0.7347\n",
      "Epoch 3/10, Batch 782/883, Training Loss: 0.8186\n",
      "Epoch 3/10, Batch 783/883, Training Loss: 0.8632\n",
      "Epoch 3/10, Batch 784/883, Training Loss: 0.7658\n",
      "Epoch 3/10, Batch 785/883, Training Loss: 0.7629\n",
      "Epoch 3/10, Batch 786/883, Training Loss: 0.6445\n",
      "Epoch 3/10, Batch 787/883, Training Loss: 0.9376\n",
      "Epoch 3/10, Batch 788/883, Training Loss: 0.8394\n",
      "Epoch 3/10, Batch 789/883, Training Loss: 0.8133\n",
      "Epoch 3/10, Batch 790/883, Training Loss: 0.8284\n",
      "Epoch 3/10, Batch 791/883, Training Loss: 0.6054\n",
      "Epoch 3/10, Batch 792/883, Training Loss: 0.8938\n",
      "Epoch 3/10, Batch 793/883, Training Loss: 0.8120\n",
      "Epoch 3/10, Batch 794/883, Training Loss: 0.5796\n",
      "Epoch 3/10, Batch 795/883, Training Loss: 0.6446\n",
      "Epoch 3/10, Batch 796/883, Training Loss: 0.6274\n",
      "Epoch 3/10, Batch 797/883, Training Loss: 0.6772\n",
      "Epoch 3/10, Batch 798/883, Training Loss: 0.6197\n",
      "Epoch 3/10, Batch 799/883, Training Loss: 0.8301\n",
      "Epoch 3/10, Batch 800/883, Training Loss: 1.0332\n",
      "Epoch 3/10, Batch 801/883, Training Loss: 0.8302\n",
      "Epoch 3/10, Batch 802/883, Training Loss: 0.9111\n",
      "Epoch 3/10, Batch 803/883, Training Loss: 0.6426\n",
      "Epoch 3/10, Batch 804/883, Training Loss: 0.9678\n",
      "Epoch 3/10, Batch 805/883, Training Loss: 0.6308\n",
      "Epoch 3/10, Batch 806/883, Training Loss: 0.6559\n",
      "Epoch 3/10, Batch 807/883, Training Loss: 0.6343\n",
      "Epoch 3/10, Batch 808/883, Training Loss: 0.6548\n",
      "Epoch 3/10, Batch 809/883, Training Loss: 0.6387\n",
      "Epoch 3/10, Batch 810/883, Training Loss: 0.7736\n",
      "Epoch 3/10, Batch 811/883, Training Loss: 0.7169\n",
      "Epoch 3/10, Batch 812/883, Training Loss: 0.7588\n",
      "Epoch 3/10, Batch 813/883, Training Loss: 0.6330\n",
      "Epoch 3/10, Batch 814/883, Training Loss: 0.8042\n",
      "Epoch 3/10, Batch 815/883, Training Loss: 0.9242\n",
      "Epoch 3/10, Batch 816/883, Training Loss: 0.8904\n",
      "Epoch 3/10, Batch 817/883, Training Loss: 0.7108\n",
      "Epoch 3/10, Batch 818/883, Training Loss: 0.9402\n",
      "Epoch 3/10, Batch 819/883, Training Loss: 0.6630\n",
      "Epoch 3/10, Batch 820/883, Training Loss: 0.8536\n",
      "Epoch 3/10, Batch 821/883, Training Loss: 0.9338\n",
      "Epoch 3/10, Batch 822/883, Training Loss: 0.7427\n",
      "Epoch 3/10, Batch 823/883, Training Loss: 0.7507\n",
      "Epoch 3/10, Batch 824/883, Training Loss: 0.7399\n",
      "Epoch 3/10, Batch 825/883, Training Loss: 0.8434\n",
      "Epoch 3/10, Batch 826/883, Training Loss: 0.7928\n",
      "Epoch 3/10, Batch 827/883, Training Loss: 0.8481\n",
      "Epoch 3/10, Batch 828/883, Training Loss: 0.5760\n",
      "Epoch 3/10, Batch 829/883, Training Loss: 0.6827\n",
      "Epoch 3/10, Batch 830/883, Training Loss: 0.6609\n",
      "Epoch 3/10, Batch 831/883, Training Loss: 0.9971\n",
      "Epoch 3/10, Batch 832/883, Training Loss: 0.9159\n",
      "Epoch 3/10, Batch 833/883, Training Loss: 0.9319\n",
      "Epoch 3/10, Batch 834/883, Training Loss: 0.8731\n",
      "Epoch 3/10, Batch 835/883, Training Loss: 1.2021\n",
      "Epoch 3/10, Batch 836/883, Training Loss: 0.9062\n",
      "Epoch 3/10, Batch 837/883, Training Loss: 0.9999\n",
      "Epoch 3/10, Batch 838/883, Training Loss: 0.6048\n",
      "Epoch 3/10, Batch 839/883, Training Loss: 0.6219\n",
      "Epoch 3/10, Batch 840/883, Training Loss: 0.7907\n",
      "Epoch 3/10, Batch 841/883, Training Loss: 0.8025\n",
      "Epoch 3/10, Batch 842/883, Training Loss: 0.8421\n",
      "Epoch 3/10, Batch 843/883, Training Loss: 0.9420\n",
      "Epoch 3/10, Batch 844/883, Training Loss: 0.7406\n",
      "Epoch 3/10, Batch 845/883, Training Loss: 0.9986\n",
      "Epoch 3/10, Batch 846/883, Training Loss: 0.8012\n",
      "Epoch 3/10, Batch 847/883, Training Loss: 0.6443\n",
      "Epoch 3/10, Batch 848/883, Training Loss: 0.6452\n",
      "Epoch 3/10, Batch 849/883, Training Loss: 0.9940\n",
      "Epoch 3/10, Batch 850/883, Training Loss: 0.7395\n",
      "Epoch 3/10, Batch 851/883, Training Loss: 0.7852\n",
      "Epoch 3/10, Batch 852/883, Training Loss: 0.7434\n",
      "Epoch 3/10, Batch 853/883, Training Loss: 0.9843\n",
      "Epoch 3/10, Batch 854/883, Training Loss: 0.6130\n",
      "Epoch 3/10, Batch 855/883, Training Loss: 1.0980\n",
      "Epoch 3/10, Batch 856/883, Training Loss: 0.8709\n",
      "Epoch 3/10, Batch 857/883, Training Loss: 0.5582\n",
      "Epoch 3/10, Batch 858/883, Training Loss: 0.7753\n",
      "Epoch 3/10, Batch 859/883, Training Loss: 0.6657\n",
      "Epoch 3/10, Batch 860/883, Training Loss: 0.9964\n",
      "Epoch 3/10, Batch 861/883, Training Loss: 0.9100\n",
      "Epoch 3/10, Batch 862/883, Training Loss: 0.9567\n",
      "Epoch 3/10, Batch 863/883, Training Loss: 0.7457\n",
      "Epoch 3/10, Batch 864/883, Training Loss: 0.7012\n",
      "Epoch 3/10, Batch 865/883, Training Loss: 0.6851\n",
      "Epoch 3/10, Batch 866/883, Training Loss: 0.9424\n",
      "Epoch 3/10, Batch 867/883, Training Loss: 0.6342\n",
      "Epoch 3/10, Batch 868/883, Training Loss: 0.6326\n",
      "Epoch 3/10, Batch 869/883, Training Loss: 0.8556\n",
      "Epoch 3/10, Batch 870/883, Training Loss: 0.9937\n",
      "Epoch 3/10, Batch 871/883, Training Loss: 0.8024\n",
      "Epoch 3/10, Batch 872/883, Training Loss: 0.8485\n",
      "Epoch 3/10, Batch 873/883, Training Loss: 0.7762\n",
      "Epoch 3/10, Batch 874/883, Training Loss: 0.5579\n",
      "Epoch 3/10, Batch 875/883, Training Loss: 0.8227\n",
      "Epoch 3/10, Batch 876/883, Training Loss: 0.9099\n",
      "Epoch 3/10, Batch 877/883, Training Loss: 0.7673\n",
      "Epoch 3/10, Batch 878/883, Training Loss: 0.7528\n",
      "Epoch 3/10, Batch 879/883, Training Loss: 0.6706\n",
      "Epoch 3/10, Batch 880/883, Training Loss: 0.7199\n",
      "Epoch 3/10, Batch 881/883, Training Loss: 0.9126\n",
      "Epoch 3/10, Batch 882/883, Training Loss: 1.2960\n",
      "Epoch 3/10, Batch 883/883, Training Loss: 0.6490\n",
      "Epoch 3/10, Training Loss: 0.7990, Validation Loss: 2.0011, Validation Accuracy: 0.5573\n",
      "Epoch 4/10, Batch 1/883, Training Loss: 0.9054\n",
      "Epoch 4/10, Batch 2/883, Training Loss: 0.8542\n",
      "Epoch 4/10, Batch 3/883, Training Loss: 0.7504\n",
      "Epoch 4/10, Batch 4/883, Training Loss: 0.5947\n",
      "Epoch 4/10, Batch 5/883, Training Loss: 0.9141\n",
      "Epoch 4/10, Batch 6/883, Training Loss: 0.8388\n",
      "Epoch 4/10, Batch 7/883, Training Loss: 0.6741\n",
      "Epoch 4/10, Batch 8/883, Training Loss: 0.7463\n",
      "Epoch 4/10, Batch 9/883, Training Loss: 1.1162\n",
      "Epoch 4/10, Batch 10/883, Training Loss: 0.7791\n",
      "Epoch 4/10, Batch 11/883, Training Loss: 0.7646\n",
      "Epoch 4/10, Batch 12/883, Training Loss: 0.7076\n",
      "Epoch 4/10, Batch 13/883, Training Loss: 0.6620\n",
      "Epoch 4/10, Batch 14/883, Training Loss: 0.5973\n",
      "Epoch 4/10, Batch 15/883, Training Loss: 0.7949\n",
      "Epoch 4/10, Batch 16/883, Training Loss: 0.6567\n",
      "Epoch 4/10, Batch 17/883, Training Loss: 0.8979\n",
      "Epoch 4/10, Batch 18/883, Training Loss: 0.6958\n",
      "Epoch 4/10, Batch 19/883, Training Loss: 0.6244\n",
      "Epoch 4/10, Batch 20/883, Training Loss: 0.7534\n",
      "Epoch 4/10, Batch 21/883, Training Loss: 0.6985\n",
      "Epoch 4/10, Batch 22/883, Training Loss: 0.8934\n",
      "Epoch 4/10, Batch 23/883, Training Loss: 0.7975\n",
      "Epoch 4/10, Batch 24/883, Training Loss: 1.2639\n",
      "Epoch 4/10, Batch 25/883, Training Loss: 0.8561\n",
      "Epoch 4/10, Batch 26/883, Training Loss: 1.0674\n",
      "Epoch 4/10, Batch 27/883, Training Loss: 0.8528\n",
      "Epoch 4/10, Batch 28/883, Training Loss: 0.7026\n",
      "Epoch 4/10, Batch 29/883, Training Loss: 0.8178\n",
      "Epoch 4/10, Batch 30/883, Training Loss: 0.8379\n",
      "Epoch 4/10, Batch 31/883, Training Loss: 0.8075\n",
      "Epoch 4/10, Batch 32/883, Training Loss: 0.6967\n",
      "Epoch 4/10, Batch 33/883, Training Loss: 0.7666\n",
      "Epoch 4/10, Batch 34/883, Training Loss: 0.8597\n",
      "Epoch 4/10, Batch 35/883, Training Loss: 0.7332\n",
      "Epoch 4/10, Batch 36/883, Training Loss: 0.7146\n",
      "Epoch 4/10, Batch 37/883, Training Loss: 0.9313\n",
      "Epoch 4/10, Batch 38/883, Training Loss: 0.9297\n",
      "Epoch 4/10, Batch 39/883, Training Loss: 1.0094\n",
      "Epoch 4/10, Batch 40/883, Training Loss: 1.1303\n",
      "Epoch 4/10, Batch 41/883, Training Loss: 0.8756\n",
      "Epoch 4/10, Batch 42/883, Training Loss: 0.7099\n",
      "Epoch 4/10, Batch 43/883, Training Loss: 0.7775\n",
      "Epoch 4/10, Batch 44/883, Training Loss: 0.7283\n",
      "Epoch 4/10, Batch 45/883, Training Loss: 0.8437\n",
      "Epoch 4/10, Batch 46/883, Training Loss: 0.9428\n",
      "Epoch 4/10, Batch 47/883, Training Loss: 0.7384\n",
      "Epoch 4/10, Batch 48/883, Training Loss: 0.6556\n",
      "Epoch 4/10, Batch 49/883, Training Loss: 0.8842\n",
      "Epoch 4/10, Batch 50/883, Training Loss: 0.7289\n",
      "Epoch 4/10, Batch 51/883, Training Loss: 0.6661\n",
      "Epoch 4/10, Batch 52/883, Training Loss: 0.6059\n",
      "Epoch 4/10, Batch 53/883, Training Loss: 0.8131\n",
      "Epoch 4/10, Batch 54/883, Training Loss: 0.8101\n",
      "Epoch 4/10, Batch 55/883, Training Loss: 0.8208\n",
      "Epoch 4/10, Batch 56/883, Training Loss: 0.8028\n",
      "Epoch 4/10, Batch 57/883, Training Loss: 0.8489\n",
      "Epoch 4/10, Batch 58/883, Training Loss: 0.7085\n",
      "Epoch 4/10, Batch 59/883, Training Loss: 0.5776\n",
      "Epoch 4/10, Batch 60/883, Training Loss: 0.8359\n",
      "Epoch 4/10, Batch 61/883, Training Loss: 0.7549\n",
      "Epoch 4/10, Batch 62/883, Training Loss: 1.3003\n",
      "Epoch 4/10, Batch 63/883, Training Loss: 1.0394\n",
      "Epoch 4/10, Batch 64/883, Training Loss: 0.7641\n",
      "Epoch 4/10, Batch 65/883, Training Loss: 0.7888\n",
      "Epoch 4/10, Batch 66/883, Training Loss: 0.8908\n",
      "Epoch 4/10, Batch 67/883, Training Loss: 0.9939\n",
      "Epoch 4/10, Batch 68/883, Training Loss: 0.7073\n",
      "Epoch 4/10, Batch 69/883, Training Loss: 0.8913\n",
      "Epoch 4/10, Batch 70/883, Training Loss: 0.9364\n",
      "Epoch 4/10, Batch 71/883, Training Loss: 0.7034\n",
      "Epoch 4/10, Batch 72/883, Training Loss: 0.5901\n",
      "Epoch 4/10, Batch 73/883, Training Loss: 0.9520\n",
      "Epoch 4/10, Batch 74/883, Training Loss: 1.0752\n",
      "Epoch 4/10, Batch 75/883, Training Loss: 0.6688\n",
      "Epoch 4/10, Batch 76/883, Training Loss: 0.8467\n",
      "Epoch 4/10, Batch 77/883, Training Loss: 0.6760\n",
      "Epoch 4/10, Batch 78/883, Training Loss: 0.7357\n",
      "Epoch 4/10, Batch 79/883, Training Loss: 0.7556\n",
      "Epoch 4/10, Batch 80/883, Training Loss: 0.7615\n",
      "Epoch 4/10, Batch 81/883, Training Loss: 0.7618\n",
      "Epoch 4/10, Batch 82/883, Training Loss: 0.7888\n",
      "Epoch 4/10, Batch 83/883, Training Loss: 0.7186\n",
      "Epoch 4/10, Batch 84/883, Training Loss: 0.8096\n",
      "Epoch 4/10, Batch 85/883, Training Loss: 0.8092\n",
      "Epoch 4/10, Batch 86/883, Training Loss: 0.8985\n",
      "Epoch 4/10, Batch 87/883, Training Loss: 0.5882\n",
      "Epoch 4/10, Batch 88/883, Training Loss: 0.4549\n",
      "Epoch 4/10, Batch 89/883, Training Loss: 0.9159\n",
      "Epoch 4/10, Batch 90/883, Training Loss: 0.9161\n",
      "Epoch 4/10, Batch 91/883, Training Loss: 0.8043\n",
      "Epoch 4/10, Batch 92/883, Training Loss: 0.8822\n",
      "Epoch 4/10, Batch 93/883, Training Loss: 0.6875\n",
      "Epoch 4/10, Batch 94/883, Training Loss: 0.8014\n",
      "Epoch 4/10, Batch 95/883, Training Loss: 0.7219\n",
      "Epoch 4/10, Batch 96/883, Training Loss: 0.7410\n",
      "Epoch 4/10, Batch 97/883, Training Loss: 0.7070\n",
      "Epoch 4/10, Batch 98/883, Training Loss: 0.6436\n",
      "Epoch 4/10, Batch 99/883, Training Loss: 0.6088\n",
      "Epoch 4/10, Batch 100/883, Training Loss: 0.6649\n",
      "Epoch 4/10, Batch 101/883, Training Loss: 0.8024\n",
      "Epoch 4/10, Batch 102/883, Training Loss: 0.5744\n",
      "Epoch 4/10, Batch 103/883, Training Loss: 0.6502\n",
      "Epoch 4/10, Batch 104/883, Training Loss: 0.9233\n",
      "Epoch 4/10, Batch 105/883, Training Loss: 0.8805\n",
      "Epoch 4/10, Batch 106/883, Training Loss: 0.8535\n",
      "Epoch 4/10, Batch 107/883, Training Loss: 0.7921\n",
      "Epoch 4/10, Batch 108/883, Training Loss: 0.7267\n",
      "Epoch 4/10, Batch 109/883, Training Loss: 0.5050\n",
      "Epoch 4/10, Batch 110/883, Training Loss: 1.1170\n",
      "Epoch 4/10, Batch 111/883, Training Loss: 0.6116\n",
      "Epoch 4/10, Batch 112/883, Training Loss: 0.6584\n",
      "Epoch 4/10, Batch 113/883, Training Loss: 0.7259\n",
      "Epoch 4/10, Batch 114/883, Training Loss: 0.7345\n",
      "Epoch 4/10, Batch 115/883, Training Loss: 0.5374\n",
      "Epoch 4/10, Batch 116/883, Training Loss: 0.8264\n",
      "Epoch 4/10, Batch 117/883, Training Loss: 0.9283\n",
      "Epoch 4/10, Batch 118/883, Training Loss: 0.9109\n",
      "Epoch 4/10, Batch 119/883, Training Loss: 0.7871\n",
      "Epoch 4/10, Batch 120/883, Training Loss: 0.8197\n",
      "Epoch 4/10, Batch 121/883, Training Loss: 0.9814\n",
      "Epoch 4/10, Batch 122/883, Training Loss: 0.8353\n",
      "Epoch 4/10, Batch 123/883, Training Loss: 0.8565\n",
      "Epoch 4/10, Batch 124/883, Training Loss: 0.6273\n",
      "Epoch 4/10, Batch 125/883, Training Loss: 0.6538\n",
      "Epoch 4/10, Batch 126/883, Training Loss: 1.0162\n",
      "Epoch 4/10, Batch 127/883, Training Loss: 0.6474\n",
      "Epoch 4/10, Batch 128/883, Training Loss: 0.8954\n",
      "Epoch 4/10, Batch 129/883, Training Loss: 0.8475\n",
      "Epoch 4/10, Batch 130/883, Training Loss: 0.7972\n",
      "Epoch 4/10, Batch 131/883, Training Loss: 0.7298\n",
      "Epoch 4/10, Batch 132/883, Training Loss: 0.5030\n",
      "Epoch 4/10, Batch 133/883, Training Loss: 0.7415\n",
      "Epoch 4/10, Batch 134/883, Training Loss: 0.4714\n",
      "Epoch 4/10, Batch 135/883, Training Loss: 0.7402\n",
      "Epoch 4/10, Batch 136/883, Training Loss: 0.7938\n",
      "Epoch 4/10, Batch 137/883, Training Loss: 0.7129\n",
      "Epoch 4/10, Batch 138/883, Training Loss: 0.9758\n",
      "Epoch 4/10, Batch 139/883, Training Loss: 0.7183\n",
      "Epoch 4/10, Batch 140/883, Training Loss: 0.8205\n",
      "Epoch 4/10, Batch 141/883, Training Loss: 0.6236\n",
      "Epoch 4/10, Batch 142/883, Training Loss: 0.5863\n",
      "Epoch 4/10, Batch 143/883, Training Loss: 0.9671\n",
      "Epoch 4/10, Batch 144/883, Training Loss: 0.6204\n",
      "Epoch 4/10, Batch 145/883, Training Loss: 0.7189\n",
      "Epoch 4/10, Batch 146/883, Training Loss: 0.5377\n",
      "Epoch 4/10, Batch 147/883, Training Loss: 0.8564\n",
      "Epoch 4/10, Batch 148/883, Training Loss: 0.6219\n",
      "Epoch 4/10, Batch 149/883, Training Loss: 0.7807\n",
      "Epoch 4/10, Batch 150/883, Training Loss: 0.6317\n",
      "Epoch 4/10, Batch 151/883, Training Loss: 0.7718\n",
      "Epoch 4/10, Batch 152/883, Training Loss: 0.8707\n",
      "Epoch 4/10, Batch 153/883, Training Loss: 0.7279\n",
      "Epoch 4/10, Batch 154/883, Training Loss: 0.7753\n",
      "Epoch 4/10, Batch 155/883, Training Loss: 0.6904\n",
      "Epoch 4/10, Batch 156/883, Training Loss: 0.6903\n",
      "Epoch 4/10, Batch 157/883, Training Loss: 0.8361\n",
      "Epoch 4/10, Batch 158/883, Training Loss: 0.9488\n",
      "Epoch 4/10, Batch 159/883, Training Loss: 0.8202\n",
      "Epoch 4/10, Batch 160/883, Training Loss: 0.4836\n",
      "Epoch 4/10, Batch 161/883, Training Loss: 0.7572\n",
      "Epoch 4/10, Batch 162/883, Training Loss: 0.4701\n",
      "Epoch 4/10, Batch 163/883, Training Loss: 0.6384\n",
      "Epoch 4/10, Batch 164/883, Training Loss: 0.7434\n",
      "Epoch 4/10, Batch 165/883, Training Loss: 0.9810\n",
      "Epoch 4/10, Batch 166/883, Training Loss: 0.9900\n",
      "Epoch 4/10, Batch 167/883, Training Loss: 0.7895\n",
      "Epoch 4/10, Batch 168/883, Training Loss: 0.4256\n",
      "Epoch 4/10, Batch 169/883, Training Loss: 0.8436\n",
      "Epoch 4/10, Batch 170/883, Training Loss: 0.8980\n",
      "Epoch 4/10, Batch 171/883, Training Loss: 0.7016\n",
      "Epoch 4/10, Batch 172/883, Training Loss: 0.7105\n",
      "Epoch 4/10, Batch 173/883, Training Loss: 0.7546\n",
      "Epoch 4/10, Batch 174/883, Training Loss: 0.8462\n",
      "Epoch 4/10, Batch 175/883, Training Loss: 0.8940\n",
      "Epoch 4/10, Batch 176/883, Training Loss: 1.2118\n",
      "Epoch 4/10, Batch 177/883, Training Loss: 0.9226\n",
      "Epoch 4/10, Batch 178/883, Training Loss: 0.8085\n",
      "Epoch 4/10, Batch 179/883, Training Loss: 0.7986\n",
      "Epoch 4/10, Batch 180/883, Training Loss: 0.8123\n",
      "Epoch 4/10, Batch 181/883, Training Loss: 0.6641\n",
      "Epoch 4/10, Batch 182/883, Training Loss: 0.9577\n",
      "Epoch 4/10, Batch 183/883, Training Loss: 0.8201\n",
      "Epoch 4/10, Batch 184/883, Training Loss: 0.6642\n",
      "Epoch 4/10, Batch 185/883, Training Loss: 0.8430\n",
      "Epoch 4/10, Batch 186/883, Training Loss: 0.5445\n",
      "Epoch 4/10, Batch 187/883, Training Loss: 0.6088\n",
      "Epoch 4/10, Batch 188/883, Training Loss: 0.8167\n",
      "Epoch 4/10, Batch 189/883, Training Loss: 0.8363\n",
      "Epoch 4/10, Batch 190/883, Training Loss: 0.5917\n",
      "Epoch 4/10, Batch 191/883, Training Loss: 0.9431\n",
      "Epoch 4/10, Batch 192/883, Training Loss: 0.7480\n",
      "Epoch 4/10, Batch 193/883, Training Loss: 0.8032\n",
      "Epoch 4/10, Batch 194/883, Training Loss: 0.7865\n",
      "Epoch 4/10, Batch 195/883, Training Loss: 0.5988\n",
      "Epoch 4/10, Batch 196/883, Training Loss: 0.8725\n",
      "Epoch 4/10, Batch 197/883, Training Loss: 0.9023\n",
      "Epoch 4/10, Batch 198/883, Training Loss: 0.9644\n",
      "Epoch 4/10, Batch 199/883, Training Loss: 0.7620\n",
      "Epoch 4/10, Batch 200/883, Training Loss: 1.0920\n",
      "Epoch 4/10, Batch 201/883, Training Loss: 0.7036\n",
      "Epoch 4/10, Batch 202/883, Training Loss: 0.5914\n",
      "Epoch 4/10, Batch 203/883, Training Loss: 0.7071\n",
      "Epoch 4/10, Batch 204/883, Training Loss: 0.9515\n",
      "Epoch 4/10, Batch 205/883, Training Loss: 0.7760\n",
      "Epoch 4/10, Batch 206/883, Training Loss: 0.9645\n",
      "Epoch 4/10, Batch 207/883, Training Loss: 0.7558\n",
      "Epoch 4/10, Batch 208/883, Training Loss: 0.8313\n",
      "Epoch 4/10, Batch 209/883, Training Loss: 0.6010\n",
      "Epoch 4/10, Batch 210/883, Training Loss: 0.6261\n",
      "Epoch 4/10, Batch 211/883, Training Loss: 0.5546\n",
      "Epoch 4/10, Batch 212/883, Training Loss: 0.4997\n",
      "Epoch 4/10, Batch 213/883, Training Loss: 0.6259\n",
      "Epoch 4/10, Batch 214/883, Training Loss: 0.6619\n",
      "Epoch 4/10, Batch 215/883, Training Loss: 0.5465\n",
      "Epoch 4/10, Batch 216/883, Training Loss: 0.5547\n",
      "Epoch 4/10, Batch 217/883, Training Loss: 0.9950\n",
      "Epoch 4/10, Batch 218/883, Training Loss: 0.7582\n",
      "Epoch 4/10, Batch 219/883, Training Loss: 0.6987\n",
      "Epoch 4/10, Batch 220/883, Training Loss: 0.7577\n",
      "Epoch 4/10, Batch 221/883, Training Loss: 0.5869\n",
      "Epoch 4/10, Batch 222/883, Training Loss: 0.9714\n",
      "Epoch 4/10, Batch 223/883, Training Loss: 1.0181\n",
      "Epoch 4/10, Batch 224/883, Training Loss: 0.7902\n",
      "Epoch 4/10, Batch 225/883, Training Loss: 0.8799\n",
      "Epoch 4/10, Batch 226/883, Training Loss: 0.8698\n",
      "Epoch 4/10, Batch 227/883, Training Loss: 1.1116\n",
      "Epoch 4/10, Batch 228/883, Training Loss: 0.7925\n",
      "Epoch 4/10, Batch 229/883, Training Loss: 0.7928\n",
      "Epoch 4/10, Batch 230/883, Training Loss: 0.9418\n",
      "Epoch 4/10, Batch 231/883, Training Loss: 0.8943\n",
      "Epoch 4/10, Batch 232/883, Training Loss: 0.5309\n",
      "Epoch 4/10, Batch 233/883, Training Loss: 0.8798\n",
      "Epoch 4/10, Batch 234/883, Training Loss: 0.6373\n",
      "Epoch 4/10, Batch 235/883, Training Loss: 0.8426\n",
      "Epoch 4/10, Batch 236/883, Training Loss: 0.6405\n",
      "Epoch 4/10, Batch 237/883, Training Loss: 0.7251\n",
      "Epoch 4/10, Batch 238/883, Training Loss: 0.6551\n",
      "Epoch 4/10, Batch 239/883, Training Loss: 0.8608\n",
      "Epoch 4/10, Batch 240/883, Training Loss: 0.6921\n",
      "Epoch 4/10, Batch 241/883, Training Loss: 0.8197\n",
      "Epoch 4/10, Batch 242/883, Training Loss: 0.8414\n",
      "Epoch 4/10, Batch 243/883, Training Loss: 0.6612\n",
      "Epoch 4/10, Batch 244/883, Training Loss: 0.7467\n",
      "Epoch 4/10, Batch 245/883, Training Loss: 0.5236\n",
      "Epoch 4/10, Batch 246/883, Training Loss: 0.6394\n",
      "Epoch 4/10, Batch 247/883, Training Loss: 0.8058\n",
      "Epoch 4/10, Batch 248/883, Training Loss: 1.1819\n",
      "Epoch 4/10, Batch 249/883, Training Loss: 0.3935\n",
      "Epoch 4/10, Batch 250/883, Training Loss: 0.4141\n",
      "Epoch 4/10, Batch 251/883, Training Loss: 0.8780\n",
      "Epoch 4/10, Batch 252/883, Training Loss: 1.3836\n",
      "Epoch 4/10, Batch 253/883, Training Loss: 1.0515\n",
      "Epoch 4/10, Batch 254/883, Training Loss: 0.7059\n",
      "Epoch 4/10, Batch 255/883, Training Loss: 1.3113\n",
      "Epoch 4/10, Batch 256/883, Training Loss: 0.8651\n",
      "Epoch 4/10, Batch 257/883, Training Loss: 0.8431\n",
      "Epoch 4/10, Batch 258/883, Training Loss: 0.5298\n",
      "Epoch 4/10, Batch 259/883, Training Loss: 0.8983\n",
      "Epoch 4/10, Batch 260/883, Training Loss: 0.6406\n",
      "Epoch 4/10, Batch 261/883, Training Loss: 0.7492\n",
      "Epoch 4/10, Batch 262/883, Training Loss: 0.7638\n",
      "Epoch 4/10, Batch 263/883, Training Loss: 0.7348\n",
      "Epoch 4/10, Batch 264/883, Training Loss: 0.5689\n",
      "Epoch 4/10, Batch 265/883, Training Loss: 0.7688\n",
      "Epoch 4/10, Batch 266/883, Training Loss: 0.8692\n",
      "Epoch 4/10, Batch 267/883, Training Loss: 0.8199\n",
      "Epoch 4/10, Batch 268/883, Training Loss: 0.8197\n",
      "Epoch 4/10, Batch 269/883, Training Loss: 0.7921\n",
      "Epoch 4/10, Batch 270/883, Training Loss: 0.6851\n",
      "Epoch 4/10, Batch 271/883, Training Loss: 0.8837\n",
      "Epoch 4/10, Batch 272/883, Training Loss: 0.5862\n",
      "Epoch 4/10, Batch 273/883, Training Loss: 0.6737\n",
      "Epoch 4/10, Batch 274/883, Training Loss: 0.7195\n",
      "Epoch 4/10, Batch 275/883, Training Loss: 1.0913\n",
      "Epoch 4/10, Batch 276/883, Training Loss: 0.7285\n",
      "Epoch 4/10, Batch 277/883, Training Loss: 0.6651\n",
      "Epoch 4/10, Batch 278/883, Training Loss: 1.2600\n",
      "Epoch 4/10, Batch 279/883, Training Loss: 0.8676\n",
      "Epoch 4/10, Batch 280/883, Training Loss: 1.0195\n",
      "Epoch 4/10, Batch 281/883, Training Loss: 0.6332\n",
      "Epoch 4/10, Batch 282/883, Training Loss: 0.5725\n",
      "Epoch 4/10, Batch 283/883, Training Loss: 0.9012\n",
      "Epoch 4/10, Batch 284/883, Training Loss: 0.6340\n",
      "Epoch 4/10, Batch 285/883, Training Loss: 0.7967\n",
      "Epoch 4/10, Batch 286/883, Training Loss: 0.5622\n",
      "Epoch 4/10, Batch 287/883, Training Loss: 0.8031\n",
      "Epoch 4/10, Batch 288/883, Training Loss: 1.0621\n",
      "Epoch 4/10, Batch 289/883, Training Loss: 0.8072\n",
      "Epoch 4/10, Batch 290/883, Training Loss: 0.7435\n",
      "Epoch 4/10, Batch 291/883, Training Loss: 0.6448\n",
      "Epoch 4/10, Batch 292/883, Training Loss: 0.8023\n",
      "Epoch 4/10, Batch 293/883, Training Loss: 0.6318\n",
      "Epoch 4/10, Batch 294/883, Training Loss: 0.8953\n",
      "Epoch 4/10, Batch 295/883, Training Loss: 0.6113\n",
      "Epoch 4/10, Batch 296/883, Training Loss: 0.7350\n",
      "Epoch 4/10, Batch 297/883, Training Loss: 0.7733\n",
      "Epoch 4/10, Batch 298/883, Training Loss: 0.6985\n",
      "Epoch 4/10, Batch 299/883, Training Loss: 0.6572\n",
      "Epoch 4/10, Batch 300/883, Training Loss: 0.9263\n",
      "Epoch 4/10, Batch 301/883, Training Loss: 0.9109\n",
      "Epoch 4/10, Batch 302/883, Training Loss: 0.5804\n",
      "Epoch 4/10, Batch 303/883, Training Loss: 0.8448\n",
      "Epoch 4/10, Batch 304/883, Training Loss: 0.5904\n",
      "Epoch 4/10, Batch 305/883, Training Loss: 0.9071\n",
      "Epoch 4/10, Batch 306/883, Training Loss: 0.4667\n",
      "Epoch 4/10, Batch 307/883, Training Loss: 0.7015\n",
      "Epoch 4/10, Batch 308/883, Training Loss: 0.6415\n",
      "Epoch 4/10, Batch 309/883, Training Loss: 1.0311\n",
      "Epoch 4/10, Batch 310/883, Training Loss: 1.1088\n",
      "Epoch 4/10, Batch 311/883, Training Loss: 0.8739\n",
      "Epoch 4/10, Batch 312/883, Training Loss: 0.8904\n",
      "Epoch 4/10, Batch 313/883, Training Loss: 0.5142\n",
      "Epoch 4/10, Batch 314/883, Training Loss: 0.8188\n",
      "Epoch 4/10, Batch 315/883, Training Loss: 0.8066\n",
      "Epoch 4/10, Batch 316/883, Training Loss: 1.0014\n",
      "Epoch 4/10, Batch 317/883, Training Loss: 1.0697\n",
      "Epoch 4/10, Batch 318/883, Training Loss: 0.8590\n",
      "Epoch 4/10, Batch 319/883, Training Loss: 0.7478\n",
      "Epoch 4/10, Batch 320/883, Training Loss: 0.7093\n",
      "Epoch 4/10, Batch 321/883, Training Loss: 0.8414\n",
      "Epoch 4/10, Batch 322/883, Training Loss: 0.7666\n",
      "Epoch 4/10, Batch 323/883, Training Loss: 0.6944\n",
      "Epoch 4/10, Batch 324/883, Training Loss: 0.7654\n",
      "Epoch 4/10, Batch 325/883, Training Loss: 1.0756\n",
      "Epoch 4/10, Batch 326/883, Training Loss: 0.6938\n",
      "Epoch 4/10, Batch 327/883, Training Loss: 0.7267\n",
      "Epoch 4/10, Batch 328/883, Training Loss: 0.7074\n",
      "Epoch 4/10, Batch 329/883, Training Loss: 0.8820\n",
      "Epoch 4/10, Batch 330/883, Training Loss: 0.6909\n",
      "Epoch 4/10, Batch 331/883, Training Loss: 0.6802\n",
      "Epoch 4/10, Batch 332/883, Training Loss: 0.8289\n",
      "Epoch 4/10, Batch 333/883, Training Loss: 0.6260\n",
      "Epoch 4/10, Batch 334/883, Training Loss: 0.9884\n",
      "Epoch 4/10, Batch 335/883, Training Loss: 0.6115\n",
      "Epoch 4/10, Batch 336/883, Training Loss: 1.0220\n",
      "Epoch 4/10, Batch 337/883, Training Loss: 0.5899\n",
      "Epoch 4/10, Batch 338/883, Training Loss: 0.8306\n",
      "Epoch 4/10, Batch 339/883, Training Loss: 0.9385\n",
      "Epoch 4/10, Batch 340/883, Training Loss: 0.7461\n",
      "Epoch 4/10, Batch 341/883, Training Loss: 0.7717\n",
      "Epoch 4/10, Batch 342/883, Training Loss: 0.6584\n",
      "Epoch 4/10, Batch 343/883, Training Loss: 0.7537\n",
      "Epoch 4/10, Batch 344/883, Training Loss: 0.6318\n",
      "Epoch 4/10, Batch 345/883, Training Loss: 0.5980\n",
      "Epoch 4/10, Batch 346/883, Training Loss: 1.0931\n",
      "Epoch 4/10, Batch 347/883, Training Loss: 0.5217\n",
      "Epoch 4/10, Batch 348/883, Training Loss: 0.7733\n",
      "Epoch 4/10, Batch 349/883, Training Loss: 0.9753\n",
      "Epoch 4/10, Batch 350/883, Training Loss: 0.5833\n",
      "Epoch 4/10, Batch 351/883, Training Loss: 1.0055\n",
      "Epoch 4/10, Batch 352/883, Training Loss: 0.8794\n",
      "Epoch 4/10, Batch 353/883, Training Loss: 0.7269\n",
      "Epoch 4/10, Batch 354/883, Training Loss: 1.0686\n",
      "Epoch 4/10, Batch 355/883, Training Loss: 0.5967\n",
      "Epoch 4/10, Batch 356/883, Training Loss: 0.5562\n",
      "Epoch 4/10, Batch 357/883, Training Loss: 0.8073\n",
      "Epoch 4/10, Batch 358/883, Training Loss: 0.6964\n",
      "Epoch 4/10, Batch 359/883, Training Loss: 0.9859\n",
      "Epoch 4/10, Batch 360/883, Training Loss: 0.6138\n",
      "Epoch 4/10, Batch 361/883, Training Loss: 0.9677\n",
      "Epoch 4/10, Batch 362/883, Training Loss: 0.8323\n",
      "Epoch 4/10, Batch 363/883, Training Loss: 0.8272\n",
      "Epoch 4/10, Batch 364/883, Training Loss: 0.6572\n",
      "Epoch 4/10, Batch 365/883, Training Loss: 0.5827\n",
      "Epoch 4/10, Batch 366/883, Training Loss: 0.8154\n",
      "Epoch 4/10, Batch 367/883, Training Loss: 0.6794\n",
      "Epoch 4/10, Batch 368/883, Training Loss: 0.7968\n",
      "Epoch 4/10, Batch 369/883, Training Loss: 0.8643\n",
      "Epoch 4/10, Batch 370/883, Training Loss: 0.7142\n",
      "Epoch 4/10, Batch 371/883, Training Loss: 0.8754\n",
      "Epoch 4/10, Batch 372/883, Training Loss: 0.6135\n",
      "Epoch 4/10, Batch 373/883, Training Loss: 0.8044\n",
      "Epoch 4/10, Batch 374/883, Training Loss: 0.7058\n",
      "Epoch 4/10, Batch 375/883, Training Loss: 0.9143\n",
      "Epoch 4/10, Batch 376/883, Training Loss: 0.7558\n",
      "Epoch 4/10, Batch 377/883, Training Loss: 0.8637\n",
      "Epoch 4/10, Batch 378/883, Training Loss: 0.7640\n",
      "Epoch 4/10, Batch 379/883, Training Loss: 0.5545\n",
      "Epoch 4/10, Batch 380/883, Training Loss: 0.5971\n",
      "Epoch 4/10, Batch 381/883, Training Loss: 0.9224\n",
      "Epoch 4/10, Batch 382/883, Training Loss: 0.6948\n",
      "Epoch 4/10, Batch 383/883, Training Loss: 0.6461\n",
      "Epoch 4/10, Batch 384/883, Training Loss: 0.8969\n",
      "Epoch 4/10, Batch 385/883, Training Loss: 0.9104\n",
      "Epoch 4/10, Batch 386/883, Training Loss: 0.7470\n",
      "Epoch 4/10, Batch 387/883, Training Loss: 0.8303\n",
      "Epoch 4/10, Batch 388/883, Training Loss: 0.9630\n",
      "Epoch 4/10, Batch 389/883, Training Loss: 0.8653\n",
      "Epoch 4/10, Batch 390/883, Training Loss: 0.7039\n",
      "Epoch 4/10, Batch 391/883, Training Loss: 0.9974\n",
      "Epoch 4/10, Batch 392/883, Training Loss: 0.6587\n",
      "Epoch 4/10, Batch 393/883, Training Loss: 0.7574\n",
      "Epoch 4/10, Batch 394/883, Training Loss: 0.8775\n",
      "Epoch 4/10, Batch 395/883, Training Loss: 0.8364\n",
      "Epoch 4/10, Batch 396/883, Training Loss: 0.7441\n",
      "Epoch 4/10, Batch 397/883, Training Loss: 0.7876\n",
      "Epoch 4/10, Batch 398/883, Training Loss: 0.6538\n",
      "Epoch 4/10, Batch 399/883, Training Loss: 0.9564\n",
      "Epoch 4/10, Batch 400/883, Training Loss: 0.8413\n",
      "Epoch 4/10, Batch 401/883, Training Loss: 0.7144\n",
      "Epoch 4/10, Batch 402/883, Training Loss: 0.5810\n",
      "Epoch 4/10, Batch 403/883, Training Loss: 0.9173\n",
      "Epoch 4/10, Batch 404/883, Training Loss: 0.7744\n",
      "Epoch 4/10, Batch 405/883, Training Loss: 0.6004\n",
      "Epoch 4/10, Batch 406/883, Training Loss: 0.7549\n",
      "Epoch 4/10, Batch 407/883, Training Loss: 0.6396\n",
      "Epoch 4/10, Batch 408/883, Training Loss: 0.6552\n",
      "Epoch 4/10, Batch 409/883, Training Loss: 0.7306\n",
      "Epoch 4/10, Batch 410/883, Training Loss: 0.8815\n",
      "Epoch 4/10, Batch 411/883, Training Loss: 0.7164\n",
      "Epoch 4/10, Batch 412/883, Training Loss: 0.8001\n",
      "Epoch 4/10, Batch 413/883, Training Loss: 0.7691\n",
      "Epoch 4/10, Batch 414/883, Training Loss: 0.7390\n",
      "Epoch 4/10, Batch 415/883, Training Loss: 0.5340\n",
      "Epoch 4/10, Batch 416/883, Training Loss: 0.7098\n",
      "Epoch 4/10, Batch 417/883, Training Loss: 0.9243\n",
      "Epoch 4/10, Batch 418/883, Training Loss: 1.0189\n",
      "Epoch 4/10, Batch 419/883, Training Loss: 0.7186\n",
      "Epoch 4/10, Batch 420/883, Training Loss: 0.7010\n",
      "Epoch 4/10, Batch 421/883, Training Loss: 0.7776\n",
      "Epoch 4/10, Batch 422/883, Training Loss: 0.6584\n",
      "Epoch 4/10, Batch 423/883, Training Loss: 0.8816\n",
      "Epoch 4/10, Batch 424/883, Training Loss: 0.8315\n",
      "Epoch 4/10, Batch 425/883, Training Loss: 1.0160\n",
      "Epoch 4/10, Batch 426/883, Training Loss: 0.8988\n",
      "Epoch 4/10, Batch 427/883, Training Loss: 0.9683\n",
      "Epoch 4/10, Batch 428/883, Training Loss: 0.7690\n",
      "Epoch 4/10, Batch 429/883, Training Loss: 0.9353\n",
      "Epoch 4/10, Batch 430/883, Training Loss: 0.7367\n",
      "Epoch 4/10, Batch 431/883, Training Loss: 0.8244\n",
      "Epoch 4/10, Batch 432/883, Training Loss: 0.5181\n",
      "Epoch 4/10, Batch 433/883, Training Loss: 0.6577\n",
      "Epoch 4/10, Batch 434/883, Training Loss: 0.8927\n",
      "Epoch 4/10, Batch 435/883, Training Loss: 0.8779\n",
      "Epoch 4/10, Batch 436/883, Training Loss: 0.6459\n",
      "Epoch 4/10, Batch 437/883, Training Loss: 0.8720\n",
      "Epoch 4/10, Batch 438/883, Training Loss: 0.6042\n",
      "Epoch 4/10, Batch 439/883, Training Loss: 0.8019\n",
      "Epoch 4/10, Batch 440/883, Training Loss: 0.6844\n",
      "Epoch 4/10, Batch 441/883, Training Loss: 0.8409\n",
      "Epoch 4/10, Batch 442/883, Training Loss: 0.5954\n",
      "Epoch 4/10, Batch 443/883, Training Loss: 0.8215\n",
      "Epoch 4/10, Batch 444/883, Training Loss: 0.7504\n",
      "Epoch 4/10, Batch 445/883, Training Loss: 0.6832\n",
      "Epoch 4/10, Batch 446/883, Training Loss: 0.7780\n",
      "Epoch 4/10, Batch 447/883, Training Loss: 0.5977\n",
      "Epoch 4/10, Batch 448/883, Training Loss: 0.8163\n",
      "Epoch 4/10, Batch 449/883, Training Loss: 0.5758\n",
      "Epoch 4/10, Batch 450/883, Training Loss: 0.8721\n",
      "Epoch 4/10, Batch 451/883, Training Loss: 0.8112\n",
      "Epoch 4/10, Batch 452/883, Training Loss: 0.8149\n",
      "Epoch 4/10, Batch 453/883, Training Loss: 0.7972\n",
      "Epoch 4/10, Batch 454/883, Training Loss: 0.8996\n",
      "Epoch 4/10, Batch 455/883, Training Loss: 0.6029\n",
      "Epoch 4/10, Batch 456/883, Training Loss: 0.7350\n",
      "Epoch 4/10, Batch 457/883, Training Loss: 0.8467\n",
      "Epoch 4/10, Batch 458/883, Training Loss: 0.7736\n",
      "Epoch 4/10, Batch 459/883, Training Loss: 1.0225\n",
      "Epoch 4/10, Batch 460/883, Training Loss: 0.8723\n",
      "Epoch 4/10, Batch 461/883, Training Loss: 0.8273\n",
      "Epoch 4/10, Batch 462/883, Training Loss: 0.6770\n",
      "Epoch 4/10, Batch 463/883, Training Loss: 0.9165\n",
      "Epoch 4/10, Batch 464/883, Training Loss: 0.8208\n",
      "Epoch 4/10, Batch 465/883, Training Loss: 0.7880\n",
      "Epoch 4/10, Batch 466/883, Training Loss: 0.7247\n",
      "Epoch 4/10, Batch 467/883, Training Loss: 0.9024\n",
      "Epoch 4/10, Batch 468/883, Training Loss: 0.5077\n",
      "Epoch 4/10, Batch 469/883, Training Loss: 0.8343\n",
      "Epoch 4/10, Batch 470/883, Training Loss: 0.9594\n",
      "Epoch 4/10, Batch 471/883, Training Loss: 0.9225\n",
      "Epoch 4/10, Batch 472/883, Training Loss: 0.7459\n",
      "Epoch 4/10, Batch 473/883, Training Loss: 0.6487\n",
      "Epoch 4/10, Batch 474/883, Training Loss: 0.5573\n",
      "Epoch 4/10, Batch 475/883, Training Loss: 0.7815\n",
      "Epoch 4/10, Batch 476/883, Training Loss: 0.8861\n",
      "Epoch 4/10, Batch 477/883, Training Loss: 0.7326\n",
      "Epoch 4/10, Batch 478/883, Training Loss: 0.5517\n",
      "Epoch 4/10, Batch 479/883, Training Loss: 0.8545\n",
      "Epoch 4/10, Batch 480/883, Training Loss: 0.6183\n",
      "Epoch 4/10, Batch 481/883, Training Loss: 0.6349\n",
      "Epoch 4/10, Batch 482/883, Training Loss: 0.8567\n",
      "Epoch 4/10, Batch 483/883, Training Loss: 1.3048\n",
      "Epoch 4/10, Batch 484/883, Training Loss: 0.5758\n",
      "Epoch 4/10, Batch 485/883, Training Loss: 0.8667\n",
      "Epoch 4/10, Batch 486/883, Training Loss: 0.7902\n",
      "Epoch 4/10, Batch 487/883, Training Loss: 0.7829\n",
      "Epoch 4/10, Batch 488/883, Training Loss: 0.4569\n",
      "Epoch 4/10, Batch 489/883, Training Loss: 0.6323\n",
      "Epoch 4/10, Batch 490/883, Training Loss: 0.9046\n",
      "Epoch 4/10, Batch 491/883, Training Loss: 0.9474\n",
      "Epoch 4/10, Batch 492/883, Training Loss: 0.8777\n",
      "Epoch 4/10, Batch 493/883, Training Loss: 0.8118\n",
      "Epoch 4/10, Batch 494/883, Training Loss: 0.5111\n",
      "Epoch 4/10, Batch 495/883, Training Loss: 0.9014\n",
      "Epoch 4/10, Batch 496/883, Training Loss: 1.2222\n",
      "Epoch 4/10, Batch 497/883, Training Loss: 0.8663\n",
      "Epoch 4/10, Batch 498/883, Training Loss: 0.5906\n",
      "Epoch 4/10, Batch 499/883, Training Loss: 0.6593\n",
      "Epoch 4/10, Batch 500/883, Training Loss: 0.7686\n",
      "Epoch 4/10, Batch 501/883, Training Loss: 0.8043\n",
      "Epoch 4/10, Batch 502/883, Training Loss: 0.6249\n",
      "Epoch 4/10, Batch 503/883, Training Loss: 0.7473\n",
      "Epoch 4/10, Batch 504/883, Training Loss: 0.7540\n",
      "Epoch 4/10, Batch 505/883, Training Loss: 0.8089\n",
      "Epoch 4/10, Batch 506/883, Training Loss: 0.8028\n",
      "Epoch 4/10, Batch 507/883, Training Loss: 0.7897\n",
      "Epoch 4/10, Batch 508/883, Training Loss: 0.6629\n",
      "Epoch 4/10, Batch 509/883, Training Loss: 0.5894\n",
      "Epoch 4/10, Batch 510/883, Training Loss: 0.7758\n",
      "Epoch 4/10, Batch 511/883, Training Loss: 0.7626\n",
      "Epoch 4/10, Batch 512/883, Training Loss: 0.9788\n",
      "Epoch 4/10, Batch 513/883, Training Loss: 1.0173\n",
      "Epoch 4/10, Batch 514/883, Training Loss: 0.6908\n",
      "Epoch 4/10, Batch 515/883, Training Loss: 0.5157\n",
      "Epoch 4/10, Batch 516/883, Training Loss: 0.7628\n",
      "Epoch 4/10, Batch 517/883, Training Loss: 1.4679\n",
      "Epoch 4/10, Batch 518/883, Training Loss: 0.8563\n",
      "Epoch 4/10, Batch 519/883, Training Loss: 0.5765\n",
      "Epoch 4/10, Batch 520/883, Training Loss: 0.8956\n",
      "Epoch 4/10, Batch 521/883, Training Loss: 0.7307\n",
      "Epoch 4/10, Batch 522/883, Training Loss: 0.7020\n",
      "Epoch 4/10, Batch 523/883, Training Loss: 0.9417\n",
      "Epoch 4/10, Batch 524/883, Training Loss: 0.7935\n",
      "Epoch 4/10, Batch 525/883, Training Loss: 0.7854\n",
      "Epoch 4/10, Batch 526/883, Training Loss: 1.0642\n",
      "Epoch 4/10, Batch 527/883, Training Loss: 0.9086\n",
      "Epoch 4/10, Batch 528/883, Training Loss: 0.6405\n",
      "Epoch 4/10, Batch 529/883, Training Loss: 1.0579\n",
      "Epoch 4/10, Batch 530/883, Training Loss: 0.5943\n",
      "Epoch 4/10, Batch 531/883, Training Loss: 0.7261\n",
      "Epoch 4/10, Batch 532/883, Training Loss: 0.6196\n",
      "Epoch 4/10, Batch 533/883, Training Loss: 0.6275\n",
      "Epoch 4/10, Batch 534/883, Training Loss: 0.6614\n",
      "Epoch 4/10, Batch 535/883, Training Loss: 0.7291\n",
      "Epoch 4/10, Batch 536/883, Training Loss: 0.6254\n",
      "Epoch 4/10, Batch 537/883, Training Loss: 0.6637\n",
      "Epoch 4/10, Batch 538/883, Training Loss: 0.7125\n",
      "Epoch 4/10, Batch 539/883, Training Loss: 0.6660\n",
      "Epoch 4/10, Batch 540/883, Training Loss: 0.7090\n",
      "Epoch 4/10, Batch 541/883, Training Loss: 0.8985\n",
      "Epoch 4/10, Batch 542/883, Training Loss: 0.7028\n",
      "Epoch 4/10, Batch 543/883, Training Loss: 0.8421\n",
      "Epoch 4/10, Batch 544/883, Training Loss: 0.7686\n",
      "Epoch 4/10, Batch 545/883, Training Loss: 1.0882\n",
      "Epoch 4/10, Batch 546/883, Training Loss: 0.6766\n",
      "Epoch 4/10, Batch 547/883, Training Loss: 0.7657\n",
      "Epoch 4/10, Batch 548/883, Training Loss: 0.9353\n",
      "Epoch 4/10, Batch 549/883, Training Loss: 0.7790\n",
      "Epoch 4/10, Batch 550/883, Training Loss: 0.8745\n",
      "Epoch 4/10, Batch 551/883, Training Loss: 0.8320\n",
      "Epoch 4/10, Batch 552/883, Training Loss: 1.0576\n",
      "Epoch 4/10, Batch 553/883, Training Loss: 0.6526\n",
      "Epoch 4/10, Batch 554/883, Training Loss: 0.6366\n",
      "Epoch 4/10, Batch 555/883, Training Loss: 0.6921\n",
      "Epoch 4/10, Batch 556/883, Training Loss: 0.5598\n",
      "Epoch 4/10, Batch 557/883, Training Loss: 0.6903\n",
      "Epoch 4/10, Batch 558/883, Training Loss: 0.7748\n",
      "Epoch 4/10, Batch 559/883, Training Loss: 0.6004\n",
      "Epoch 4/10, Batch 560/883, Training Loss: 0.7059\n",
      "Epoch 4/10, Batch 561/883, Training Loss: 0.7778\n",
      "Epoch 4/10, Batch 562/883, Training Loss: 0.8834\n",
      "Epoch 4/10, Batch 563/883, Training Loss: 0.7623\n",
      "Epoch 4/10, Batch 564/883, Training Loss: 0.7949\n",
      "Epoch 4/10, Batch 565/883, Training Loss: 0.6608\n",
      "Epoch 4/10, Batch 566/883, Training Loss: 0.7339\n",
      "Epoch 4/10, Batch 567/883, Training Loss: 0.8998\n",
      "Epoch 4/10, Batch 568/883, Training Loss: 1.1374\n",
      "Epoch 4/10, Batch 569/883, Training Loss: 0.9312\n",
      "Epoch 4/10, Batch 570/883, Training Loss: 0.9359\n",
      "Epoch 4/10, Batch 571/883, Training Loss: 0.6050\n",
      "Epoch 4/10, Batch 572/883, Training Loss: 0.9632\n",
      "Epoch 4/10, Batch 573/883, Training Loss: 0.7635\n",
      "Epoch 4/10, Batch 574/883, Training Loss: 0.8256\n",
      "Epoch 4/10, Batch 575/883, Training Loss: 0.8349\n",
      "Epoch 4/10, Batch 576/883, Training Loss: 0.6421\n",
      "Epoch 4/10, Batch 577/883, Training Loss: 0.8386\n",
      "Epoch 4/10, Batch 578/883, Training Loss: 0.6643\n",
      "Epoch 4/10, Batch 579/883, Training Loss: 1.0379\n",
      "Epoch 4/10, Batch 580/883, Training Loss: 0.7328\n",
      "Epoch 4/10, Batch 581/883, Training Loss: 0.5804\n",
      "Epoch 4/10, Batch 582/883, Training Loss: 0.7924\n",
      "Epoch 4/10, Batch 583/883, Training Loss: 0.9428\n",
      "Epoch 4/10, Batch 584/883, Training Loss: 0.7227\n",
      "Epoch 4/10, Batch 585/883, Training Loss: 0.9069\n",
      "Epoch 4/10, Batch 586/883, Training Loss: 0.6690\n",
      "Epoch 4/10, Batch 587/883, Training Loss: 0.7360\n",
      "Epoch 4/10, Batch 588/883, Training Loss: 0.8938\n",
      "Epoch 4/10, Batch 589/883, Training Loss: 0.7992\n",
      "Epoch 4/10, Batch 590/883, Training Loss: 0.9174\n",
      "Epoch 4/10, Batch 591/883, Training Loss: 0.8249\n",
      "Epoch 4/10, Batch 592/883, Training Loss: 0.7955\n",
      "Epoch 4/10, Batch 593/883, Training Loss: 0.6936\n",
      "Epoch 4/10, Batch 594/883, Training Loss: 0.8675\n",
      "Epoch 4/10, Batch 595/883, Training Loss: 0.7375\n",
      "Epoch 4/10, Batch 596/883, Training Loss: 0.6459\n",
      "Epoch 4/10, Batch 597/883, Training Loss: 0.5669\n",
      "Epoch 4/10, Batch 598/883, Training Loss: 0.5210\n",
      "Epoch 4/10, Batch 599/883, Training Loss: 1.0158\n",
      "Epoch 4/10, Batch 600/883, Training Loss: 0.7263\n",
      "Epoch 4/10, Batch 601/883, Training Loss: 0.6103\n",
      "Epoch 4/10, Batch 602/883, Training Loss: 0.7730\n",
      "Epoch 4/10, Batch 603/883, Training Loss: 1.1755\n",
      "Epoch 4/10, Batch 604/883, Training Loss: 0.8136\n",
      "Epoch 4/10, Batch 605/883, Training Loss: 0.4830\n",
      "Epoch 4/10, Batch 606/883, Training Loss: 0.7962\n",
      "Epoch 4/10, Batch 607/883, Training Loss: 0.7734\n",
      "Epoch 4/10, Batch 608/883, Training Loss: 0.7197\n",
      "Epoch 4/10, Batch 609/883, Training Loss: 0.6448\n",
      "Epoch 4/10, Batch 610/883, Training Loss: 0.9998\n",
      "Epoch 4/10, Batch 611/883, Training Loss: 0.8308\n",
      "Epoch 4/10, Batch 612/883, Training Loss: 1.1770\n",
      "Epoch 4/10, Batch 613/883, Training Loss: 0.7850\n",
      "Epoch 4/10, Batch 614/883, Training Loss: 0.7431\n",
      "Epoch 4/10, Batch 615/883, Training Loss: 0.9605\n",
      "Epoch 4/10, Batch 616/883, Training Loss: 1.0208\n",
      "Epoch 4/10, Batch 617/883, Training Loss: 0.7585\n",
      "Epoch 4/10, Batch 618/883, Training Loss: 0.5905\n",
      "Epoch 4/10, Batch 619/883, Training Loss: 0.7874\n",
      "Epoch 4/10, Batch 620/883, Training Loss: 0.5927\n",
      "Epoch 4/10, Batch 621/883, Training Loss: 0.7172\n",
      "Epoch 4/10, Batch 622/883, Training Loss: 1.0130\n",
      "Epoch 4/10, Batch 623/883, Training Loss: 0.8148\n",
      "Epoch 4/10, Batch 624/883, Training Loss: 0.6224\n",
      "Epoch 4/10, Batch 625/883, Training Loss: 0.6689\n",
      "Epoch 4/10, Batch 626/883, Training Loss: 0.7768\n",
      "Epoch 4/10, Batch 627/883, Training Loss: 0.5829\n",
      "Epoch 4/10, Batch 628/883, Training Loss: 0.6802\n",
      "Epoch 4/10, Batch 629/883, Training Loss: 0.7021\n",
      "Epoch 4/10, Batch 630/883, Training Loss: 0.7539\n",
      "Epoch 4/10, Batch 631/883, Training Loss: 1.1414\n",
      "Epoch 4/10, Batch 632/883, Training Loss: 0.6298\n",
      "Epoch 4/10, Batch 633/883, Training Loss: 0.5912\n",
      "Epoch 4/10, Batch 634/883, Training Loss: 0.4924\n",
      "Epoch 4/10, Batch 635/883, Training Loss: 0.5682\n",
      "Epoch 4/10, Batch 636/883, Training Loss: 0.9189\n",
      "Epoch 4/10, Batch 637/883, Training Loss: 0.9187\n",
      "Epoch 4/10, Batch 638/883, Training Loss: 0.9472\n",
      "Epoch 4/10, Batch 639/883, Training Loss: 0.6180\n",
      "Epoch 4/10, Batch 640/883, Training Loss: 0.4956\n",
      "Epoch 4/10, Batch 641/883, Training Loss: 0.6942\n",
      "Epoch 4/10, Batch 642/883, Training Loss: 1.0362\n",
      "Epoch 4/10, Batch 643/883, Training Loss: 0.8234\n",
      "Epoch 4/10, Batch 644/883, Training Loss: 0.5377\n",
      "Epoch 4/10, Batch 645/883, Training Loss: 0.8744\n",
      "Epoch 4/10, Batch 646/883, Training Loss: 0.6956\n",
      "Epoch 4/10, Batch 647/883, Training Loss: 0.5281\n",
      "Epoch 4/10, Batch 648/883, Training Loss: 0.5692\n",
      "Epoch 4/10, Batch 649/883, Training Loss: 0.8234\n",
      "Epoch 4/10, Batch 650/883, Training Loss: 0.7541\n",
      "Epoch 4/10, Batch 651/883, Training Loss: 1.0646\n",
      "Epoch 4/10, Batch 652/883, Training Loss: 0.5491\n",
      "Epoch 4/10, Batch 653/883, Training Loss: 1.2260\n",
      "Epoch 4/10, Batch 654/883, Training Loss: 0.5928\n",
      "Epoch 4/10, Batch 655/883, Training Loss: 0.7489\n",
      "Epoch 4/10, Batch 656/883, Training Loss: 0.5335\n",
      "Epoch 4/10, Batch 657/883, Training Loss: 0.4877\n",
      "Epoch 4/10, Batch 658/883, Training Loss: 0.7113\n",
      "Epoch 4/10, Batch 659/883, Training Loss: 0.7181\n",
      "Epoch 4/10, Batch 660/883, Training Loss: 0.7458\n",
      "Epoch 4/10, Batch 661/883, Training Loss: 0.9504\n",
      "Epoch 4/10, Batch 662/883, Training Loss: 0.8598\n",
      "Epoch 4/10, Batch 663/883, Training Loss: 0.9011\n",
      "Epoch 4/10, Batch 664/883, Training Loss: 0.7336\n",
      "Epoch 4/10, Batch 665/883, Training Loss: 0.6718\n",
      "Epoch 4/10, Batch 666/883, Training Loss: 0.6719\n",
      "Epoch 4/10, Batch 667/883, Training Loss: 0.6877\n",
      "Epoch 4/10, Batch 668/883, Training Loss: 0.7742\n",
      "Epoch 4/10, Batch 669/883, Training Loss: 0.5791\n",
      "Epoch 4/10, Batch 670/883, Training Loss: 0.8420\n",
      "Epoch 4/10, Batch 671/883, Training Loss: 0.9913\n",
      "Epoch 4/10, Batch 672/883, Training Loss: 0.7697\n",
      "Epoch 4/10, Batch 673/883, Training Loss: 0.8766\n",
      "Epoch 4/10, Batch 674/883, Training Loss: 0.9865\n",
      "Epoch 4/10, Batch 675/883, Training Loss: 1.1347\n",
      "Epoch 4/10, Batch 676/883, Training Loss: 0.8394\n",
      "Epoch 4/10, Batch 677/883, Training Loss: 0.9275\n",
      "Epoch 4/10, Batch 678/883, Training Loss: 0.9051\n",
      "Epoch 4/10, Batch 679/883, Training Loss: 0.6574\n",
      "Epoch 4/10, Batch 680/883, Training Loss: 0.7604\n",
      "Epoch 4/10, Batch 681/883, Training Loss: 1.2355\n",
      "Epoch 4/10, Batch 682/883, Training Loss: 0.7722\n",
      "Epoch 4/10, Batch 683/883, Training Loss: 1.1952\n",
      "Epoch 4/10, Batch 684/883, Training Loss: 0.8350\n",
      "Epoch 4/10, Batch 685/883, Training Loss: 0.5427\n",
      "Epoch 4/10, Batch 686/883, Training Loss: 0.7128\n",
      "Epoch 4/10, Batch 687/883, Training Loss: 0.8032\n",
      "Epoch 4/10, Batch 688/883, Training Loss: 0.8514\n",
      "Epoch 4/10, Batch 689/883, Training Loss: 0.6903\n",
      "Epoch 4/10, Batch 690/883, Training Loss: 0.8384\n",
      "Epoch 4/10, Batch 691/883, Training Loss: 0.7735\n",
      "Epoch 4/10, Batch 692/883, Training Loss: 0.9548\n",
      "Epoch 4/10, Batch 693/883, Training Loss: 0.6213\n",
      "Epoch 4/10, Batch 694/883, Training Loss: 0.6376\n",
      "Epoch 4/10, Batch 695/883, Training Loss: 0.9873\n",
      "Epoch 4/10, Batch 696/883, Training Loss: 0.5899\n",
      "Epoch 4/10, Batch 697/883, Training Loss: 0.7448\n",
      "Epoch 4/10, Batch 698/883, Training Loss: 0.6933\n",
      "Epoch 4/10, Batch 699/883, Training Loss: 0.6052\n",
      "Epoch 4/10, Batch 700/883, Training Loss: 0.6242\n",
      "Epoch 4/10, Batch 701/883, Training Loss: 0.8085\n",
      "Epoch 4/10, Batch 702/883, Training Loss: 0.6695\n",
      "Epoch 4/10, Batch 703/883, Training Loss: 1.0634\n",
      "Epoch 4/10, Batch 704/883, Training Loss: 0.8362\n",
      "Epoch 4/10, Batch 705/883, Training Loss: 0.6622\n",
      "Epoch 4/10, Batch 706/883, Training Loss: 0.5808\n",
      "Epoch 4/10, Batch 707/883, Training Loss: 0.8147\n",
      "Epoch 4/10, Batch 708/883, Training Loss: 0.8978\n",
      "Epoch 4/10, Batch 709/883, Training Loss: 0.7234\n",
      "Epoch 4/10, Batch 710/883, Training Loss: 0.6876\n",
      "Epoch 4/10, Batch 711/883, Training Loss: 0.6234\n",
      "Epoch 4/10, Batch 712/883, Training Loss: 0.8937\n",
      "Epoch 4/10, Batch 713/883, Training Loss: 0.5884\n",
      "Epoch 4/10, Batch 714/883, Training Loss: 0.9291\n",
      "Epoch 4/10, Batch 715/883, Training Loss: 0.7182\n",
      "Epoch 4/10, Batch 716/883, Training Loss: 0.6893\n",
      "Epoch 4/10, Batch 717/883, Training Loss: 0.9379\n",
      "Epoch 4/10, Batch 718/883, Training Loss: 0.8313\n",
      "Epoch 4/10, Batch 719/883, Training Loss: 0.7314\n",
      "Epoch 4/10, Batch 720/883, Training Loss: 0.8035\n",
      "Epoch 4/10, Batch 721/883, Training Loss: 0.7431\n",
      "Epoch 4/10, Batch 722/883, Training Loss: 0.8024\n",
      "Epoch 4/10, Batch 723/883, Training Loss: 0.9475\n",
      "Epoch 4/10, Batch 724/883, Training Loss: 0.6232\n",
      "Epoch 4/10, Batch 725/883, Training Loss: 0.5933\n",
      "Epoch 4/10, Batch 726/883, Training Loss: 0.5585\n",
      "Epoch 4/10, Batch 727/883, Training Loss: 0.6057\n",
      "Epoch 4/10, Batch 728/883, Training Loss: 0.6103\n",
      "Epoch 4/10, Batch 729/883, Training Loss: 0.5728\n",
      "Epoch 4/10, Batch 730/883, Training Loss: 0.7784\n",
      "Epoch 4/10, Batch 731/883, Training Loss: 0.5395\n",
      "Epoch 4/10, Batch 732/883, Training Loss: 0.9173\n",
      "Epoch 4/10, Batch 733/883, Training Loss: 1.1069\n",
      "Epoch 4/10, Batch 734/883, Training Loss: 0.7145\n",
      "Epoch 4/10, Batch 735/883, Training Loss: 1.0696\n",
      "Epoch 4/10, Batch 736/883, Training Loss: 0.6869\n",
      "Epoch 4/10, Batch 737/883, Training Loss: 0.8443\n",
      "Epoch 4/10, Batch 738/883, Training Loss: 1.0668\n",
      "Epoch 4/10, Batch 739/883, Training Loss: 0.7599\n",
      "Epoch 4/10, Batch 740/883, Training Loss: 0.7073\n",
      "Epoch 4/10, Batch 741/883, Training Loss: 1.0202\n",
      "Epoch 4/10, Batch 742/883, Training Loss: 0.7548\n",
      "Epoch 4/10, Batch 743/883, Training Loss: 0.9353\n",
      "Epoch 4/10, Batch 744/883, Training Loss: 0.7010\n",
      "Epoch 4/10, Batch 745/883, Training Loss: 0.6926\n",
      "Epoch 4/10, Batch 746/883, Training Loss: 0.6340\n",
      "Epoch 4/10, Batch 747/883, Training Loss: 0.7593\n",
      "Epoch 4/10, Batch 748/883, Training Loss: 0.6262\n",
      "Epoch 4/10, Batch 749/883, Training Loss: 0.7540\n",
      "Epoch 4/10, Batch 750/883, Training Loss: 0.5416\n",
      "Epoch 4/10, Batch 751/883, Training Loss: 0.6075\n",
      "Epoch 4/10, Batch 752/883, Training Loss: 0.7862\n",
      "Epoch 4/10, Batch 753/883, Training Loss: 0.6485\n",
      "Epoch 4/10, Batch 754/883, Training Loss: 0.8400\n",
      "Epoch 4/10, Batch 755/883, Training Loss: 0.9155\n",
      "Epoch 4/10, Batch 756/883, Training Loss: 0.6831\n",
      "Epoch 4/10, Batch 757/883, Training Loss: 0.7460\n",
      "Epoch 4/10, Batch 758/883, Training Loss: 0.4753\n",
      "Epoch 4/10, Batch 759/883, Training Loss: 0.9789\n",
      "Epoch 4/10, Batch 760/883, Training Loss: 0.5792\n",
      "Epoch 4/10, Batch 761/883, Training Loss: 0.5102\n",
      "Epoch 4/10, Batch 762/883, Training Loss: 0.8104\n",
      "Epoch 4/10, Batch 763/883, Training Loss: 0.6531\n",
      "Epoch 4/10, Batch 764/883, Training Loss: 0.6774\n",
      "Epoch 4/10, Batch 765/883, Training Loss: 0.8463\n",
      "Epoch 4/10, Batch 766/883, Training Loss: 0.5114\n",
      "Epoch 4/10, Batch 767/883, Training Loss: 0.7465\n",
      "Epoch 4/10, Batch 768/883, Training Loss: 0.8723\n",
      "Epoch 4/10, Batch 769/883, Training Loss: 0.9584\n",
      "Epoch 4/10, Batch 770/883, Training Loss: 0.7634\n",
      "Epoch 4/10, Batch 771/883, Training Loss: 0.9187\n",
      "Epoch 4/10, Batch 772/883, Training Loss: 0.7956\n",
      "Epoch 4/10, Batch 773/883, Training Loss: 0.6334\n",
      "Epoch 4/10, Batch 774/883, Training Loss: 1.0311\n",
      "Epoch 4/10, Batch 775/883, Training Loss: 1.0256\n",
      "Epoch 4/10, Batch 776/883, Training Loss: 0.8788\n",
      "Epoch 4/10, Batch 777/883, Training Loss: 0.8448\n",
      "Epoch 4/10, Batch 778/883, Training Loss: 0.8537\n",
      "Epoch 4/10, Batch 779/883, Training Loss: 0.7379\n",
      "Epoch 4/10, Batch 780/883, Training Loss: 0.7959\n",
      "Epoch 4/10, Batch 781/883, Training Loss: 0.6386\n",
      "Epoch 4/10, Batch 782/883, Training Loss: 0.7541\n",
      "Epoch 4/10, Batch 783/883, Training Loss: 0.7220\n",
      "Epoch 4/10, Batch 784/883, Training Loss: 0.7242\n",
      "Epoch 4/10, Batch 785/883, Training Loss: 0.7015\n",
      "Epoch 4/10, Batch 786/883, Training Loss: 0.8230\n",
      "Epoch 4/10, Batch 787/883, Training Loss: 0.7331\n",
      "Epoch 4/10, Batch 788/883, Training Loss: 0.8475\n",
      "Epoch 4/10, Batch 789/883, Training Loss: 0.7224\n",
      "Epoch 4/10, Batch 790/883, Training Loss: 0.9377\n",
      "Epoch 4/10, Batch 791/883, Training Loss: 0.8064\n",
      "Epoch 4/10, Batch 792/883, Training Loss: 0.5827\n",
      "Epoch 4/10, Batch 793/883, Training Loss: 0.8840\n",
      "Epoch 4/10, Batch 794/883, Training Loss: 1.0170\n",
      "Epoch 4/10, Batch 795/883, Training Loss: 0.7399\n",
      "Epoch 4/10, Batch 796/883, Training Loss: 0.8400\n",
      "Epoch 4/10, Batch 797/883, Training Loss: 0.7458\n",
      "Epoch 4/10, Batch 798/883, Training Loss: 0.7815\n",
      "Epoch 4/10, Batch 799/883, Training Loss: 0.7104\n",
      "Epoch 4/10, Batch 800/883, Training Loss: 0.6237\n",
      "Epoch 4/10, Batch 801/883, Training Loss: 0.5250\n",
      "Epoch 4/10, Batch 802/883, Training Loss: 0.6886\n",
      "Epoch 4/10, Batch 803/883, Training Loss: 0.5907\n",
      "Epoch 4/10, Batch 804/883, Training Loss: 0.5895\n",
      "Epoch 4/10, Batch 805/883, Training Loss: 0.6974\n",
      "Epoch 4/10, Batch 806/883, Training Loss: 0.8538\n",
      "Epoch 4/10, Batch 807/883, Training Loss: 1.1197\n",
      "Epoch 4/10, Batch 808/883, Training Loss: 0.7690\n",
      "Epoch 4/10, Batch 809/883, Training Loss: 0.9922\n",
      "Epoch 4/10, Batch 810/883, Training Loss: 0.8542\n",
      "Epoch 4/10, Batch 811/883, Training Loss: 0.8859\n",
      "Epoch 4/10, Batch 812/883, Training Loss: 0.7302\n",
      "Epoch 4/10, Batch 813/883, Training Loss: 0.8515\n",
      "Epoch 4/10, Batch 814/883, Training Loss: 0.6175\n",
      "Epoch 4/10, Batch 815/883, Training Loss: 0.8147\n",
      "Epoch 4/10, Batch 816/883, Training Loss: 0.5919\n",
      "Epoch 4/10, Batch 817/883, Training Loss: 0.5998\n",
      "Epoch 4/10, Batch 818/883, Training Loss: 0.8311\n",
      "Epoch 4/10, Batch 819/883, Training Loss: 1.0956\n",
      "Epoch 4/10, Batch 820/883, Training Loss: 0.6379\n",
      "Epoch 4/10, Batch 821/883, Training Loss: 0.9181\n",
      "Epoch 4/10, Batch 822/883, Training Loss: 0.6883\n",
      "Epoch 4/10, Batch 823/883, Training Loss: 0.5826\n",
      "Epoch 4/10, Batch 824/883, Training Loss: 0.6661\n",
      "Epoch 4/10, Batch 825/883, Training Loss: 0.6681\n",
      "Epoch 4/10, Batch 826/883, Training Loss: 0.8102\n",
      "Epoch 4/10, Batch 827/883, Training Loss: 0.8675\n",
      "Epoch 4/10, Batch 828/883, Training Loss: 0.7614\n",
      "Epoch 4/10, Batch 829/883, Training Loss: 0.6744\n",
      "Epoch 4/10, Batch 830/883, Training Loss: 0.6757\n",
      "Epoch 4/10, Batch 831/883, Training Loss: 0.8535\n",
      "Epoch 4/10, Batch 832/883, Training Loss: 0.8570\n",
      "Epoch 4/10, Batch 833/883, Training Loss: 0.5646\n",
      "Epoch 4/10, Batch 834/883, Training Loss: 1.0557\n",
      "Epoch 4/10, Batch 835/883, Training Loss: 0.9458\n",
      "Epoch 4/10, Batch 836/883, Training Loss: 0.7217\n",
      "Epoch 4/10, Batch 837/883, Training Loss: 0.6380\n",
      "Epoch 4/10, Batch 838/883, Training Loss: 0.8031\n",
      "Epoch 4/10, Batch 839/883, Training Loss: 0.9062\n",
      "Epoch 4/10, Batch 840/883, Training Loss: 0.6296\n",
      "Epoch 4/10, Batch 841/883, Training Loss: 0.8361\n",
      "Epoch 4/10, Batch 842/883, Training Loss: 0.8599\n",
      "Epoch 4/10, Batch 843/883, Training Loss: 0.6229\n",
      "Epoch 4/10, Batch 844/883, Training Loss: 0.6248\n",
      "Epoch 4/10, Batch 845/883, Training Loss: 0.7939\n",
      "Epoch 4/10, Batch 846/883, Training Loss: 0.8475\n",
      "Epoch 4/10, Batch 847/883, Training Loss: 1.1928\n",
      "Epoch 4/10, Batch 848/883, Training Loss: 0.6838\n",
      "Epoch 4/10, Batch 849/883, Training Loss: 0.7964\n",
      "Epoch 4/10, Batch 850/883, Training Loss: 0.6318\n",
      "Epoch 4/10, Batch 851/883, Training Loss: 0.9360\n",
      "Epoch 4/10, Batch 852/883, Training Loss: 0.8009\n",
      "Epoch 4/10, Batch 853/883, Training Loss: 0.6351\n",
      "Epoch 4/10, Batch 854/883, Training Loss: 0.8747\n",
      "Epoch 4/10, Batch 855/883, Training Loss: 0.7873\n",
      "Epoch 4/10, Batch 856/883, Training Loss: 0.7155\n",
      "Epoch 4/10, Batch 857/883, Training Loss: 0.9085\n",
      "Epoch 4/10, Batch 858/883, Training Loss: 0.7871\n",
      "Epoch 4/10, Batch 859/883, Training Loss: 0.8316\n",
      "Epoch 4/10, Batch 860/883, Training Loss: 0.5861\n",
      "Epoch 4/10, Batch 861/883, Training Loss: 0.5764\n",
      "Epoch 4/10, Batch 862/883, Training Loss: 0.7233\n",
      "Epoch 4/10, Batch 863/883, Training Loss: 0.7347\n",
      "Epoch 4/10, Batch 864/883, Training Loss: 0.6868\n",
      "Epoch 4/10, Batch 865/883, Training Loss: 0.6839\n",
      "Epoch 4/10, Batch 866/883, Training Loss: 0.5636\n",
      "Epoch 4/10, Batch 867/883, Training Loss: 0.7008\n",
      "Epoch 4/10, Batch 868/883, Training Loss: 0.6268\n",
      "Epoch 4/10, Batch 869/883, Training Loss: 0.6622\n",
      "Epoch 4/10, Batch 870/883, Training Loss: 0.4918\n",
      "Epoch 4/10, Batch 871/883, Training Loss: 1.1040\n",
      "Epoch 4/10, Batch 872/883, Training Loss: 0.6142\n",
      "Epoch 4/10, Batch 873/883, Training Loss: 0.9861\n",
      "Epoch 4/10, Batch 874/883, Training Loss: 0.9572\n",
      "Epoch 4/10, Batch 875/883, Training Loss: 0.6702\n",
      "Epoch 4/10, Batch 876/883, Training Loss: 0.4733\n",
      "Epoch 4/10, Batch 877/883, Training Loss: 0.5324\n",
      "Epoch 4/10, Batch 878/883, Training Loss: 0.5106\n",
      "Epoch 4/10, Batch 879/883, Training Loss: 0.8537\n",
      "Epoch 4/10, Batch 880/883, Training Loss: 0.6167\n",
      "Epoch 4/10, Batch 881/883, Training Loss: 0.6926\n",
      "Epoch 4/10, Batch 882/883, Training Loss: 0.9191\n",
      "Epoch 4/10, Batch 883/883, Training Loss: 1.4602\n",
      "Epoch 4/10, Training Loss: 0.7768, Validation Loss: 0.7569, Validation Accuracy: 0.6205\n",
      "Epoch 5/10, Batch 1/883, Training Loss: 0.6765\n",
      "Epoch 5/10, Batch 2/883, Training Loss: 0.5183\n",
      "Epoch 5/10, Batch 3/883, Training Loss: 0.8079\n",
      "Epoch 5/10, Batch 4/883, Training Loss: 0.8032\n",
      "Epoch 5/10, Batch 5/883, Training Loss: 0.8133\n",
      "Epoch 5/10, Batch 6/883, Training Loss: 0.5354\n",
      "Epoch 5/10, Batch 7/883, Training Loss: 0.7169\n",
      "Epoch 5/10, Batch 8/883, Training Loss: 0.5411\n",
      "Epoch 5/10, Batch 9/883, Training Loss: 0.7732\n",
      "Epoch 5/10, Batch 10/883, Training Loss: 0.7235\n",
      "Epoch 5/10, Batch 11/883, Training Loss: 1.0235\n",
      "Epoch 5/10, Batch 12/883, Training Loss: 1.0000\n",
      "Epoch 5/10, Batch 13/883, Training Loss: 0.5808\n",
      "Epoch 5/10, Batch 14/883, Training Loss: 0.5790\n",
      "Epoch 5/10, Batch 15/883, Training Loss: 0.7467\n",
      "Epoch 5/10, Batch 16/883, Training Loss: 0.7513\n",
      "Epoch 5/10, Batch 17/883, Training Loss: 0.9248\n",
      "Epoch 5/10, Batch 18/883, Training Loss: 0.6153\n",
      "Epoch 5/10, Batch 19/883, Training Loss: 0.7919\n",
      "Epoch 5/10, Batch 20/883, Training Loss: 0.8120\n",
      "Epoch 5/10, Batch 21/883, Training Loss: 0.6956\n",
      "Epoch 5/10, Batch 22/883, Training Loss: 0.7931\n",
      "Epoch 5/10, Batch 23/883, Training Loss: 0.8164\n",
      "Epoch 5/10, Batch 24/883, Training Loss: 0.7398\n",
      "Epoch 5/10, Batch 25/883, Training Loss: 0.9812\n",
      "Epoch 5/10, Batch 26/883, Training Loss: 0.6897\n",
      "Epoch 5/10, Batch 27/883, Training Loss: 0.8275\n",
      "Epoch 5/10, Batch 28/883, Training Loss: 0.7142\n",
      "Epoch 5/10, Batch 29/883, Training Loss: 0.6221\n",
      "Epoch 5/10, Batch 30/883, Training Loss: 0.6127\n",
      "Epoch 5/10, Batch 31/883, Training Loss: 0.7140\n",
      "Epoch 5/10, Batch 32/883, Training Loss: 0.9622\n",
      "Epoch 5/10, Batch 33/883, Training Loss: 1.0873\n",
      "Epoch 5/10, Batch 34/883, Training Loss: 0.6488\n",
      "Epoch 5/10, Batch 35/883, Training Loss: 1.0514\n",
      "Epoch 5/10, Batch 36/883, Training Loss: 0.8321\n",
      "Epoch 5/10, Batch 37/883, Training Loss: 0.7032\n",
      "Epoch 5/10, Batch 38/883, Training Loss: 0.6512\n",
      "Epoch 5/10, Batch 39/883, Training Loss: 0.6585\n",
      "Epoch 5/10, Batch 40/883, Training Loss: 0.5702\n",
      "Epoch 5/10, Batch 41/883, Training Loss: 0.7617\n",
      "Epoch 5/10, Batch 42/883, Training Loss: 0.5608\n",
      "Epoch 5/10, Batch 43/883, Training Loss: 0.7579\n",
      "Epoch 5/10, Batch 44/883, Training Loss: 0.8575\n",
      "Epoch 5/10, Batch 45/883, Training Loss: 0.7179\n",
      "Epoch 5/10, Batch 46/883, Training Loss: 0.5976\n",
      "Epoch 5/10, Batch 47/883, Training Loss: 0.7580\n",
      "Epoch 5/10, Batch 48/883, Training Loss: 0.6237\n",
      "Epoch 5/10, Batch 49/883, Training Loss: 0.6251\n",
      "Epoch 5/10, Batch 50/883, Training Loss: 0.6066\n",
      "Epoch 5/10, Batch 51/883, Training Loss: 0.7430\n",
      "Epoch 5/10, Batch 52/883, Training Loss: 0.8236\n",
      "Epoch 5/10, Batch 53/883, Training Loss: 0.7878\n",
      "Epoch 5/10, Batch 54/883, Training Loss: 0.6279\n",
      "Epoch 5/10, Batch 55/883, Training Loss: 0.9711\n",
      "Epoch 5/10, Batch 56/883, Training Loss: 0.7434\n",
      "Epoch 5/10, Batch 57/883, Training Loss: 0.6798\n",
      "Epoch 5/10, Batch 58/883, Training Loss: 0.6473\n",
      "Epoch 5/10, Batch 59/883, Training Loss: 0.6866\n",
      "Epoch 5/10, Batch 60/883, Training Loss: 0.6207\n",
      "Epoch 5/10, Batch 61/883, Training Loss: 0.8792\n",
      "Epoch 5/10, Batch 62/883, Training Loss: 0.9738\n",
      "Epoch 5/10, Batch 63/883, Training Loss: 0.7533\n",
      "Epoch 5/10, Batch 64/883, Training Loss: 0.5532\n",
      "Epoch 5/10, Batch 65/883, Training Loss: 0.7209\n",
      "Epoch 5/10, Batch 66/883, Training Loss: 0.7378\n",
      "Epoch 5/10, Batch 67/883, Training Loss: 0.6212\n",
      "Epoch 5/10, Batch 68/883, Training Loss: 0.6541\n",
      "Epoch 5/10, Batch 69/883, Training Loss: 0.5987\n",
      "Epoch 5/10, Batch 70/883, Training Loss: 0.7419\n",
      "Epoch 5/10, Batch 71/883, Training Loss: 0.6925\n",
      "Epoch 5/10, Batch 72/883, Training Loss: 0.7133\n",
      "Epoch 5/10, Batch 73/883, Training Loss: 0.7092\n",
      "Epoch 5/10, Batch 74/883, Training Loss: 0.6304\n",
      "Epoch 5/10, Batch 75/883, Training Loss: 0.6610\n",
      "Epoch 5/10, Batch 76/883, Training Loss: 0.8263\n",
      "Epoch 5/10, Batch 77/883, Training Loss: 0.8004\n",
      "Epoch 5/10, Batch 78/883, Training Loss: 0.4912\n",
      "Epoch 5/10, Batch 79/883, Training Loss: 0.8681\n",
      "Epoch 5/10, Batch 80/883, Training Loss: 0.6444\n",
      "Epoch 5/10, Batch 81/883, Training Loss: 0.6465\n",
      "Epoch 5/10, Batch 82/883, Training Loss: 0.7183\n",
      "Epoch 5/10, Batch 83/883, Training Loss: 0.7322\n",
      "Epoch 5/10, Batch 84/883, Training Loss: 0.8812\n",
      "Epoch 5/10, Batch 85/883, Training Loss: 0.7030\n",
      "Epoch 5/10, Batch 86/883, Training Loss: 0.7361\n",
      "Epoch 5/10, Batch 87/883, Training Loss: 0.6524\n",
      "Epoch 5/10, Batch 88/883, Training Loss: 0.8276\n",
      "Epoch 5/10, Batch 89/883, Training Loss: 1.2824\n",
      "Epoch 5/10, Batch 90/883, Training Loss: 0.6067\n",
      "Epoch 5/10, Batch 91/883, Training Loss: 0.5237\n",
      "Epoch 5/10, Batch 92/883, Training Loss: 1.0901\n",
      "Epoch 5/10, Batch 93/883, Training Loss: 0.5633\n",
      "Epoch 5/10, Batch 94/883, Training Loss: 0.6352\n",
      "Epoch 5/10, Batch 95/883, Training Loss: 0.5877\n",
      "Epoch 5/10, Batch 96/883, Training Loss: 0.6401\n",
      "Epoch 5/10, Batch 97/883, Training Loss: 0.6676\n",
      "Epoch 5/10, Batch 98/883, Training Loss: 1.0416\n",
      "Epoch 5/10, Batch 99/883, Training Loss: 0.5423\n",
      "Epoch 5/10, Batch 100/883, Training Loss: 0.8236\n",
      "Epoch 5/10, Batch 101/883, Training Loss: 0.6405\n",
      "Epoch 5/10, Batch 102/883, Training Loss: 0.5123\n",
      "Epoch 5/10, Batch 103/883, Training Loss: 0.7778\n",
      "Epoch 5/10, Batch 104/883, Training Loss: 0.6921\n",
      "Epoch 5/10, Batch 105/883, Training Loss: 0.7909\n",
      "Epoch 5/10, Batch 106/883, Training Loss: 0.7503\n",
      "Epoch 5/10, Batch 107/883, Training Loss: 0.5426\n",
      "Epoch 5/10, Batch 108/883, Training Loss: 0.6024\n",
      "Epoch 5/10, Batch 109/883, Training Loss: 0.7673\n",
      "Epoch 5/10, Batch 110/883, Training Loss: 0.7292\n",
      "Epoch 5/10, Batch 111/883, Training Loss: 0.8553\n",
      "Epoch 5/10, Batch 112/883, Training Loss: 0.9401\n",
      "Epoch 5/10, Batch 113/883, Training Loss: 0.8044\n",
      "Epoch 5/10, Batch 114/883, Training Loss: 0.8481\n",
      "Epoch 5/10, Batch 115/883, Training Loss: 0.7431\n",
      "Epoch 5/10, Batch 116/883, Training Loss: 0.6667\n",
      "Epoch 5/10, Batch 117/883, Training Loss: 0.5866\n",
      "Epoch 5/10, Batch 118/883, Training Loss: 0.9390\n",
      "Epoch 5/10, Batch 119/883, Training Loss: 1.0373\n",
      "Epoch 5/10, Batch 120/883, Training Loss: 0.8128\n",
      "Epoch 5/10, Batch 121/883, Training Loss: 1.2001\n",
      "Epoch 5/10, Batch 122/883, Training Loss: 0.7282\n",
      "Epoch 5/10, Batch 123/883, Training Loss: 0.7268\n",
      "Epoch 5/10, Batch 124/883, Training Loss: 1.1748\n",
      "Epoch 5/10, Batch 125/883, Training Loss: 0.7435\n",
      "Epoch 5/10, Batch 126/883, Training Loss: 1.0748\n",
      "Epoch 5/10, Batch 127/883, Training Loss: 0.7737\n",
      "Epoch 5/10, Batch 128/883, Training Loss: 0.6358\n",
      "Epoch 5/10, Batch 129/883, Training Loss: 0.5510\n",
      "Epoch 5/10, Batch 130/883, Training Loss: 0.6460\n",
      "Epoch 5/10, Batch 131/883, Training Loss: 0.5960\n",
      "Epoch 5/10, Batch 132/883, Training Loss: 0.8742\n",
      "Epoch 5/10, Batch 133/883, Training Loss: 0.8095\n",
      "Epoch 5/10, Batch 134/883, Training Loss: 0.8324\n",
      "Epoch 5/10, Batch 135/883, Training Loss: 0.5296\n",
      "Epoch 5/10, Batch 136/883, Training Loss: 0.8735\n",
      "Epoch 5/10, Batch 137/883, Training Loss: 0.9359\n",
      "Epoch 5/10, Batch 138/883, Training Loss: 0.8871\n",
      "Epoch 5/10, Batch 139/883, Training Loss: 0.6216\n",
      "Epoch 5/10, Batch 140/883, Training Loss: 0.7021\n",
      "Epoch 5/10, Batch 141/883, Training Loss: 0.5267\n",
      "Epoch 5/10, Batch 142/883, Training Loss: 0.8651\n",
      "Epoch 5/10, Batch 143/883, Training Loss: 0.7194\n",
      "Epoch 5/10, Batch 144/883, Training Loss: 0.6245\n",
      "Epoch 5/10, Batch 145/883, Training Loss: 0.9830\n",
      "Epoch 5/10, Batch 146/883, Training Loss: 0.7972\n",
      "Epoch 5/10, Batch 147/883, Training Loss: 1.1816\n",
      "Epoch 5/10, Batch 148/883, Training Loss: 0.6478\n",
      "Epoch 5/10, Batch 149/883, Training Loss: 0.5050\n",
      "Epoch 5/10, Batch 150/883, Training Loss: 0.8430\n",
      "Epoch 5/10, Batch 151/883, Training Loss: 0.5838\n",
      "Epoch 5/10, Batch 152/883, Training Loss: 0.6487\n",
      "Epoch 5/10, Batch 153/883, Training Loss: 0.5483\n",
      "Epoch 5/10, Batch 154/883, Training Loss: 0.9377\n",
      "Epoch 5/10, Batch 155/883, Training Loss: 0.9487\n",
      "Epoch 5/10, Batch 156/883, Training Loss: 0.8573\n",
      "Epoch 5/10, Batch 157/883, Training Loss: 0.8093\n",
      "Epoch 5/10, Batch 158/883, Training Loss: 0.7865\n",
      "Epoch 5/10, Batch 159/883, Training Loss: 0.6823\n",
      "Epoch 5/10, Batch 160/883, Training Loss: 0.7192\n",
      "Epoch 5/10, Batch 161/883, Training Loss: 0.6539\n",
      "Epoch 5/10, Batch 162/883, Training Loss: 0.7935\n",
      "Epoch 5/10, Batch 163/883, Training Loss: 0.6817\n",
      "Epoch 5/10, Batch 164/883, Training Loss: 0.8327\n",
      "Epoch 5/10, Batch 165/883, Training Loss: 0.5243\n",
      "Epoch 5/10, Batch 166/883, Training Loss: 0.6024\n",
      "Epoch 5/10, Batch 167/883, Training Loss: 0.6084\n",
      "Epoch 5/10, Batch 168/883, Training Loss: 0.6531\n",
      "Epoch 5/10, Batch 169/883, Training Loss: 0.6622\n",
      "Epoch 5/10, Batch 170/883, Training Loss: 0.7516\n",
      "Epoch 5/10, Batch 171/883, Training Loss: 0.6629\n",
      "Epoch 5/10, Batch 172/883, Training Loss: 0.5788\n",
      "Epoch 5/10, Batch 173/883, Training Loss: 0.7694\n",
      "Epoch 5/10, Batch 174/883, Training Loss: 0.7555\n",
      "Epoch 5/10, Batch 175/883, Training Loss: 1.0432\n",
      "Epoch 5/10, Batch 176/883, Training Loss: 0.6165\n",
      "Epoch 5/10, Batch 177/883, Training Loss: 0.7082\n",
      "Epoch 5/10, Batch 178/883, Training Loss: 0.8005\n",
      "Epoch 5/10, Batch 179/883, Training Loss: 0.7503\n",
      "Epoch 5/10, Batch 180/883, Training Loss: 0.8852\n",
      "Epoch 5/10, Batch 181/883, Training Loss: 0.6838\n",
      "Epoch 5/10, Batch 182/883, Training Loss: 0.6925\n",
      "Epoch 5/10, Batch 183/883, Training Loss: 0.5219\n",
      "Epoch 5/10, Batch 184/883, Training Loss: 0.7875\n",
      "Epoch 5/10, Batch 185/883, Training Loss: 0.6435\n",
      "Epoch 5/10, Batch 186/883, Training Loss: 0.6146\n",
      "Epoch 5/10, Batch 187/883, Training Loss: 0.5884\n",
      "Epoch 5/10, Batch 188/883, Training Loss: 0.7770\n",
      "Epoch 5/10, Batch 189/883, Training Loss: 1.2591\n",
      "Epoch 5/10, Batch 190/883, Training Loss: 0.9424\n",
      "Epoch 5/10, Batch 191/883, Training Loss: 0.6643\n",
      "Epoch 5/10, Batch 192/883, Training Loss: 0.6257\n",
      "Epoch 5/10, Batch 193/883, Training Loss: 1.1363\n",
      "Epoch 5/10, Batch 194/883, Training Loss: 0.5277\n",
      "Epoch 5/10, Batch 195/883, Training Loss: 0.7191\n",
      "Epoch 5/10, Batch 196/883, Training Loss: 0.9462\n",
      "Epoch 5/10, Batch 197/883, Training Loss: 0.7481\n",
      "Epoch 5/10, Batch 198/883, Training Loss: 0.6933\n",
      "Epoch 5/10, Batch 199/883, Training Loss: 0.5250\n",
      "Epoch 5/10, Batch 200/883, Training Loss: 0.7812\n",
      "Epoch 5/10, Batch 201/883, Training Loss: 0.7641\n",
      "Epoch 5/10, Batch 202/883, Training Loss: 0.8271\n",
      "Epoch 5/10, Batch 203/883, Training Loss: 0.5196\n",
      "Epoch 5/10, Batch 204/883, Training Loss: 0.7154\n",
      "Epoch 5/10, Batch 205/883, Training Loss: 0.6691\n",
      "Epoch 5/10, Batch 206/883, Training Loss: 0.4818\n",
      "Epoch 5/10, Batch 207/883, Training Loss: 1.2681\n",
      "Epoch 5/10, Batch 208/883, Training Loss: 0.5879\n",
      "Epoch 5/10, Batch 209/883, Training Loss: 0.6975\n",
      "Epoch 5/10, Batch 210/883, Training Loss: 0.8017\n",
      "Epoch 5/10, Batch 211/883, Training Loss: 0.6899\n",
      "Epoch 5/10, Batch 212/883, Training Loss: 0.6404\n",
      "Epoch 5/10, Batch 213/883, Training Loss: 0.5861\n",
      "Epoch 5/10, Batch 214/883, Training Loss: 0.6851\n",
      "Epoch 5/10, Batch 215/883, Training Loss: 0.7697\n",
      "Epoch 5/10, Batch 216/883, Training Loss: 0.6669\n",
      "Epoch 5/10, Batch 217/883, Training Loss: 0.5493\n",
      "Epoch 5/10, Batch 218/883, Training Loss: 0.8472\n",
      "Epoch 5/10, Batch 219/883, Training Loss: 0.7405\n",
      "Epoch 5/10, Batch 220/883, Training Loss: 0.7788\n",
      "Epoch 5/10, Batch 221/883, Training Loss: 0.7102\n",
      "Epoch 5/10, Batch 222/883, Training Loss: 0.8612\n",
      "Epoch 5/10, Batch 223/883, Training Loss: 0.7475\n",
      "Epoch 5/10, Batch 224/883, Training Loss: 0.3973\n",
      "Epoch 5/10, Batch 225/883, Training Loss: 0.7417\n",
      "Epoch 5/10, Batch 226/883, Training Loss: 0.6894\n",
      "Epoch 5/10, Batch 227/883, Training Loss: 0.5331\n",
      "Epoch 5/10, Batch 228/883, Training Loss: 0.7736\n",
      "Epoch 5/10, Batch 229/883, Training Loss: 0.8019\n",
      "Epoch 5/10, Batch 230/883, Training Loss: 0.6938\n",
      "Epoch 5/10, Batch 231/883, Training Loss: 0.5142\n",
      "Epoch 5/10, Batch 232/883, Training Loss: 0.5527\n",
      "Epoch 5/10, Batch 233/883, Training Loss: 0.4931\n",
      "Epoch 5/10, Batch 234/883, Training Loss: 0.5203\n",
      "Epoch 5/10, Batch 235/883, Training Loss: 0.7596\n",
      "Epoch 5/10, Batch 236/883, Training Loss: 0.8783\n",
      "Epoch 5/10, Batch 237/883, Training Loss: 0.9351\n",
      "Epoch 5/10, Batch 238/883, Training Loss: 0.7821\n",
      "Epoch 5/10, Batch 239/883, Training Loss: 0.6182\n",
      "Epoch 5/10, Batch 240/883, Training Loss: 0.7650\n",
      "Epoch 5/10, Batch 241/883, Training Loss: 0.6444\n",
      "Epoch 5/10, Batch 242/883, Training Loss: 0.6785\n",
      "Epoch 5/10, Batch 243/883, Training Loss: 0.8277\n",
      "Epoch 5/10, Batch 244/883, Training Loss: 0.8489\n",
      "Epoch 5/10, Batch 245/883, Training Loss: 0.6963\n",
      "Epoch 5/10, Batch 246/883, Training Loss: 1.0077\n",
      "Epoch 5/10, Batch 247/883, Training Loss: 0.8844\n",
      "Epoch 5/10, Batch 248/883, Training Loss: 0.8701\n",
      "Epoch 5/10, Batch 249/883, Training Loss: 0.9137\n",
      "Epoch 5/10, Batch 250/883, Training Loss: 1.1981\n",
      "Epoch 5/10, Batch 251/883, Training Loss: 0.5602\n",
      "Epoch 5/10, Batch 252/883, Training Loss: 0.7060\n",
      "Epoch 5/10, Batch 253/883, Training Loss: 0.4913\n",
      "Epoch 5/10, Batch 254/883, Training Loss: 0.8500\n",
      "Epoch 5/10, Batch 255/883, Training Loss: 0.5695\n",
      "Epoch 5/10, Batch 256/883, Training Loss: 0.6333\n",
      "Epoch 5/10, Batch 257/883, Training Loss: 0.9613\n",
      "Epoch 5/10, Batch 258/883, Training Loss: 0.8305\n",
      "Epoch 5/10, Batch 259/883, Training Loss: 0.7606\n",
      "Epoch 5/10, Batch 260/883, Training Loss: 0.6002\n",
      "Epoch 5/10, Batch 261/883, Training Loss: 0.6987\n",
      "Epoch 5/10, Batch 262/883, Training Loss: 0.6202\n",
      "Epoch 5/10, Batch 263/883, Training Loss: 0.6680\n",
      "Epoch 5/10, Batch 264/883, Training Loss: 1.3638\n",
      "Epoch 5/10, Batch 265/883, Training Loss: 0.7009\n",
      "Epoch 5/10, Batch 266/883, Training Loss: 0.6872\n",
      "Epoch 5/10, Batch 267/883, Training Loss: 0.7165\n",
      "Epoch 5/10, Batch 268/883, Training Loss: 0.9503\n",
      "Epoch 5/10, Batch 269/883, Training Loss: 0.5937\n",
      "Epoch 5/10, Batch 270/883, Training Loss: 0.6675\n",
      "Epoch 5/10, Batch 271/883, Training Loss: 0.7468\n",
      "Epoch 5/10, Batch 272/883, Training Loss: 0.6079\n",
      "Epoch 5/10, Batch 273/883, Training Loss: 0.7854\n",
      "Epoch 5/10, Batch 274/883, Training Loss: 0.7027\n",
      "Epoch 5/10, Batch 275/883, Training Loss: 0.8878\n",
      "Epoch 5/10, Batch 276/883, Training Loss: 0.6246\n",
      "Epoch 5/10, Batch 277/883, Training Loss: 0.6279\n",
      "Epoch 5/10, Batch 278/883, Training Loss: 0.5260\n",
      "Epoch 5/10, Batch 279/883, Training Loss: 0.5963\n",
      "Epoch 5/10, Batch 280/883, Training Loss: 1.1547\n",
      "Epoch 5/10, Batch 281/883, Training Loss: 0.5242\n",
      "Epoch 5/10, Batch 282/883, Training Loss: 0.6064\n",
      "Epoch 5/10, Batch 283/883, Training Loss: 0.9004\n",
      "Epoch 5/10, Batch 284/883, Training Loss: 0.6729\n",
      "Epoch 5/10, Batch 285/883, Training Loss: 0.4771\n",
      "Epoch 5/10, Batch 286/883, Training Loss: 0.7204\n",
      "Epoch 5/10, Batch 287/883, Training Loss: 0.4111\n",
      "Epoch 5/10, Batch 288/883, Training Loss: 1.0086\n",
      "Epoch 5/10, Batch 289/883, Training Loss: 0.4873\n",
      "Epoch 5/10, Batch 290/883, Training Loss: 0.9191\n",
      "Epoch 5/10, Batch 291/883, Training Loss: 0.8116\n",
      "Epoch 5/10, Batch 292/883, Training Loss: 0.7146\n",
      "Epoch 5/10, Batch 293/883, Training Loss: 0.9905\n",
      "Epoch 5/10, Batch 294/883, Training Loss: 0.3953\n",
      "Epoch 5/10, Batch 295/883, Training Loss: 0.7443\n",
      "Epoch 5/10, Batch 296/883, Training Loss: 0.7805\n",
      "Epoch 5/10, Batch 297/883, Training Loss: 0.7927\n",
      "Epoch 5/10, Batch 298/883, Training Loss: 1.0730\n",
      "Epoch 5/10, Batch 299/883, Training Loss: 0.6803\n",
      "Epoch 5/10, Batch 300/883, Training Loss: 0.9904\n",
      "Epoch 5/10, Batch 301/883, Training Loss: 0.9717\n",
      "Epoch 5/10, Batch 302/883, Training Loss: 0.8282\n",
      "Epoch 5/10, Batch 303/883, Training Loss: 0.6122\n",
      "Epoch 5/10, Batch 304/883, Training Loss: 0.7070\n",
      "Epoch 5/10, Batch 305/883, Training Loss: 0.5456\n",
      "Epoch 5/10, Batch 306/883, Training Loss: 0.7721\n",
      "Epoch 5/10, Batch 307/883, Training Loss: 0.5390\n",
      "Epoch 5/10, Batch 308/883, Training Loss: 0.7396\n",
      "Epoch 5/10, Batch 309/883, Training Loss: 0.7231\n",
      "Epoch 5/10, Batch 310/883, Training Loss: 0.5028\n",
      "Epoch 5/10, Batch 311/883, Training Loss: 0.6546\n",
      "Epoch 5/10, Batch 312/883, Training Loss: 0.7655\n",
      "Epoch 5/10, Batch 313/883, Training Loss: 0.8939\n",
      "Epoch 5/10, Batch 314/883, Training Loss: 0.5899\n",
      "Epoch 5/10, Batch 315/883, Training Loss: 1.0029\n",
      "Epoch 5/10, Batch 316/883, Training Loss: 0.7420\n",
      "Epoch 5/10, Batch 317/883, Training Loss: 0.7626\n",
      "Epoch 5/10, Batch 318/883, Training Loss: 0.6312\n",
      "Epoch 5/10, Batch 319/883, Training Loss: 0.9518\n",
      "Epoch 5/10, Batch 320/883, Training Loss: 0.8966\n",
      "Epoch 5/10, Batch 321/883, Training Loss: 0.7416\n",
      "Epoch 5/10, Batch 322/883, Training Loss: 0.6024\n",
      "Epoch 5/10, Batch 323/883, Training Loss: 0.6817\n",
      "Epoch 5/10, Batch 324/883, Training Loss: 0.7596\n",
      "Epoch 5/10, Batch 325/883, Training Loss: 0.6189\n",
      "Epoch 5/10, Batch 326/883, Training Loss: 0.5214\n",
      "Epoch 5/10, Batch 327/883, Training Loss: 0.6356\n",
      "Epoch 5/10, Batch 328/883, Training Loss: 0.9457\n",
      "Epoch 5/10, Batch 329/883, Training Loss: 0.5801\n",
      "Epoch 5/10, Batch 330/883, Training Loss: 0.5895\n",
      "Epoch 5/10, Batch 331/883, Training Loss: 0.6686\n",
      "Epoch 5/10, Batch 332/883, Training Loss: 0.8258\n",
      "Epoch 5/10, Batch 333/883, Training Loss: 0.7745\n",
      "Epoch 5/10, Batch 334/883, Training Loss: 0.8909\n",
      "Epoch 5/10, Batch 335/883, Training Loss: 0.6835\n",
      "Epoch 5/10, Batch 336/883, Training Loss: 0.6523\n",
      "Epoch 5/10, Batch 337/883, Training Loss: 0.8035\n",
      "Epoch 5/10, Batch 338/883, Training Loss: 0.9737\n",
      "Epoch 5/10, Batch 339/883, Training Loss: 0.4269\n",
      "Epoch 5/10, Batch 340/883, Training Loss: 0.7869\n",
      "Epoch 5/10, Batch 341/883, Training Loss: 0.6701\n",
      "Epoch 5/10, Batch 342/883, Training Loss: 0.6042\n",
      "Epoch 5/10, Batch 343/883, Training Loss: 0.8663\n",
      "Epoch 5/10, Batch 344/883, Training Loss: 0.6644\n",
      "Epoch 5/10, Batch 345/883, Training Loss: 0.9008\n",
      "Epoch 5/10, Batch 346/883, Training Loss: 0.8087\n",
      "Epoch 5/10, Batch 347/883, Training Loss: 0.6880\n",
      "Epoch 5/10, Batch 348/883, Training Loss: 0.9434\n",
      "Epoch 5/10, Batch 349/883, Training Loss: 0.5888\n",
      "Epoch 5/10, Batch 350/883, Training Loss: 0.9559\n",
      "Epoch 5/10, Batch 351/883, Training Loss: 0.7529\n",
      "Epoch 5/10, Batch 352/883, Training Loss: 0.8669\n",
      "Epoch 5/10, Batch 353/883, Training Loss: 0.7447\n",
      "Epoch 5/10, Batch 354/883, Training Loss: 0.8063\n",
      "Epoch 5/10, Batch 355/883, Training Loss: 0.4746\n",
      "Epoch 5/10, Batch 356/883, Training Loss: 0.6888\n",
      "Epoch 5/10, Batch 357/883, Training Loss: 0.5167\n",
      "Epoch 5/10, Batch 358/883, Training Loss: 0.5127\n",
      "Epoch 5/10, Batch 359/883, Training Loss: 0.7073\n",
      "Epoch 5/10, Batch 360/883, Training Loss: 0.6061\n",
      "Epoch 5/10, Batch 361/883, Training Loss: 0.7352\n",
      "Epoch 5/10, Batch 362/883, Training Loss: 0.4975\n",
      "Epoch 5/10, Batch 363/883, Training Loss: 0.7432\n",
      "Epoch 5/10, Batch 364/883, Training Loss: 0.5655\n",
      "Epoch 5/10, Batch 365/883, Training Loss: 0.6536\n",
      "Epoch 5/10, Batch 366/883, Training Loss: 0.8694\n",
      "Epoch 5/10, Batch 367/883, Training Loss: 0.7800\n",
      "Epoch 5/10, Batch 368/883, Training Loss: 0.4814\n",
      "Epoch 5/10, Batch 369/883, Training Loss: 0.6935\n",
      "Epoch 5/10, Batch 370/883, Training Loss: 0.6706\n",
      "Epoch 5/10, Batch 371/883, Training Loss: 0.7593\n",
      "Epoch 5/10, Batch 372/883, Training Loss: 0.7452\n",
      "Epoch 5/10, Batch 373/883, Training Loss: 0.6916\n",
      "Epoch 5/10, Batch 374/883, Training Loss: 0.9331\n",
      "Epoch 5/10, Batch 375/883, Training Loss: 0.7937\n",
      "Epoch 5/10, Batch 376/883, Training Loss: 0.8065\n",
      "Epoch 5/10, Batch 377/883, Training Loss: 0.9045\n",
      "Epoch 5/10, Batch 378/883, Training Loss: 0.9426\n",
      "Epoch 5/10, Batch 379/883, Training Loss: 0.6199\n",
      "Epoch 5/10, Batch 380/883, Training Loss: 0.5902\n",
      "Epoch 5/10, Batch 381/883, Training Loss: 1.0669\n",
      "Epoch 5/10, Batch 382/883, Training Loss: 0.5312\n",
      "Epoch 5/10, Batch 383/883, Training Loss: 0.6792\n",
      "Epoch 5/10, Batch 384/883, Training Loss: 0.6357\n",
      "Epoch 5/10, Batch 385/883, Training Loss: 0.9046\n",
      "Epoch 5/10, Batch 386/883, Training Loss: 0.9655\n",
      "Epoch 5/10, Batch 387/883, Training Loss: 0.6429\n",
      "Epoch 5/10, Batch 388/883, Training Loss: 0.7932\n",
      "Epoch 5/10, Batch 389/883, Training Loss: 0.4925\n",
      "Epoch 5/10, Batch 390/883, Training Loss: 0.6776\n",
      "Epoch 5/10, Batch 391/883, Training Loss: 0.5880\n",
      "Epoch 5/10, Batch 392/883, Training Loss: 0.5864\n",
      "Epoch 5/10, Batch 393/883, Training Loss: 0.7777\n",
      "Epoch 5/10, Batch 394/883, Training Loss: 0.6696\n",
      "Epoch 5/10, Batch 395/883, Training Loss: 1.1972\n",
      "Epoch 5/10, Batch 396/883, Training Loss: 0.7814\n",
      "Epoch 5/10, Batch 397/883, Training Loss: 0.8605\n",
      "Epoch 5/10, Batch 398/883, Training Loss: 0.6937\n",
      "Epoch 5/10, Batch 399/883, Training Loss: 0.7969\n",
      "Epoch 5/10, Batch 400/883, Training Loss: 0.7355\n",
      "Epoch 5/10, Batch 401/883, Training Loss: 0.8014\n",
      "Epoch 5/10, Batch 402/883, Training Loss: 0.5213\n",
      "Epoch 5/10, Batch 403/883, Training Loss: 0.7376\n",
      "Epoch 5/10, Batch 404/883, Training Loss: 0.8623\n",
      "Epoch 5/10, Batch 405/883, Training Loss: 0.7103\n",
      "Epoch 5/10, Batch 406/883, Training Loss: 0.6852\n",
      "Epoch 5/10, Batch 407/883, Training Loss: 0.8102\n",
      "Epoch 5/10, Batch 408/883, Training Loss: 0.5222\n",
      "Epoch 5/10, Batch 409/883, Training Loss: 0.7815\n",
      "Epoch 5/10, Batch 410/883, Training Loss: 0.9281\n",
      "Epoch 5/10, Batch 411/883, Training Loss: 0.4238\n",
      "Epoch 5/10, Batch 412/883, Training Loss: 0.4931\n",
      "Epoch 5/10, Batch 413/883, Training Loss: 0.8329\n",
      "Epoch 5/10, Batch 414/883, Training Loss: 0.5337\n",
      "Epoch 5/10, Batch 415/883, Training Loss: 0.8738\n",
      "Epoch 5/10, Batch 416/883, Training Loss: 0.6295\n",
      "Epoch 5/10, Batch 417/883, Training Loss: 0.6860\n",
      "Epoch 5/10, Batch 418/883, Training Loss: 0.8969\n",
      "Epoch 5/10, Batch 419/883, Training Loss: 0.6633\n",
      "Epoch 5/10, Batch 420/883, Training Loss: 0.8847\n",
      "Epoch 5/10, Batch 421/883, Training Loss: 0.8440\n",
      "Epoch 5/10, Batch 422/883, Training Loss: 0.8586\n",
      "Epoch 5/10, Batch 423/883, Training Loss: 0.7694\n",
      "Epoch 5/10, Batch 424/883, Training Loss: 0.6383\n",
      "Epoch 5/10, Batch 425/883, Training Loss: 0.6254\n",
      "Epoch 5/10, Batch 426/883, Training Loss: 0.6175\n",
      "Epoch 5/10, Batch 427/883, Training Loss: 0.8326\n",
      "Epoch 5/10, Batch 428/883, Training Loss: 0.7140\n",
      "Epoch 5/10, Batch 429/883, Training Loss: 0.6755\n",
      "Epoch 5/10, Batch 430/883, Training Loss: 0.7034\n",
      "Epoch 5/10, Batch 431/883, Training Loss: 0.8646\n",
      "Epoch 5/10, Batch 432/883, Training Loss: 0.6984\n",
      "Epoch 5/10, Batch 433/883, Training Loss: 0.7106\n",
      "Epoch 5/10, Batch 434/883, Training Loss: 0.6017\n",
      "Epoch 5/10, Batch 435/883, Training Loss: 0.6320\n",
      "Epoch 5/10, Batch 436/883, Training Loss: 0.7343\n",
      "Epoch 5/10, Batch 437/883, Training Loss: 0.8504\n",
      "Epoch 5/10, Batch 438/883, Training Loss: 0.5947\n",
      "Epoch 5/10, Batch 439/883, Training Loss: 0.7067\n",
      "Epoch 5/10, Batch 440/883, Training Loss: 0.6223\n",
      "Epoch 5/10, Batch 441/883, Training Loss: 0.5560\n",
      "Epoch 5/10, Batch 442/883, Training Loss: 1.0414\n",
      "Epoch 5/10, Batch 443/883, Training Loss: 0.7122\n",
      "Epoch 5/10, Batch 444/883, Training Loss: 0.5710\n",
      "Epoch 5/10, Batch 445/883, Training Loss: 0.7413\n",
      "Epoch 5/10, Batch 446/883, Training Loss: 0.5702\n",
      "Epoch 5/10, Batch 447/883, Training Loss: 0.6557\n",
      "Epoch 5/10, Batch 448/883, Training Loss: 0.7596\n",
      "Epoch 5/10, Batch 449/883, Training Loss: 0.7041\n",
      "Epoch 5/10, Batch 450/883, Training Loss: 0.7916\n",
      "Epoch 5/10, Batch 451/883, Training Loss: 0.5823\n",
      "Epoch 5/10, Batch 452/883, Training Loss: 0.5064\n",
      "Epoch 5/10, Batch 453/883, Training Loss: 1.1017\n",
      "Epoch 5/10, Batch 454/883, Training Loss: 0.6518\n",
      "Epoch 5/10, Batch 455/883, Training Loss: 0.7410\n",
      "Epoch 5/10, Batch 456/883, Training Loss: 0.8796\n",
      "Epoch 5/10, Batch 457/883, Training Loss: 0.6305\n",
      "Epoch 5/10, Batch 458/883, Training Loss: 0.4576\n",
      "Epoch 5/10, Batch 459/883, Training Loss: 0.9728\n",
      "Epoch 5/10, Batch 460/883, Training Loss: 0.4702\n",
      "Epoch 5/10, Batch 461/883, Training Loss: 0.4561\n",
      "Epoch 5/10, Batch 462/883, Training Loss: 0.7318\n",
      "Epoch 5/10, Batch 463/883, Training Loss: 0.9041\n",
      "Epoch 5/10, Batch 464/883, Training Loss: 1.1659\n",
      "Epoch 5/10, Batch 465/883, Training Loss: 0.6120\n",
      "Epoch 5/10, Batch 466/883, Training Loss: 0.5924\n",
      "Epoch 5/10, Batch 467/883, Training Loss: 1.1595\n",
      "Epoch 5/10, Batch 468/883, Training Loss: 0.6997\n",
      "Epoch 5/10, Batch 469/883, Training Loss: 0.7619\n",
      "Epoch 5/10, Batch 470/883, Training Loss: 1.1390\n",
      "Epoch 5/10, Batch 471/883, Training Loss: 0.9317\n",
      "Epoch 5/10, Batch 472/883, Training Loss: 0.6358\n",
      "Epoch 5/10, Batch 473/883, Training Loss: 0.8249\n",
      "Epoch 5/10, Batch 474/883, Training Loss: 0.8782\n",
      "Epoch 5/10, Batch 475/883, Training Loss: 0.7034\n",
      "Epoch 5/10, Batch 476/883, Training Loss: 0.6902\n",
      "Epoch 5/10, Batch 477/883, Training Loss: 0.8653\n",
      "Epoch 5/10, Batch 478/883, Training Loss: 0.5737\n",
      "Epoch 5/10, Batch 479/883, Training Loss: 0.8163\n",
      "Epoch 5/10, Batch 480/883, Training Loss: 0.8460\n",
      "Epoch 5/10, Batch 481/883, Training Loss: 0.6604\n",
      "Epoch 5/10, Batch 482/883, Training Loss: 0.6292\n",
      "Epoch 5/10, Batch 483/883, Training Loss: 0.5002\n",
      "Epoch 5/10, Batch 484/883, Training Loss: 0.7897\n",
      "Epoch 5/10, Batch 485/883, Training Loss: 0.8790\n",
      "Epoch 5/10, Batch 486/883, Training Loss: 1.1688\n",
      "Epoch 5/10, Batch 487/883, Training Loss: 0.7620\n",
      "Epoch 5/10, Batch 488/883, Training Loss: 0.6731\n",
      "Epoch 5/10, Batch 489/883, Training Loss: 0.7918\n",
      "Epoch 5/10, Batch 490/883, Training Loss: 0.8858\n",
      "Epoch 5/10, Batch 491/883, Training Loss: 0.7134\n",
      "Epoch 5/10, Batch 492/883, Training Loss: 0.8831\n",
      "Epoch 5/10, Batch 493/883, Training Loss: 0.6617\n",
      "Epoch 5/10, Batch 494/883, Training Loss: 0.7288\n",
      "Epoch 5/10, Batch 495/883, Training Loss: 0.8276\n",
      "Epoch 5/10, Batch 496/883, Training Loss: 0.7393\n",
      "Epoch 5/10, Batch 497/883, Training Loss: 0.6087\n",
      "Epoch 5/10, Batch 498/883, Training Loss: 0.8456\n",
      "Epoch 5/10, Batch 499/883, Training Loss: 0.6668\n",
      "Epoch 5/10, Batch 500/883, Training Loss: 0.7220\n",
      "Epoch 5/10, Batch 501/883, Training Loss: 0.7518\n",
      "Epoch 5/10, Batch 502/883, Training Loss: 0.5576\n",
      "Epoch 5/10, Batch 503/883, Training Loss: 0.6078\n",
      "Epoch 5/10, Batch 504/883, Training Loss: 0.8213\n",
      "Epoch 5/10, Batch 505/883, Training Loss: 0.8415\n",
      "Epoch 5/10, Batch 506/883, Training Loss: 0.6459\n",
      "Epoch 5/10, Batch 507/883, Training Loss: 0.8300\n",
      "Epoch 5/10, Batch 508/883, Training Loss: 0.8871\n",
      "Epoch 5/10, Batch 509/883, Training Loss: 0.8491\n",
      "Epoch 5/10, Batch 510/883, Training Loss: 0.7792\n",
      "Epoch 5/10, Batch 511/883, Training Loss: 0.6312\n",
      "Epoch 5/10, Batch 512/883, Training Loss: 0.6461\n",
      "Epoch 5/10, Batch 513/883, Training Loss: 0.8958\n",
      "Epoch 5/10, Batch 514/883, Training Loss: 0.6043\n",
      "Epoch 5/10, Batch 515/883, Training Loss: 0.6368\n",
      "Epoch 5/10, Batch 516/883, Training Loss: 0.7423\n",
      "Epoch 5/10, Batch 517/883, Training Loss: 0.6745\n",
      "Epoch 5/10, Batch 518/883, Training Loss: 1.0408\n",
      "Epoch 5/10, Batch 519/883, Training Loss: 0.9430\n",
      "Epoch 5/10, Batch 520/883, Training Loss: 0.9540\n",
      "Epoch 5/10, Batch 521/883, Training Loss: 0.7630\n",
      "Epoch 5/10, Batch 522/883, Training Loss: 0.8571\n",
      "Epoch 5/10, Batch 523/883, Training Loss: 0.7988\n",
      "Epoch 5/10, Batch 524/883, Training Loss: 0.8031\n",
      "Epoch 5/10, Batch 525/883, Training Loss: 0.5987\n",
      "Epoch 5/10, Batch 526/883, Training Loss: 0.5611\n",
      "Epoch 5/10, Batch 527/883, Training Loss: 0.7428\n",
      "Epoch 5/10, Batch 528/883, Training Loss: 0.8175\n",
      "Epoch 5/10, Batch 529/883, Training Loss: 0.9363\n",
      "Epoch 5/10, Batch 530/883, Training Loss: 0.8746\n",
      "Epoch 5/10, Batch 531/883, Training Loss: 0.5339\n",
      "Epoch 5/10, Batch 532/883, Training Loss: 0.8169\n",
      "Epoch 5/10, Batch 533/883, Training Loss: 0.5368\n",
      "Epoch 5/10, Batch 534/883, Training Loss: 0.6582\n",
      "Epoch 5/10, Batch 535/883, Training Loss: 0.9504\n",
      "Epoch 5/10, Batch 536/883, Training Loss: 0.6325\n",
      "Epoch 5/10, Batch 537/883, Training Loss: 0.4895\n",
      "Epoch 5/10, Batch 538/883, Training Loss: 0.7680\n",
      "Epoch 5/10, Batch 539/883, Training Loss: 0.7481\n",
      "Epoch 5/10, Batch 540/883, Training Loss: 0.7020\n",
      "Epoch 5/10, Batch 541/883, Training Loss: 0.5518\n",
      "Epoch 5/10, Batch 542/883, Training Loss: 0.5625\n",
      "Epoch 5/10, Batch 543/883, Training Loss: 0.6951\n",
      "Epoch 5/10, Batch 544/883, Training Loss: 0.7400\n",
      "Epoch 5/10, Batch 545/883, Training Loss: 0.7012\n",
      "Epoch 5/10, Batch 546/883, Training Loss: 0.6327\n",
      "Epoch 5/10, Batch 547/883, Training Loss: 0.7570\n",
      "Epoch 5/10, Batch 548/883, Training Loss: 0.7515\n",
      "Epoch 5/10, Batch 549/883, Training Loss: 0.6670\n",
      "Epoch 5/10, Batch 550/883, Training Loss: 0.9420\n",
      "Epoch 5/10, Batch 551/883, Training Loss: 0.6465\n",
      "Epoch 5/10, Batch 552/883, Training Loss: 0.8407\n",
      "Epoch 5/10, Batch 553/883, Training Loss: 0.6382\n",
      "Epoch 5/10, Batch 554/883, Training Loss: 0.5956\n",
      "Epoch 5/10, Batch 555/883, Training Loss: 0.6981\n",
      "Epoch 5/10, Batch 556/883, Training Loss: 0.6932\n",
      "Epoch 5/10, Batch 557/883, Training Loss: 0.4881\n",
      "Epoch 5/10, Batch 558/883, Training Loss: 0.9256\n",
      "Epoch 5/10, Batch 559/883, Training Loss: 0.6046\n",
      "Epoch 5/10, Batch 560/883, Training Loss: 1.0106\n",
      "Epoch 5/10, Batch 561/883, Training Loss: 0.6509\n",
      "Epoch 5/10, Batch 562/883, Training Loss: 0.7298\n",
      "Epoch 5/10, Batch 563/883, Training Loss: 0.7674\n",
      "Epoch 5/10, Batch 564/883, Training Loss: 0.7837\n",
      "Epoch 5/10, Batch 565/883, Training Loss: 1.0372\n",
      "Epoch 5/10, Batch 566/883, Training Loss: 0.7357\n",
      "Epoch 5/10, Batch 567/883, Training Loss: 0.9116\n",
      "Epoch 5/10, Batch 568/883, Training Loss: 0.8127\n",
      "Epoch 5/10, Batch 569/883, Training Loss: 0.7289\n",
      "Epoch 5/10, Batch 570/883, Training Loss: 0.6845\n",
      "Epoch 5/10, Batch 571/883, Training Loss: 0.8536\n",
      "Epoch 5/10, Batch 572/883, Training Loss: 0.7023\n",
      "Epoch 5/10, Batch 573/883, Training Loss: 0.4844\n",
      "Epoch 5/10, Batch 574/883, Training Loss: 1.0205\n",
      "Epoch 5/10, Batch 575/883, Training Loss: 0.7874\n",
      "Epoch 5/10, Batch 576/883, Training Loss: 1.0108\n",
      "Epoch 5/10, Batch 577/883, Training Loss: 0.5166\n",
      "Epoch 5/10, Batch 578/883, Training Loss: 0.7370\n",
      "Epoch 5/10, Batch 579/883, Training Loss: 1.0566\n",
      "Epoch 5/10, Batch 580/883, Training Loss: 0.5817\n",
      "Epoch 5/10, Batch 581/883, Training Loss: 0.8880\n",
      "Epoch 5/10, Batch 582/883, Training Loss: 0.5454\n",
      "Epoch 5/10, Batch 583/883, Training Loss: 0.6959\n",
      "Epoch 5/10, Batch 584/883, Training Loss: 0.5575\n",
      "Epoch 5/10, Batch 585/883, Training Loss: 0.6274\n",
      "Epoch 5/10, Batch 586/883, Training Loss: 0.4899\n",
      "Epoch 5/10, Batch 587/883, Training Loss: 0.7244\n",
      "Epoch 5/10, Batch 588/883, Training Loss: 0.8388\n",
      "Epoch 5/10, Batch 589/883, Training Loss: 0.9316\n",
      "Epoch 5/10, Batch 590/883, Training Loss: 0.8293\n",
      "Epoch 5/10, Batch 591/883, Training Loss: 0.6956\n",
      "Epoch 5/10, Batch 592/883, Training Loss: 0.7202\n",
      "Epoch 5/10, Batch 593/883, Training Loss: 0.6422\n",
      "Epoch 5/10, Batch 594/883, Training Loss: 0.8969\n",
      "Epoch 5/10, Batch 595/883, Training Loss: 0.6716\n",
      "Epoch 5/10, Batch 596/883, Training Loss: 0.7632\n",
      "Epoch 5/10, Batch 597/883, Training Loss: 0.7441\n",
      "Epoch 5/10, Batch 598/883, Training Loss: 0.9574\n",
      "Epoch 5/10, Batch 599/883, Training Loss: 0.5933\n",
      "Epoch 5/10, Batch 600/883, Training Loss: 0.8028\n",
      "Epoch 5/10, Batch 601/883, Training Loss: 0.6103\n",
      "Epoch 5/10, Batch 602/883, Training Loss: 0.7523\n",
      "Epoch 5/10, Batch 603/883, Training Loss: 0.9064\n",
      "Epoch 5/10, Batch 604/883, Training Loss: 0.8030\n",
      "Epoch 5/10, Batch 605/883, Training Loss: 0.7145\n",
      "Epoch 5/10, Batch 606/883, Training Loss: 0.7610\n",
      "Epoch 5/10, Batch 607/883, Training Loss: 0.8305\n",
      "Epoch 5/10, Batch 608/883, Training Loss: 0.8458\n",
      "Epoch 5/10, Batch 609/883, Training Loss: 0.7183\n",
      "Epoch 5/10, Batch 610/883, Training Loss: 0.7656\n",
      "Epoch 5/10, Batch 611/883, Training Loss: 0.6333\n",
      "Epoch 5/10, Batch 612/883, Training Loss: 0.6489\n",
      "Epoch 5/10, Batch 613/883, Training Loss: 0.6995\n",
      "Epoch 5/10, Batch 614/883, Training Loss: 1.0675\n",
      "Epoch 5/10, Batch 615/883, Training Loss: 0.8091\n",
      "Epoch 5/10, Batch 616/883, Training Loss: 1.0734\n",
      "Epoch 5/10, Batch 617/883, Training Loss: 0.9243\n",
      "Epoch 5/10, Batch 618/883, Training Loss: 0.7149\n",
      "Epoch 5/10, Batch 619/883, Training Loss: 0.8405\n",
      "Epoch 5/10, Batch 620/883, Training Loss: 0.7482\n",
      "Epoch 5/10, Batch 621/883, Training Loss: 0.5558\n",
      "Epoch 5/10, Batch 622/883, Training Loss: 0.6439\n",
      "Epoch 5/10, Batch 623/883, Training Loss: 0.7152\n",
      "Epoch 5/10, Batch 624/883, Training Loss: 1.0121\n",
      "Epoch 5/10, Batch 625/883, Training Loss: 0.7043\n",
      "Epoch 5/10, Batch 626/883, Training Loss: 0.9162\n",
      "Epoch 5/10, Batch 627/883, Training Loss: 0.8224\n",
      "Epoch 5/10, Batch 628/883, Training Loss: 0.6368\n",
      "Epoch 5/10, Batch 629/883, Training Loss: 0.7222\n",
      "Epoch 5/10, Batch 630/883, Training Loss: 0.5930\n",
      "Epoch 5/10, Batch 631/883, Training Loss: 0.6915\n",
      "Epoch 5/10, Batch 632/883, Training Loss: 0.5910\n",
      "Epoch 5/10, Batch 633/883, Training Loss: 0.7960\n",
      "Epoch 5/10, Batch 634/883, Training Loss: 1.0810\n",
      "Epoch 5/10, Batch 635/883, Training Loss: 0.6415\n",
      "Epoch 5/10, Batch 636/883, Training Loss: 0.6161\n",
      "Epoch 5/10, Batch 637/883, Training Loss: 0.7658\n",
      "Epoch 5/10, Batch 638/883, Training Loss: 0.6854\n",
      "Epoch 5/10, Batch 639/883, Training Loss: 0.5550\n",
      "Epoch 5/10, Batch 640/883, Training Loss: 0.4457\n",
      "Epoch 5/10, Batch 641/883, Training Loss: 0.6023\n",
      "Epoch 5/10, Batch 642/883, Training Loss: 0.8133\n",
      "Epoch 5/10, Batch 643/883, Training Loss: 0.6098\n",
      "Epoch 5/10, Batch 644/883, Training Loss: 0.5745\n",
      "Epoch 5/10, Batch 645/883, Training Loss: 0.6012\n",
      "Epoch 5/10, Batch 646/883, Training Loss: 0.6244\n",
      "Epoch 5/10, Batch 647/883, Training Loss: 0.8515\n",
      "Epoch 5/10, Batch 648/883, Training Loss: 0.8345\n",
      "Epoch 5/10, Batch 649/883, Training Loss: 0.6066\n",
      "Epoch 5/10, Batch 650/883, Training Loss: 0.6289\n",
      "Epoch 5/10, Batch 651/883, Training Loss: 0.6606\n",
      "Epoch 5/10, Batch 652/883, Training Loss: 0.7570\n",
      "Epoch 5/10, Batch 653/883, Training Loss: 0.6790\n",
      "Epoch 5/10, Batch 654/883, Training Loss: 0.9463\n",
      "Epoch 5/10, Batch 655/883, Training Loss: 0.6740\n",
      "Epoch 5/10, Batch 656/883, Training Loss: 0.6292\n",
      "Epoch 5/10, Batch 657/883, Training Loss: 0.7501\n",
      "Epoch 5/10, Batch 658/883, Training Loss: 0.5016\n",
      "Epoch 5/10, Batch 659/883, Training Loss: 0.8743\n",
      "Epoch 5/10, Batch 660/883, Training Loss: 0.5366\n",
      "Epoch 5/10, Batch 661/883, Training Loss: 0.5502\n",
      "Epoch 5/10, Batch 662/883, Training Loss: 0.7876\n",
      "Epoch 5/10, Batch 663/883, Training Loss: 0.7235\n",
      "Epoch 5/10, Batch 664/883, Training Loss: 0.6927\n",
      "Epoch 5/10, Batch 665/883, Training Loss: 0.7016\n",
      "Epoch 5/10, Batch 666/883, Training Loss: 0.6529\n",
      "Epoch 5/10, Batch 667/883, Training Loss: 0.6671\n",
      "Epoch 5/10, Batch 668/883, Training Loss: 0.5760\n",
      "Epoch 5/10, Batch 669/883, Training Loss: 0.7048\n",
      "Epoch 5/10, Batch 670/883, Training Loss: 0.5377\n",
      "Epoch 5/10, Batch 671/883, Training Loss: 1.1736\n",
      "Epoch 5/10, Batch 672/883, Training Loss: 0.7581\n",
      "Epoch 5/10, Batch 673/883, Training Loss: 0.5423\n",
      "Epoch 5/10, Batch 674/883, Training Loss: 0.8079\n",
      "Epoch 5/10, Batch 675/883, Training Loss: 0.7118\n",
      "Epoch 5/10, Batch 676/883, Training Loss: 0.9450\n",
      "Epoch 5/10, Batch 677/883, Training Loss: 0.6470\n",
      "Epoch 5/10, Batch 678/883, Training Loss: 0.6293\n",
      "Epoch 5/10, Batch 679/883, Training Loss: 0.9290\n",
      "Epoch 5/10, Batch 680/883, Training Loss: 0.7767\n",
      "Epoch 5/10, Batch 681/883, Training Loss: 0.4522\n",
      "Epoch 5/10, Batch 682/883, Training Loss: 0.6233\n",
      "Epoch 5/10, Batch 683/883, Training Loss: 0.7841\n",
      "Epoch 5/10, Batch 684/883, Training Loss: 0.6996\n",
      "Epoch 5/10, Batch 685/883, Training Loss: 0.5505\n",
      "Epoch 5/10, Batch 686/883, Training Loss: 0.5221\n",
      "Epoch 5/10, Batch 687/883, Training Loss: 0.8152\n",
      "Epoch 5/10, Batch 688/883, Training Loss: 0.5909\n",
      "Epoch 5/10, Batch 689/883, Training Loss: 0.7509\n",
      "Epoch 5/10, Batch 690/883, Training Loss: 0.6513\n",
      "Epoch 5/10, Batch 691/883, Training Loss: 0.6950\n",
      "Epoch 5/10, Batch 692/883, Training Loss: 0.6976\n",
      "Epoch 5/10, Batch 693/883, Training Loss: 0.5119\n",
      "Epoch 5/10, Batch 694/883, Training Loss: 0.6165\n",
      "Epoch 5/10, Batch 695/883, Training Loss: 0.7887\n",
      "Epoch 5/10, Batch 696/883, Training Loss: 0.6283\n",
      "Epoch 5/10, Batch 697/883, Training Loss: 1.1854\n",
      "Epoch 5/10, Batch 698/883, Training Loss: 0.5562\n",
      "Epoch 5/10, Batch 699/883, Training Loss: 0.8263\n",
      "Epoch 5/10, Batch 700/883, Training Loss: 0.4979\n",
      "Epoch 5/10, Batch 701/883, Training Loss: 0.8083\n",
      "Epoch 5/10, Batch 702/883, Training Loss: 0.5908\n",
      "Epoch 5/10, Batch 703/883, Training Loss: 0.6390\n",
      "Epoch 5/10, Batch 704/883, Training Loss: 0.5142\n",
      "Epoch 5/10, Batch 705/883, Training Loss: 0.6744\n",
      "Epoch 5/10, Batch 706/883, Training Loss: 0.5185\n",
      "Epoch 5/10, Batch 707/883, Training Loss: 0.6708\n",
      "Epoch 5/10, Batch 708/883, Training Loss: 0.6181\n",
      "Epoch 5/10, Batch 709/883, Training Loss: 0.9005\n",
      "Epoch 5/10, Batch 710/883, Training Loss: 0.6161\n",
      "Epoch 5/10, Batch 711/883, Training Loss: 0.6424\n",
      "Epoch 5/10, Batch 712/883, Training Loss: 0.6416\n",
      "Epoch 5/10, Batch 713/883, Training Loss: 0.6196\n",
      "Epoch 5/10, Batch 714/883, Training Loss: 0.6832\n",
      "Epoch 5/10, Batch 715/883, Training Loss: 0.7548\n",
      "Epoch 5/10, Batch 716/883, Training Loss: 0.8223\n",
      "Epoch 5/10, Batch 717/883, Training Loss: 0.8640\n",
      "Epoch 5/10, Batch 718/883, Training Loss: 0.9501\n",
      "Epoch 5/10, Batch 719/883, Training Loss: 0.8596\n",
      "Epoch 5/10, Batch 720/883, Training Loss: 0.7592\n",
      "Epoch 5/10, Batch 721/883, Training Loss: 0.6183\n",
      "Epoch 5/10, Batch 722/883, Training Loss: 1.1821\n",
      "Epoch 5/10, Batch 723/883, Training Loss: 0.8661\n",
      "Epoch 5/10, Batch 724/883, Training Loss: 0.6512\n",
      "Epoch 5/10, Batch 725/883, Training Loss: 0.5739\n",
      "Epoch 5/10, Batch 726/883, Training Loss: 0.5942\n",
      "Epoch 5/10, Batch 727/883, Training Loss: 0.6104\n",
      "Epoch 5/10, Batch 728/883, Training Loss: 0.7835\n",
      "Epoch 5/10, Batch 729/883, Training Loss: 0.5835\n",
      "Epoch 5/10, Batch 730/883, Training Loss: 0.7426\n",
      "Epoch 5/10, Batch 731/883, Training Loss: 0.7185\n",
      "Epoch 5/10, Batch 732/883, Training Loss: 0.6715\n",
      "Epoch 5/10, Batch 733/883, Training Loss: 0.5593\n",
      "Epoch 5/10, Batch 734/883, Training Loss: 0.6736\n",
      "Epoch 5/10, Batch 735/883, Training Loss: 0.4622\n",
      "Epoch 5/10, Batch 736/883, Training Loss: 0.7525\n",
      "Epoch 5/10, Batch 737/883, Training Loss: 0.8036\n",
      "Epoch 5/10, Batch 738/883, Training Loss: 0.5138\n",
      "Epoch 5/10, Batch 739/883, Training Loss: 1.1127\n",
      "Epoch 5/10, Batch 740/883, Training Loss: 0.6153\n",
      "Epoch 5/10, Batch 741/883, Training Loss: 1.1020\n",
      "Epoch 5/10, Batch 742/883, Training Loss: 0.6280\n",
      "Epoch 5/10, Batch 743/883, Training Loss: 0.7388\n",
      "Epoch 5/10, Batch 744/883, Training Loss: 0.9236\n",
      "Epoch 5/10, Batch 745/883, Training Loss: 0.7116\n",
      "Epoch 5/10, Batch 746/883, Training Loss: 0.8835\n",
      "Epoch 5/10, Batch 747/883, Training Loss: 1.0539\n",
      "Epoch 5/10, Batch 748/883, Training Loss: 0.7834\n",
      "Epoch 5/10, Batch 749/883, Training Loss: 0.7841\n",
      "Epoch 5/10, Batch 750/883, Training Loss: 0.8998\n",
      "Epoch 5/10, Batch 751/883, Training Loss: 0.9049\n",
      "Epoch 5/10, Batch 752/883, Training Loss: 0.6667\n",
      "Epoch 5/10, Batch 753/883, Training Loss: 0.6846\n",
      "Epoch 5/10, Batch 754/883, Training Loss: 0.6591\n",
      "Epoch 5/10, Batch 755/883, Training Loss: 0.5887\n",
      "Epoch 5/10, Batch 756/883, Training Loss: 0.6436\n",
      "Epoch 5/10, Batch 757/883, Training Loss: 0.7877\n",
      "Epoch 5/10, Batch 758/883, Training Loss: 0.6290\n",
      "Epoch 5/10, Batch 759/883, Training Loss: 0.7428\n",
      "Epoch 5/10, Batch 760/883, Training Loss: 0.7197\n",
      "Epoch 5/10, Batch 761/883, Training Loss: 0.8393\n",
      "Epoch 5/10, Batch 762/883, Training Loss: 0.8536\n",
      "Epoch 5/10, Batch 763/883, Training Loss: 0.6394\n",
      "Epoch 5/10, Batch 764/883, Training Loss: 0.7020\n",
      "Epoch 5/10, Batch 765/883, Training Loss: 1.1861\n",
      "Epoch 5/10, Batch 766/883, Training Loss: 0.7577\n",
      "Epoch 5/10, Batch 767/883, Training Loss: 0.4963\n",
      "Epoch 5/10, Batch 768/883, Training Loss: 0.5965\n",
      "Epoch 5/10, Batch 769/883, Training Loss: 0.6018\n",
      "Epoch 5/10, Batch 770/883, Training Loss: 0.8874\n",
      "Epoch 5/10, Batch 771/883, Training Loss: 0.7361\n",
      "Epoch 5/10, Batch 772/883, Training Loss: 0.8874\n",
      "Epoch 5/10, Batch 773/883, Training Loss: 0.9452\n",
      "Epoch 5/10, Batch 774/883, Training Loss: 0.8572\n",
      "Epoch 5/10, Batch 775/883, Training Loss: 0.6722\n",
      "Epoch 5/10, Batch 776/883, Training Loss: 0.6835\n",
      "Epoch 5/10, Batch 777/883, Training Loss: 0.6391\n",
      "Epoch 5/10, Batch 778/883, Training Loss: 1.1407\n",
      "Epoch 5/10, Batch 779/883, Training Loss: 0.5260\n",
      "Epoch 5/10, Batch 780/883, Training Loss: 0.6806\n",
      "Epoch 5/10, Batch 781/883, Training Loss: 0.7324\n",
      "Epoch 5/10, Batch 782/883, Training Loss: 0.5732\n",
      "Epoch 5/10, Batch 783/883, Training Loss: 0.6475\n",
      "Epoch 5/10, Batch 784/883, Training Loss: 0.7016\n",
      "Epoch 5/10, Batch 785/883, Training Loss: 0.6833\n",
      "Epoch 5/10, Batch 786/883, Training Loss: 0.7702\n",
      "Epoch 5/10, Batch 787/883, Training Loss: 0.8026\n",
      "Epoch 5/10, Batch 788/883, Training Loss: 0.7485\n",
      "Epoch 5/10, Batch 789/883, Training Loss: 0.6052\n",
      "Epoch 5/10, Batch 790/883, Training Loss: 0.7654\n",
      "Epoch 5/10, Batch 791/883, Training Loss: 0.6496\n",
      "Epoch 5/10, Batch 792/883, Training Loss: 0.5656\n",
      "Epoch 5/10, Batch 793/883, Training Loss: 0.6730\n",
      "Epoch 5/10, Batch 794/883, Training Loss: 0.7568\n",
      "Epoch 5/10, Batch 795/883, Training Loss: 0.7974\n",
      "Epoch 5/10, Batch 796/883, Training Loss: 0.8528\n",
      "Epoch 5/10, Batch 797/883, Training Loss: 0.6662\n",
      "Epoch 5/10, Batch 798/883, Training Loss: 0.6769\n",
      "Epoch 5/10, Batch 799/883, Training Loss: 0.8227\n",
      "Epoch 5/10, Batch 800/883, Training Loss: 1.2765\n",
      "Epoch 5/10, Batch 801/883, Training Loss: 0.6460\n",
      "Epoch 5/10, Batch 802/883, Training Loss: 0.5021\n",
      "Epoch 5/10, Batch 803/883, Training Loss: 0.7461\n",
      "Epoch 5/10, Batch 804/883, Training Loss: 0.6221\n",
      "Epoch 5/10, Batch 805/883, Training Loss: 0.5873\n",
      "Epoch 5/10, Batch 806/883, Training Loss: 0.5695\n",
      "Epoch 5/10, Batch 807/883, Training Loss: 0.5068\n",
      "Epoch 5/10, Batch 808/883, Training Loss: 0.6473\n",
      "Epoch 5/10, Batch 809/883, Training Loss: 0.8109\n",
      "Epoch 5/10, Batch 810/883, Training Loss: 0.6671\n",
      "Epoch 5/10, Batch 811/883, Training Loss: 0.4450\n",
      "Epoch 5/10, Batch 812/883, Training Loss: 0.7343\n",
      "Epoch 5/10, Batch 813/883, Training Loss: 0.9026\n",
      "Epoch 5/10, Batch 814/883, Training Loss: 0.6634\n",
      "Epoch 5/10, Batch 815/883, Training Loss: 0.7048\n",
      "Epoch 5/10, Batch 816/883, Training Loss: 0.9580\n",
      "Epoch 5/10, Batch 817/883, Training Loss: 0.5545\n",
      "Epoch 5/10, Batch 818/883, Training Loss: 0.7795\n",
      "Epoch 5/10, Batch 819/883, Training Loss: 0.6218\n",
      "Epoch 5/10, Batch 820/883, Training Loss: 0.8903\n",
      "Epoch 5/10, Batch 821/883, Training Loss: 1.3670\n",
      "Epoch 5/10, Batch 822/883, Training Loss: 0.7444\n",
      "Epoch 5/10, Batch 823/883, Training Loss: 0.7855\n",
      "Epoch 5/10, Batch 824/883, Training Loss: 0.6951\n",
      "Epoch 5/10, Batch 825/883, Training Loss: 0.6032\n",
      "Epoch 5/10, Batch 826/883, Training Loss: 0.4937\n",
      "Epoch 5/10, Batch 827/883, Training Loss: 0.6106\n",
      "Epoch 5/10, Batch 828/883, Training Loss: 0.6329\n",
      "Epoch 5/10, Batch 829/883, Training Loss: 0.6102\n",
      "Epoch 5/10, Batch 830/883, Training Loss: 0.7457\n",
      "Epoch 5/10, Batch 831/883, Training Loss: 0.9820\n",
      "Epoch 5/10, Batch 832/883, Training Loss: 0.6111\n",
      "Epoch 5/10, Batch 833/883, Training Loss: 0.6684\n",
      "Epoch 5/10, Batch 834/883, Training Loss: 0.9700\n",
      "Epoch 5/10, Batch 835/883, Training Loss: 0.5396\n",
      "Epoch 5/10, Batch 836/883, Training Loss: 0.6251\n",
      "Epoch 5/10, Batch 837/883, Training Loss: 0.6913\n",
      "Epoch 5/10, Batch 838/883, Training Loss: 0.6148\n",
      "Epoch 5/10, Batch 839/883, Training Loss: 0.6553\n",
      "Epoch 5/10, Batch 840/883, Training Loss: 0.9137\n",
      "Epoch 5/10, Batch 841/883, Training Loss: 0.5763\n",
      "Epoch 5/10, Batch 842/883, Training Loss: 0.7486\n",
      "Epoch 5/10, Batch 843/883, Training Loss: 0.5081\n",
      "Epoch 5/10, Batch 844/883, Training Loss: 0.8402\n",
      "Epoch 5/10, Batch 845/883, Training Loss: 0.8583\n",
      "Epoch 5/10, Batch 846/883, Training Loss: 0.6794\n",
      "Epoch 5/10, Batch 847/883, Training Loss: 0.9621\n",
      "Epoch 5/10, Batch 848/883, Training Loss: 0.7033\n",
      "Epoch 5/10, Batch 849/883, Training Loss: 0.8430\n",
      "Epoch 5/10, Batch 850/883, Training Loss: 0.7659\n",
      "Epoch 5/10, Batch 851/883, Training Loss: 0.6730\n",
      "Epoch 5/10, Batch 852/883, Training Loss: 0.6079\n",
      "Epoch 5/10, Batch 853/883, Training Loss: 1.4037\n",
      "Epoch 5/10, Batch 854/883, Training Loss: 0.5262\n",
      "Epoch 5/10, Batch 855/883, Training Loss: 0.7755\n",
      "Epoch 5/10, Batch 856/883, Training Loss: 0.8843\n",
      "Epoch 5/10, Batch 857/883, Training Loss: 0.6351\n",
      "Epoch 5/10, Batch 858/883, Training Loss: 0.7632\n",
      "Epoch 5/10, Batch 859/883, Training Loss: 0.5749\n",
      "Epoch 5/10, Batch 860/883, Training Loss: 0.4800\n",
      "Epoch 5/10, Batch 861/883, Training Loss: 0.7096\n",
      "Epoch 5/10, Batch 862/883, Training Loss: 0.5192\n",
      "Epoch 5/10, Batch 863/883, Training Loss: 0.7490\n",
      "Epoch 5/10, Batch 864/883, Training Loss: 0.6009\n",
      "Epoch 5/10, Batch 865/883, Training Loss: 0.8748\n",
      "Epoch 5/10, Batch 866/883, Training Loss: 0.9232\n",
      "Epoch 5/10, Batch 867/883, Training Loss: 0.6366\n",
      "Epoch 5/10, Batch 868/883, Training Loss: 0.4725\n",
      "Epoch 5/10, Batch 869/883, Training Loss: 0.8110\n",
      "Epoch 5/10, Batch 870/883, Training Loss: 0.7929\n",
      "Epoch 5/10, Batch 871/883, Training Loss: 0.8746\n",
      "Epoch 5/10, Batch 872/883, Training Loss: 1.0163\n",
      "Epoch 5/10, Batch 873/883, Training Loss: 0.7192\n",
      "Epoch 5/10, Batch 874/883, Training Loss: 0.6190\n",
      "Epoch 5/10, Batch 875/883, Training Loss: 0.6098\n",
      "Epoch 5/10, Batch 876/883, Training Loss: 0.6892\n",
      "Epoch 5/10, Batch 877/883, Training Loss: 0.6927\n",
      "Epoch 5/10, Batch 878/883, Training Loss: 0.5679\n",
      "Epoch 5/10, Batch 879/883, Training Loss: 0.8942\n",
      "Epoch 5/10, Batch 880/883, Training Loss: 0.5123\n",
      "Epoch 5/10, Batch 881/883, Training Loss: 0.9601\n",
      "Epoch 5/10, Batch 882/883, Training Loss: 0.7215\n",
      "Epoch 5/10, Batch 883/883, Training Loss: 0.6036\n",
      "Epoch 5/10, Training Loss: 0.7341, Validation Loss: 0.6919, Validation Accuracy: 0.6838\n",
      "Epoch 6/10, Batch 1/883, Training Loss: 0.5606\n",
      "Epoch 6/10, Batch 2/883, Training Loss: 0.8599\n",
      "Epoch 6/10, Batch 3/883, Training Loss: 0.5011\n",
      "Epoch 6/10, Batch 4/883, Training Loss: 0.5468\n",
      "Epoch 6/10, Batch 5/883, Training Loss: 0.5315\n",
      "Epoch 6/10, Batch 6/883, Training Loss: 0.5834\n",
      "Epoch 6/10, Batch 7/883, Training Loss: 1.1506\n",
      "Epoch 6/10, Batch 8/883, Training Loss: 0.7417\n",
      "Epoch 6/10, Batch 9/883, Training Loss: 0.6686\n",
      "Epoch 6/10, Batch 10/883, Training Loss: 0.4419\n",
      "Epoch 6/10, Batch 11/883, Training Loss: 0.6529\n",
      "Epoch 6/10, Batch 12/883, Training Loss: 0.8092\n",
      "Epoch 6/10, Batch 13/883, Training Loss: 0.9117\n",
      "Epoch 6/10, Batch 14/883, Training Loss: 0.7162\n",
      "Epoch 6/10, Batch 15/883, Training Loss: 0.7569\n",
      "Epoch 6/10, Batch 16/883, Training Loss: 0.7450\n",
      "Epoch 6/10, Batch 17/883, Training Loss: 1.4737\n",
      "Epoch 6/10, Batch 18/883, Training Loss: 0.9983\n",
      "Epoch 6/10, Batch 19/883, Training Loss: 0.8541\n",
      "Epoch 6/10, Batch 20/883, Training Loss: 1.0990\n",
      "Epoch 6/10, Batch 21/883, Training Loss: 0.6237\n",
      "Epoch 6/10, Batch 22/883, Training Loss: 0.7334\n",
      "Epoch 6/10, Batch 23/883, Training Loss: 1.1591\n",
      "Epoch 6/10, Batch 24/883, Training Loss: 0.6359\n",
      "Epoch 6/10, Batch 25/883, Training Loss: 0.6881\n",
      "Epoch 6/10, Batch 26/883, Training Loss: 0.5908\n",
      "Epoch 6/10, Batch 27/883, Training Loss: 0.5371\n",
      "Epoch 6/10, Batch 28/883, Training Loss: 0.8820\n",
      "Epoch 6/10, Batch 29/883, Training Loss: 0.5924\n",
      "Epoch 6/10, Batch 30/883, Training Loss: 0.6759\n",
      "Epoch 6/10, Batch 31/883, Training Loss: 0.6517\n",
      "Epoch 6/10, Batch 32/883, Training Loss: 0.6138\n",
      "Epoch 6/10, Batch 33/883, Training Loss: 0.6634\n",
      "Epoch 6/10, Batch 34/883, Training Loss: 0.7444\n",
      "Epoch 6/10, Batch 35/883, Training Loss: 0.4187\n",
      "Epoch 6/10, Batch 36/883, Training Loss: 0.5326\n",
      "Epoch 6/10, Batch 37/883, Training Loss: 0.5441\n",
      "Epoch 6/10, Batch 38/883, Training Loss: 0.8549\n",
      "Epoch 6/10, Batch 39/883, Training Loss: 0.6351\n",
      "Epoch 6/10, Batch 40/883, Training Loss: 0.6019\n",
      "Epoch 6/10, Batch 41/883, Training Loss: 0.6119\n",
      "Epoch 6/10, Batch 42/883, Training Loss: 0.6112\n",
      "Epoch 6/10, Batch 43/883, Training Loss: 0.7353\n",
      "Epoch 6/10, Batch 44/883, Training Loss: 0.4661\n",
      "Epoch 6/10, Batch 45/883, Training Loss: 0.5187\n",
      "Epoch 6/10, Batch 46/883, Training Loss: 1.1452\n",
      "Epoch 6/10, Batch 47/883, Training Loss: 0.9157\n",
      "Epoch 6/10, Batch 48/883, Training Loss: 0.6919\n",
      "Epoch 6/10, Batch 49/883, Training Loss: 0.6812\n",
      "Epoch 6/10, Batch 50/883, Training Loss: 0.6119\n",
      "Epoch 6/10, Batch 51/883, Training Loss: 0.8283\n",
      "Epoch 6/10, Batch 52/883, Training Loss: 0.8777\n",
      "Epoch 6/10, Batch 53/883, Training Loss: 0.8506\n",
      "Epoch 6/10, Batch 54/883, Training Loss: 0.4895\n",
      "Epoch 6/10, Batch 55/883, Training Loss: 0.6943\n",
      "Epoch 6/10, Batch 56/883, Training Loss: 0.9114\n",
      "Epoch 6/10, Batch 57/883, Training Loss: 0.3868\n",
      "Epoch 6/10, Batch 58/883, Training Loss: 0.5181\n",
      "Epoch 6/10, Batch 59/883, Training Loss: 0.6292\n",
      "Epoch 6/10, Batch 60/883, Training Loss: 0.8043\n",
      "Epoch 6/10, Batch 61/883, Training Loss: 0.4202\n",
      "Epoch 6/10, Batch 62/883, Training Loss: 0.6529\n",
      "Epoch 6/10, Batch 63/883, Training Loss: 0.8326\n",
      "Epoch 6/10, Batch 64/883, Training Loss: 0.5514\n",
      "Epoch 6/10, Batch 65/883, Training Loss: 0.6877\n",
      "Epoch 6/10, Batch 66/883, Training Loss: 0.8571\n",
      "Epoch 6/10, Batch 67/883, Training Loss: 0.4781\n",
      "Epoch 6/10, Batch 68/883, Training Loss: 0.3993\n",
      "Epoch 6/10, Batch 69/883, Training Loss: 0.7611\n",
      "Epoch 6/10, Batch 70/883, Training Loss: 0.6459\n",
      "Epoch 6/10, Batch 71/883, Training Loss: 0.7450\n",
      "Epoch 6/10, Batch 72/883, Training Loss: 0.5054\n",
      "Epoch 6/10, Batch 73/883, Training Loss: 0.7303\n",
      "Epoch 6/10, Batch 74/883, Training Loss: 0.6878\n",
      "Epoch 6/10, Batch 75/883, Training Loss: 0.3612\n",
      "Epoch 6/10, Batch 76/883, Training Loss: 0.7978\n",
      "Epoch 6/10, Batch 77/883, Training Loss: 0.9768\n",
      "Epoch 6/10, Batch 78/883, Training Loss: 0.6173\n",
      "Epoch 6/10, Batch 79/883, Training Loss: 0.5918\n",
      "Epoch 6/10, Batch 80/883, Training Loss: 0.5412\n",
      "Epoch 6/10, Batch 81/883, Training Loss: 0.7169\n",
      "Epoch 6/10, Batch 82/883, Training Loss: 0.6834\n",
      "Epoch 6/10, Batch 83/883, Training Loss: 0.8783\n",
      "Epoch 6/10, Batch 84/883, Training Loss: 0.8288\n",
      "Epoch 6/10, Batch 85/883, Training Loss: 0.6810\n",
      "Epoch 6/10, Batch 86/883, Training Loss: 0.8685\n",
      "Epoch 6/10, Batch 87/883, Training Loss: 0.4877\n",
      "Epoch 6/10, Batch 88/883, Training Loss: 0.9010\n",
      "Epoch 6/10, Batch 89/883, Training Loss: 0.6675\n",
      "Epoch 6/10, Batch 90/883, Training Loss: 1.0485\n",
      "Epoch 6/10, Batch 91/883, Training Loss: 0.7381\n",
      "Epoch 6/10, Batch 92/883, Training Loss: 1.1780\n",
      "Epoch 6/10, Batch 93/883, Training Loss: 0.7718\n",
      "Epoch 6/10, Batch 94/883, Training Loss: 0.9352\n",
      "Epoch 6/10, Batch 95/883, Training Loss: 0.7936\n",
      "Epoch 6/10, Batch 96/883, Training Loss: 0.7602\n",
      "Epoch 6/10, Batch 97/883, Training Loss: 0.5845\n",
      "Epoch 6/10, Batch 98/883, Training Loss: 0.6560\n",
      "Epoch 6/10, Batch 99/883, Training Loss: 0.7779\n",
      "Epoch 6/10, Batch 100/883, Training Loss: 0.5742\n",
      "Epoch 6/10, Batch 101/883, Training Loss: 0.5973\n",
      "Epoch 6/10, Batch 102/883, Training Loss: 0.5417\n",
      "Epoch 6/10, Batch 103/883, Training Loss: 0.7390\n",
      "Epoch 6/10, Batch 104/883, Training Loss: 0.7794\n",
      "Epoch 6/10, Batch 105/883, Training Loss: 0.8844\n",
      "Epoch 6/10, Batch 106/883, Training Loss: 0.4526\n",
      "Epoch 6/10, Batch 107/883, Training Loss: 0.5866\n",
      "Epoch 6/10, Batch 108/883, Training Loss: 0.6210\n",
      "Epoch 6/10, Batch 109/883, Training Loss: 0.5275\n",
      "Epoch 6/10, Batch 110/883, Training Loss: 0.9381\n",
      "Epoch 6/10, Batch 111/883, Training Loss: 0.5629\n",
      "Epoch 6/10, Batch 112/883, Training Loss: 0.6071\n",
      "Epoch 6/10, Batch 113/883, Training Loss: 0.5158\n",
      "Epoch 6/10, Batch 114/883, Training Loss: 0.4789\n",
      "Epoch 6/10, Batch 115/883, Training Loss: 0.6940\n",
      "Epoch 6/10, Batch 116/883, Training Loss: 0.8503\n",
      "Epoch 6/10, Batch 117/883, Training Loss: 0.5127\n",
      "Epoch 6/10, Batch 118/883, Training Loss: 0.6302\n",
      "Epoch 6/10, Batch 119/883, Training Loss: 0.9078\n",
      "Epoch 6/10, Batch 120/883, Training Loss: 0.6139\n",
      "Epoch 6/10, Batch 121/883, Training Loss: 0.5454\n",
      "Epoch 6/10, Batch 122/883, Training Loss: 0.5826\n",
      "Epoch 6/10, Batch 123/883, Training Loss: 0.5910\n",
      "Epoch 6/10, Batch 124/883, Training Loss: 0.8204\n",
      "Epoch 6/10, Batch 125/883, Training Loss: 0.4917\n",
      "Epoch 6/10, Batch 126/883, Training Loss: 0.5107\n",
      "Epoch 6/10, Batch 127/883, Training Loss: 0.6544\n",
      "Epoch 6/10, Batch 128/883, Training Loss: 0.9638\n",
      "Epoch 6/10, Batch 129/883, Training Loss: 0.9687\n",
      "Epoch 6/10, Batch 130/883, Training Loss: 0.6051\n",
      "Epoch 6/10, Batch 131/883, Training Loss: 0.7300\n",
      "Epoch 6/10, Batch 132/883, Training Loss: 0.6070\n",
      "Epoch 6/10, Batch 133/883, Training Loss: 0.5722\n",
      "Epoch 6/10, Batch 134/883, Training Loss: 0.4617\n",
      "Epoch 6/10, Batch 135/883, Training Loss: 0.8073\n",
      "Epoch 6/10, Batch 136/883, Training Loss: 0.4786\n",
      "Epoch 6/10, Batch 137/883, Training Loss: 0.7194\n",
      "Epoch 6/10, Batch 138/883, Training Loss: 0.9814\n",
      "Epoch 6/10, Batch 139/883, Training Loss: 0.6727\n",
      "Epoch 6/10, Batch 140/883, Training Loss: 0.6742\n",
      "Epoch 6/10, Batch 141/883, Training Loss: 0.7045\n",
      "Epoch 6/10, Batch 142/883, Training Loss: 0.6332\n",
      "Epoch 6/10, Batch 143/883, Training Loss: 0.7122\n",
      "Epoch 6/10, Batch 144/883, Training Loss: 1.1117\n",
      "Epoch 6/10, Batch 145/883, Training Loss: 0.6426\n",
      "Epoch 6/10, Batch 146/883, Training Loss: 0.8953\n",
      "Epoch 6/10, Batch 147/883, Training Loss: 0.6275\n",
      "Epoch 6/10, Batch 148/883, Training Loss: 0.7070\n",
      "Epoch 6/10, Batch 149/883, Training Loss: 0.5881\n",
      "Epoch 6/10, Batch 150/883, Training Loss: 1.0310\n",
      "Epoch 6/10, Batch 151/883, Training Loss: 0.6270\n",
      "Epoch 6/10, Batch 152/883, Training Loss: 0.4356\n",
      "Epoch 6/10, Batch 153/883, Training Loss: 0.5233\n",
      "Epoch 6/10, Batch 154/883, Training Loss: 0.6283\n",
      "Epoch 6/10, Batch 155/883, Training Loss: 0.7464\n",
      "Epoch 6/10, Batch 156/883, Training Loss: 0.6203\n",
      "Epoch 6/10, Batch 157/883, Training Loss: 0.8550\n",
      "Epoch 6/10, Batch 158/883, Training Loss: 0.9620\n",
      "Epoch 6/10, Batch 159/883, Training Loss: 0.5324\n",
      "Epoch 6/10, Batch 160/883, Training Loss: 0.6840\n",
      "Epoch 6/10, Batch 161/883, Training Loss: 1.0507\n",
      "Epoch 6/10, Batch 162/883, Training Loss: 0.5408\n",
      "Epoch 6/10, Batch 163/883, Training Loss: 0.5580\n",
      "Epoch 6/10, Batch 164/883, Training Loss: 0.5457\n",
      "Epoch 6/10, Batch 165/883, Training Loss: 0.8346\n",
      "Epoch 6/10, Batch 166/883, Training Loss: 0.7500\n",
      "Epoch 6/10, Batch 167/883, Training Loss: 0.7075\n",
      "Epoch 6/10, Batch 168/883, Training Loss: 0.4953\n",
      "Epoch 6/10, Batch 169/883, Training Loss: 0.8894\n",
      "Epoch 6/10, Batch 170/883, Training Loss: 0.6227\n",
      "Epoch 6/10, Batch 171/883, Training Loss: 0.5676\n",
      "Epoch 6/10, Batch 172/883, Training Loss: 0.8690\n",
      "Epoch 6/10, Batch 173/883, Training Loss: 0.7943\n",
      "Epoch 6/10, Batch 174/883, Training Loss: 0.6484\n",
      "Epoch 6/10, Batch 175/883, Training Loss: 0.7883\n",
      "Epoch 6/10, Batch 176/883, Training Loss: 0.6993\n",
      "Epoch 6/10, Batch 177/883, Training Loss: 0.6528\n",
      "Epoch 6/10, Batch 178/883, Training Loss: 1.0017\n",
      "Epoch 6/10, Batch 179/883, Training Loss: 0.7813\n",
      "Epoch 6/10, Batch 180/883, Training Loss: 0.5019\n",
      "Epoch 6/10, Batch 181/883, Training Loss: 0.5279\n",
      "Epoch 6/10, Batch 182/883, Training Loss: 0.9642\n",
      "Epoch 6/10, Batch 183/883, Training Loss: 0.8146\n",
      "Epoch 6/10, Batch 184/883, Training Loss: 0.6379\n",
      "Epoch 6/10, Batch 185/883, Training Loss: 0.9674\n",
      "Epoch 6/10, Batch 186/883, Training Loss: 0.8289\n",
      "Epoch 6/10, Batch 187/883, Training Loss: 0.7662\n",
      "Epoch 6/10, Batch 188/883, Training Loss: 0.7551\n",
      "Epoch 6/10, Batch 189/883, Training Loss: 0.9516\n",
      "Epoch 6/10, Batch 190/883, Training Loss: 0.6414\n",
      "Epoch 6/10, Batch 191/883, Training Loss: 0.6607\n",
      "Epoch 6/10, Batch 192/883, Training Loss: 0.7066\n",
      "Epoch 6/10, Batch 193/883, Training Loss: 0.6373\n",
      "Epoch 6/10, Batch 194/883, Training Loss: 0.5326\n",
      "Epoch 6/10, Batch 195/883, Training Loss: 0.8818\n",
      "Epoch 6/10, Batch 196/883, Training Loss: 0.4799\n",
      "Epoch 6/10, Batch 197/883, Training Loss: 0.6172\n",
      "Epoch 6/10, Batch 198/883, Training Loss: 0.5444\n",
      "Epoch 6/10, Batch 199/883, Training Loss: 0.4283\n",
      "Epoch 6/10, Batch 200/883, Training Loss: 0.7343\n",
      "Epoch 6/10, Batch 201/883, Training Loss: 0.5824\n",
      "Epoch 6/10, Batch 202/883, Training Loss: 0.6404\n",
      "Epoch 6/10, Batch 203/883, Training Loss: 0.5595\n",
      "Epoch 6/10, Batch 204/883, Training Loss: 0.8950\n",
      "Epoch 6/10, Batch 205/883, Training Loss: 0.8787\n",
      "Epoch 6/10, Batch 206/883, Training Loss: 0.4744\n",
      "Epoch 6/10, Batch 207/883, Training Loss: 0.7307\n",
      "Epoch 6/10, Batch 208/883, Training Loss: 0.6718\n",
      "Epoch 6/10, Batch 209/883, Training Loss: 0.6351\n",
      "Epoch 6/10, Batch 210/883, Training Loss: 0.6312\n",
      "Epoch 6/10, Batch 211/883, Training Loss: 0.9111\n",
      "Epoch 6/10, Batch 212/883, Training Loss: 0.5885\n",
      "Epoch 6/10, Batch 213/883, Training Loss: 0.5251\n",
      "Epoch 6/10, Batch 214/883, Training Loss: 0.8400\n",
      "Epoch 6/10, Batch 215/883, Training Loss: 0.8065\n",
      "Epoch 6/10, Batch 216/883, Training Loss: 0.5492\n",
      "Epoch 6/10, Batch 217/883, Training Loss: 0.6676\n",
      "Epoch 6/10, Batch 218/883, Training Loss: 0.5764\n",
      "Epoch 6/10, Batch 219/883, Training Loss: 1.0289\n",
      "Epoch 6/10, Batch 220/883, Training Loss: 0.7747\n",
      "Epoch 6/10, Batch 221/883, Training Loss: 0.5013\n",
      "Epoch 6/10, Batch 222/883, Training Loss: 0.6027\n",
      "Epoch 6/10, Batch 223/883, Training Loss: 0.4831\n",
      "Epoch 6/10, Batch 224/883, Training Loss: 0.8207\n",
      "Epoch 6/10, Batch 225/883, Training Loss: 0.4706\n",
      "Epoch 6/10, Batch 226/883, Training Loss: 0.8543\n",
      "Epoch 6/10, Batch 227/883, Training Loss: 0.6859\n",
      "Epoch 6/10, Batch 228/883, Training Loss: 0.6303\n",
      "Epoch 6/10, Batch 229/883, Training Loss: 0.8958\n",
      "Epoch 6/10, Batch 230/883, Training Loss: 0.3979\n",
      "Epoch 6/10, Batch 231/883, Training Loss: 0.5685\n",
      "Epoch 6/10, Batch 232/883, Training Loss: 0.5312\n",
      "Epoch 6/10, Batch 233/883, Training Loss: 0.8691\n",
      "Epoch 6/10, Batch 234/883, Training Loss: 0.7385\n",
      "Epoch 6/10, Batch 235/883, Training Loss: 0.3970\n",
      "Epoch 6/10, Batch 236/883, Training Loss: 0.6552\n",
      "Epoch 6/10, Batch 237/883, Training Loss: 1.0326\n",
      "Epoch 6/10, Batch 238/883, Training Loss: 0.4494\n",
      "Epoch 6/10, Batch 239/883, Training Loss: 0.5910\n",
      "Epoch 6/10, Batch 240/883, Training Loss: 0.5415\n",
      "Epoch 6/10, Batch 241/883, Training Loss: 0.6055\n",
      "Epoch 6/10, Batch 242/883, Training Loss: 0.5034\n",
      "Epoch 6/10, Batch 243/883, Training Loss: 0.6459\n",
      "Epoch 6/10, Batch 244/883, Training Loss: 0.5584\n",
      "Epoch 6/10, Batch 245/883, Training Loss: 1.1711\n",
      "Epoch 6/10, Batch 246/883, Training Loss: 0.4340\n",
      "Epoch 6/10, Batch 247/883, Training Loss: 0.8867\n",
      "Epoch 6/10, Batch 248/883, Training Loss: 0.6988\n",
      "Epoch 6/10, Batch 249/883, Training Loss: 0.7481\n",
      "Epoch 6/10, Batch 250/883, Training Loss: 0.4678\n",
      "Epoch 6/10, Batch 251/883, Training Loss: 0.4727\n",
      "Epoch 6/10, Batch 252/883, Training Loss: 0.5605\n",
      "Epoch 6/10, Batch 253/883, Training Loss: 0.7033\n",
      "Epoch 6/10, Batch 254/883, Training Loss: 0.8659\n",
      "Epoch 6/10, Batch 255/883, Training Loss: 0.5150\n",
      "Epoch 6/10, Batch 256/883, Training Loss: 0.7232\n",
      "Epoch 6/10, Batch 257/883, Training Loss: 0.8743\n",
      "Epoch 6/10, Batch 258/883, Training Loss: 0.9382\n",
      "Epoch 6/10, Batch 259/883, Training Loss: 0.5785\n",
      "Epoch 6/10, Batch 260/883, Training Loss: 0.8415\n",
      "Epoch 6/10, Batch 261/883, Training Loss: 0.5762\n",
      "Epoch 6/10, Batch 262/883, Training Loss: 0.7156\n",
      "Epoch 6/10, Batch 263/883, Training Loss: 0.6396\n",
      "Epoch 6/10, Batch 264/883, Training Loss: 0.7865\n",
      "Epoch 6/10, Batch 265/883, Training Loss: 0.6288\n",
      "Epoch 6/10, Batch 266/883, Training Loss: 0.8648\n",
      "Epoch 6/10, Batch 267/883, Training Loss: 0.7424\n",
      "Epoch 6/10, Batch 268/883, Training Loss: 0.7482\n",
      "Epoch 6/10, Batch 269/883, Training Loss: 0.5364\n",
      "Epoch 6/10, Batch 270/883, Training Loss: 0.7894\n",
      "Epoch 6/10, Batch 271/883, Training Loss: 0.5312\n",
      "Epoch 6/10, Batch 272/883, Training Loss: 0.4629\n",
      "Epoch 6/10, Batch 273/883, Training Loss: 0.7528\n",
      "Epoch 6/10, Batch 274/883, Training Loss: 0.4199\n",
      "Epoch 6/10, Batch 275/883, Training Loss: 0.6294\n",
      "Epoch 6/10, Batch 276/883, Training Loss: 0.6444\n",
      "Epoch 6/10, Batch 277/883, Training Loss: 0.6980\n",
      "Epoch 6/10, Batch 278/883, Training Loss: 0.6333\n",
      "Epoch 6/10, Batch 279/883, Training Loss: 0.7073\n",
      "Epoch 6/10, Batch 280/883, Training Loss: 1.0601\n",
      "Epoch 6/10, Batch 281/883, Training Loss: 0.8810\n",
      "Epoch 6/10, Batch 282/883, Training Loss: 0.4111\n",
      "Epoch 6/10, Batch 283/883, Training Loss: 0.6833\n",
      "Epoch 6/10, Batch 284/883, Training Loss: 0.9028\n",
      "Epoch 6/10, Batch 285/883, Training Loss: 0.5261\n",
      "Epoch 6/10, Batch 286/883, Training Loss: 0.4867\n",
      "Epoch 6/10, Batch 287/883, Training Loss: 0.4059\n",
      "Epoch 6/10, Batch 288/883, Training Loss: 0.8726\n",
      "Epoch 6/10, Batch 289/883, Training Loss: 0.8288\n",
      "Epoch 6/10, Batch 290/883, Training Loss: 0.5170\n",
      "Epoch 6/10, Batch 291/883, Training Loss: 0.8092\n",
      "Epoch 6/10, Batch 292/883, Training Loss: 0.6924\n",
      "Epoch 6/10, Batch 293/883, Training Loss: 0.8726\n",
      "Epoch 6/10, Batch 294/883, Training Loss: 0.6119\n",
      "Epoch 6/10, Batch 295/883, Training Loss: 0.7082\n",
      "Epoch 6/10, Batch 296/883, Training Loss: 0.7643\n",
      "Epoch 6/10, Batch 297/883, Training Loss: 0.7441\n",
      "Epoch 6/10, Batch 298/883, Training Loss: 0.5949\n",
      "Epoch 6/10, Batch 299/883, Training Loss: 0.5661\n",
      "Epoch 6/10, Batch 300/883, Training Loss: 0.4938\n",
      "Epoch 6/10, Batch 301/883, Training Loss: 0.6122\n",
      "Epoch 6/10, Batch 302/883, Training Loss: 0.6864\n",
      "Epoch 6/10, Batch 303/883, Training Loss: 0.9769\n",
      "Epoch 6/10, Batch 304/883, Training Loss: 0.6562\n",
      "Epoch 6/10, Batch 305/883, Training Loss: 0.9051\n",
      "Epoch 6/10, Batch 306/883, Training Loss: 0.5091\n",
      "Epoch 6/10, Batch 307/883, Training Loss: 0.7763\n",
      "Epoch 6/10, Batch 308/883, Training Loss: 0.6273\n",
      "Epoch 6/10, Batch 309/883, Training Loss: 0.8075\n",
      "Epoch 6/10, Batch 310/883, Training Loss: 0.6139\n",
      "Epoch 6/10, Batch 311/883, Training Loss: 0.9094\n",
      "Epoch 6/10, Batch 312/883, Training Loss: 0.6346\n",
      "Epoch 6/10, Batch 313/883, Training Loss: 1.0796\n",
      "Epoch 6/10, Batch 314/883, Training Loss: 0.8107\n",
      "Epoch 6/10, Batch 315/883, Training Loss: 0.5066\n",
      "Epoch 6/10, Batch 316/883, Training Loss: 0.8439\n",
      "Epoch 6/10, Batch 317/883, Training Loss: 0.6927\n",
      "Epoch 6/10, Batch 318/883, Training Loss: 0.6727\n",
      "Epoch 6/10, Batch 319/883, Training Loss: 0.7311\n",
      "Epoch 6/10, Batch 320/883, Training Loss: 0.6782\n",
      "Epoch 6/10, Batch 321/883, Training Loss: 0.5708\n",
      "Epoch 6/10, Batch 322/883, Training Loss: 0.8587\n",
      "Epoch 6/10, Batch 323/883, Training Loss: 0.5432\n",
      "Epoch 6/10, Batch 324/883, Training Loss: 0.7520\n",
      "Epoch 6/10, Batch 325/883, Training Loss: 0.7202\n",
      "Epoch 6/10, Batch 326/883, Training Loss: 0.7660\n",
      "Epoch 6/10, Batch 327/883, Training Loss: 0.5587\n",
      "Epoch 6/10, Batch 328/883, Training Loss: 0.6853\n",
      "Epoch 6/10, Batch 329/883, Training Loss: 0.5988\n",
      "Epoch 6/10, Batch 330/883, Training Loss: 0.5551\n",
      "Epoch 6/10, Batch 331/883, Training Loss: 0.4682\n",
      "Epoch 6/10, Batch 332/883, Training Loss: 0.8878\n",
      "Epoch 6/10, Batch 333/883, Training Loss: 0.8406\n",
      "Epoch 6/10, Batch 334/883, Training Loss: 0.8389\n",
      "Epoch 6/10, Batch 335/883, Training Loss: 0.5401\n",
      "Epoch 6/10, Batch 336/883, Training Loss: 0.4358\n",
      "Epoch 6/10, Batch 337/883, Training Loss: 0.7412\n",
      "Epoch 6/10, Batch 338/883, Training Loss: 0.5962\n",
      "Epoch 6/10, Batch 339/883, Training Loss: 0.6495\n",
      "Epoch 6/10, Batch 340/883, Training Loss: 0.5850\n",
      "Epoch 6/10, Batch 341/883, Training Loss: 0.4475\n",
      "Epoch 6/10, Batch 342/883, Training Loss: 0.5491\n",
      "Epoch 6/10, Batch 343/883, Training Loss: 0.8814\n",
      "Epoch 6/10, Batch 344/883, Training Loss: 0.6864\n",
      "Epoch 6/10, Batch 345/883, Training Loss: 0.5899\n",
      "Epoch 6/10, Batch 346/883, Training Loss: 0.6400\n",
      "Epoch 6/10, Batch 347/883, Training Loss: 0.6216\n",
      "Epoch 6/10, Batch 348/883, Training Loss: 0.6052\n",
      "Epoch 6/10, Batch 349/883, Training Loss: 0.6750\n",
      "Epoch 6/10, Batch 350/883, Training Loss: 0.5395\n",
      "Epoch 6/10, Batch 351/883, Training Loss: 0.7677\n",
      "Epoch 6/10, Batch 352/883, Training Loss: 0.7499\n",
      "Epoch 6/10, Batch 353/883, Training Loss: 0.5673\n",
      "Epoch 6/10, Batch 354/883, Training Loss: 0.6480\n",
      "Epoch 6/10, Batch 355/883, Training Loss: 0.6751\n",
      "Epoch 6/10, Batch 356/883, Training Loss: 0.6745\n",
      "Epoch 6/10, Batch 357/883, Training Loss: 0.6833\n",
      "Epoch 6/10, Batch 358/883, Training Loss: 0.7383\n",
      "Epoch 6/10, Batch 359/883, Training Loss: 0.6600\n",
      "Epoch 6/10, Batch 360/883, Training Loss: 0.7329\n",
      "Epoch 6/10, Batch 361/883, Training Loss: 0.5351\n",
      "Epoch 6/10, Batch 362/883, Training Loss: 0.6395\n",
      "Epoch 6/10, Batch 363/883, Training Loss: 1.3124\n",
      "Epoch 6/10, Batch 364/883, Training Loss: 0.9071\n",
      "Epoch 6/10, Batch 365/883, Training Loss: 0.6309\n",
      "Epoch 6/10, Batch 366/883, Training Loss: 0.4269\n",
      "Epoch 6/10, Batch 367/883, Training Loss: 0.6273\n",
      "Epoch 6/10, Batch 368/883, Training Loss: 0.5863\n",
      "Epoch 6/10, Batch 369/883, Training Loss: 0.7562\n",
      "Epoch 6/10, Batch 370/883, Training Loss: 1.1803\n",
      "Epoch 6/10, Batch 371/883, Training Loss: 0.6171\n",
      "Epoch 6/10, Batch 372/883, Training Loss: 0.6561\n",
      "Epoch 6/10, Batch 373/883, Training Loss: 0.8485\n",
      "Epoch 6/10, Batch 374/883, Training Loss: 0.8591\n",
      "Epoch 6/10, Batch 375/883, Training Loss: 0.4917\n",
      "Epoch 6/10, Batch 376/883, Training Loss: 0.5413\n",
      "Epoch 6/10, Batch 377/883, Training Loss: 0.5688\n",
      "Epoch 6/10, Batch 378/883, Training Loss: 0.4531\n",
      "Epoch 6/10, Batch 379/883, Training Loss: 0.9832\n",
      "Epoch 6/10, Batch 380/883, Training Loss: 0.5768\n",
      "Epoch 6/10, Batch 381/883, Training Loss: 0.6339\n",
      "Epoch 6/10, Batch 382/883, Training Loss: 0.6890\n",
      "Epoch 6/10, Batch 383/883, Training Loss: 0.7433\n",
      "Epoch 6/10, Batch 384/883, Training Loss: 0.8819\n",
      "Epoch 6/10, Batch 385/883, Training Loss: 0.7376\n",
      "Epoch 6/10, Batch 386/883, Training Loss: 0.6772\n",
      "Epoch 6/10, Batch 387/883, Training Loss: 1.0563\n",
      "Epoch 6/10, Batch 388/883, Training Loss: 0.8428\n",
      "Epoch 6/10, Batch 389/883, Training Loss: 0.6126\n",
      "Epoch 6/10, Batch 390/883, Training Loss: 0.7231\n",
      "Epoch 6/10, Batch 391/883, Training Loss: 0.8198\n",
      "Epoch 6/10, Batch 392/883, Training Loss: 0.5662\n",
      "Epoch 6/10, Batch 393/883, Training Loss: 0.5400\n",
      "Epoch 6/10, Batch 394/883, Training Loss: 0.7194\n",
      "Epoch 6/10, Batch 395/883, Training Loss: 0.5838\n",
      "Epoch 6/10, Batch 396/883, Training Loss: 0.7626\n",
      "Epoch 6/10, Batch 397/883, Training Loss: 0.5344\n",
      "Epoch 6/10, Batch 398/883, Training Loss: 0.5241\n",
      "Epoch 6/10, Batch 399/883, Training Loss: 0.5981\n",
      "Epoch 6/10, Batch 400/883, Training Loss: 0.9780\n",
      "Epoch 6/10, Batch 401/883, Training Loss: 0.7092\n",
      "Epoch 6/10, Batch 402/883, Training Loss: 0.6306\n",
      "Epoch 6/10, Batch 403/883, Training Loss: 0.4596\n",
      "Epoch 6/10, Batch 404/883, Training Loss: 0.8922\n",
      "Epoch 6/10, Batch 405/883, Training Loss: 0.4956\n",
      "Epoch 6/10, Batch 406/883, Training Loss: 0.8888\n",
      "Epoch 6/10, Batch 407/883, Training Loss: 0.8302\n",
      "Epoch 6/10, Batch 408/883, Training Loss: 0.6211\n",
      "Epoch 6/10, Batch 409/883, Training Loss: 0.7744\n",
      "Epoch 6/10, Batch 410/883, Training Loss: 0.7203\n",
      "Epoch 6/10, Batch 411/883, Training Loss: 0.6582\n",
      "Epoch 6/10, Batch 412/883, Training Loss: 0.9131\n",
      "Epoch 6/10, Batch 413/883, Training Loss: 0.6564\n",
      "Epoch 6/10, Batch 414/883, Training Loss: 0.5114\n",
      "Epoch 6/10, Batch 415/883, Training Loss: 0.7859\n",
      "Epoch 6/10, Batch 416/883, Training Loss: 0.6426\n",
      "Epoch 6/10, Batch 417/883, Training Loss: 0.7920\n",
      "Epoch 6/10, Batch 418/883, Training Loss: 0.7461\n",
      "Epoch 6/10, Batch 419/883, Training Loss: 0.7779\n",
      "Epoch 6/10, Batch 420/883, Training Loss: 0.7547\n",
      "Epoch 6/10, Batch 421/883, Training Loss: 0.5991\n",
      "Epoch 6/10, Batch 422/883, Training Loss: 0.6762\n",
      "Epoch 6/10, Batch 423/883, Training Loss: 0.7294\n",
      "Epoch 6/10, Batch 424/883, Training Loss: 0.6694\n",
      "Epoch 6/10, Batch 425/883, Training Loss: 0.5047\n",
      "Epoch 6/10, Batch 426/883, Training Loss: 0.4187\n",
      "Epoch 6/10, Batch 427/883, Training Loss: 0.6401\n",
      "Epoch 6/10, Batch 428/883, Training Loss: 1.1001\n",
      "Epoch 6/10, Batch 429/883, Training Loss: 0.5494\n",
      "Epoch 6/10, Batch 430/883, Training Loss: 0.5426\n",
      "Epoch 6/10, Batch 431/883, Training Loss: 0.5244\n",
      "Epoch 6/10, Batch 432/883, Training Loss: 0.7008\n",
      "Epoch 6/10, Batch 433/883, Training Loss: 0.7854\n",
      "Epoch 6/10, Batch 434/883, Training Loss: 0.5374\n",
      "Epoch 6/10, Batch 435/883, Training Loss: 0.6234\n",
      "Epoch 6/10, Batch 436/883, Training Loss: 1.1143\n",
      "Epoch 6/10, Batch 437/883, Training Loss: 0.5502\n",
      "Epoch 6/10, Batch 438/883, Training Loss: 1.4413\n",
      "Epoch 6/10, Batch 439/883, Training Loss: 0.8574\n",
      "Epoch 6/10, Batch 440/883, Training Loss: 0.8305\n",
      "Epoch 6/10, Batch 441/883, Training Loss: 1.1084\n",
      "Epoch 6/10, Batch 442/883, Training Loss: 0.5129\n",
      "Epoch 6/10, Batch 443/883, Training Loss: 0.6480\n",
      "Epoch 6/10, Batch 444/883, Training Loss: 0.7170\n",
      "Epoch 6/10, Batch 445/883, Training Loss: 0.9349\n",
      "Epoch 6/10, Batch 446/883, Training Loss: 1.2448\n",
      "Epoch 6/10, Batch 447/883, Training Loss: 0.5331\n",
      "Epoch 6/10, Batch 448/883, Training Loss: 0.7597\n",
      "Epoch 6/10, Batch 449/883, Training Loss: 0.6917\n",
      "Epoch 6/10, Batch 450/883, Training Loss: 0.6117\n",
      "Epoch 6/10, Batch 451/883, Training Loss: 0.7509\n",
      "Epoch 6/10, Batch 452/883, Training Loss: 0.6520\n",
      "Epoch 6/10, Batch 453/883, Training Loss: 0.5851\n",
      "Epoch 6/10, Batch 454/883, Training Loss: 0.7656\n",
      "Epoch 6/10, Batch 455/883, Training Loss: 0.7204\n",
      "Epoch 6/10, Batch 456/883, Training Loss: 1.0793\n",
      "Epoch 6/10, Batch 457/883, Training Loss: 0.6959\n",
      "Epoch 6/10, Batch 458/883, Training Loss: 0.8442\n",
      "Epoch 6/10, Batch 459/883, Training Loss: 0.9991\n",
      "Epoch 6/10, Batch 460/883, Training Loss: 0.6493\n",
      "Epoch 6/10, Batch 461/883, Training Loss: 0.8203\n",
      "Epoch 6/10, Batch 462/883, Training Loss: 0.8703\n",
      "Epoch 6/10, Batch 463/883, Training Loss: 0.8481\n",
      "Epoch 6/10, Batch 464/883, Training Loss: 0.8264\n",
      "Epoch 6/10, Batch 465/883, Training Loss: 0.6411\n",
      "Epoch 6/10, Batch 466/883, Training Loss: 0.6360\n",
      "Epoch 6/10, Batch 467/883, Training Loss: 0.6492\n",
      "Epoch 6/10, Batch 468/883, Training Loss: 0.7261\n",
      "Epoch 6/10, Batch 469/883, Training Loss: 0.5199\n",
      "Epoch 6/10, Batch 470/883, Training Loss: 0.7119\n",
      "Epoch 6/10, Batch 471/883, Training Loss: 0.7210\n",
      "Epoch 6/10, Batch 472/883, Training Loss: 0.7260\n",
      "Epoch 6/10, Batch 473/883, Training Loss: 0.6179\n",
      "Epoch 6/10, Batch 474/883, Training Loss: 0.5799\n",
      "Epoch 6/10, Batch 475/883, Training Loss: 0.6901\n",
      "Epoch 6/10, Batch 476/883, Training Loss: 0.6120\n",
      "Epoch 6/10, Batch 477/883, Training Loss: 0.6148\n",
      "Epoch 6/10, Batch 478/883, Training Loss: 0.7698\n",
      "Epoch 6/10, Batch 479/883, Training Loss: 0.5167\n",
      "Epoch 6/10, Batch 480/883, Training Loss: 0.6199\n",
      "Epoch 6/10, Batch 481/883, Training Loss: 0.9676\n",
      "Epoch 6/10, Batch 482/883, Training Loss: 0.5205\n",
      "Epoch 6/10, Batch 483/883, Training Loss: 1.0518\n",
      "Epoch 6/10, Batch 484/883, Training Loss: 0.6146\n",
      "Epoch 6/10, Batch 485/883, Training Loss: 1.0653\n",
      "Epoch 6/10, Batch 486/883, Training Loss: 0.7585\n",
      "Epoch 6/10, Batch 487/883, Training Loss: 0.6726\n",
      "Epoch 6/10, Batch 488/883, Training Loss: 0.5088\n",
      "Epoch 6/10, Batch 489/883, Training Loss: 0.6802\n",
      "Epoch 6/10, Batch 490/883, Training Loss: 1.0097\n",
      "Epoch 6/10, Batch 491/883, Training Loss: 0.7073\n",
      "Epoch 6/10, Batch 492/883, Training Loss: 0.4340\n",
      "Epoch 6/10, Batch 493/883, Training Loss: 0.8390\n",
      "Epoch 6/10, Batch 494/883, Training Loss: 0.8315\n",
      "Epoch 6/10, Batch 495/883, Training Loss: 0.6366\n",
      "Epoch 6/10, Batch 496/883, Training Loss: 0.6788\n",
      "Epoch 6/10, Batch 497/883, Training Loss: 0.6897\n",
      "Epoch 6/10, Batch 498/883, Training Loss: 0.9253\n",
      "Epoch 6/10, Batch 499/883, Training Loss: 0.6125\n",
      "Epoch 6/10, Batch 500/883, Training Loss: 0.8688\n",
      "Epoch 6/10, Batch 501/883, Training Loss: 0.4655\n",
      "Epoch 6/10, Batch 502/883, Training Loss: 0.4715\n",
      "Epoch 6/10, Batch 503/883, Training Loss: 0.6252\n",
      "Epoch 6/10, Batch 504/883, Training Loss: 0.6737\n",
      "Epoch 6/10, Batch 505/883, Training Loss: 0.6449\n",
      "Epoch 6/10, Batch 506/883, Training Loss: 0.5046\n",
      "Epoch 6/10, Batch 507/883, Training Loss: 0.5844\n",
      "Epoch 6/10, Batch 508/883, Training Loss: 0.7107\n",
      "Epoch 6/10, Batch 509/883, Training Loss: 0.4373\n",
      "Epoch 6/10, Batch 510/883, Training Loss: 0.7476\n",
      "Epoch 6/10, Batch 511/883, Training Loss: 0.7663\n",
      "Epoch 6/10, Batch 512/883, Training Loss: 0.5800\n",
      "Epoch 6/10, Batch 513/883, Training Loss: 0.4802\n",
      "Epoch 6/10, Batch 514/883, Training Loss: 0.7868\n",
      "Epoch 6/10, Batch 515/883, Training Loss: 0.7596\n",
      "Epoch 6/10, Batch 516/883, Training Loss: 0.7309\n",
      "Epoch 6/10, Batch 517/883, Training Loss: 0.6721\n",
      "Epoch 6/10, Batch 518/883, Training Loss: 0.6813\n",
      "Epoch 6/10, Batch 519/883, Training Loss: 0.5436\n",
      "Epoch 6/10, Batch 520/883, Training Loss: 0.7160\n",
      "Epoch 6/10, Batch 521/883, Training Loss: 0.5095\n",
      "Epoch 6/10, Batch 522/883, Training Loss: 0.6208\n",
      "Epoch 6/10, Batch 523/883, Training Loss: 0.5724\n",
      "Epoch 6/10, Batch 524/883, Training Loss: 0.5892\n",
      "Epoch 6/10, Batch 525/883, Training Loss: 0.5937\n",
      "Epoch 6/10, Batch 526/883, Training Loss: 0.7050\n",
      "Epoch 6/10, Batch 527/883, Training Loss: 0.5970\n",
      "Epoch 6/10, Batch 528/883, Training Loss: 0.4736\n",
      "Epoch 6/10, Batch 529/883, Training Loss: 0.6030\n",
      "Epoch 6/10, Batch 530/883, Training Loss: 0.6562\n",
      "Epoch 6/10, Batch 531/883, Training Loss: 0.7177\n",
      "Epoch 6/10, Batch 532/883, Training Loss: 0.8842\n",
      "Epoch 6/10, Batch 533/883, Training Loss: 0.5767\n",
      "Epoch 6/10, Batch 534/883, Training Loss: 0.7487\n",
      "Epoch 6/10, Batch 535/883, Training Loss: 0.4851\n",
      "Epoch 6/10, Batch 536/883, Training Loss: 0.4356\n",
      "Epoch 6/10, Batch 537/883, Training Loss: 0.5093\n",
      "Epoch 6/10, Batch 538/883, Training Loss: 0.5525\n",
      "Epoch 6/10, Batch 539/883, Training Loss: 0.5615\n",
      "Epoch 6/10, Batch 540/883, Training Loss: 0.5898\n",
      "Epoch 6/10, Batch 541/883, Training Loss: 0.5358\n",
      "Epoch 6/10, Batch 542/883, Training Loss: 0.5413\n",
      "Epoch 6/10, Batch 543/883, Training Loss: 0.6672\n",
      "Epoch 6/10, Batch 544/883, Training Loss: 0.4716\n",
      "Epoch 6/10, Batch 545/883, Training Loss: 0.7891\n",
      "Epoch 6/10, Batch 546/883, Training Loss: 0.6366\n",
      "Epoch 6/10, Batch 547/883, Training Loss: 0.7803\n",
      "Epoch 6/10, Batch 548/883, Training Loss: 0.7911\n",
      "Epoch 6/10, Batch 549/883, Training Loss: 0.5733\n",
      "Epoch 6/10, Batch 550/883, Training Loss: 0.5401\n",
      "Epoch 6/10, Batch 551/883, Training Loss: 0.5566\n",
      "Epoch 6/10, Batch 552/883, Training Loss: 0.7211\n",
      "Epoch 6/10, Batch 553/883, Training Loss: 0.9335\n",
      "Epoch 6/10, Batch 554/883, Training Loss: 0.7482\n",
      "Epoch 6/10, Batch 555/883, Training Loss: 0.7712\n",
      "Epoch 6/10, Batch 556/883, Training Loss: 0.7412\n",
      "Epoch 6/10, Batch 557/883, Training Loss: 0.5126\n",
      "Epoch 6/10, Batch 558/883, Training Loss: 0.5203\n",
      "Epoch 6/10, Batch 559/883, Training Loss: 0.5091\n",
      "Epoch 6/10, Batch 560/883, Training Loss: 0.6348\n",
      "Epoch 6/10, Batch 561/883, Training Loss: 0.5024\n",
      "Epoch 6/10, Batch 562/883, Training Loss: 0.8379\n",
      "Epoch 6/10, Batch 563/883, Training Loss: 0.5547\n",
      "Epoch 6/10, Batch 564/883, Training Loss: 0.6243\n",
      "Epoch 6/10, Batch 565/883, Training Loss: 0.5981\n",
      "Epoch 6/10, Batch 566/883, Training Loss: 0.5227\n",
      "Epoch 6/10, Batch 567/883, Training Loss: 1.0271\n",
      "Epoch 6/10, Batch 568/883, Training Loss: 0.6406\n",
      "Epoch 6/10, Batch 569/883, Training Loss: 0.7537\n",
      "Epoch 6/10, Batch 570/883, Training Loss: 0.5411\n",
      "Epoch 6/10, Batch 571/883, Training Loss: 0.7144\n",
      "Epoch 6/10, Batch 572/883, Training Loss: 0.7134\n",
      "Epoch 6/10, Batch 573/883, Training Loss: 0.9859\n",
      "Epoch 6/10, Batch 574/883, Training Loss: 0.7711\n",
      "Epoch 6/10, Batch 575/883, Training Loss: 0.5676\n",
      "Epoch 6/10, Batch 576/883, Training Loss: 0.7006\n",
      "Epoch 6/10, Batch 577/883, Training Loss: 0.5845\n",
      "Epoch 6/10, Batch 578/883, Training Loss: 0.6757\n",
      "Epoch 6/10, Batch 579/883, Training Loss: 0.6069\n",
      "Epoch 6/10, Batch 580/883, Training Loss: 0.6993\n",
      "Epoch 6/10, Batch 581/883, Training Loss: 0.6498\n",
      "Epoch 6/10, Batch 582/883, Training Loss: 0.8138\n",
      "Epoch 6/10, Batch 583/883, Training Loss: 0.6808\n",
      "Epoch 6/10, Batch 584/883, Training Loss: 0.6045\n",
      "Epoch 6/10, Batch 585/883, Training Loss: 0.6369\n",
      "Epoch 6/10, Batch 586/883, Training Loss: 0.6061\n",
      "Epoch 6/10, Batch 587/883, Training Loss: 0.5030\n",
      "Epoch 6/10, Batch 588/883, Training Loss: 0.8867\n",
      "Epoch 6/10, Batch 589/883, Training Loss: 0.6371\n",
      "Epoch 6/10, Batch 590/883, Training Loss: 0.6245\n",
      "Epoch 6/10, Batch 591/883, Training Loss: 0.5278\n",
      "Epoch 6/10, Batch 592/883, Training Loss: 0.5862\n",
      "Epoch 6/10, Batch 593/883, Training Loss: 0.6597\n",
      "Epoch 6/10, Batch 594/883, Training Loss: 0.8087\n",
      "Epoch 6/10, Batch 595/883, Training Loss: 0.5489\n",
      "Epoch 6/10, Batch 596/883, Training Loss: 0.6267\n",
      "Epoch 6/10, Batch 597/883, Training Loss: 1.2330\n",
      "Epoch 6/10, Batch 598/883, Training Loss: 1.1425\n",
      "Epoch 6/10, Batch 599/883, Training Loss: 0.4927\n",
      "Epoch 6/10, Batch 600/883, Training Loss: 0.5393\n",
      "Epoch 6/10, Batch 601/883, Training Loss: 0.7328\n",
      "Epoch 6/10, Batch 602/883, Training Loss: 0.8544\n",
      "Epoch 6/10, Batch 603/883, Training Loss: 0.9038\n",
      "Epoch 6/10, Batch 604/883, Training Loss: 0.6940\n",
      "Epoch 6/10, Batch 605/883, Training Loss: 1.1913\n",
      "Epoch 6/10, Batch 606/883, Training Loss: 0.6561\n",
      "Epoch 6/10, Batch 607/883, Training Loss: 0.8792\n",
      "Epoch 6/10, Batch 608/883, Training Loss: 0.5213\n",
      "Epoch 6/10, Batch 609/883, Training Loss: 0.8452\n",
      "Epoch 6/10, Batch 610/883, Training Loss: 0.4541\n",
      "Epoch 6/10, Batch 611/883, Training Loss: 0.9336\n",
      "Epoch 6/10, Batch 612/883, Training Loss: 0.7602\n",
      "Epoch 6/10, Batch 613/883, Training Loss: 0.9647\n",
      "Epoch 6/10, Batch 614/883, Training Loss: 1.0660\n",
      "Epoch 6/10, Batch 615/883, Training Loss: 0.5936\n",
      "Epoch 6/10, Batch 616/883, Training Loss: 0.7444\n",
      "Epoch 6/10, Batch 617/883, Training Loss: 0.6529\n",
      "Epoch 6/10, Batch 618/883, Training Loss: 0.5996\n",
      "Epoch 6/10, Batch 619/883, Training Loss: 0.7680\n",
      "Epoch 6/10, Batch 620/883, Training Loss: 0.5863\n",
      "Epoch 6/10, Batch 621/883, Training Loss: 0.9096\n",
      "Epoch 6/10, Batch 622/883, Training Loss: 0.8626\n",
      "Epoch 6/10, Batch 623/883, Training Loss: 0.7417\n",
      "Epoch 6/10, Batch 624/883, Training Loss: 0.8971\n",
      "Epoch 6/10, Batch 625/883, Training Loss: 0.6360\n",
      "Epoch 6/10, Batch 626/883, Training Loss: 0.8719\n",
      "Epoch 6/10, Batch 627/883, Training Loss: 0.7810\n",
      "Epoch 6/10, Batch 628/883, Training Loss: 0.9147\n",
      "Epoch 6/10, Batch 629/883, Training Loss: 0.6313\n",
      "Epoch 6/10, Batch 630/883, Training Loss: 0.5217\n",
      "Epoch 6/10, Batch 631/883, Training Loss: 0.4819\n",
      "Epoch 6/10, Batch 632/883, Training Loss: 0.6911\n",
      "Epoch 6/10, Batch 633/883, Training Loss: 0.6939\n",
      "Epoch 6/10, Batch 634/883, Training Loss: 0.6567\n",
      "Epoch 6/10, Batch 635/883, Training Loss: 0.8509\n",
      "Epoch 6/10, Batch 636/883, Training Loss: 0.7058\n",
      "Epoch 6/10, Batch 637/883, Training Loss: 0.5126\n",
      "Epoch 6/10, Batch 638/883, Training Loss: 0.6908\n",
      "Epoch 6/10, Batch 639/883, Training Loss: 0.7204\n",
      "Epoch 6/10, Batch 640/883, Training Loss: 0.4306\n",
      "Epoch 6/10, Batch 641/883, Training Loss: 0.6931\n",
      "Epoch 6/10, Batch 642/883, Training Loss: 0.6324\n",
      "Epoch 6/10, Batch 643/883, Training Loss: 0.7345\n",
      "Epoch 6/10, Batch 644/883, Training Loss: 0.5865\n",
      "Epoch 6/10, Batch 645/883, Training Loss: 0.7652\n",
      "Epoch 6/10, Batch 646/883, Training Loss: 0.6673\n",
      "Epoch 6/10, Batch 647/883, Training Loss: 0.6356\n",
      "Epoch 6/10, Batch 648/883, Training Loss: 0.5803\n",
      "Epoch 6/10, Batch 649/883, Training Loss: 0.7968\n",
      "Epoch 6/10, Batch 650/883, Training Loss: 0.8654\n",
      "Epoch 6/10, Batch 651/883, Training Loss: 0.6520\n",
      "Epoch 6/10, Batch 652/883, Training Loss: 0.5917\n",
      "Epoch 6/10, Batch 653/883, Training Loss: 1.1188\n",
      "Epoch 6/10, Batch 654/883, Training Loss: 0.5883\n",
      "Epoch 6/10, Batch 655/883, Training Loss: 0.5287\n",
      "Epoch 6/10, Batch 656/883, Training Loss: 1.0094\n",
      "Epoch 6/10, Batch 657/883, Training Loss: 0.8749\n",
      "Epoch 6/10, Batch 658/883, Training Loss: 0.6843\n",
      "Epoch 6/10, Batch 659/883, Training Loss: 0.9131\n",
      "Epoch 6/10, Batch 660/883, Training Loss: 0.4977\n",
      "Epoch 6/10, Batch 661/883, Training Loss: 0.5952\n",
      "Epoch 6/10, Batch 662/883, Training Loss: 0.6519\n",
      "Epoch 6/10, Batch 663/883, Training Loss: 0.8127\n",
      "Epoch 6/10, Batch 664/883, Training Loss: 0.8289\n",
      "Epoch 6/10, Batch 665/883, Training Loss: 0.7849\n",
      "Epoch 6/10, Batch 666/883, Training Loss: 0.6349\n",
      "Epoch 6/10, Batch 667/883, Training Loss: 0.6998\n",
      "Epoch 6/10, Batch 668/883, Training Loss: 0.5810\n",
      "Epoch 6/10, Batch 669/883, Training Loss: 0.4121\n",
      "Epoch 6/10, Batch 670/883, Training Loss: 0.7287\n",
      "Epoch 6/10, Batch 671/883, Training Loss: 0.8826\n",
      "Epoch 6/10, Batch 672/883, Training Loss: 0.4783\n",
      "Epoch 6/10, Batch 673/883, Training Loss: 0.6389\n",
      "Epoch 6/10, Batch 674/883, Training Loss: 0.6611\n",
      "Epoch 6/10, Batch 675/883, Training Loss: 0.7065\n",
      "Epoch 6/10, Batch 676/883, Training Loss: 0.9636\n",
      "Epoch 6/10, Batch 677/883, Training Loss: 0.9027\n",
      "Epoch 6/10, Batch 678/883, Training Loss: 0.6014\n",
      "Epoch 6/10, Batch 679/883, Training Loss: 0.8613\n",
      "Epoch 6/10, Batch 680/883, Training Loss: 0.6823\n",
      "Epoch 6/10, Batch 681/883, Training Loss: 1.4114\n",
      "Epoch 6/10, Batch 682/883, Training Loss: 0.5705\n",
      "Epoch 6/10, Batch 683/883, Training Loss: 0.6578\n",
      "Epoch 6/10, Batch 684/883, Training Loss: 0.6629\n",
      "Epoch 6/10, Batch 685/883, Training Loss: 0.6902\n",
      "Epoch 6/10, Batch 686/883, Training Loss: 0.6055\n",
      "Epoch 6/10, Batch 687/883, Training Loss: 0.8019\n",
      "Epoch 6/10, Batch 688/883, Training Loss: 1.1491\n",
      "Epoch 6/10, Batch 689/883, Training Loss: 0.7084\n",
      "Epoch 6/10, Batch 690/883, Training Loss: 0.9229\n",
      "Epoch 6/10, Batch 691/883, Training Loss: 0.7352\n",
      "Epoch 6/10, Batch 692/883, Training Loss: 0.8577\n",
      "Epoch 6/10, Batch 693/883, Training Loss: 0.9086\n",
      "Epoch 6/10, Batch 694/883, Training Loss: 0.7907\n",
      "Epoch 6/10, Batch 695/883, Training Loss: 0.8529\n",
      "Epoch 6/10, Batch 696/883, Training Loss: 0.5845\n",
      "Epoch 6/10, Batch 697/883, Training Loss: 0.8791\n",
      "Epoch 6/10, Batch 698/883, Training Loss: 1.1775\n",
      "Epoch 6/10, Batch 699/883, Training Loss: 0.9361\n",
      "Epoch 6/10, Batch 700/883, Training Loss: 0.5327\n",
      "Epoch 6/10, Batch 701/883, Training Loss: 1.1390\n",
      "Epoch 6/10, Batch 702/883, Training Loss: 0.6057\n",
      "Epoch 6/10, Batch 703/883, Training Loss: 0.6631\n",
      "Epoch 6/10, Batch 704/883, Training Loss: 0.7077\n",
      "Epoch 6/10, Batch 705/883, Training Loss: 0.7357\n",
      "Epoch 6/10, Batch 706/883, Training Loss: 0.6158\n",
      "Epoch 6/10, Batch 707/883, Training Loss: 0.9121\n",
      "Epoch 6/10, Batch 708/883, Training Loss: 0.7356\n",
      "Epoch 6/10, Batch 709/883, Training Loss: 0.5710\n",
      "Epoch 6/10, Batch 710/883, Training Loss: 0.6069\n",
      "Epoch 6/10, Batch 711/883, Training Loss: 0.7440\n",
      "Epoch 6/10, Batch 712/883, Training Loss: 0.6337\n",
      "Epoch 6/10, Batch 713/883, Training Loss: 0.8193\n",
      "Epoch 6/10, Batch 714/883, Training Loss: 0.5890\n",
      "Epoch 6/10, Batch 715/883, Training Loss: 0.6945\n",
      "Epoch 6/10, Batch 716/883, Training Loss: 0.5765\n",
      "Epoch 6/10, Batch 717/883, Training Loss: 0.7507\n",
      "Epoch 6/10, Batch 718/883, Training Loss: 0.7927\n",
      "Epoch 6/10, Batch 719/883, Training Loss: 0.8036\n",
      "Epoch 6/10, Batch 720/883, Training Loss: 0.6311\n",
      "Epoch 6/10, Batch 721/883, Training Loss: 0.6885\n",
      "Epoch 6/10, Batch 722/883, Training Loss: 0.5215\n",
      "Epoch 6/10, Batch 723/883, Training Loss: 0.7250\n",
      "Epoch 6/10, Batch 724/883, Training Loss: 0.7233\n",
      "Epoch 6/10, Batch 725/883, Training Loss: 0.7594\n",
      "Epoch 6/10, Batch 726/883, Training Loss: 0.4368\n",
      "Epoch 6/10, Batch 727/883, Training Loss: 0.7669\n",
      "Epoch 6/10, Batch 728/883, Training Loss: 0.9096\n",
      "Epoch 6/10, Batch 729/883, Training Loss: 0.9468\n",
      "Epoch 6/10, Batch 730/883, Training Loss: 0.7211\n",
      "Epoch 6/10, Batch 731/883, Training Loss: 0.6749\n",
      "Epoch 6/10, Batch 732/883, Training Loss: 0.5595\n",
      "Epoch 6/10, Batch 733/883, Training Loss: 0.7575\n",
      "Epoch 6/10, Batch 734/883, Training Loss: 0.7988\n",
      "Epoch 6/10, Batch 735/883, Training Loss: 0.7466\n",
      "Epoch 6/10, Batch 736/883, Training Loss: 0.6223\n",
      "Epoch 6/10, Batch 737/883, Training Loss: 0.7378\n",
      "Epoch 6/10, Batch 738/883, Training Loss: 0.6524\n",
      "Epoch 6/10, Batch 739/883, Training Loss: 0.7975\n",
      "Epoch 6/10, Batch 740/883, Training Loss: 0.7027\n",
      "Epoch 6/10, Batch 741/883, Training Loss: 0.7519\n",
      "Epoch 6/10, Batch 742/883, Training Loss: 0.6547\n",
      "Epoch 6/10, Batch 743/883, Training Loss: 0.6977\n",
      "Epoch 6/10, Batch 744/883, Training Loss: 0.8340\n",
      "Epoch 6/10, Batch 745/883, Training Loss: 0.5546\n",
      "Epoch 6/10, Batch 746/883, Training Loss: 0.5520\n",
      "Epoch 6/10, Batch 747/883, Training Loss: 0.5285\n",
      "Epoch 6/10, Batch 748/883, Training Loss: 0.4970\n",
      "Epoch 6/10, Batch 749/883, Training Loss: 0.6887\n",
      "Epoch 6/10, Batch 750/883, Training Loss: 0.9089\n",
      "Epoch 6/10, Batch 751/883, Training Loss: 0.6843\n",
      "Epoch 6/10, Batch 752/883, Training Loss: 0.8045\n",
      "Epoch 6/10, Batch 753/883, Training Loss: 0.4691\n",
      "Epoch 6/10, Batch 754/883, Training Loss: 0.6295\n",
      "Epoch 6/10, Batch 755/883, Training Loss: 0.7275\n",
      "Epoch 6/10, Batch 756/883, Training Loss: 0.7260\n",
      "Epoch 6/10, Batch 757/883, Training Loss: 0.6944\n",
      "Epoch 6/10, Batch 758/883, Training Loss: 0.7120\n",
      "Epoch 6/10, Batch 759/883, Training Loss: 0.8514\n",
      "Epoch 6/10, Batch 760/883, Training Loss: 0.4623\n",
      "Epoch 6/10, Batch 761/883, Training Loss: 0.6717\n",
      "Epoch 6/10, Batch 762/883, Training Loss: 0.4459\n",
      "Epoch 6/10, Batch 763/883, Training Loss: 0.7076\n",
      "Epoch 6/10, Batch 764/883, Training Loss: 0.7125\n",
      "Epoch 6/10, Batch 765/883, Training Loss: 0.4898\n",
      "Epoch 6/10, Batch 766/883, Training Loss: 0.6213\n",
      "Epoch 6/10, Batch 767/883, Training Loss: 0.4897\n",
      "Epoch 6/10, Batch 768/883, Training Loss: 0.7252\n",
      "Epoch 6/10, Batch 769/883, Training Loss: 0.8459\n",
      "Epoch 6/10, Batch 770/883, Training Loss: 0.7551\n",
      "Epoch 6/10, Batch 771/883, Training Loss: 0.6059\n",
      "Epoch 6/10, Batch 772/883, Training Loss: 0.7247\n",
      "Epoch 6/10, Batch 773/883, Training Loss: 0.5759\n",
      "Epoch 6/10, Batch 774/883, Training Loss: 0.6306\n",
      "Epoch 6/10, Batch 775/883, Training Loss: 0.4631\n",
      "Epoch 6/10, Batch 776/883, Training Loss: 0.7450\n",
      "Epoch 6/10, Batch 777/883, Training Loss: 0.6402\n",
      "Epoch 6/10, Batch 778/883, Training Loss: 0.4973\n",
      "Epoch 6/10, Batch 779/883, Training Loss: 0.7034\n",
      "Epoch 6/10, Batch 780/883, Training Loss: 0.4874\n",
      "Epoch 6/10, Batch 781/883, Training Loss: 0.6304\n",
      "Epoch 6/10, Batch 782/883, Training Loss: 0.7070\n",
      "Epoch 6/10, Batch 783/883, Training Loss: 0.7470\n",
      "Epoch 6/10, Batch 784/883, Training Loss: 0.8166\n",
      "Epoch 6/10, Batch 785/883, Training Loss: 0.7816\n",
      "Epoch 6/10, Batch 786/883, Training Loss: 0.8080\n",
      "Epoch 6/10, Batch 787/883, Training Loss: 0.7629\n",
      "Epoch 6/10, Batch 788/883, Training Loss: 0.8952\n",
      "Epoch 6/10, Batch 789/883, Training Loss: 0.5346\n",
      "Epoch 6/10, Batch 790/883, Training Loss: 0.7206\n",
      "Epoch 6/10, Batch 791/883, Training Loss: 0.9159\n",
      "Epoch 6/10, Batch 792/883, Training Loss: 0.5281\n",
      "Epoch 6/10, Batch 793/883, Training Loss: 0.6254\n",
      "Epoch 6/10, Batch 794/883, Training Loss: 0.5280\n",
      "Epoch 6/10, Batch 795/883, Training Loss: 0.5726\n",
      "Epoch 6/10, Batch 796/883, Training Loss: 0.5492\n",
      "Epoch 6/10, Batch 797/883, Training Loss: 0.8039\n",
      "Epoch 6/10, Batch 798/883, Training Loss: 0.7704\n",
      "Epoch 6/10, Batch 799/883, Training Loss: 0.8304\n",
      "Epoch 6/10, Batch 800/883, Training Loss: 0.6384\n",
      "Epoch 6/10, Batch 801/883, Training Loss: 0.6966\n",
      "Epoch 6/10, Batch 802/883, Training Loss: 0.6870\n",
      "Epoch 6/10, Batch 803/883, Training Loss: 0.6163\n",
      "Epoch 6/10, Batch 804/883, Training Loss: 0.4755\n",
      "Epoch 6/10, Batch 805/883, Training Loss: 0.5915\n",
      "Epoch 6/10, Batch 806/883, Training Loss: 0.7525\n",
      "Epoch 6/10, Batch 807/883, Training Loss: 0.9150\n",
      "Epoch 6/10, Batch 808/883, Training Loss: 0.6738\n",
      "Epoch 6/10, Batch 809/883, Training Loss: 0.5579\n",
      "Epoch 6/10, Batch 810/883, Training Loss: 0.6691\n",
      "Epoch 6/10, Batch 811/883, Training Loss: 0.5784\n",
      "Epoch 6/10, Batch 812/883, Training Loss: 0.6375\n",
      "Epoch 6/10, Batch 813/883, Training Loss: 0.7538\n",
      "Epoch 6/10, Batch 814/883, Training Loss: 0.9365\n",
      "Epoch 6/10, Batch 815/883, Training Loss: 0.6900\n",
      "Epoch 6/10, Batch 816/883, Training Loss: 0.7509\n",
      "Epoch 6/10, Batch 817/883, Training Loss: 0.6503\n",
      "Epoch 6/10, Batch 818/883, Training Loss: 1.0095\n",
      "Epoch 6/10, Batch 819/883, Training Loss: 0.4897\n",
      "Epoch 6/10, Batch 820/883, Training Loss: 0.5347\n",
      "Epoch 6/10, Batch 821/883, Training Loss: 0.5169\n",
      "Epoch 6/10, Batch 822/883, Training Loss: 0.4196\n",
      "Epoch 6/10, Batch 823/883, Training Loss: 0.8192\n",
      "Epoch 6/10, Batch 824/883, Training Loss: 0.5103\n",
      "Epoch 6/10, Batch 825/883, Training Loss: 0.8105\n",
      "Epoch 6/10, Batch 826/883, Training Loss: 0.5018\n",
      "Epoch 6/10, Batch 827/883, Training Loss: 1.1434\n",
      "Epoch 6/10, Batch 828/883, Training Loss: 0.4908\n",
      "Epoch 6/10, Batch 829/883, Training Loss: 0.4221\n",
      "Epoch 6/10, Batch 830/883, Training Loss: 0.4469\n",
      "Epoch 6/10, Batch 831/883, Training Loss: 0.6459\n",
      "Epoch 6/10, Batch 832/883, Training Loss: 0.6079\n",
      "Epoch 6/10, Batch 833/883, Training Loss: 0.5382\n",
      "Epoch 6/10, Batch 834/883, Training Loss: 0.6084\n",
      "Epoch 6/10, Batch 835/883, Training Loss: 0.6036\n",
      "Epoch 6/10, Batch 836/883, Training Loss: 0.5765\n",
      "Epoch 6/10, Batch 837/883, Training Loss: 0.6540\n",
      "Epoch 6/10, Batch 838/883, Training Loss: 0.6701\n",
      "Epoch 6/10, Batch 839/883, Training Loss: 0.8435\n",
      "Epoch 6/10, Batch 840/883, Training Loss: 0.9418\n",
      "Epoch 6/10, Batch 841/883, Training Loss: 0.6133\n",
      "Epoch 6/10, Batch 842/883, Training Loss: 0.5981\n",
      "Epoch 6/10, Batch 843/883, Training Loss: 0.5747\n",
      "Epoch 6/10, Batch 844/883, Training Loss: 0.5228\n",
      "Epoch 6/10, Batch 845/883, Training Loss: 0.8823\n",
      "Epoch 6/10, Batch 846/883, Training Loss: 0.8090\n",
      "Epoch 6/10, Batch 847/883, Training Loss: 0.6560\n",
      "Epoch 6/10, Batch 848/883, Training Loss: 0.6224\n",
      "Epoch 6/10, Batch 849/883, Training Loss: 0.5601\n",
      "Epoch 6/10, Batch 850/883, Training Loss: 0.6206\n",
      "Epoch 6/10, Batch 851/883, Training Loss: 0.5153\n",
      "Epoch 6/10, Batch 852/883, Training Loss: 0.7678\n",
      "Epoch 6/10, Batch 853/883, Training Loss: 0.6241\n",
      "Epoch 6/10, Batch 854/883, Training Loss: 0.4909\n",
      "Epoch 6/10, Batch 855/883, Training Loss: 0.5861\n",
      "Epoch 6/10, Batch 856/883, Training Loss: 0.7877\n",
      "Epoch 6/10, Batch 857/883, Training Loss: 0.7300\n",
      "Epoch 6/10, Batch 858/883, Training Loss: 0.4355\n",
      "Epoch 6/10, Batch 859/883, Training Loss: 0.7296\n",
      "Epoch 6/10, Batch 860/883, Training Loss: 0.6280\n",
      "Epoch 6/10, Batch 861/883, Training Loss: 0.8509\n",
      "Epoch 6/10, Batch 862/883, Training Loss: 1.0455\n",
      "Epoch 6/10, Batch 863/883, Training Loss: 0.2804\n",
      "Epoch 6/10, Batch 864/883, Training Loss: 0.7026\n",
      "Epoch 6/10, Batch 865/883, Training Loss: 0.7517\n",
      "Epoch 6/10, Batch 866/883, Training Loss: 0.9512\n",
      "Epoch 6/10, Batch 867/883, Training Loss: 0.6388\n",
      "Epoch 6/10, Batch 868/883, Training Loss: 0.7734\n",
      "Epoch 6/10, Batch 869/883, Training Loss: 0.8466\n",
      "Epoch 6/10, Batch 870/883, Training Loss: 0.5863\n",
      "Epoch 6/10, Batch 871/883, Training Loss: 1.0652\n",
      "Epoch 6/10, Batch 872/883, Training Loss: 0.4532\n",
      "Epoch 6/10, Batch 873/883, Training Loss: 0.6095\n",
      "Epoch 6/10, Batch 874/883, Training Loss: 0.4249\n",
      "Epoch 6/10, Batch 875/883, Training Loss: 0.5628\n",
      "Epoch 6/10, Batch 876/883, Training Loss: 0.7044\n",
      "Epoch 6/10, Batch 877/883, Training Loss: 0.7924\n",
      "Epoch 6/10, Batch 878/883, Training Loss: 0.7402\n",
      "Epoch 6/10, Batch 879/883, Training Loss: 0.8212\n",
      "Epoch 6/10, Batch 880/883, Training Loss: 0.5794\n",
      "Epoch 6/10, Batch 881/883, Training Loss: 0.8141\n",
      "Epoch 6/10, Batch 882/883, Training Loss: 0.5746\n",
      "Epoch 6/10, Batch 883/883, Training Loss: 0.4131\n",
      "Epoch 6/10, Training Loss: 0.6950, Validation Loss: 0.6974, Validation Accuracy: 0.6654\n",
      "Epoch 7/10, Batch 1/883, Training Loss: 0.6653\n",
      "Epoch 7/10, Batch 2/883, Training Loss: 0.4004\n",
      "Epoch 7/10, Batch 3/883, Training Loss: 0.8757\n",
      "Epoch 7/10, Batch 4/883, Training Loss: 0.7207\n",
      "Epoch 7/10, Batch 5/883, Training Loss: 0.5516\n",
      "Epoch 7/10, Batch 6/883, Training Loss: 0.6645\n",
      "Epoch 7/10, Batch 7/883, Training Loss: 0.8139\n",
      "Epoch 7/10, Batch 8/883, Training Loss: 0.9196\n",
      "Epoch 7/10, Batch 9/883, Training Loss: 0.7610\n",
      "Epoch 7/10, Batch 10/883, Training Loss: 0.8049\n",
      "Epoch 7/10, Batch 11/883, Training Loss: 0.9522\n",
      "Epoch 7/10, Batch 12/883, Training Loss: 1.0637\n",
      "Epoch 7/10, Batch 13/883, Training Loss: 0.5025\n",
      "Epoch 7/10, Batch 14/883, Training Loss: 0.5515\n",
      "Epoch 7/10, Batch 15/883, Training Loss: 0.6219\n",
      "Epoch 7/10, Batch 16/883, Training Loss: 0.5944\n",
      "Epoch 7/10, Batch 17/883, Training Loss: 0.7015\n",
      "Epoch 7/10, Batch 18/883, Training Loss: 0.7073\n",
      "Epoch 7/10, Batch 19/883, Training Loss: 0.6966\n",
      "Epoch 7/10, Batch 20/883, Training Loss: 0.6202\n",
      "Epoch 7/10, Batch 21/883, Training Loss: 0.7090\n",
      "Epoch 7/10, Batch 22/883, Training Loss: 0.6505\n",
      "Epoch 7/10, Batch 23/883, Training Loss: 0.6700\n",
      "Epoch 7/10, Batch 24/883, Training Loss: 0.6389\n",
      "Epoch 7/10, Batch 25/883, Training Loss: 0.5631\n",
      "Epoch 7/10, Batch 26/883, Training Loss: 0.7675\n",
      "Epoch 7/10, Batch 27/883, Training Loss: 0.5813\n",
      "Epoch 7/10, Batch 28/883, Training Loss: 0.8360\n",
      "Epoch 7/10, Batch 29/883, Training Loss: 0.6017\n",
      "Epoch 7/10, Batch 30/883, Training Loss: 0.5175\n",
      "Epoch 7/10, Batch 31/883, Training Loss: 0.6595\n",
      "Epoch 7/10, Batch 32/883, Training Loss: 0.5890\n",
      "Epoch 7/10, Batch 33/883, Training Loss: 0.5734\n",
      "Epoch 7/10, Batch 34/883, Training Loss: 0.6875\n",
      "Epoch 7/10, Batch 35/883, Training Loss: 0.5608\n",
      "Epoch 7/10, Batch 36/883, Training Loss: 0.6538\n",
      "Epoch 7/10, Batch 37/883, Training Loss: 0.4371\n",
      "Epoch 7/10, Batch 38/883, Training Loss: 0.9829\n",
      "Epoch 7/10, Batch 39/883, Training Loss: 0.6091\n",
      "Epoch 7/10, Batch 40/883, Training Loss: 0.5719\n",
      "Epoch 7/10, Batch 41/883, Training Loss: 0.5137\n",
      "Epoch 7/10, Batch 42/883, Training Loss: 0.7426\n",
      "Epoch 7/10, Batch 43/883, Training Loss: 0.6072\n",
      "Epoch 7/10, Batch 44/883, Training Loss: 0.9057\n",
      "Epoch 7/10, Batch 45/883, Training Loss: 0.6244\n",
      "Epoch 7/10, Batch 46/883, Training Loss: 0.7047\n",
      "Epoch 7/10, Batch 47/883, Training Loss: 0.8175\n",
      "Epoch 7/10, Batch 48/883, Training Loss: 0.6198\n",
      "Epoch 7/10, Batch 49/883, Training Loss: 0.8996\n",
      "Epoch 7/10, Batch 50/883, Training Loss: 0.7087\n",
      "Epoch 7/10, Batch 51/883, Training Loss: 0.6905\n",
      "Epoch 7/10, Batch 52/883, Training Loss: 0.6160\n",
      "Epoch 7/10, Batch 53/883, Training Loss: 0.5798\n",
      "Epoch 7/10, Batch 54/883, Training Loss: 0.6293\n",
      "Epoch 7/10, Batch 55/883, Training Loss: 0.8603\n",
      "Epoch 7/10, Batch 56/883, Training Loss: 0.6447\n",
      "Epoch 7/10, Batch 57/883, Training Loss: 0.7249\n",
      "Epoch 7/10, Batch 58/883, Training Loss: 0.5001\n",
      "Epoch 7/10, Batch 59/883, Training Loss: 0.7119\n",
      "Epoch 7/10, Batch 60/883, Training Loss: 0.4306\n",
      "Epoch 7/10, Batch 61/883, Training Loss: 0.6627\n",
      "Epoch 7/10, Batch 62/883, Training Loss: 0.8030\n",
      "Epoch 7/10, Batch 63/883, Training Loss: 0.6176\n",
      "Epoch 7/10, Batch 64/883, Training Loss: 0.4788\n",
      "Epoch 7/10, Batch 65/883, Training Loss: 0.7329\n",
      "Epoch 7/10, Batch 66/883, Training Loss: 0.7842\n",
      "Epoch 7/10, Batch 67/883, Training Loss: 0.6436\n",
      "Epoch 7/10, Batch 68/883, Training Loss: 0.6561\n",
      "Epoch 7/10, Batch 69/883, Training Loss: 0.9319\n",
      "Epoch 7/10, Batch 70/883, Training Loss: 0.7243\n",
      "Epoch 7/10, Batch 71/883, Training Loss: 0.5839\n",
      "Epoch 7/10, Batch 72/883, Training Loss: 0.6218\n",
      "Epoch 7/10, Batch 73/883, Training Loss: 0.6170\n",
      "Epoch 7/10, Batch 74/883, Training Loss: 0.7538\n",
      "Epoch 7/10, Batch 75/883, Training Loss: 0.6147\n",
      "Epoch 7/10, Batch 76/883, Training Loss: 0.6863\n",
      "Epoch 7/10, Batch 77/883, Training Loss: 0.7631\n",
      "Epoch 7/10, Batch 78/883, Training Loss: 0.5544\n",
      "Epoch 7/10, Batch 79/883, Training Loss: 0.9617\n",
      "Epoch 7/10, Batch 80/883, Training Loss: 0.5068\n",
      "Epoch 7/10, Batch 81/883, Training Loss: 0.7511\n",
      "Epoch 7/10, Batch 82/883, Training Loss: 0.5574\n",
      "Epoch 7/10, Batch 83/883, Training Loss: 0.6045\n",
      "Epoch 7/10, Batch 84/883, Training Loss: 1.0163\n",
      "Epoch 7/10, Batch 85/883, Training Loss: 0.7128\n",
      "Epoch 7/10, Batch 86/883, Training Loss: 0.7320\n",
      "Epoch 7/10, Batch 87/883, Training Loss: 0.7648\n",
      "Epoch 7/10, Batch 88/883, Training Loss: 0.3407\n",
      "Epoch 7/10, Batch 89/883, Training Loss: 0.4015\n",
      "Epoch 7/10, Batch 90/883, Training Loss: 0.4461\n",
      "Epoch 7/10, Batch 91/883, Training Loss: 0.9462\n",
      "Epoch 7/10, Batch 92/883, Training Loss: 0.3489\n",
      "Epoch 7/10, Batch 93/883, Training Loss: 0.7821\n",
      "Epoch 7/10, Batch 94/883, Training Loss: 0.8072\n",
      "Epoch 7/10, Batch 95/883, Training Loss: 0.6837\n",
      "Epoch 7/10, Batch 96/883, Training Loss: 0.7948\n",
      "Epoch 7/10, Batch 97/883, Training Loss: 0.5144\n",
      "Epoch 7/10, Batch 98/883, Training Loss: 0.9262\n",
      "Epoch 7/10, Batch 99/883, Training Loss: 1.0424\n",
      "Epoch 7/10, Batch 100/883, Training Loss: 0.5591\n",
      "Epoch 7/10, Batch 101/883, Training Loss: 0.6491\n",
      "Epoch 7/10, Batch 102/883, Training Loss: 0.4445\n",
      "Epoch 7/10, Batch 103/883, Training Loss: 0.7120\n",
      "Epoch 7/10, Batch 104/883, Training Loss: 0.4226\n",
      "Epoch 7/10, Batch 105/883, Training Loss: 0.9460\n",
      "Epoch 7/10, Batch 106/883, Training Loss: 0.6409\n",
      "Epoch 7/10, Batch 107/883, Training Loss: 0.7560\n",
      "Epoch 7/10, Batch 108/883, Training Loss: 1.0895\n",
      "Epoch 7/10, Batch 109/883, Training Loss: 0.8067\n",
      "Epoch 7/10, Batch 110/883, Training Loss: 0.6919\n",
      "Epoch 7/10, Batch 111/883, Training Loss: 0.7584\n",
      "Epoch 7/10, Batch 112/883, Training Loss: 0.5475\n",
      "Epoch 7/10, Batch 113/883, Training Loss: 0.4439\n",
      "Epoch 7/10, Batch 114/883, Training Loss: 0.7056\n",
      "Epoch 7/10, Batch 115/883, Training Loss: 0.5481\n",
      "Epoch 7/10, Batch 116/883, Training Loss: 0.7507\n",
      "Epoch 7/10, Batch 117/883, Training Loss: 0.6112\n",
      "Epoch 7/10, Batch 118/883, Training Loss: 0.7735\n",
      "Epoch 7/10, Batch 119/883, Training Loss: 0.5083\n",
      "Epoch 7/10, Batch 120/883, Training Loss: 0.6363\n",
      "Epoch 7/10, Batch 121/883, Training Loss: 0.4482\n",
      "Epoch 7/10, Batch 122/883, Training Loss: 0.7331\n",
      "Epoch 7/10, Batch 123/883, Training Loss: 0.7247\n",
      "Epoch 7/10, Batch 124/883, Training Loss: 1.0321\n",
      "Epoch 7/10, Batch 125/883, Training Loss: 0.3489\n",
      "Epoch 7/10, Batch 126/883, Training Loss: 0.7920\n",
      "Epoch 7/10, Batch 127/883, Training Loss: 0.7155\n",
      "Epoch 7/10, Batch 128/883, Training Loss: 1.0224\n",
      "Epoch 7/10, Batch 129/883, Training Loss: 0.5712\n",
      "Epoch 7/10, Batch 130/883, Training Loss: 0.6202\n",
      "Epoch 7/10, Batch 131/883, Training Loss: 0.6645\n",
      "Epoch 7/10, Batch 132/883, Training Loss: 0.7870\n",
      "Epoch 7/10, Batch 133/883, Training Loss: 0.6569\n",
      "Epoch 7/10, Batch 134/883, Training Loss: 0.6007\n",
      "Epoch 7/10, Batch 135/883, Training Loss: 0.6824\n",
      "Epoch 7/10, Batch 136/883, Training Loss: 0.5667\n",
      "Epoch 7/10, Batch 137/883, Training Loss: 0.7005\n",
      "Epoch 7/10, Batch 138/883, Training Loss: 0.8081\n",
      "Epoch 7/10, Batch 139/883, Training Loss: 0.6036\n",
      "Epoch 7/10, Batch 140/883, Training Loss: 0.7194\n",
      "Epoch 7/10, Batch 141/883, Training Loss: 0.8713\n",
      "Epoch 7/10, Batch 142/883, Training Loss: 0.4935\n",
      "Epoch 7/10, Batch 143/883, Training Loss: 0.5212\n",
      "Epoch 7/10, Batch 144/883, Training Loss: 0.6997\n",
      "Epoch 7/10, Batch 145/883, Training Loss: 0.5428\n",
      "Epoch 7/10, Batch 146/883, Training Loss: 0.7914\n",
      "Epoch 7/10, Batch 147/883, Training Loss: 0.5338\n",
      "Epoch 7/10, Batch 148/883, Training Loss: 0.4620\n",
      "Epoch 7/10, Batch 149/883, Training Loss: 0.5684\n",
      "Epoch 7/10, Batch 150/883, Training Loss: 0.8022\n",
      "Epoch 7/10, Batch 151/883, Training Loss: 0.5223\n",
      "Epoch 7/10, Batch 152/883, Training Loss: 0.8307\n",
      "Epoch 7/10, Batch 153/883, Training Loss: 0.4358\n",
      "Epoch 7/10, Batch 154/883, Training Loss: 0.6386\n",
      "Epoch 7/10, Batch 155/883, Training Loss: 0.6468\n",
      "Epoch 7/10, Batch 156/883, Training Loss: 0.6509\n",
      "Epoch 7/10, Batch 157/883, Training Loss: 0.8074\n",
      "Epoch 7/10, Batch 158/883, Training Loss: 0.7307\n",
      "Epoch 7/10, Batch 159/883, Training Loss: 0.5578\n",
      "Epoch 7/10, Batch 160/883, Training Loss: 1.1917\n",
      "Epoch 7/10, Batch 161/883, Training Loss: 0.5800\n",
      "Epoch 7/10, Batch 162/883, Training Loss: 0.5446\n",
      "Epoch 7/10, Batch 163/883, Training Loss: 0.6772\n",
      "Epoch 7/10, Batch 164/883, Training Loss: 0.6574\n",
      "Epoch 7/10, Batch 165/883, Training Loss: 0.6406\n",
      "Epoch 7/10, Batch 166/883, Training Loss: 1.1806\n",
      "Epoch 7/10, Batch 167/883, Training Loss: 0.3278\n",
      "Epoch 7/10, Batch 168/883, Training Loss: 0.5104\n",
      "Epoch 7/10, Batch 169/883, Training Loss: 0.7398\n",
      "Epoch 7/10, Batch 170/883, Training Loss: 0.4758\n",
      "Epoch 7/10, Batch 171/883, Training Loss: 0.5892\n",
      "Epoch 7/10, Batch 172/883, Training Loss: 0.6700\n",
      "Epoch 7/10, Batch 173/883, Training Loss: 0.7155\n",
      "Epoch 7/10, Batch 174/883, Training Loss: 0.5535\n",
      "Epoch 7/10, Batch 175/883, Training Loss: 0.5400\n",
      "Epoch 7/10, Batch 176/883, Training Loss: 0.6055\n",
      "Epoch 7/10, Batch 177/883, Training Loss: 0.7670\n",
      "Epoch 7/10, Batch 178/883, Training Loss: 0.4758\n",
      "Epoch 7/10, Batch 179/883, Training Loss: 1.1751\n",
      "Epoch 7/10, Batch 180/883, Training Loss: 0.8755\n",
      "Epoch 7/10, Batch 181/883, Training Loss: 0.5702\n",
      "Epoch 7/10, Batch 182/883, Training Loss: 0.5711\n",
      "Epoch 7/10, Batch 183/883, Training Loss: 0.4356\n",
      "Epoch 7/10, Batch 184/883, Training Loss: 0.8578\n",
      "Epoch 7/10, Batch 185/883, Training Loss: 0.5852\n",
      "Epoch 7/10, Batch 186/883, Training Loss: 0.5297\n",
      "Epoch 7/10, Batch 187/883, Training Loss: 0.5653\n",
      "Epoch 7/10, Batch 188/883, Training Loss: 0.7135\n",
      "Epoch 7/10, Batch 189/883, Training Loss: 0.4476\n",
      "Epoch 7/10, Batch 190/883, Training Loss: 0.7607\n",
      "Epoch 7/10, Batch 191/883, Training Loss: 0.5901\n",
      "Epoch 7/10, Batch 192/883, Training Loss: 0.4606\n",
      "Epoch 7/10, Batch 193/883, Training Loss: 0.7240\n",
      "Epoch 7/10, Batch 194/883, Training Loss: 0.6114\n",
      "Epoch 7/10, Batch 195/883, Training Loss: 0.6197\n",
      "Epoch 7/10, Batch 196/883, Training Loss: 0.5236\n",
      "Epoch 7/10, Batch 197/883, Training Loss: 0.6962\n",
      "Epoch 7/10, Batch 198/883, Training Loss: 0.6092\n",
      "Epoch 7/10, Batch 199/883, Training Loss: 0.3872\n",
      "Epoch 7/10, Batch 200/883, Training Loss: 0.8867\n",
      "Epoch 7/10, Batch 201/883, Training Loss: 0.4914\n",
      "Epoch 7/10, Batch 202/883, Training Loss: 0.5988\n",
      "Epoch 7/10, Batch 203/883, Training Loss: 0.5396\n",
      "Epoch 7/10, Batch 204/883, Training Loss: 0.5806\n",
      "Epoch 7/10, Batch 205/883, Training Loss: 1.0454\n",
      "Epoch 7/10, Batch 206/883, Training Loss: 0.7765\n",
      "Epoch 7/10, Batch 207/883, Training Loss: 0.4914\n",
      "Epoch 7/10, Batch 208/883, Training Loss: 0.6146\n",
      "Epoch 7/10, Batch 209/883, Training Loss: 0.5652\n",
      "Epoch 7/10, Batch 210/883, Training Loss: 0.7023\n",
      "Epoch 7/10, Batch 211/883, Training Loss: 0.6789\n",
      "Epoch 7/10, Batch 212/883, Training Loss: 0.3454\n",
      "Epoch 7/10, Batch 213/883, Training Loss: 0.5849\n",
      "Epoch 7/10, Batch 214/883, Training Loss: 0.6072\n",
      "Epoch 7/10, Batch 215/883, Training Loss: 0.5657\n",
      "Epoch 7/10, Batch 216/883, Training Loss: 0.5790\n",
      "Epoch 7/10, Batch 217/883, Training Loss: 0.7052\n",
      "Epoch 7/10, Batch 218/883, Training Loss: 0.9522\n",
      "Epoch 7/10, Batch 219/883, Training Loss: 0.6650\n",
      "Epoch 7/10, Batch 220/883, Training Loss: 0.7138\n",
      "Epoch 7/10, Batch 221/883, Training Loss: 0.6145\n",
      "Epoch 7/10, Batch 222/883, Training Loss: 0.5835\n",
      "Epoch 7/10, Batch 223/883, Training Loss: 0.5585\n",
      "Epoch 7/10, Batch 224/883, Training Loss: 0.6869\n",
      "Epoch 7/10, Batch 225/883, Training Loss: 0.5611\n",
      "Epoch 7/10, Batch 226/883, Training Loss: 0.3791\n",
      "Epoch 7/10, Batch 227/883, Training Loss: 0.8480\n",
      "Epoch 7/10, Batch 228/883, Training Loss: 0.7994\n",
      "Epoch 7/10, Batch 229/883, Training Loss: 0.6248\n",
      "Epoch 7/10, Batch 230/883, Training Loss: 0.8593\n",
      "Epoch 7/10, Batch 231/883, Training Loss: 0.6858\n",
      "Epoch 7/10, Batch 232/883, Training Loss: 0.6521\n",
      "Epoch 7/10, Batch 233/883, Training Loss: 0.7299\n",
      "Epoch 7/10, Batch 234/883, Training Loss: 0.9211\n",
      "Epoch 7/10, Batch 235/883, Training Loss: 0.5256\n",
      "Epoch 7/10, Batch 236/883, Training Loss: 0.7180\n",
      "Epoch 7/10, Batch 237/883, Training Loss: 0.6986\n",
      "Epoch 7/10, Batch 238/883, Training Loss: 0.7608\n",
      "Epoch 7/10, Batch 239/883, Training Loss: 0.5939\n",
      "Epoch 7/10, Batch 240/883, Training Loss: 0.5279\n",
      "Epoch 7/10, Batch 241/883, Training Loss: 0.6984\n",
      "Epoch 7/10, Batch 242/883, Training Loss: 0.7521\n",
      "Epoch 7/10, Batch 243/883, Training Loss: 0.8843\n",
      "Epoch 7/10, Batch 244/883, Training Loss: 0.6664\n",
      "Epoch 7/10, Batch 245/883, Training Loss: 0.5135\n",
      "Epoch 7/10, Batch 246/883, Training Loss: 0.5997\n",
      "Epoch 7/10, Batch 247/883, Training Loss: 0.4889\n",
      "Epoch 7/10, Batch 248/883, Training Loss: 0.6776\n",
      "Epoch 7/10, Batch 249/883, Training Loss: 0.6954\n",
      "Epoch 7/10, Batch 250/883, Training Loss: 0.7267\n",
      "Epoch 7/10, Batch 251/883, Training Loss: 0.6335\n",
      "Epoch 7/10, Batch 252/883, Training Loss: 0.6826\n",
      "Epoch 7/10, Batch 253/883, Training Loss: 0.4777\n",
      "Epoch 7/10, Batch 254/883, Training Loss: 0.6263\n",
      "Epoch 7/10, Batch 255/883, Training Loss: 0.7205\n",
      "Epoch 7/10, Batch 256/883, Training Loss: 0.8961\n",
      "Epoch 7/10, Batch 257/883, Training Loss: 0.9153\n",
      "Epoch 7/10, Batch 258/883, Training Loss: 0.5810\n",
      "Epoch 7/10, Batch 259/883, Training Loss: 0.7360\n",
      "Epoch 7/10, Batch 260/883, Training Loss: 0.5823\n",
      "Epoch 7/10, Batch 261/883, Training Loss: 0.9076\n",
      "Epoch 7/10, Batch 262/883, Training Loss: 0.7603\n",
      "Epoch 7/10, Batch 263/883, Training Loss: 0.5633\n",
      "Epoch 7/10, Batch 264/883, Training Loss: 0.8014\n",
      "Epoch 7/10, Batch 265/883, Training Loss: 0.5543\n",
      "Epoch 7/10, Batch 266/883, Training Loss: 0.6106\n",
      "Epoch 7/10, Batch 267/883, Training Loss: 0.6797\n",
      "Epoch 7/10, Batch 268/883, Training Loss: 0.4414\n",
      "Epoch 7/10, Batch 269/883, Training Loss: 0.8020\n",
      "Epoch 7/10, Batch 270/883, Training Loss: 0.5700\n",
      "Epoch 7/10, Batch 271/883, Training Loss: 1.1009\n",
      "Epoch 7/10, Batch 272/883, Training Loss: 0.4566\n",
      "Epoch 7/10, Batch 273/883, Training Loss: 0.7369\n",
      "Epoch 7/10, Batch 274/883, Training Loss: 0.7861\n",
      "Epoch 7/10, Batch 275/883, Training Loss: 0.7801\n",
      "Epoch 7/10, Batch 276/883, Training Loss: 0.3763\n",
      "Epoch 7/10, Batch 277/883, Training Loss: 0.4730\n",
      "Epoch 7/10, Batch 278/883, Training Loss: 0.9631\n",
      "Epoch 7/10, Batch 279/883, Training Loss: 0.7013\n",
      "Epoch 7/10, Batch 280/883, Training Loss: 0.5374\n",
      "Epoch 7/10, Batch 281/883, Training Loss: 0.6642\n",
      "Epoch 7/10, Batch 282/883, Training Loss: 0.8797\n",
      "Epoch 7/10, Batch 283/883, Training Loss: 0.4786\n",
      "Epoch 7/10, Batch 284/883, Training Loss: 0.6865\n",
      "Epoch 7/10, Batch 285/883, Training Loss: 0.5787\n",
      "Epoch 7/10, Batch 286/883, Training Loss: 0.6406\n",
      "Epoch 7/10, Batch 287/883, Training Loss: 0.9442\n",
      "Epoch 7/10, Batch 288/883, Training Loss: 0.5101\n",
      "Epoch 7/10, Batch 289/883, Training Loss: 0.4860\n",
      "Epoch 7/10, Batch 290/883, Training Loss: 1.0962\n",
      "Epoch 7/10, Batch 291/883, Training Loss: 0.7455\n",
      "Epoch 7/10, Batch 292/883, Training Loss: 0.7133\n",
      "Epoch 7/10, Batch 293/883, Training Loss: 0.5983\n",
      "Epoch 7/10, Batch 294/883, Training Loss: 0.5676\n",
      "Epoch 7/10, Batch 295/883, Training Loss: 0.7328\n",
      "Epoch 7/10, Batch 296/883, Training Loss: 0.9649\n",
      "Epoch 7/10, Batch 297/883, Training Loss: 0.6638\n",
      "Epoch 7/10, Batch 298/883, Training Loss: 0.5218\n",
      "Epoch 7/10, Batch 299/883, Training Loss: 0.5393\n",
      "Epoch 7/10, Batch 300/883, Training Loss: 0.4947\n",
      "Epoch 7/10, Batch 301/883, Training Loss: 0.6865\n",
      "Epoch 7/10, Batch 302/883, Training Loss: 0.6361\n",
      "Epoch 7/10, Batch 303/883, Training Loss: 0.6091\n",
      "Epoch 7/10, Batch 304/883, Training Loss: 0.6739\n",
      "Epoch 7/10, Batch 305/883, Training Loss: 0.7490\n",
      "Epoch 7/10, Batch 306/883, Training Loss: 0.6299\n",
      "Epoch 7/10, Batch 307/883, Training Loss: 0.7044\n",
      "Epoch 7/10, Batch 308/883, Training Loss: 0.7102\n",
      "Epoch 7/10, Batch 309/883, Training Loss: 0.6850\n",
      "Epoch 7/10, Batch 310/883, Training Loss: 0.7302\n",
      "Epoch 7/10, Batch 311/883, Training Loss: 0.6957\n",
      "Epoch 7/10, Batch 312/883, Training Loss: 0.5779\n",
      "Epoch 7/10, Batch 313/883, Training Loss: 0.8395\n",
      "Epoch 7/10, Batch 314/883, Training Loss: 0.6632\n",
      "Epoch 7/10, Batch 315/883, Training Loss: 0.7612\n",
      "Epoch 7/10, Batch 316/883, Training Loss: 0.5871\n",
      "Epoch 7/10, Batch 317/883, Training Loss: 1.1964\n",
      "Epoch 7/10, Batch 318/883, Training Loss: 0.6469\n",
      "Epoch 7/10, Batch 319/883, Training Loss: 0.7169\n",
      "Epoch 7/10, Batch 320/883, Training Loss: 0.5777\n",
      "Epoch 7/10, Batch 321/883, Training Loss: 0.5324\n",
      "Epoch 7/10, Batch 322/883, Training Loss: 0.7707\n",
      "Epoch 7/10, Batch 323/883, Training Loss: 0.8288\n",
      "Epoch 7/10, Batch 324/883, Training Loss: 0.6837\n",
      "Epoch 7/10, Batch 325/883, Training Loss: 0.7607\n",
      "Epoch 7/10, Batch 326/883, Training Loss: 0.6698\n",
      "Epoch 7/10, Batch 327/883, Training Loss: 0.4956\n",
      "Epoch 7/10, Batch 328/883, Training Loss: 0.7231\n",
      "Epoch 7/10, Batch 329/883, Training Loss: 0.5273\n",
      "Epoch 7/10, Batch 330/883, Training Loss: 0.6499\n",
      "Epoch 7/10, Batch 331/883, Training Loss: 0.5105\n",
      "Epoch 7/10, Batch 332/883, Training Loss: 1.0990\n",
      "Epoch 7/10, Batch 333/883, Training Loss: 0.8128\n",
      "Epoch 7/10, Batch 334/883, Training Loss: 0.3995\n",
      "Epoch 7/10, Batch 335/883, Training Loss: 0.4169\n",
      "Epoch 7/10, Batch 336/883, Training Loss: 0.8967\n",
      "Epoch 7/10, Batch 337/883, Training Loss: 0.5377\n",
      "Epoch 7/10, Batch 338/883, Training Loss: 0.5424\n",
      "Epoch 7/10, Batch 339/883, Training Loss: 0.7742\n",
      "Epoch 7/10, Batch 340/883, Training Loss: 0.4667\n",
      "Epoch 7/10, Batch 341/883, Training Loss: 0.6377\n",
      "Epoch 7/10, Batch 342/883, Training Loss: 0.5745\n",
      "Epoch 7/10, Batch 343/883, Training Loss: 0.4162\n",
      "Epoch 7/10, Batch 344/883, Training Loss: 0.5214\n",
      "Epoch 7/10, Batch 345/883, Training Loss: 0.5497\n",
      "Epoch 7/10, Batch 346/883, Training Loss: 0.5762\n",
      "Epoch 7/10, Batch 347/883, Training Loss: 0.4045\n",
      "Epoch 7/10, Batch 348/883, Training Loss: 0.4685\n",
      "Epoch 7/10, Batch 349/883, Training Loss: 0.5207\n",
      "Epoch 7/10, Batch 350/883, Training Loss: 0.5752\n",
      "Epoch 7/10, Batch 351/883, Training Loss: 0.5408\n",
      "Epoch 7/10, Batch 352/883, Training Loss: 0.6683\n",
      "Epoch 7/10, Batch 353/883, Training Loss: 0.6615\n",
      "Epoch 7/10, Batch 354/883, Training Loss: 0.5472\n",
      "Epoch 7/10, Batch 355/883, Training Loss: 0.7343\n",
      "Epoch 7/10, Batch 356/883, Training Loss: 0.7490\n",
      "Epoch 7/10, Batch 357/883, Training Loss: 0.8967\n",
      "Epoch 7/10, Batch 358/883, Training Loss: 0.7681\n",
      "Epoch 7/10, Batch 359/883, Training Loss: 0.4296\n",
      "Epoch 7/10, Batch 360/883, Training Loss: 0.9268\n",
      "Epoch 7/10, Batch 361/883, Training Loss: 0.7056\n",
      "Epoch 7/10, Batch 362/883, Training Loss: 0.6256\n",
      "Epoch 7/10, Batch 363/883, Training Loss: 0.6872\n",
      "Epoch 7/10, Batch 364/883, Training Loss: 0.7819\n",
      "Epoch 7/10, Batch 365/883, Training Loss: 0.8878\n",
      "Epoch 7/10, Batch 366/883, Training Loss: 0.8243\n",
      "Epoch 7/10, Batch 367/883, Training Loss: 1.0586\n",
      "Epoch 7/10, Batch 368/883, Training Loss: 0.7023\n",
      "Epoch 7/10, Batch 369/883, Training Loss: 0.5925\n",
      "Epoch 7/10, Batch 370/883, Training Loss: 0.6605\n",
      "Epoch 7/10, Batch 371/883, Training Loss: 0.5229\n",
      "Epoch 7/10, Batch 372/883, Training Loss: 0.5400\n",
      "Epoch 7/10, Batch 373/883, Training Loss: 0.4310\n",
      "Epoch 7/10, Batch 374/883, Training Loss: 0.4724\n",
      "Epoch 7/10, Batch 375/883, Training Loss: 0.6776\n",
      "Epoch 7/10, Batch 376/883, Training Loss: 0.7025\n",
      "Epoch 7/10, Batch 377/883, Training Loss: 0.6644\n",
      "Epoch 7/10, Batch 378/883, Training Loss: 0.6585\n",
      "Epoch 7/10, Batch 379/883, Training Loss: 0.5980\n",
      "Epoch 7/10, Batch 380/883, Training Loss: 0.7113\n",
      "Epoch 7/10, Batch 381/883, Training Loss: 0.5264\n",
      "Epoch 7/10, Batch 382/883, Training Loss: 0.5751\n",
      "Epoch 7/10, Batch 383/883, Training Loss: 0.7823\n",
      "Epoch 7/10, Batch 384/883, Training Loss: 1.0324\n",
      "Epoch 7/10, Batch 385/883, Training Loss: 0.6597\n",
      "Epoch 7/10, Batch 386/883, Training Loss: 0.5611\n",
      "Epoch 7/10, Batch 387/883, Training Loss: 0.4583\n",
      "Epoch 7/10, Batch 388/883, Training Loss: 0.5829\n",
      "Epoch 7/10, Batch 389/883, Training Loss: 0.9620\n",
      "Epoch 7/10, Batch 390/883, Training Loss: 0.5750\n",
      "Epoch 7/10, Batch 391/883, Training Loss: 0.8627\n",
      "Epoch 7/10, Batch 392/883, Training Loss: 0.6222\n",
      "Epoch 7/10, Batch 393/883, Training Loss: 0.8396\n",
      "Epoch 7/10, Batch 394/883, Training Loss: 0.5625\n",
      "Epoch 7/10, Batch 395/883, Training Loss: 0.6812\n",
      "Epoch 7/10, Batch 396/883, Training Loss: 0.3827\n",
      "Epoch 7/10, Batch 397/883, Training Loss: 0.6569\n",
      "Epoch 7/10, Batch 398/883, Training Loss: 0.5429\n",
      "Epoch 7/10, Batch 399/883, Training Loss: 0.9156\n",
      "Epoch 7/10, Batch 400/883, Training Loss: 0.4989\n",
      "Epoch 7/10, Batch 401/883, Training Loss: 0.7664\n",
      "Epoch 7/10, Batch 402/883, Training Loss: 0.6707\n",
      "Epoch 7/10, Batch 403/883, Training Loss: 0.6799\n",
      "Epoch 7/10, Batch 404/883, Training Loss: 0.5242\n",
      "Epoch 7/10, Batch 405/883, Training Loss: 0.6411\n",
      "Epoch 7/10, Batch 406/883, Training Loss: 0.5545\n",
      "Epoch 7/10, Batch 407/883, Training Loss: 0.8622\n",
      "Epoch 7/10, Batch 408/883, Training Loss: 0.5765\n",
      "Epoch 7/10, Batch 409/883, Training Loss: 0.8445\n",
      "Epoch 7/10, Batch 410/883, Training Loss: 0.4833\n",
      "Epoch 7/10, Batch 411/883, Training Loss: 0.7834\n",
      "Epoch 7/10, Batch 412/883, Training Loss: 0.5558\n",
      "Epoch 7/10, Batch 413/883, Training Loss: 0.7861\n",
      "Epoch 7/10, Batch 414/883, Training Loss: 0.6341\n",
      "Epoch 7/10, Batch 415/883, Training Loss: 0.8047\n",
      "Epoch 7/10, Batch 416/883, Training Loss: 0.7430\n",
      "Epoch 7/10, Batch 417/883, Training Loss: 0.8659\n",
      "Epoch 7/10, Batch 418/883, Training Loss: 0.5302\n",
      "Epoch 7/10, Batch 419/883, Training Loss: 0.9291\n",
      "Epoch 7/10, Batch 420/883, Training Loss: 0.7434\n",
      "Epoch 7/10, Batch 421/883, Training Loss: 0.5044\n",
      "Epoch 7/10, Batch 422/883, Training Loss: 0.5695\n",
      "Epoch 7/10, Batch 423/883, Training Loss: 0.6291\n",
      "Epoch 7/10, Batch 424/883, Training Loss: 0.6104\n",
      "Epoch 7/10, Batch 425/883, Training Loss: 0.6852\n",
      "Epoch 7/10, Batch 426/883, Training Loss: 0.5800\n",
      "Epoch 7/10, Batch 427/883, Training Loss: 0.7986\n",
      "Epoch 7/10, Batch 428/883, Training Loss: 0.8040\n",
      "Epoch 7/10, Batch 429/883, Training Loss: 0.8057\n",
      "Epoch 7/10, Batch 430/883, Training Loss: 0.7119\n",
      "Epoch 7/10, Batch 431/883, Training Loss: 0.4510\n",
      "Epoch 7/10, Batch 432/883, Training Loss: 0.5653\n",
      "Epoch 7/10, Batch 433/883, Training Loss: 0.7493\n",
      "Epoch 7/10, Batch 434/883, Training Loss: 0.7434\n",
      "Epoch 7/10, Batch 435/883, Training Loss: 1.0346\n",
      "Epoch 7/10, Batch 436/883, Training Loss: 0.5780\n",
      "Epoch 7/10, Batch 437/883, Training Loss: 0.5954\n",
      "Epoch 7/10, Batch 438/883, Training Loss: 0.4793\n",
      "Epoch 7/10, Batch 439/883, Training Loss: 0.9148\n",
      "Epoch 7/10, Batch 440/883, Training Loss: 0.8592\n",
      "Epoch 7/10, Batch 441/883, Training Loss: 1.0093\n",
      "Epoch 7/10, Batch 442/883, Training Loss: 0.7402\n",
      "Epoch 7/10, Batch 443/883, Training Loss: 0.4201\n",
      "Epoch 7/10, Batch 444/883, Training Loss: 0.7708\n",
      "Epoch 7/10, Batch 445/883, Training Loss: 0.7416\n",
      "Epoch 7/10, Batch 446/883, Training Loss: 0.7899\n",
      "Epoch 7/10, Batch 447/883, Training Loss: 0.4821\n",
      "Epoch 7/10, Batch 448/883, Training Loss: 0.5755\n",
      "Epoch 7/10, Batch 449/883, Training Loss: 0.4711\n",
      "Epoch 7/10, Batch 450/883, Training Loss: 0.4183\n",
      "Epoch 7/10, Batch 451/883, Training Loss: 0.5124\n",
      "Epoch 7/10, Batch 452/883, Training Loss: 0.4333\n",
      "Epoch 7/10, Batch 453/883, Training Loss: 0.9138\n",
      "Epoch 7/10, Batch 454/883, Training Loss: 0.7471\n",
      "Epoch 7/10, Batch 455/883, Training Loss: 0.7610\n",
      "Epoch 7/10, Batch 456/883, Training Loss: 0.5864\n",
      "Epoch 7/10, Batch 457/883, Training Loss: 0.6925\n",
      "Epoch 7/10, Batch 458/883, Training Loss: 0.9607\n",
      "Epoch 7/10, Batch 459/883, Training Loss: 0.6087\n",
      "Epoch 7/10, Batch 460/883, Training Loss: 0.6713\n",
      "Epoch 7/10, Batch 461/883, Training Loss: 0.5153\n",
      "Epoch 7/10, Batch 462/883, Training Loss: 0.7262\n",
      "Epoch 7/10, Batch 463/883, Training Loss: 0.6094\n",
      "Epoch 7/10, Batch 464/883, Training Loss: 0.6957\n",
      "Epoch 7/10, Batch 465/883, Training Loss: 0.7645\n",
      "Epoch 7/10, Batch 466/883, Training Loss: 0.5733\n",
      "Epoch 7/10, Batch 467/883, Training Loss: 0.5896\n",
      "Epoch 7/10, Batch 468/883, Training Loss: 0.3934\n",
      "Epoch 7/10, Batch 469/883, Training Loss: 0.5191\n",
      "Epoch 7/10, Batch 470/883, Training Loss: 0.5447\n",
      "Epoch 7/10, Batch 471/883, Training Loss: 0.4051\n",
      "Epoch 7/10, Batch 472/883, Training Loss: 0.7205\n",
      "Epoch 7/10, Batch 473/883, Training Loss: 0.7465\n",
      "Epoch 7/10, Batch 474/883, Training Loss: 0.7604\n",
      "Epoch 7/10, Batch 475/883, Training Loss: 0.7050\n",
      "Epoch 7/10, Batch 476/883, Training Loss: 0.5612\n",
      "Epoch 7/10, Batch 477/883, Training Loss: 0.7313\n",
      "Epoch 7/10, Batch 478/883, Training Loss: 0.7065\n",
      "Epoch 7/10, Batch 479/883, Training Loss: 0.4656\n",
      "Epoch 7/10, Batch 480/883, Training Loss: 0.9832\n",
      "Epoch 7/10, Batch 481/883, Training Loss: 0.5032\n",
      "Epoch 7/10, Batch 482/883, Training Loss: 0.5781\n",
      "Epoch 7/10, Batch 483/883, Training Loss: 0.7320\n",
      "Epoch 7/10, Batch 484/883, Training Loss: 0.5274\n",
      "Epoch 7/10, Batch 485/883, Training Loss: 0.6795\n",
      "Epoch 7/10, Batch 486/883, Training Loss: 0.8326\n",
      "Epoch 7/10, Batch 487/883, Training Loss: 0.8057\n",
      "Epoch 7/10, Batch 488/883, Training Loss: 0.6177\n",
      "Epoch 7/10, Batch 489/883, Training Loss: 0.7339\n",
      "Epoch 7/10, Batch 490/883, Training Loss: 0.6098\n",
      "Epoch 7/10, Batch 491/883, Training Loss: 0.5388\n",
      "Epoch 7/10, Batch 492/883, Training Loss: 0.8679\n",
      "Epoch 7/10, Batch 493/883, Training Loss: 0.5573\n",
      "Epoch 7/10, Batch 494/883, Training Loss: 0.6188\n",
      "Epoch 7/10, Batch 495/883, Training Loss: 0.5968\n",
      "Epoch 7/10, Batch 496/883, Training Loss: 0.4981\n",
      "Epoch 7/10, Batch 497/883, Training Loss: 0.6212\n",
      "Epoch 7/10, Batch 498/883, Training Loss: 0.7818\n",
      "Epoch 7/10, Batch 499/883, Training Loss: 0.9385\n",
      "Epoch 7/10, Batch 500/883, Training Loss: 0.4803\n",
      "Epoch 7/10, Batch 501/883, Training Loss: 0.8929\n",
      "Epoch 7/10, Batch 502/883, Training Loss: 0.9025\n",
      "Epoch 7/10, Batch 503/883, Training Loss: 0.8854\n",
      "Epoch 7/10, Batch 504/883, Training Loss: 0.8016\n",
      "Epoch 7/10, Batch 505/883, Training Loss: 0.7130\n",
      "Epoch 7/10, Batch 506/883, Training Loss: 0.7205\n",
      "Epoch 7/10, Batch 507/883, Training Loss: 0.5306\n",
      "Epoch 7/10, Batch 508/883, Training Loss: 0.5464\n",
      "Epoch 7/10, Batch 509/883, Training Loss: 0.5029\n",
      "Epoch 7/10, Batch 510/883, Training Loss: 0.7788\n",
      "Epoch 7/10, Batch 511/883, Training Loss: 0.5636\n",
      "Epoch 7/10, Batch 512/883, Training Loss: 0.6693\n",
      "Epoch 7/10, Batch 513/883, Training Loss: 0.5084\n",
      "Epoch 7/10, Batch 514/883, Training Loss: 0.9220\n",
      "Epoch 7/10, Batch 515/883, Training Loss: 0.6901\n",
      "Epoch 7/10, Batch 516/883, Training Loss: 0.6435\n",
      "Epoch 7/10, Batch 517/883, Training Loss: 0.6044\n",
      "Epoch 7/10, Batch 518/883, Training Loss: 0.7132\n",
      "Epoch 7/10, Batch 519/883, Training Loss: 0.7087\n",
      "Epoch 7/10, Batch 520/883, Training Loss: 1.1418\n",
      "Epoch 7/10, Batch 521/883, Training Loss: 0.5168\n",
      "Epoch 7/10, Batch 522/883, Training Loss: 0.5982\n",
      "Epoch 7/10, Batch 523/883, Training Loss: 0.5668\n",
      "Epoch 7/10, Batch 524/883, Training Loss: 0.6885\n",
      "Epoch 7/10, Batch 525/883, Training Loss: 0.7117\n",
      "Epoch 7/10, Batch 526/883, Training Loss: 0.6621\n",
      "Epoch 7/10, Batch 527/883, Training Loss: 0.7177\n",
      "Epoch 7/10, Batch 528/883, Training Loss: 0.6707\n",
      "Epoch 7/10, Batch 529/883, Training Loss: 0.5951\n",
      "Epoch 7/10, Batch 530/883, Training Loss: 0.8634\n",
      "Epoch 7/10, Batch 531/883, Training Loss: 0.4497\n",
      "Epoch 7/10, Batch 532/883, Training Loss: 0.6078\n",
      "Epoch 7/10, Batch 533/883, Training Loss: 0.7365\n",
      "Epoch 7/10, Batch 534/883, Training Loss: 0.5461\n",
      "Epoch 7/10, Batch 535/883, Training Loss: 0.6090\n",
      "Epoch 7/10, Batch 536/883, Training Loss: 0.6745\n",
      "Epoch 7/10, Batch 537/883, Training Loss: 0.3874\n",
      "Epoch 7/10, Batch 538/883, Training Loss: 0.5935\n",
      "Epoch 7/10, Batch 539/883, Training Loss: 0.5198\n",
      "Epoch 7/10, Batch 540/883, Training Loss: 0.7199\n",
      "Epoch 7/10, Batch 541/883, Training Loss: 0.7877\n",
      "Epoch 7/10, Batch 542/883, Training Loss: 0.5498\n",
      "Epoch 7/10, Batch 543/883, Training Loss: 0.5154\n",
      "Epoch 7/10, Batch 544/883, Training Loss: 0.6565\n",
      "Epoch 7/10, Batch 545/883, Training Loss: 0.9812\n",
      "Epoch 7/10, Batch 546/883, Training Loss: 0.8807\n",
      "Epoch 7/10, Batch 547/883, Training Loss: 0.6135\n",
      "Epoch 7/10, Batch 548/883, Training Loss: 0.7613\n",
      "Epoch 7/10, Batch 549/883, Training Loss: 0.4288\n",
      "Epoch 7/10, Batch 550/883, Training Loss: 0.4691\n",
      "Epoch 7/10, Batch 551/883, Training Loss: 0.6969\n",
      "Epoch 7/10, Batch 552/883, Training Loss: 0.4929\n",
      "Epoch 7/10, Batch 553/883, Training Loss: 0.4197\n",
      "Epoch 7/10, Batch 554/883, Training Loss: 0.6040\n",
      "Epoch 7/10, Batch 555/883, Training Loss: 0.5188\n",
      "Epoch 7/10, Batch 556/883, Training Loss: 1.0393\n",
      "Epoch 7/10, Batch 557/883, Training Loss: 0.5938\n",
      "Epoch 7/10, Batch 558/883, Training Loss: 0.4575\n",
      "Epoch 7/10, Batch 559/883, Training Loss: 0.5588\n",
      "Epoch 7/10, Batch 560/883, Training Loss: 0.4481\n",
      "Epoch 7/10, Batch 561/883, Training Loss: 0.6579\n",
      "Epoch 7/10, Batch 562/883, Training Loss: 0.6511\n",
      "Epoch 7/10, Batch 563/883, Training Loss: 0.5418\n",
      "Epoch 7/10, Batch 564/883, Training Loss: 0.5528\n",
      "Epoch 7/10, Batch 565/883, Training Loss: 0.7842\n",
      "Epoch 7/10, Batch 566/883, Training Loss: 0.8487\n",
      "Epoch 7/10, Batch 567/883, Training Loss: 0.4628\n",
      "Epoch 7/10, Batch 568/883, Training Loss: 0.7047\n",
      "Epoch 7/10, Batch 569/883, Training Loss: 0.5281\n",
      "Epoch 7/10, Batch 570/883, Training Loss: 0.6013\n",
      "Epoch 7/10, Batch 571/883, Training Loss: 0.3706\n",
      "Epoch 7/10, Batch 572/883, Training Loss: 0.8113\n",
      "Epoch 7/10, Batch 573/883, Training Loss: 0.6378\n",
      "Epoch 7/10, Batch 574/883, Training Loss: 0.4864\n",
      "Epoch 7/10, Batch 575/883, Training Loss: 0.6424\n",
      "Epoch 7/10, Batch 576/883, Training Loss: 0.4092\n",
      "Epoch 7/10, Batch 577/883, Training Loss: 0.5517\n",
      "Epoch 7/10, Batch 578/883, Training Loss: 0.5238\n",
      "Epoch 7/10, Batch 579/883, Training Loss: 0.3719\n",
      "Epoch 7/10, Batch 580/883, Training Loss: 0.4493\n",
      "Epoch 7/10, Batch 581/883, Training Loss: 0.5482\n",
      "Epoch 7/10, Batch 582/883, Training Loss: 0.7319\n",
      "Epoch 7/10, Batch 583/883, Training Loss: 0.7677\n",
      "Epoch 7/10, Batch 584/883, Training Loss: 0.5004\n",
      "Epoch 7/10, Batch 585/883, Training Loss: 0.6780\n",
      "Epoch 7/10, Batch 586/883, Training Loss: 0.6993\n",
      "Epoch 7/10, Batch 587/883, Training Loss: 0.5532\n",
      "Epoch 7/10, Batch 588/883, Training Loss: 0.6114\n",
      "Epoch 7/10, Batch 589/883, Training Loss: 0.5213\n",
      "Epoch 7/10, Batch 590/883, Training Loss: 0.5166\n",
      "Epoch 7/10, Batch 591/883, Training Loss: 0.3591\n",
      "Epoch 7/10, Batch 592/883, Training Loss: 0.7182\n",
      "Epoch 7/10, Batch 593/883, Training Loss: 0.6649\n",
      "Epoch 7/10, Batch 594/883, Training Loss: 0.4739\n",
      "Epoch 7/10, Batch 595/883, Training Loss: 0.5009\n",
      "Epoch 7/10, Batch 596/883, Training Loss: 0.6119\n",
      "Epoch 7/10, Batch 597/883, Training Loss: 0.6909\n",
      "Epoch 7/10, Batch 598/883, Training Loss: 0.8399\n",
      "Epoch 7/10, Batch 599/883, Training Loss: 0.5416\n",
      "Epoch 7/10, Batch 600/883, Training Loss: 0.6922\n",
      "Epoch 7/10, Batch 601/883, Training Loss: 0.7194\n",
      "Epoch 7/10, Batch 602/883, Training Loss: 0.5023\n",
      "Epoch 7/10, Batch 603/883, Training Loss: 0.7085\n",
      "Epoch 7/10, Batch 604/883, Training Loss: 0.7360\n",
      "Epoch 7/10, Batch 605/883, Training Loss: 0.6716\n",
      "Epoch 7/10, Batch 606/883, Training Loss: 0.5645\n",
      "Epoch 7/10, Batch 607/883, Training Loss: 0.7292\n",
      "Epoch 7/10, Batch 608/883, Training Loss: 0.4110\n",
      "Epoch 7/10, Batch 609/883, Training Loss: 0.7174\n",
      "Epoch 7/10, Batch 610/883, Training Loss: 0.9617\n",
      "Epoch 7/10, Batch 611/883, Training Loss: 0.4565\n",
      "Epoch 7/10, Batch 612/883, Training Loss: 0.8706\n",
      "Epoch 7/10, Batch 613/883, Training Loss: 0.6459\n",
      "Epoch 7/10, Batch 614/883, Training Loss: 0.8221\n",
      "Epoch 7/10, Batch 615/883, Training Loss: 0.7535\n",
      "Epoch 7/10, Batch 616/883, Training Loss: 0.5932\n",
      "Epoch 7/10, Batch 617/883, Training Loss: 0.5352\n",
      "Epoch 7/10, Batch 618/883, Training Loss: 0.6662\n",
      "Epoch 7/10, Batch 619/883, Training Loss: 0.4970\n",
      "Epoch 7/10, Batch 620/883, Training Loss: 0.4478\n",
      "Epoch 7/10, Batch 621/883, Training Loss: 0.9732\n",
      "Epoch 7/10, Batch 622/883, Training Loss: 0.4480\n",
      "Epoch 7/10, Batch 623/883, Training Loss: 0.7629\n",
      "Epoch 7/10, Batch 624/883, Training Loss: 0.4967\n",
      "Epoch 7/10, Batch 625/883, Training Loss: 0.6238\n",
      "Epoch 7/10, Batch 626/883, Training Loss: 0.5596\n",
      "Epoch 7/10, Batch 627/883, Training Loss: 0.7938\n",
      "Epoch 7/10, Batch 628/883, Training Loss: 0.4167\n",
      "Epoch 7/10, Batch 629/883, Training Loss: 0.7798\n",
      "Epoch 7/10, Batch 630/883, Training Loss: 0.5896\n",
      "Epoch 7/10, Batch 631/883, Training Loss: 0.6635\n",
      "Epoch 7/10, Batch 632/883, Training Loss: 0.4690\n",
      "Epoch 7/10, Batch 633/883, Training Loss: 0.5300\n",
      "Epoch 7/10, Batch 634/883, Training Loss: 0.4819\n",
      "Epoch 7/10, Batch 635/883, Training Loss: 0.5523\n",
      "Epoch 7/10, Batch 636/883, Training Loss: 0.7039\n",
      "Epoch 7/10, Batch 637/883, Training Loss: 0.5183\n",
      "Epoch 7/10, Batch 638/883, Training Loss: 0.4800\n",
      "Epoch 7/10, Batch 639/883, Training Loss: 0.8049\n",
      "Epoch 7/10, Batch 640/883, Training Loss: 0.4128\n",
      "Epoch 7/10, Batch 641/883, Training Loss: 0.7344\n",
      "Epoch 7/10, Batch 642/883, Training Loss: 0.6304\n",
      "Epoch 7/10, Batch 643/883, Training Loss: 0.3913\n",
      "Epoch 7/10, Batch 644/883, Training Loss: 0.6753\n",
      "Epoch 7/10, Batch 645/883, Training Loss: 0.5103\n",
      "Epoch 7/10, Batch 646/883, Training Loss: 0.4233\n",
      "Epoch 7/10, Batch 647/883, Training Loss: 0.4116\n",
      "Epoch 7/10, Batch 648/883, Training Loss: 0.3187\n",
      "Epoch 7/10, Batch 649/883, Training Loss: 0.6541\n",
      "Epoch 7/10, Batch 650/883, Training Loss: 0.7834\n",
      "Epoch 7/10, Batch 651/883, Training Loss: 0.5296\n",
      "Epoch 7/10, Batch 652/883, Training Loss: 0.4579\n",
      "Epoch 7/10, Batch 653/883, Training Loss: 0.4739\n",
      "Epoch 7/10, Batch 654/883, Training Loss: 0.6430\n",
      "Epoch 7/10, Batch 655/883, Training Loss: 0.6897\n",
      "Epoch 7/10, Batch 656/883, Training Loss: 0.3784\n",
      "Epoch 7/10, Batch 657/883, Training Loss: 0.8830\n",
      "Epoch 7/10, Batch 658/883, Training Loss: 0.6777\n",
      "Epoch 7/10, Batch 659/883, Training Loss: 0.6973\n",
      "Epoch 7/10, Batch 660/883, Training Loss: 0.4010\n",
      "Epoch 7/10, Batch 661/883, Training Loss: 0.3505\n",
      "Epoch 7/10, Batch 662/883, Training Loss: 0.7421\n",
      "Epoch 7/10, Batch 663/883, Training Loss: 0.5986\n",
      "Epoch 7/10, Batch 664/883, Training Loss: 0.8267\n",
      "Epoch 7/10, Batch 665/883, Training Loss: 0.6790\n",
      "Epoch 7/10, Batch 666/883, Training Loss: 1.4639\n",
      "Epoch 7/10, Batch 667/883, Training Loss: 0.6688\n",
      "Epoch 7/10, Batch 668/883, Training Loss: 0.6764\n",
      "Epoch 7/10, Batch 669/883, Training Loss: 0.6389\n",
      "Epoch 7/10, Batch 670/883, Training Loss: 0.5205\n",
      "Epoch 7/10, Batch 671/883, Training Loss: 0.5857\n",
      "Epoch 7/10, Batch 672/883, Training Loss: 0.5722\n",
      "Epoch 7/10, Batch 673/883, Training Loss: 0.6547\n",
      "Epoch 7/10, Batch 674/883, Training Loss: 0.8058\n",
      "Epoch 7/10, Batch 675/883, Training Loss: 0.5096\n",
      "Epoch 7/10, Batch 676/883, Training Loss: 0.4672\n",
      "Epoch 7/10, Batch 677/883, Training Loss: 0.4107\n",
      "Epoch 7/10, Batch 678/883, Training Loss: 0.4805\n",
      "Epoch 7/10, Batch 679/883, Training Loss: 0.6052\n",
      "Epoch 7/10, Batch 680/883, Training Loss: 0.6621\n",
      "Epoch 7/10, Batch 681/883, Training Loss: 0.7403\n",
      "Epoch 7/10, Batch 682/883, Training Loss: 0.7203\n",
      "Epoch 7/10, Batch 683/883, Training Loss: 0.4552\n",
      "Epoch 7/10, Batch 684/883, Training Loss: 0.8158\n",
      "Epoch 7/10, Batch 685/883, Training Loss: 0.5115\n",
      "Epoch 7/10, Batch 686/883, Training Loss: 0.5903\n",
      "Epoch 7/10, Batch 687/883, Training Loss: 0.9966\n",
      "Epoch 7/10, Batch 688/883, Training Loss: 0.6640\n",
      "Epoch 7/10, Batch 689/883, Training Loss: 0.5017\n",
      "Epoch 7/10, Batch 690/883, Training Loss: 0.7633\n",
      "Epoch 7/10, Batch 691/883, Training Loss: 0.3706\n",
      "Epoch 7/10, Batch 692/883, Training Loss: 0.3886\n",
      "Epoch 7/10, Batch 693/883, Training Loss: 0.5094\n",
      "Epoch 7/10, Batch 694/883, Training Loss: 0.5418\n",
      "Epoch 7/10, Batch 695/883, Training Loss: 0.5931\n",
      "Epoch 7/10, Batch 696/883, Training Loss: 0.9194\n",
      "Epoch 7/10, Batch 697/883, Training Loss: 0.8461\n",
      "Epoch 7/10, Batch 698/883, Training Loss: 0.6459\n",
      "Epoch 7/10, Batch 699/883, Training Loss: 0.7961\n",
      "Epoch 7/10, Batch 700/883, Training Loss: 0.6236\n",
      "Epoch 7/10, Batch 701/883, Training Loss: 0.8232\n",
      "Epoch 7/10, Batch 702/883, Training Loss: 0.4812\n",
      "Epoch 7/10, Batch 703/883, Training Loss: 0.9563\n",
      "Epoch 7/10, Batch 704/883, Training Loss: 1.3190\n",
      "Epoch 7/10, Batch 705/883, Training Loss: 0.6230\n",
      "Epoch 7/10, Batch 706/883, Training Loss: 0.5974\n",
      "Epoch 7/10, Batch 707/883, Training Loss: 0.4765\n",
      "Epoch 7/10, Batch 708/883, Training Loss: 0.5845\n",
      "Epoch 7/10, Batch 709/883, Training Loss: 0.5794\n",
      "Epoch 7/10, Batch 710/883, Training Loss: 0.4208\n",
      "Epoch 7/10, Batch 711/883, Training Loss: 0.6261\n",
      "Epoch 7/10, Batch 712/883, Training Loss: 0.5780\n",
      "Epoch 7/10, Batch 713/883, Training Loss: 0.9516\n",
      "Epoch 7/10, Batch 714/883, Training Loss: 0.7847\n",
      "Epoch 7/10, Batch 715/883, Training Loss: 0.5549\n",
      "Epoch 7/10, Batch 716/883, Training Loss: 0.6255\n",
      "Epoch 7/10, Batch 717/883, Training Loss: 0.9656\n",
      "Epoch 7/10, Batch 718/883, Training Loss: 0.5660\n",
      "Epoch 7/10, Batch 719/883, Training Loss: 0.8265\n",
      "Epoch 7/10, Batch 720/883, Training Loss: 0.4733\n",
      "Epoch 7/10, Batch 721/883, Training Loss: 0.6640\n",
      "Epoch 7/10, Batch 722/883, Training Loss: 0.8273\n",
      "Epoch 7/10, Batch 723/883, Training Loss: 0.7023\n",
      "Epoch 7/10, Batch 724/883, Training Loss: 0.5835\n",
      "Epoch 7/10, Batch 725/883, Training Loss: 0.9103\n",
      "Epoch 7/10, Batch 726/883, Training Loss: 0.4933\n",
      "Epoch 7/10, Batch 727/883, Training Loss: 0.6895\n",
      "Epoch 7/10, Batch 728/883, Training Loss: 0.7508\n",
      "Epoch 7/10, Batch 729/883, Training Loss: 0.5082\n",
      "Epoch 7/10, Batch 730/883, Training Loss: 0.6878\n",
      "Epoch 7/10, Batch 731/883, Training Loss: 0.4597\n",
      "Epoch 7/10, Batch 732/883, Training Loss: 0.6833\n",
      "Epoch 7/10, Batch 733/883, Training Loss: 0.6180\n",
      "Epoch 7/10, Batch 734/883, Training Loss: 0.4215\n",
      "Epoch 7/10, Batch 735/883, Training Loss: 0.6226\n",
      "Epoch 7/10, Batch 736/883, Training Loss: 0.8496\n",
      "Epoch 7/10, Batch 737/883, Training Loss: 0.6692\n",
      "Epoch 7/10, Batch 738/883, Training Loss: 0.7683\n",
      "Epoch 7/10, Batch 739/883, Training Loss: 0.4944\n",
      "Epoch 7/10, Batch 740/883, Training Loss: 0.7213\n",
      "Epoch 7/10, Batch 741/883, Training Loss: 0.6315\n",
      "Epoch 7/10, Batch 742/883, Training Loss: 0.4346\n",
      "Epoch 7/10, Batch 743/883, Training Loss: 0.5364\n",
      "Epoch 7/10, Batch 744/883, Training Loss: 0.4323\n",
      "Epoch 7/10, Batch 745/883, Training Loss: 0.8634\n",
      "Epoch 7/10, Batch 746/883, Training Loss: 0.5591\n",
      "Epoch 7/10, Batch 747/883, Training Loss: 0.5860\n",
      "Epoch 7/10, Batch 748/883, Training Loss: 0.4536\n",
      "Epoch 7/10, Batch 749/883, Training Loss: 0.5437\n",
      "Epoch 7/10, Batch 750/883, Training Loss: 0.9434\n",
      "Epoch 7/10, Batch 751/883, Training Loss: 0.7847\n",
      "Epoch 7/10, Batch 752/883, Training Loss: 0.8186\n",
      "Epoch 7/10, Batch 753/883, Training Loss: 1.0487\n",
      "Epoch 7/10, Batch 754/883, Training Loss: 0.5340\n",
      "Epoch 7/10, Batch 755/883, Training Loss: 0.7942\n",
      "Epoch 7/10, Batch 756/883, Training Loss: 0.6237\n",
      "Epoch 7/10, Batch 757/883, Training Loss: 1.0024\n",
      "Epoch 7/10, Batch 758/883, Training Loss: 0.7410\n",
      "Epoch 7/10, Batch 759/883, Training Loss: 0.4381\n",
      "Epoch 7/10, Batch 760/883, Training Loss: 0.4769\n",
      "Epoch 7/10, Batch 761/883, Training Loss: 0.6549\n",
      "Epoch 7/10, Batch 762/883, Training Loss: 0.4944\n",
      "Epoch 7/10, Batch 763/883, Training Loss: 0.6143\n",
      "Epoch 7/10, Batch 764/883, Training Loss: 0.6735\n",
      "Epoch 7/10, Batch 765/883, Training Loss: 0.5128\n",
      "Epoch 7/10, Batch 766/883, Training Loss: 0.5142\n",
      "Epoch 7/10, Batch 767/883, Training Loss: 0.6345\n",
      "Epoch 7/10, Batch 768/883, Training Loss: 0.6780\n",
      "Epoch 7/10, Batch 769/883, Training Loss: 0.6504\n",
      "Epoch 7/10, Batch 770/883, Training Loss: 0.7241\n",
      "Epoch 7/10, Batch 771/883, Training Loss: 0.9025\n",
      "Epoch 7/10, Batch 772/883, Training Loss: 0.3941\n",
      "Epoch 7/10, Batch 773/883, Training Loss: 0.4817\n",
      "Epoch 7/10, Batch 774/883, Training Loss: 0.4717\n",
      "Epoch 7/10, Batch 775/883, Training Loss: 0.7403\n",
      "Epoch 7/10, Batch 776/883, Training Loss: 0.9732\n",
      "Epoch 7/10, Batch 777/883, Training Loss: 0.5947\n",
      "Epoch 7/10, Batch 778/883, Training Loss: 0.8536\n",
      "Epoch 7/10, Batch 779/883, Training Loss: 0.5718\n",
      "Epoch 7/10, Batch 780/883, Training Loss: 0.6059\n",
      "Epoch 7/10, Batch 781/883, Training Loss: 0.5656\n",
      "Epoch 7/10, Batch 782/883, Training Loss: 0.6796\n",
      "Epoch 7/10, Batch 783/883, Training Loss: 0.8004\n",
      "Epoch 7/10, Batch 784/883, Training Loss: 0.8295\n",
      "Epoch 7/10, Batch 785/883, Training Loss: 0.8376\n",
      "Epoch 7/10, Batch 786/883, Training Loss: 0.4971\n",
      "Epoch 7/10, Batch 787/883, Training Loss: 0.6472\n",
      "Epoch 7/10, Batch 788/883, Training Loss: 0.6469\n",
      "Epoch 7/10, Batch 789/883, Training Loss: 0.9654\n",
      "Epoch 7/10, Batch 790/883, Training Loss: 0.6402\n",
      "Epoch 7/10, Batch 791/883, Training Loss: 0.6996\n",
      "Epoch 7/10, Batch 792/883, Training Loss: 0.5123\n",
      "Epoch 7/10, Batch 793/883, Training Loss: 0.6536\n",
      "Epoch 7/10, Batch 794/883, Training Loss: 0.8341\n",
      "Epoch 7/10, Batch 795/883, Training Loss: 0.6581\n",
      "Epoch 7/10, Batch 796/883, Training Loss: 0.9786\n",
      "Epoch 7/10, Batch 797/883, Training Loss: 0.8774\n",
      "Epoch 7/10, Batch 798/883, Training Loss: 0.7345\n",
      "Epoch 7/10, Batch 799/883, Training Loss: 0.3748\n",
      "Epoch 7/10, Batch 800/883, Training Loss: 0.7338\n",
      "Epoch 7/10, Batch 801/883, Training Loss: 0.9182\n",
      "Epoch 7/10, Batch 802/883, Training Loss: 0.7800\n",
      "Epoch 7/10, Batch 803/883, Training Loss: 1.2102\n",
      "Epoch 7/10, Batch 804/883, Training Loss: 0.4330\n",
      "Epoch 7/10, Batch 805/883, Training Loss: 0.8648\n",
      "Epoch 7/10, Batch 806/883, Training Loss: 0.6774\n",
      "Epoch 7/10, Batch 807/883, Training Loss: 0.7692\n",
      "Epoch 7/10, Batch 808/883, Training Loss: 0.3930\n",
      "Epoch 7/10, Batch 809/883, Training Loss: 0.7784\n",
      "Epoch 7/10, Batch 810/883, Training Loss: 0.6135\n",
      "Epoch 7/10, Batch 811/883, Training Loss: 0.4239\n",
      "Epoch 7/10, Batch 812/883, Training Loss: 0.6269\n",
      "Epoch 7/10, Batch 813/883, Training Loss: 0.5836\n",
      "Epoch 7/10, Batch 814/883, Training Loss: 0.5664\n",
      "Epoch 7/10, Batch 815/883, Training Loss: 0.4327\n",
      "Epoch 7/10, Batch 816/883, Training Loss: 0.7877\n",
      "Epoch 7/10, Batch 817/883, Training Loss: 0.5375\n",
      "Epoch 7/10, Batch 818/883, Training Loss: 1.2469\n",
      "Epoch 7/10, Batch 819/883, Training Loss: 0.5858\n",
      "Epoch 7/10, Batch 820/883, Training Loss: 0.7447\n",
      "Epoch 7/10, Batch 821/883, Training Loss: 0.4279\n",
      "Epoch 7/10, Batch 822/883, Training Loss: 0.5665\n",
      "Epoch 7/10, Batch 823/883, Training Loss: 0.6291\n",
      "Epoch 7/10, Batch 824/883, Training Loss: 0.5527\n",
      "Epoch 7/10, Batch 825/883, Training Loss: 0.9913\n",
      "Epoch 7/10, Batch 826/883, Training Loss: 0.5829\n",
      "Epoch 7/10, Batch 827/883, Training Loss: 0.8555\n",
      "Epoch 7/10, Batch 828/883, Training Loss: 0.6076\n",
      "Epoch 7/10, Batch 829/883, Training Loss: 0.4818\n",
      "Epoch 7/10, Batch 830/883, Training Loss: 0.7688\n",
      "Epoch 7/10, Batch 831/883, Training Loss: 0.7723\n",
      "Epoch 7/10, Batch 832/883, Training Loss: 1.0719\n",
      "Epoch 7/10, Batch 833/883, Training Loss: 0.5427\n",
      "Epoch 7/10, Batch 834/883, Training Loss: 0.6498\n",
      "Epoch 7/10, Batch 835/883, Training Loss: 0.8645\n",
      "Epoch 7/10, Batch 836/883, Training Loss: 0.6993\n",
      "Epoch 7/10, Batch 837/883, Training Loss: 0.6868\n",
      "Epoch 7/10, Batch 838/883, Training Loss: 0.8520\n",
      "Epoch 7/10, Batch 839/883, Training Loss: 0.4416\n",
      "Epoch 7/10, Batch 840/883, Training Loss: 0.4867\n",
      "Epoch 7/10, Batch 841/883, Training Loss: 0.5217\n",
      "Epoch 7/10, Batch 842/883, Training Loss: 0.5964\n",
      "Epoch 7/10, Batch 843/883, Training Loss: 0.4521\n",
      "Epoch 7/10, Batch 844/883, Training Loss: 0.5380\n",
      "Epoch 7/10, Batch 845/883, Training Loss: 0.8041\n",
      "Epoch 7/10, Batch 846/883, Training Loss: 0.8231\n",
      "Epoch 7/10, Batch 847/883, Training Loss: 0.9564\n",
      "Epoch 7/10, Batch 848/883, Training Loss: 0.7074\n",
      "Epoch 7/10, Batch 849/883, Training Loss: 0.6504\n",
      "Epoch 7/10, Batch 850/883, Training Loss: 0.9996\n",
      "Epoch 7/10, Batch 851/883, Training Loss: 0.5545\n",
      "Epoch 7/10, Batch 852/883, Training Loss: 0.5099\n",
      "Epoch 7/10, Batch 853/883, Training Loss: 0.6030\n",
      "Epoch 7/10, Batch 854/883, Training Loss: 0.5625\n",
      "Epoch 7/10, Batch 855/883, Training Loss: 0.9916\n",
      "Epoch 7/10, Batch 856/883, Training Loss: 0.4528\n",
      "Epoch 7/10, Batch 857/883, Training Loss: 1.0177\n",
      "Epoch 7/10, Batch 858/883, Training Loss: 0.5413\n",
      "Epoch 7/10, Batch 859/883, Training Loss: 0.4423\n",
      "Epoch 7/10, Batch 860/883, Training Loss: 0.7020\n",
      "Epoch 7/10, Batch 861/883, Training Loss: 0.4806\n",
      "Epoch 7/10, Batch 862/883, Training Loss: 0.5375\n",
      "Epoch 7/10, Batch 863/883, Training Loss: 0.6601\n",
      "Epoch 7/10, Batch 864/883, Training Loss: 0.5659\n",
      "Epoch 7/10, Batch 865/883, Training Loss: 0.6661\n",
      "Epoch 7/10, Batch 866/883, Training Loss: 0.4586\n",
      "Epoch 7/10, Batch 867/883, Training Loss: 0.4948\n",
      "Epoch 7/10, Batch 868/883, Training Loss: 0.6442\n",
      "Epoch 7/10, Batch 869/883, Training Loss: 0.6795\n",
      "Epoch 7/10, Batch 870/883, Training Loss: 0.4449\n",
      "Epoch 7/10, Batch 871/883, Training Loss: 0.8780\n",
      "Epoch 7/10, Batch 872/883, Training Loss: 0.5505\n",
      "Epoch 7/10, Batch 873/883, Training Loss: 1.0452\n",
      "Epoch 7/10, Batch 874/883, Training Loss: 0.7225\n",
      "Epoch 7/10, Batch 875/883, Training Loss: 0.4431\n",
      "Epoch 7/10, Batch 876/883, Training Loss: 0.6998\n",
      "Epoch 7/10, Batch 877/883, Training Loss: 0.9558\n",
      "Epoch 7/10, Batch 878/883, Training Loss: 0.4744\n",
      "Epoch 7/10, Batch 879/883, Training Loss: 0.6015\n",
      "Epoch 7/10, Batch 880/883, Training Loss: 0.5302\n",
      "Epoch 7/10, Batch 881/883, Training Loss: 0.7653\n",
      "Epoch 7/10, Batch 882/883, Training Loss: 0.6553\n",
      "Epoch 7/10, Batch 883/883, Training Loss: 0.3458\n",
      "Epoch 7/10, Training Loss: 0.6586, Validation Loss: 0.6359, Validation Accuracy: 0.7183\n",
      "Epoch 8/10, Batch 1/883, Training Loss: 0.6986\n",
      "Epoch 8/10, Batch 2/883, Training Loss: 0.6010\n",
      "Epoch 8/10, Batch 3/883, Training Loss: 0.6699\n",
      "Epoch 8/10, Batch 4/883, Training Loss: 0.6095\n",
      "Epoch 8/10, Batch 5/883, Training Loss: 0.4532\n",
      "Epoch 8/10, Batch 6/883, Training Loss: 0.7242\n",
      "Epoch 8/10, Batch 7/883, Training Loss: 0.4661\n",
      "Epoch 8/10, Batch 8/883, Training Loss: 0.6726\n",
      "Epoch 8/10, Batch 9/883, Training Loss: 0.6670\n",
      "Epoch 8/10, Batch 10/883, Training Loss: 0.8323\n",
      "Epoch 8/10, Batch 11/883, Training Loss: 0.8899\n",
      "Epoch 8/10, Batch 12/883, Training Loss: 0.3561\n",
      "Epoch 8/10, Batch 13/883, Training Loss: 0.5957\n",
      "Epoch 8/10, Batch 14/883, Training Loss: 0.4437\n",
      "Epoch 8/10, Batch 15/883, Training Loss: 0.5957\n",
      "Epoch 8/10, Batch 16/883, Training Loss: 0.4794\n",
      "Epoch 8/10, Batch 17/883, Training Loss: 0.8968\n",
      "Epoch 8/10, Batch 18/883, Training Loss: 0.4041\n",
      "Epoch 8/10, Batch 19/883, Training Loss: 0.4530\n",
      "Epoch 8/10, Batch 20/883, Training Loss: 0.6889\n",
      "Epoch 8/10, Batch 21/883, Training Loss: 0.7198\n",
      "Epoch 8/10, Batch 22/883, Training Loss: 0.6064\n",
      "Epoch 8/10, Batch 23/883, Training Loss: 0.7331\n",
      "Epoch 8/10, Batch 24/883, Training Loss: 0.5443\n",
      "Epoch 8/10, Batch 25/883, Training Loss: 0.5538\n",
      "Epoch 8/10, Batch 26/883, Training Loss: 0.7577\n",
      "Epoch 8/10, Batch 27/883, Training Loss: 0.4634\n",
      "Epoch 8/10, Batch 28/883, Training Loss: 0.5695\n",
      "Epoch 8/10, Batch 29/883, Training Loss: 0.5393\n",
      "Epoch 8/10, Batch 30/883, Training Loss: 0.7109\n",
      "Epoch 8/10, Batch 31/883, Training Loss: 0.5282\n",
      "Epoch 8/10, Batch 32/883, Training Loss: 0.6617\n",
      "Epoch 8/10, Batch 33/883, Training Loss: 0.5047\n",
      "Epoch 8/10, Batch 34/883, Training Loss: 0.6215\n",
      "Epoch 8/10, Batch 35/883, Training Loss: 0.7765\n",
      "Epoch 8/10, Batch 36/883, Training Loss: 0.6370\n",
      "Epoch 8/10, Batch 37/883, Training Loss: 0.7476\n",
      "Epoch 8/10, Batch 38/883, Training Loss: 0.7537\n",
      "Epoch 8/10, Batch 39/883, Training Loss: 0.8184\n",
      "Epoch 8/10, Batch 40/883, Training Loss: 0.7231\n",
      "Epoch 8/10, Batch 41/883, Training Loss: 0.4271\n",
      "Epoch 8/10, Batch 42/883, Training Loss: 0.7291\n",
      "Epoch 8/10, Batch 43/883, Training Loss: 0.6336\n",
      "Epoch 8/10, Batch 44/883, Training Loss: 0.6950\n",
      "Epoch 8/10, Batch 45/883, Training Loss: 0.4783\n",
      "Epoch 8/10, Batch 46/883, Training Loss: 0.2934\n",
      "Epoch 8/10, Batch 47/883, Training Loss: 0.9001\n",
      "Epoch 8/10, Batch 48/883, Training Loss: 0.5993\n",
      "Epoch 8/10, Batch 49/883, Training Loss: 0.7804\n",
      "Epoch 8/10, Batch 50/883, Training Loss: 0.5826\n",
      "Epoch 8/10, Batch 51/883, Training Loss: 0.5344\n",
      "Epoch 8/10, Batch 52/883, Training Loss: 0.5563\n",
      "Epoch 8/10, Batch 53/883, Training Loss: 0.3984\n",
      "Epoch 8/10, Batch 54/883, Training Loss: 0.7137\n",
      "Epoch 8/10, Batch 55/883, Training Loss: 0.6785\n",
      "Epoch 8/10, Batch 56/883, Training Loss: 0.3474\n",
      "Epoch 8/10, Batch 57/883, Training Loss: 0.9447\n",
      "Epoch 8/10, Batch 58/883, Training Loss: 0.5214\n",
      "Epoch 8/10, Batch 59/883, Training Loss: 0.6066\n",
      "Epoch 8/10, Batch 60/883, Training Loss: 0.4315\n",
      "Epoch 8/10, Batch 61/883, Training Loss: 0.4773\n",
      "Epoch 8/10, Batch 62/883, Training Loss: 0.5344\n",
      "Epoch 8/10, Batch 63/883, Training Loss: 0.5462\n",
      "Epoch 8/10, Batch 64/883, Training Loss: 0.3490\n",
      "Epoch 8/10, Batch 65/883, Training Loss: 0.5992\n",
      "Epoch 8/10, Batch 66/883, Training Loss: 0.4940\n",
      "Epoch 8/10, Batch 67/883, Training Loss: 0.7792\n",
      "Epoch 8/10, Batch 68/883, Training Loss: 0.6491\n",
      "Epoch 8/10, Batch 69/883, Training Loss: 0.7515\n",
      "Epoch 8/10, Batch 70/883, Training Loss: 0.5813\n",
      "Epoch 8/10, Batch 71/883, Training Loss: 0.4370\n",
      "Epoch 8/10, Batch 72/883, Training Loss: 0.5107\n",
      "Epoch 8/10, Batch 73/883, Training Loss: 0.4833\n",
      "Epoch 8/10, Batch 74/883, Training Loss: 0.4191\n",
      "Epoch 8/10, Batch 75/883, Training Loss: 0.6254\n",
      "Epoch 8/10, Batch 76/883, Training Loss: 0.8726\n",
      "Epoch 8/10, Batch 77/883, Training Loss: 0.5645\n",
      "Epoch 8/10, Batch 78/883, Training Loss: 0.8615\n",
      "Epoch 8/10, Batch 79/883, Training Loss: 0.6132\n",
      "Epoch 8/10, Batch 80/883, Training Loss: 0.7724\n",
      "Epoch 8/10, Batch 81/883, Training Loss: 0.3659\n",
      "Epoch 8/10, Batch 82/883, Training Loss: 0.4647\n",
      "Epoch 8/10, Batch 83/883, Training Loss: 0.5564\n",
      "Epoch 8/10, Batch 84/883, Training Loss: 0.5690\n",
      "Epoch 8/10, Batch 85/883, Training Loss: 0.5200\n",
      "Epoch 8/10, Batch 86/883, Training Loss: 0.3497\n",
      "Epoch 8/10, Batch 87/883, Training Loss: 0.7194\n",
      "Epoch 8/10, Batch 88/883, Training Loss: 0.5395\n",
      "Epoch 8/10, Batch 89/883, Training Loss: 0.9728\n",
      "Epoch 8/10, Batch 90/883, Training Loss: 0.5707\n",
      "Epoch 8/10, Batch 91/883, Training Loss: 0.7940\n",
      "Epoch 8/10, Batch 92/883, Training Loss: 0.4929\n",
      "Epoch 8/10, Batch 93/883, Training Loss: 0.4320\n",
      "Epoch 8/10, Batch 94/883, Training Loss: 0.7025\n",
      "Epoch 8/10, Batch 95/883, Training Loss: 0.8220\n",
      "Epoch 8/10, Batch 96/883, Training Loss: 0.8587\n",
      "Epoch 8/10, Batch 97/883, Training Loss: 0.6844\n",
      "Epoch 8/10, Batch 98/883, Training Loss: 0.7452\n",
      "Epoch 8/10, Batch 99/883, Training Loss: 0.2490\n",
      "Epoch 8/10, Batch 100/883, Training Loss: 0.7417\n",
      "Epoch 8/10, Batch 101/883, Training Loss: 1.1072\n",
      "Epoch 8/10, Batch 102/883, Training Loss: 0.7564\n",
      "Epoch 8/10, Batch 103/883, Training Loss: 0.7308\n",
      "Epoch 8/10, Batch 104/883, Training Loss: 0.7500\n",
      "Epoch 8/10, Batch 105/883, Training Loss: 0.5741\n",
      "Epoch 8/10, Batch 106/883, Training Loss: 0.5627\n",
      "Epoch 8/10, Batch 107/883, Training Loss: 0.5550\n",
      "Epoch 8/10, Batch 108/883, Training Loss: 0.6015\n",
      "Epoch 8/10, Batch 109/883, Training Loss: 0.4897\n",
      "Epoch 8/10, Batch 110/883, Training Loss: 0.5905\n",
      "Epoch 8/10, Batch 111/883, Training Loss: 0.8925\n",
      "Epoch 8/10, Batch 112/883, Training Loss: 0.7023\n",
      "Epoch 8/10, Batch 113/883, Training Loss: 0.6697\n",
      "Epoch 8/10, Batch 114/883, Training Loss: 1.2376\n",
      "Epoch 8/10, Batch 115/883, Training Loss: 0.3786\n",
      "Epoch 8/10, Batch 116/883, Training Loss: 0.3497\n",
      "Epoch 8/10, Batch 117/883, Training Loss: 0.4867\n",
      "Epoch 8/10, Batch 118/883, Training Loss: 0.5951\n",
      "Epoch 8/10, Batch 119/883, Training Loss: 0.5353\n",
      "Epoch 8/10, Batch 120/883, Training Loss: 0.6192\n",
      "Epoch 8/10, Batch 121/883, Training Loss: 0.4723\n",
      "Epoch 8/10, Batch 122/883, Training Loss: 0.7093\n",
      "Epoch 8/10, Batch 123/883, Training Loss: 0.6479\n",
      "Epoch 8/10, Batch 124/883, Training Loss: 0.9667\n",
      "Epoch 8/10, Batch 125/883, Training Loss: 0.4194\n",
      "Epoch 8/10, Batch 126/883, Training Loss: 0.4855\n",
      "Epoch 8/10, Batch 127/883, Training Loss: 0.7322\n",
      "Epoch 8/10, Batch 128/883, Training Loss: 0.3468\n",
      "Epoch 8/10, Batch 129/883, Training Loss: 0.5847\n",
      "Epoch 8/10, Batch 130/883, Training Loss: 0.5442\n",
      "Epoch 8/10, Batch 131/883, Training Loss: 0.3983\n",
      "Epoch 8/10, Batch 132/883, Training Loss: 0.6164\n",
      "Epoch 8/10, Batch 133/883, Training Loss: 0.6431\n",
      "Epoch 8/10, Batch 134/883, Training Loss: 0.8097\n",
      "Epoch 8/10, Batch 135/883, Training Loss: 0.5044\n",
      "Epoch 8/10, Batch 136/883, Training Loss: 0.3790\n",
      "Epoch 8/10, Batch 137/883, Training Loss: 0.4676\n",
      "Epoch 8/10, Batch 138/883, Training Loss: 0.8604\n",
      "Epoch 8/10, Batch 139/883, Training Loss: 0.8456\n",
      "Epoch 8/10, Batch 140/883, Training Loss: 1.0040\n",
      "Epoch 8/10, Batch 141/883, Training Loss: 0.7053\n",
      "Epoch 8/10, Batch 142/883, Training Loss: 0.6999\n",
      "Epoch 8/10, Batch 143/883, Training Loss: 0.6074\n",
      "Epoch 8/10, Batch 144/883, Training Loss: 0.3353\n",
      "Epoch 8/10, Batch 145/883, Training Loss: 0.4548\n",
      "Epoch 8/10, Batch 146/883, Training Loss: 0.7284\n",
      "Epoch 8/10, Batch 147/883, Training Loss: 0.6416\n",
      "Epoch 8/10, Batch 148/883, Training Loss: 1.0383\n",
      "Epoch 8/10, Batch 149/883, Training Loss: 0.7208\n",
      "Epoch 8/10, Batch 150/883, Training Loss: 0.8568\n",
      "Epoch 8/10, Batch 151/883, Training Loss: 0.5970\n",
      "Epoch 8/10, Batch 152/883, Training Loss: 0.8465\n",
      "Epoch 8/10, Batch 153/883, Training Loss: 0.5198\n",
      "Epoch 8/10, Batch 154/883, Training Loss: 0.4627\n",
      "Epoch 8/10, Batch 155/883, Training Loss: 0.5732\n",
      "Epoch 8/10, Batch 156/883, Training Loss: 0.6149\n",
      "Epoch 8/10, Batch 157/883, Training Loss: 0.4550\n",
      "Epoch 8/10, Batch 158/883, Training Loss: 0.7223\n",
      "Epoch 8/10, Batch 159/883, Training Loss: 0.6399\n",
      "Epoch 8/10, Batch 160/883, Training Loss: 0.8403\n",
      "Epoch 8/10, Batch 161/883, Training Loss: 0.5712\n",
      "Epoch 8/10, Batch 162/883, Training Loss: 0.6080\n",
      "Epoch 8/10, Batch 163/883, Training Loss: 0.5616\n",
      "Epoch 8/10, Batch 164/883, Training Loss: 0.7064\n",
      "Epoch 8/10, Batch 165/883, Training Loss: 0.4716\n",
      "Epoch 8/10, Batch 166/883, Training Loss: 0.3749\n",
      "Epoch 8/10, Batch 167/883, Training Loss: 0.6894\n",
      "Epoch 8/10, Batch 168/883, Training Loss: 0.4395\n",
      "Epoch 8/10, Batch 169/883, Training Loss: 0.7713\n",
      "Epoch 8/10, Batch 170/883, Training Loss: 1.0596\n",
      "Epoch 8/10, Batch 171/883, Training Loss: 0.6149\n",
      "Epoch 8/10, Batch 172/883, Training Loss: 0.5121\n",
      "Epoch 8/10, Batch 173/883, Training Loss: 0.6460\n",
      "Epoch 8/10, Batch 174/883, Training Loss: 0.5708\n",
      "Epoch 8/10, Batch 175/883, Training Loss: 0.8072\n",
      "Epoch 8/10, Batch 176/883, Training Loss: 0.6749\n",
      "Epoch 8/10, Batch 177/883, Training Loss: 0.9042\n",
      "Epoch 8/10, Batch 178/883, Training Loss: 0.8130\n",
      "Epoch 8/10, Batch 179/883, Training Loss: 0.4147\n",
      "Epoch 8/10, Batch 180/883, Training Loss: 0.5401\n",
      "Epoch 8/10, Batch 181/883, Training Loss: 0.8454\n",
      "Epoch 8/10, Batch 182/883, Training Loss: 0.5731\n",
      "Epoch 8/10, Batch 183/883, Training Loss: 0.5077\n",
      "Epoch 8/10, Batch 184/883, Training Loss: 0.6899\n",
      "Epoch 8/10, Batch 185/883, Training Loss: 0.6243\n",
      "Epoch 8/10, Batch 186/883, Training Loss: 0.4324\n",
      "Epoch 8/10, Batch 187/883, Training Loss: 0.7521\n",
      "Epoch 8/10, Batch 188/883, Training Loss: 0.8463\n",
      "Epoch 8/10, Batch 189/883, Training Loss: 1.0025\n",
      "Epoch 8/10, Batch 190/883, Training Loss: 0.6241\n",
      "Epoch 8/10, Batch 191/883, Training Loss: 0.8162\n",
      "Epoch 8/10, Batch 192/883, Training Loss: 0.7630\n",
      "Epoch 8/10, Batch 193/883, Training Loss: 0.5156\n",
      "Epoch 8/10, Batch 194/883, Training Loss: 0.9138\n",
      "Epoch 8/10, Batch 195/883, Training Loss: 0.5619\n",
      "Epoch 8/10, Batch 196/883, Training Loss: 0.5985\n",
      "Epoch 8/10, Batch 197/883, Training Loss: 0.6679\n",
      "Epoch 8/10, Batch 198/883, Training Loss: 0.7192\n",
      "Epoch 8/10, Batch 199/883, Training Loss: 0.5039\n",
      "Epoch 8/10, Batch 200/883, Training Loss: 0.7508\n",
      "Epoch 8/10, Batch 201/883, Training Loss: 0.5864\n",
      "Epoch 8/10, Batch 202/883, Training Loss: 0.6729\n",
      "Epoch 8/10, Batch 203/883, Training Loss: 0.7469\n",
      "Epoch 8/10, Batch 204/883, Training Loss: 0.6414\n",
      "Epoch 8/10, Batch 205/883, Training Loss: 0.7870\n",
      "Epoch 8/10, Batch 206/883, Training Loss: 0.7550\n",
      "Epoch 8/10, Batch 207/883, Training Loss: 0.6211\n",
      "Epoch 8/10, Batch 208/883, Training Loss: 0.6465\n",
      "Epoch 8/10, Batch 209/883, Training Loss: 0.4393\n",
      "Epoch 8/10, Batch 210/883, Training Loss: 0.7407\n",
      "Epoch 8/10, Batch 211/883, Training Loss: 1.1063\n",
      "Epoch 8/10, Batch 212/883, Training Loss: 0.8680\n",
      "Epoch 8/10, Batch 213/883, Training Loss: 0.5987\n",
      "Epoch 8/10, Batch 214/883, Training Loss: 0.4889\n",
      "Epoch 8/10, Batch 215/883, Training Loss: 0.3989\n",
      "Epoch 8/10, Batch 216/883, Training Loss: 0.4227\n",
      "Epoch 8/10, Batch 217/883, Training Loss: 0.7391\n",
      "Epoch 8/10, Batch 218/883, Training Loss: 0.4578\n",
      "Epoch 8/10, Batch 219/883, Training Loss: 0.6482\n",
      "Epoch 8/10, Batch 220/883, Training Loss: 0.3876\n",
      "Epoch 8/10, Batch 221/883, Training Loss: 0.6990\n",
      "Epoch 8/10, Batch 222/883, Training Loss: 0.6685\n",
      "Epoch 8/10, Batch 223/883, Training Loss: 0.9340\n",
      "Epoch 8/10, Batch 224/883, Training Loss: 0.5229\n",
      "Epoch 8/10, Batch 225/883, Training Loss: 0.4705\n",
      "Epoch 8/10, Batch 226/883, Training Loss: 0.5621\n",
      "Epoch 8/10, Batch 227/883, Training Loss: 0.9832\n",
      "Epoch 8/10, Batch 228/883, Training Loss: 0.4056\n",
      "Epoch 8/10, Batch 229/883, Training Loss: 0.6719\n",
      "Epoch 8/10, Batch 230/883, Training Loss: 0.8211\n",
      "Epoch 8/10, Batch 231/883, Training Loss: 0.5785\n",
      "Epoch 8/10, Batch 232/883, Training Loss: 0.5899\n",
      "Epoch 8/10, Batch 233/883, Training Loss: 0.8257\n",
      "Epoch 8/10, Batch 234/883, Training Loss: 0.5693\n",
      "Epoch 8/10, Batch 235/883, Training Loss: 0.4046\n",
      "Epoch 8/10, Batch 236/883, Training Loss: 0.4022\n",
      "Epoch 8/10, Batch 237/883, Training Loss: 0.7009\n",
      "Epoch 8/10, Batch 238/883, Training Loss: 0.4932\n",
      "Epoch 8/10, Batch 239/883, Training Loss: 0.4787\n",
      "Epoch 8/10, Batch 240/883, Training Loss: 0.8192\n",
      "Epoch 8/10, Batch 241/883, Training Loss: 0.6108\n",
      "Epoch 8/10, Batch 242/883, Training Loss: 0.5642\n",
      "Epoch 8/10, Batch 243/883, Training Loss: 0.4063\n",
      "Epoch 8/10, Batch 244/883, Training Loss: 0.5798\n",
      "Epoch 8/10, Batch 245/883, Training Loss: 0.8509\n",
      "Epoch 8/10, Batch 246/883, Training Loss: 0.4357\n",
      "Epoch 8/10, Batch 247/883, Training Loss: 0.6226\n",
      "Epoch 8/10, Batch 248/883, Training Loss: 0.4937\n",
      "Epoch 8/10, Batch 249/883, Training Loss: 0.6732\n",
      "Epoch 8/10, Batch 250/883, Training Loss: 0.5466\n",
      "Epoch 8/10, Batch 251/883, Training Loss: 0.6139\n",
      "Epoch 8/10, Batch 252/883, Training Loss: 0.4679\n",
      "Epoch 8/10, Batch 253/883, Training Loss: 0.4080\n",
      "Epoch 8/10, Batch 254/883, Training Loss: 0.4776\n",
      "Epoch 8/10, Batch 255/883, Training Loss: 0.5371\n",
      "Epoch 8/10, Batch 256/883, Training Loss: 0.6038\n",
      "Epoch 8/10, Batch 257/883, Training Loss: 1.1765\n",
      "Epoch 8/10, Batch 258/883, Training Loss: 0.6159\n",
      "Epoch 8/10, Batch 259/883, Training Loss: 0.6913\n",
      "Epoch 8/10, Batch 260/883, Training Loss: 0.6731\n",
      "Epoch 8/10, Batch 261/883, Training Loss: 0.5656\n",
      "Epoch 8/10, Batch 262/883, Training Loss: 0.4154\n",
      "Epoch 8/10, Batch 263/883, Training Loss: 0.5057\n",
      "Epoch 8/10, Batch 264/883, Training Loss: 0.3751\n",
      "Epoch 8/10, Batch 265/883, Training Loss: 0.5074\n",
      "Epoch 8/10, Batch 266/883, Training Loss: 0.5386\n",
      "Epoch 8/10, Batch 267/883, Training Loss: 0.4776\n",
      "Epoch 8/10, Batch 268/883, Training Loss: 0.4333\n",
      "Epoch 8/10, Batch 269/883, Training Loss: 0.5305\n",
      "Epoch 8/10, Batch 270/883, Training Loss: 0.9453\n",
      "Epoch 8/10, Batch 271/883, Training Loss: 0.6416\n",
      "Epoch 8/10, Batch 272/883, Training Loss: 0.4515\n",
      "Epoch 8/10, Batch 273/883, Training Loss: 0.8611\n",
      "Epoch 8/10, Batch 274/883, Training Loss: 0.5741\n",
      "Epoch 8/10, Batch 275/883, Training Loss: 0.4534\n",
      "Epoch 8/10, Batch 276/883, Training Loss: 0.7461\n",
      "Epoch 8/10, Batch 277/883, Training Loss: 0.5863\n",
      "Epoch 8/10, Batch 278/883, Training Loss: 0.7946\n",
      "Epoch 8/10, Batch 279/883, Training Loss: 1.2891\n",
      "Epoch 8/10, Batch 280/883, Training Loss: 0.6360\n",
      "Epoch 8/10, Batch 281/883, Training Loss: 0.5635\n",
      "Epoch 8/10, Batch 282/883, Training Loss: 0.8299\n",
      "Epoch 8/10, Batch 283/883, Training Loss: 0.9073\n",
      "Epoch 8/10, Batch 284/883, Training Loss: 0.7742\n",
      "Epoch 8/10, Batch 285/883, Training Loss: 0.8427\n",
      "Epoch 8/10, Batch 286/883, Training Loss: 0.4392\n",
      "Epoch 8/10, Batch 287/883, Training Loss: 0.4775\n",
      "Epoch 8/10, Batch 288/883, Training Loss: 0.6558\n",
      "Epoch 8/10, Batch 289/883, Training Loss: 0.4753\n",
      "Epoch 8/10, Batch 290/883, Training Loss: 0.6226\n",
      "Epoch 8/10, Batch 291/883, Training Loss: 1.0972\n",
      "Epoch 8/10, Batch 292/883, Training Loss: 0.6575\n",
      "Epoch 8/10, Batch 293/883, Training Loss: 0.5461\n",
      "Epoch 8/10, Batch 294/883, Training Loss: 0.6484\n",
      "Epoch 8/10, Batch 295/883, Training Loss: 0.5067\n",
      "Epoch 8/10, Batch 296/883, Training Loss: 0.6964\n",
      "Epoch 8/10, Batch 297/883, Training Loss: 0.3536\n",
      "Epoch 8/10, Batch 298/883, Training Loss: 0.8060\n",
      "Epoch 8/10, Batch 299/883, Training Loss: 0.5337\n",
      "Epoch 8/10, Batch 300/883, Training Loss: 0.5370\n",
      "Epoch 8/10, Batch 301/883, Training Loss: 0.7823\n",
      "Epoch 8/10, Batch 302/883, Training Loss: 0.6203\n",
      "Epoch 8/10, Batch 303/883, Training Loss: 0.3470\n",
      "Epoch 8/10, Batch 304/883, Training Loss: 0.7771\n",
      "Epoch 8/10, Batch 305/883, Training Loss: 0.6373\n",
      "Epoch 8/10, Batch 306/883, Training Loss: 0.6429\n",
      "Epoch 8/10, Batch 307/883, Training Loss: 1.3558\n",
      "Epoch 8/10, Batch 308/883, Training Loss: 0.9607\n",
      "Epoch 8/10, Batch 309/883, Training Loss: 0.7110\n",
      "Epoch 8/10, Batch 310/883, Training Loss: 0.6393\n",
      "Epoch 8/10, Batch 311/883, Training Loss: 0.6005\n",
      "Epoch 8/10, Batch 312/883, Training Loss: 0.5841\n",
      "Epoch 8/10, Batch 313/883, Training Loss: 0.5198\n",
      "Epoch 8/10, Batch 314/883, Training Loss: 0.5583\n",
      "Epoch 8/10, Batch 315/883, Training Loss: 0.5640\n",
      "Epoch 8/10, Batch 316/883, Training Loss: 0.6548\n",
      "Epoch 8/10, Batch 317/883, Training Loss: 0.5005\n",
      "Epoch 8/10, Batch 318/883, Training Loss: 0.5327\n",
      "Epoch 8/10, Batch 319/883, Training Loss: 0.7280\n",
      "Epoch 8/10, Batch 320/883, Training Loss: 0.4729\n",
      "Epoch 8/10, Batch 321/883, Training Loss: 0.8610\n",
      "Epoch 8/10, Batch 322/883, Training Loss: 0.4412\n",
      "Epoch 8/10, Batch 323/883, Training Loss: 0.4928\n",
      "Epoch 8/10, Batch 324/883, Training Loss: 0.8039\n",
      "Epoch 8/10, Batch 325/883, Training Loss: 0.7379\n",
      "Epoch 8/10, Batch 326/883, Training Loss: 0.5302\n",
      "Epoch 8/10, Batch 327/883, Training Loss: 0.7094\n",
      "Epoch 8/10, Batch 328/883, Training Loss: 0.4087\n",
      "Epoch 8/10, Batch 329/883, Training Loss: 0.3873\n",
      "Epoch 8/10, Batch 330/883, Training Loss: 0.6289\n",
      "Epoch 8/10, Batch 331/883, Training Loss: 0.7567\n",
      "Epoch 8/10, Batch 332/883, Training Loss: 0.4630\n",
      "Epoch 8/10, Batch 333/883, Training Loss: 0.6062\n",
      "Epoch 8/10, Batch 334/883, Training Loss: 0.6263\n",
      "Epoch 8/10, Batch 335/883, Training Loss: 0.5811\n",
      "Epoch 8/10, Batch 336/883, Training Loss: 0.6736\n",
      "Epoch 8/10, Batch 337/883, Training Loss: 0.9110\n",
      "Epoch 8/10, Batch 338/883, Training Loss: 0.7505\n",
      "Epoch 8/10, Batch 339/883, Training Loss: 0.5299\n",
      "Epoch 8/10, Batch 340/883, Training Loss: 0.4403\n",
      "Epoch 8/10, Batch 341/883, Training Loss: 0.8745\n",
      "Epoch 8/10, Batch 342/883, Training Loss: 0.4934\n",
      "Epoch 8/10, Batch 343/883, Training Loss: 0.5960\n",
      "Epoch 8/10, Batch 344/883, Training Loss: 0.6154\n",
      "Epoch 8/10, Batch 345/883, Training Loss: 0.6798\n",
      "Epoch 8/10, Batch 346/883, Training Loss: 0.5578\n",
      "Epoch 8/10, Batch 347/883, Training Loss: 0.5424\n",
      "Epoch 8/10, Batch 348/883, Training Loss: 0.5735\n",
      "Epoch 8/10, Batch 349/883, Training Loss: 0.5636\n",
      "Epoch 8/10, Batch 350/883, Training Loss: 0.6575\n",
      "Epoch 8/10, Batch 351/883, Training Loss: 0.8312\n",
      "Epoch 8/10, Batch 352/883, Training Loss: 0.5166\n",
      "Epoch 8/10, Batch 353/883, Training Loss: 0.8204\n",
      "Epoch 8/10, Batch 354/883, Training Loss: 0.4719\n",
      "Epoch 8/10, Batch 355/883, Training Loss: 0.8242\n",
      "Epoch 8/10, Batch 356/883, Training Loss: 0.5143\n",
      "Epoch 8/10, Batch 357/883, Training Loss: 0.8558\n",
      "Epoch 8/10, Batch 358/883, Training Loss: 0.8083\n",
      "Epoch 8/10, Batch 359/883, Training Loss: 0.7233\n",
      "Epoch 8/10, Batch 360/883, Training Loss: 0.8934\n",
      "Epoch 8/10, Batch 361/883, Training Loss: 0.5603\n",
      "Epoch 8/10, Batch 362/883, Training Loss: 0.9711\n",
      "Epoch 8/10, Batch 363/883, Training Loss: 0.4415\n",
      "Epoch 8/10, Batch 364/883, Training Loss: 0.5808\n",
      "Epoch 8/10, Batch 365/883, Training Loss: 0.5125\n",
      "Epoch 8/10, Batch 366/883, Training Loss: 0.9227\n",
      "Epoch 8/10, Batch 367/883, Training Loss: 0.4969\n",
      "Epoch 8/10, Batch 368/883, Training Loss: 0.5769\n",
      "Epoch 8/10, Batch 369/883, Training Loss: 0.5732\n",
      "Epoch 8/10, Batch 370/883, Training Loss: 0.5345\n",
      "Epoch 8/10, Batch 371/883, Training Loss: 0.5517\n",
      "Epoch 8/10, Batch 372/883, Training Loss: 0.5864\n",
      "Epoch 8/10, Batch 373/883, Training Loss: 0.6257\n",
      "Epoch 8/10, Batch 374/883, Training Loss: 0.5007\n",
      "Epoch 8/10, Batch 375/883, Training Loss: 0.5168\n",
      "Epoch 8/10, Batch 376/883, Training Loss: 0.5261\n",
      "Epoch 8/10, Batch 377/883, Training Loss: 0.7334\n",
      "Epoch 8/10, Batch 378/883, Training Loss: 0.7359\n",
      "Epoch 8/10, Batch 379/883, Training Loss: 0.4215\n",
      "Epoch 8/10, Batch 380/883, Training Loss: 0.9341\n",
      "Epoch 8/10, Batch 381/883, Training Loss: 0.6007\n",
      "Epoch 8/10, Batch 382/883, Training Loss: 0.7559\n",
      "Epoch 8/10, Batch 383/883, Training Loss: 0.7967\n",
      "Epoch 8/10, Batch 384/883, Training Loss: 0.5208\n",
      "Epoch 8/10, Batch 385/883, Training Loss: 0.8102\n",
      "Epoch 8/10, Batch 386/883, Training Loss: 0.6227\n",
      "Epoch 8/10, Batch 387/883, Training Loss: 0.7828\n",
      "Epoch 8/10, Batch 388/883, Training Loss: 0.8509\n",
      "Epoch 8/10, Batch 389/883, Training Loss: 0.8284\n",
      "Epoch 8/10, Batch 390/883, Training Loss: 0.5638\n",
      "Epoch 8/10, Batch 391/883, Training Loss: 0.6974\n",
      "Epoch 8/10, Batch 392/883, Training Loss: 0.5356\n",
      "Epoch 8/10, Batch 393/883, Training Loss: 0.6970\n",
      "Epoch 8/10, Batch 394/883, Training Loss: 1.1034\n",
      "Epoch 8/10, Batch 395/883, Training Loss: 1.1235\n",
      "Epoch 8/10, Batch 396/883, Training Loss: 0.5249\n",
      "Epoch 8/10, Batch 397/883, Training Loss: 0.8212\n",
      "Epoch 8/10, Batch 398/883, Training Loss: 0.4979\n",
      "Epoch 8/10, Batch 399/883, Training Loss: 0.8304\n",
      "Epoch 8/10, Batch 400/883, Training Loss: 0.7778\n",
      "Epoch 8/10, Batch 401/883, Training Loss: 0.4467\n",
      "Epoch 8/10, Batch 402/883, Training Loss: 0.8064\n",
      "Epoch 8/10, Batch 403/883, Training Loss: 0.5697\n",
      "Epoch 8/10, Batch 404/883, Training Loss: 0.4795\n",
      "Epoch 8/10, Batch 405/883, Training Loss: 0.5402\n",
      "Epoch 8/10, Batch 406/883, Training Loss: 0.4173\n",
      "Epoch 8/10, Batch 407/883, Training Loss: 0.6546\n",
      "Epoch 8/10, Batch 408/883, Training Loss: 0.4580\n",
      "Epoch 8/10, Batch 409/883, Training Loss: 0.4770\n",
      "Epoch 8/10, Batch 410/883, Training Loss: 0.8387\n",
      "Epoch 8/10, Batch 411/883, Training Loss: 0.7347\n",
      "Epoch 8/10, Batch 412/883, Training Loss: 0.7182\n",
      "Epoch 8/10, Batch 413/883, Training Loss: 0.5019\n",
      "Epoch 8/10, Batch 414/883, Training Loss: 0.5772\n",
      "Epoch 8/10, Batch 415/883, Training Loss: 0.5849\n",
      "Epoch 8/10, Batch 416/883, Training Loss: 0.4670\n",
      "Epoch 8/10, Batch 417/883, Training Loss: 0.5348\n",
      "Epoch 8/10, Batch 418/883, Training Loss: 0.6574\n",
      "Epoch 8/10, Batch 419/883, Training Loss: 0.7200\n",
      "Epoch 8/10, Batch 420/883, Training Loss: 0.5006\n",
      "Epoch 8/10, Batch 421/883, Training Loss: 0.4382\n",
      "Epoch 8/10, Batch 422/883, Training Loss: 0.7326\n",
      "Epoch 8/10, Batch 423/883, Training Loss: 0.7122\n",
      "Epoch 8/10, Batch 424/883, Training Loss: 0.4263\n",
      "Epoch 8/10, Batch 425/883, Training Loss: 0.3635\n",
      "Epoch 8/10, Batch 426/883, Training Loss: 0.6087\n",
      "Epoch 8/10, Batch 427/883, Training Loss: 0.4023\n",
      "Epoch 8/10, Batch 428/883, Training Loss: 0.5700\n",
      "Epoch 8/10, Batch 429/883, Training Loss: 0.3326\n",
      "Epoch 8/10, Batch 430/883, Training Loss: 0.5425\n",
      "Epoch 8/10, Batch 431/883, Training Loss: 0.7539\n",
      "Epoch 8/10, Batch 432/883, Training Loss: 0.4810\n",
      "Epoch 8/10, Batch 433/883, Training Loss: 0.4011\n",
      "Epoch 8/10, Batch 434/883, Training Loss: 0.5145\n",
      "Epoch 8/10, Batch 435/883, Training Loss: 0.5621\n",
      "Epoch 8/10, Batch 436/883, Training Loss: 0.5254\n",
      "Epoch 8/10, Batch 437/883, Training Loss: 0.4871\n",
      "Epoch 8/10, Batch 438/883, Training Loss: 0.5769\n",
      "Epoch 8/10, Batch 439/883, Training Loss: 0.7209\n",
      "Epoch 8/10, Batch 440/883, Training Loss: 0.7381\n",
      "Epoch 8/10, Batch 441/883, Training Loss: 0.7057\n",
      "Epoch 8/10, Batch 442/883, Training Loss: 0.4153\n",
      "Epoch 8/10, Batch 443/883, Training Loss: 0.4922\n",
      "Epoch 8/10, Batch 444/883, Training Loss: 0.7548\n",
      "Epoch 8/10, Batch 445/883, Training Loss: 0.6038\n",
      "Epoch 8/10, Batch 446/883, Training Loss: 0.7416\n",
      "Epoch 8/10, Batch 447/883, Training Loss: 1.0415\n",
      "Epoch 8/10, Batch 448/883, Training Loss: 0.4867\n",
      "Epoch 8/10, Batch 449/883, Training Loss: 0.6334\n",
      "Epoch 8/10, Batch 450/883, Training Loss: 0.3430\n",
      "Epoch 8/10, Batch 451/883, Training Loss: 0.9821\n",
      "Epoch 8/10, Batch 452/883, Training Loss: 0.6826\n",
      "Epoch 8/10, Batch 453/883, Training Loss: 0.6388\n",
      "Epoch 8/10, Batch 454/883, Training Loss: 0.5754\n",
      "Epoch 8/10, Batch 455/883, Training Loss: 0.5346\n",
      "Epoch 8/10, Batch 456/883, Training Loss: 0.7194\n",
      "Epoch 8/10, Batch 457/883, Training Loss: 0.5442\n",
      "Epoch 8/10, Batch 458/883, Training Loss: 0.9135\n",
      "Epoch 8/10, Batch 459/883, Training Loss: 0.3945\n",
      "Epoch 8/10, Batch 460/883, Training Loss: 0.7000\n",
      "Epoch 8/10, Batch 461/883, Training Loss: 0.5187\n",
      "Epoch 8/10, Batch 462/883, Training Loss: 0.3915\n",
      "Epoch 8/10, Batch 463/883, Training Loss: 0.5571\n",
      "Epoch 8/10, Batch 464/883, Training Loss: 0.5695\n",
      "Epoch 8/10, Batch 465/883, Training Loss: 0.8100\n",
      "Epoch 8/10, Batch 466/883, Training Loss: 0.5724\n",
      "Epoch 8/10, Batch 467/883, Training Loss: 0.4407\n",
      "Epoch 8/10, Batch 468/883, Training Loss: 0.4869\n",
      "Epoch 8/10, Batch 469/883, Training Loss: 0.3263\n",
      "Epoch 8/10, Batch 470/883, Training Loss: 0.3524\n",
      "Epoch 8/10, Batch 471/883, Training Loss: 0.4705\n",
      "Epoch 8/10, Batch 472/883, Training Loss: 0.3940\n",
      "Epoch 8/10, Batch 473/883, Training Loss: 0.6408\n",
      "Epoch 8/10, Batch 474/883, Training Loss: 0.4221\n",
      "Epoch 8/10, Batch 475/883, Training Loss: 0.5755\n",
      "Epoch 8/10, Batch 476/883, Training Loss: 0.8151\n",
      "Epoch 8/10, Batch 477/883, Training Loss: 0.5793\n",
      "Epoch 8/10, Batch 478/883, Training Loss: 0.7727\n",
      "Epoch 8/10, Batch 479/883, Training Loss: 0.6768\n",
      "Epoch 8/10, Batch 480/883, Training Loss: 0.4195\n",
      "Epoch 8/10, Batch 481/883, Training Loss: 0.5255\n",
      "Epoch 8/10, Batch 482/883, Training Loss: 0.5199\n",
      "Epoch 8/10, Batch 483/883, Training Loss: 0.6208\n",
      "Epoch 8/10, Batch 484/883, Training Loss: 0.4922\n",
      "Epoch 8/10, Batch 485/883, Training Loss: 0.4425\n",
      "Epoch 8/10, Batch 486/883, Training Loss: 0.6914\n",
      "Epoch 8/10, Batch 487/883, Training Loss: 0.5042\n",
      "Epoch 8/10, Batch 488/883, Training Loss: 0.8411\n",
      "Epoch 8/10, Batch 489/883, Training Loss: 0.6793\n",
      "Epoch 8/10, Batch 490/883, Training Loss: 0.3660\n",
      "Epoch 8/10, Batch 491/883, Training Loss: 0.4537\n",
      "Epoch 8/10, Batch 492/883, Training Loss: 0.9202\n",
      "Epoch 8/10, Batch 493/883, Training Loss: 0.6624\n",
      "Epoch 8/10, Batch 494/883, Training Loss: 0.7476\n",
      "Epoch 8/10, Batch 495/883, Training Loss: 0.4115\n",
      "Epoch 8/10, Batch 496/883, Training Loss: 0.9214\n",
      "Epoch 8/10, Batch 497/883, Training Loss: 0.3216\n",
      "Epoch 8/10, Batch 498/883, Training Loss: 0.4311\n",
      "Epoch 8/10, Batch 499/883, Training Loss: 0.6975\n",
      "Epoch 8/10, Batch 500/883, Training Loss: 0.7093\n",
      "Epoch 8/10, Batch 501/883, Training Loss: 0.8306\n",
      "Epoch 8/10, Batch 502/883, Training Loss: 0.9133\n",
      "Epoch 8/10, Batch 503/883, Training Loss: 0.3127\n",
      "Epoch 8/10, Batch 504/883, Training Loss: 0.5563\n",
      "Epoch 8/10, Batch 505/883, Training Loss: 0.9532\n",
      "Epoch 8/10, Batch 506/883, Training Loss: 1.0622\n",
      "Epoch 8/10, Batch 507/883, Training Loss: 0.6016\n",
      "Epoch 8/10, Batch 508/883, Training Loss: 0.6294\n",
      "Epoch 8/10, Batch 509/883, Training Loss: 0.7084\n",
      "Epoch 8/10, Batch 510/883, Training Loss: 0.7872\n",
      "Epoch 8/10, Batch 511/883, Training Loss: 0.4918\n",
      "Epoch 8/10, Batch 512/883, Training Loss: 0.5212\n",
      "Epoch 8/10, Batch 513/883, Training Loss: 0.7004\n",
      "Epoch 8/10, Batch 514/883, Training Loss: 0.4491\n",
      "Epoch 8/10, Batch 515/883, Training Loss: 0.6876\n",
      "Epoch 8/10, Batch 516/883, Training Loss: 0.6088\n",
      "Epoch 8/10, Batch 517/883, Training Loss: 0.7093\n",
      "Epoch 8/10, Batch 518/883, Training Loss: 0.4142\n",
      "Epoch 8/10, Batch 519/883, Training Loss: 0.5701\n",
      "Epoch 8/10, Batch 520/883, Training Loss: 0.4639\n",
      "Epoch 8/10, Batch 521/883, Training Loss: 0.7893\n",
      "Epoch 8/10, Batch 522/883, Training Loss: 0.9345\n",
      "Epoch 8/10, Batch 523/883, Training Loss: 0.6316\n",
      "Epoch 8/10, Batch 524/883, Training Loss: 0.5210\n",
      "Epoch 8/10, Batch 525/883, Training Loss: 0.7301\n",
      "Epoch 8/10, Batch 526/883, Training Loss: 0.5747\n",
      "Epoch 8/10, Batch 527/883, Training Loss: 0.6245\n",
      "Epoch 8/10, Batch 528/883, Training Loss: 0.3894\n",
      "Epoch 8/10, Batch 529/883, Training Loss: 0.6815\n",
      "Epoch 8/10, Batch 530/883, Training Loss: 0.4890\n",
      "Epoch 8/10, Batch 531/883, Training Loss: 0.4563\n",
      "Epoch 8/10, Batch 532/883, Training Loss: 0.8808\n",
      "Epoch 8/10, Batch 533/883, Training Loss: 0.4263\n",
      "Epoch 8/10, Batch 534/883, Training Loss: 0.4808\n",
      "Epoch 8/10, Batch 535/883, Training Loss: 0.6664\n",
      "Epoch 8/10, Batch 536/883, Training Loss: 0.5193\n",
      "Epoch 8/10, Batch 537/883, Training Loss: 0.8350\n",
      "Epoch 8/10, Batch 538/883, Training Loss: 0.9055\n",
      "Epoch 8/10, Batch 539/883, Training Loss: 0.5057\n",
      "Epoch 8/10, Batch 540/883, Training Loss: 0.3458\n",
      "Epoch 8/10, Batch 541/883, Training Loss: 0.5620\n",
      "Epoch 8/10, Batch 542/883, Training Loss: 0.5661\n",
      "Epoch 8/10, Batch 543/883, Training Loss: 0.7272\n",
      "Epoch 8/10, Batch 544/883, Training Loss: 0.8823\n",
      "Epoch 8/10, Batch 545/883, Training Loss: 0.5422\n",
      "Epoch 8/10, Batch 546/883, Training Loss: 0.5939\n",
      "Epoch 8/10, Batch 547/883, Training Loss: 0.5979\n",
      "Epoch 8/10, Batch 548/883, Training Loss: 0.7317\n",
      "Epoch 8/10, Batch 549/883, Training Loss: 0.6159\n",
      "Epoch 8/10, Batch 550/883, Training Loss: 0.9621\n",
      "Epoch 8/10, Batch 551/883, Training Loss: 0.4618\n",
      "Epoch 8/10, Batch 552/883, Training Loss: 0.5032\n",
      "Epoch 8/10, Batch 553/883, Training Loss: 0.7463\n",
      "Epoch 8/10, Batch 554/883, Training Loss: 0.9055\n",
      "Epoch 8/10, Batch 555/883, Training Loss: 0.6106\n",
      "Epoch 8/10, Batch 556/883, Training Loss: 0.5156\n",
      "Epoch 8/10, Batch 557/883, Training Loss: 0.6312\n",
      "Epoch 8/10, Batch 558/883, Training Loss: 0.6589\n",
      "Epoch 8/10, Batch 559/883, Training Loss: 0.7651\n",
      "Epoch 8/10, Batch 560/883, Training Loss: 0.7108\n",
      "Epoch 8/10, Batch 561/883, Training Loss: 0.3695\n",
      "Epoch 8/10, Batch 562/883, Training Loss: 0.4216\n",
      "Epoch 8/10, Batch 563/883, Training Loss: 0.6492\n",
      "Epoch 8/10, Batch 564/883, Training Loss: 0.6584\n",
      "Epoch 8/10, Batch 565/883, Training Loss: 0.7705\n",
      "Epoch 8/10, Batch 566/883, Training Loss: 0.6228\n",
      "Epoch 8/10, Batch 567/883, Training Loss: 0.5178\n",
      "Epoch 8/10, Batch 568/883, Training Loss: 0.6355\n",
      "Epoch 8/10, Batch 569/883, Training Loss: 0.7209\n",
      "Epoch 8/10, Batch 570/883, Training Loss: 0.9711\n",
      "Epoch 8/10, Batch 571/883, Training Loss: 0.3770\n",
      "Epoch 8/10, Batch 572/883, Training Loss: 0.4182\n",
      "Epoch 8/10, Batch 573/883, Training Loss: 0.7205\n",
      "Epoch 8/10, Batch 574/883, Training Loss: 0.8394\n",
      "Epoch 8/10, Batch 575/883, Training Loss: 0.5106\n",
      "Epoch 8/10, Batch 576/883, Training Loss: 0.5625\n",
      "Epoch 8/10, Batch 577/883, Training Loss: 0.7922\n",
      "Epoch 8/10, Batch 578/883, Training Loss: 0.4829\n",
      "Epoch 8/10, Batch 579/883, Training Loss: 0.3891\n",
      "Epoch 8/10, Batch 580/883, Training Loss: 0.7200\n",
      "Epoch 8/10, Batch 581/883, Training Loss: 0.3620\n",
      "Epoch 8/10, Batch 582/883, Training Loss: 0.9026\n",
      "Epoch 8/10, Batch 583/883, Training Loss: 0.3737\n",
      "Epoch 8/10, Batch 584/883, Training Loss: 0.5389\n",
      "Epoch 8/10, Batch 585/883, Training Loss: 1.0831\n",
      "Epoch 8/10, Batch 586/883, Training Loss: 0.4083\n",
      "Epoch 8/10, Batch 587/883, Training Loss: 0.5873\n",
      "Epoch 8/10, Batch 588/883, Training Loss: 1.1269\n",
      "Epoch 8/10, Batch 589/883, Training Loss: 0.5542\n",
      "Epoch 8/10, Batch 590/883, Training Loss: 0.7091\n",
      "Epoch 8/10, Batch 591/883, Training Loss: 0.6375\n",
      "Epoch 8/10, Batch 592/883, Training Loss: 0.4847\n",
      "Epoch 8/10, Batch 593/883, Training Loss: 0.5663\n",
      "Epoch 8/10, Batch 594/883, Training Loss: 0.7498\n",
      "Epoch 8/10, Batch 595/883, Training Loss: 0.5344\n",
      "Epoch 8/10, Batch 596/883, Training Loss: 1.0738\n",
      "Epoch 8/10, Batch 597/883, Training Loss: 0.6049\n",
      "Epoch 8/10, Batch 598/883, Training Loss: 0.4238\n",
      "Epoch 8/10, Batch 599/883, Training Loss: 0.6425\n",
      "Epoch 8/10, Batch 600/883, Training Loss: 0.3667\n",
      "Epoch 8/10, Batch 601/883, Training Loss: 0.7045\n",
      "Epoch 8/10, Batch 602/883, Training Loss: 0.5935\n",
      "Epoch 8/10, Batch 603/883, Training Loss: 0.3385\n",
      "Epoch 8/10, Batch 604/883, Training Loss: 0.7698\n",
      "Epoch 8/10, Batch 605/883, Training Loss: 0.4978\n",
      "Epoch 8/10, Batch 606/883, Training Loss: 0.6150\n",
      "Epoch 8/10, Batch 607/883, Training Loss: 0.5106\n",
      "Epoch 8/10, Batch 608/883, Training Loss: 0.7807\n",
      "Epoch 8/10, Batch 609/883, Training Loss: 0.9459\n",
      "Epoch 8/10, Batch 610/883, Training Loss: 0.9315\n",
      "Epoch 8/10, Batch 611/883, Training Loss: 0.6432\n",
      "Epoch 8/10, Batch 612/883, Training Loss: 0.4957\n",
      "Epoch 8/10, Batch 613/883, Training Loss: 0.5093\n",
      "Epoch 8/10, Batch 614/883, Training Loss: 0.6985\n",
      "Epoch 8/10, Batch 615/883, Training Loss: 0.7075\n",
      "Epoch 8/10, Batch 616/883, Training Loss: 0.6764\n",
      "Epoch 8/10, Batch 617/883, Training Loss: 0.4570\n",
      "Epoch 8/10, Batch 618/883, Training Loss: 0.6387\n",
      "Epoch 8/10, Batch 619/883, Training Loss: 0.6936\n",
      "Epoch 8/10, Batch 620/883, Training Loss: 0.4844\n",
      "Epoch 8/10, Batch 621/883, Training Loss: 0.3867\n",
      "Epoch 8/10, Batch 622/883, Training Loss: 0.5630\n",
      "Epoch 8/10, Batch 623/883, Training Loss: 0.5497\n",
      "Epoch 8/10, Batch 624/883, Training Loss: 0.7337\n",
      "Epoch 8/10, Batch 625/883, Training Loss: 0.5591\n",
      "Epoch 8/10, Batch 626/883, Training Loss: 0.6934\n",
      "Epoch 8/10, Batch 627/883, Training Loss: 0.5785\n",
      "Epoch 8/10, Batch 628/883, Training Loss: 0.7851\n",
      "Epoch 8/10, Batch 629/883, Training Loss: 0.4601\n",
      "Epoch 8/10, Batch 630/883, Training Loss: 0.4639\n",
      "Epoch 8/10, Batch 631/883, Training Loss: 0.5719\n",
      "Epoch 8/10, Batch 632/883, Training Loss: 0.9306\n",
      "Epoch 8/10, Batch 633/883, Training Loss: 0.4741\n",
      "Epoch 8/10, Batch 634/883, Training Loss: 0.4474\n",
      "Epoch 8/10, Batch 635/883, Training Loss: 0.6243\n",
      "Epoch 8/10, Batch 636/883, Training Loss: 0.5260\n",
      "Epoch 8/10, Batch 637/883, Training Loss: 0.3419\n",
      "Epoch 8/10, Batch 638/883, Training Loss: 0.9691\n",
      "Epoch 8/10, Batch 639/883, Training Loss: 0.4250\n",
      "Epoch 8/10, Batch 640/883, Training Loss: 0.6459\n",
      "Epoch 8/10, Batch 641/883, Training Loss: 0.8513\n",
      "Epoch 8/10, Batch 642/883, Training Loss: 0.4878\n",
      "Epoch 8/10, Batch 643/883, Training Loss: 0.7005\n",
      "Epoch 8/10, Batch 644/883, Training Loss: 0.4180\n",
      "Epoch 8/10, Batch 645/883, Training Loss: 0.3970\n",
      "Epoch 8/10, Batch 646/883, Training Loss: 0.6396\n",
      "Epoch 8/10, Batch 647/883, Training Loss: 0.6438\n",
      "Epoch 8/10, Batch 648/883, Training Loss: 0.5833\n",
      "Epoch 8/10, Batch 649/883, Training Loss: 0.4784\n",
      "Epoch 8/10, Batch 650/883, Training Loss: 0.5032\n",
      "Epoch 8/10, Batch 651/883, Training Loss: 1.0577\n",
      "Epoch 8/10, Batch 652/883, Training Loss: 0.6997\n",
      "Epoch 8/10, Batch 653/883, Training Loss: 0.5303\n",
      "Epoch 8/10, Batch 654/883, Training Loss: 0.5261\n",
      "Epoch 8/10, Batch 655/883, Training Loss: 0.3909\n",
      "Epoch 8/10, Batch 656/883, Training Loss: 0.6197\n",
      "Epoch 8/10, Batch 657/883, Training Loss: 0.5246\n",
      "Epoch 8/10, Batch 658/883, Training Loss: 0.4334\n",
      "Epoch 8/10, Batch 659/883, Training Loss: 0.6814\n",
      "Epoch 8/10, Batch 660/883, Training Loss: 0.5452\n",
      "Epoch 8/10, Batch 661/883, Training Loss: 0.5499\n",
      "Epoch 8/10, Batch 662/883, Training Loss: 0.5173\n",
      "Epoch 8/10, Batch 663/883, Training Loss: 0.9527\n",
      "Epoch 8/10, Batch 664/883, Training Loss: 0.4140\n",
      "Epoch 8/10, Batch 665/883, Training Loss: 0.7168\n",
      "Epoch 8/10, Batch 666/883, Training Loss: 0.5448\n",
      "Epoch 8/10, Batch 667/883, Training Loss: 0.8996\n",
      "Epoch 8/10, Batch 668/883, Training Loss: 0.5629\n",
      "Epoch 8/10, Batch 669/883, Training Loss: 0.6296\n",
      "Epoch 8/10, Batch 670/883, Training Loss: 0.8436\n",
      "Epoch 8/10, Batch 671/883, Training Loss: 0.8372\n",
      "Epoch 8/10, Batch 672/883, Training Loss: 0.5185\n",
      "Epoch 8/10, Batch 673/883, Training Loss: 0.6466\n",
      "Epoch 8/10, Batch 674/883, Training Loss: 0.4783\n",
      "Epoch 8/10, Batch 675/883, Training Loss: 0.4550\n",
      "Epoch 8/10, Batch 676/883, Training Loss: 0.6669\n",
      "Epoch 8/10, Batch 677/883, Training Loss: 0.5865\n",
      "Epoch 8/10, Batch 678/883, Training Loss: 0.6194\n",
      "Epoch 8/10, Batch 679/883, Training Loss: 0.3715\n",
      "Epoch 8/10, Batch 680/883, Training Loss: 0.6135\n",
      "Epoch 8/10, Batch 681/883, Training Loss: 0.6298\n",
      "Epoch 8/10, Batch 682/883, Training Loss: 0.4390\n",
      "Epoch 8/10, Batch 683/883, Training Loss: 0.6975\n",
      "Epoch 8/10, Batch 684/883, Training Loss: 0.5652\n",
      "Epoch 8/10, Batch 685/883, Training Loss: 0.8225\n",
      "Epoch 8/10, Batch 686/883, Training Loss: 0.5667\n",
      "Epoch 8/10, Batch 687/883, Training Loss: 0.4684\n",
      "Epoch 8/10, Batch 688/883, Training Loss: 0.4119\n",
      "Epoch 8/10, Batch 689/883, Training Loss: 0.5960\n",
      "Epoch 8/10, Batch 690/883, Training Loss: 0.4953\n",
      "Epoch 8/10, Batch 691/883, Training Loss: 0.5960\n",
      "Epoch 8/10, Batch 692/883, Training Loss: 0.4645\n",
      "Epoch 8/10, Batch 693/883, Training Loss: 0.6222\n",
      "Epoch 8/10, Batch 694/883, Training Loss: 0.6810\n",
      "Epoch 8/10, Batch 695/883, Training Loss: 0.4755\n",
      "Epoch 8/10, Batch 696/883, Training Loss: 0.4168\n",
      "Epoch 8/10, Batch 697/883, Training Loss: 0.7401\n",
      "Epoch 8/10, Batch 698/883, Training Loss: 0.4999\n",
      "Epoch 8/10, Batch 699/883, Training Loss: 0.7527\n",
      "Epoch 8/10, Batch 700/883, Training Loss: 0.4386\n",
      "Epoch 8/10, Batch 701/883, Training Loss: 0.5603\n",
      "Epoch 8/10, Batch 702/883, Training Loss: 0.4783\n",
      "Epoch 8/10, Batch 703/883, Training Loss: 0.7645\n",
      "Epoch 8/10, Batch 704/883, Training Loss: 0.6206\n",
      "Epoch 8/10, Batch 705/883, Training Loss: 0.6471\n",
      "Epoch 8/10, Batch 706/883, Training Loss: 0.6547\n",
      "Epoch 8/10, Batch 707/883, Training Loss: 0.3856\n",
      "Epoch 8/10, Batch 708/883, Training Loss: 0.3452\n",
      "Epoch 8/10, Batch 709/883, Training Loss: 0.9233\n",
      "Epoch 8/10, Batch 710/883, Training Loss: 0.5455\n",
      "Epoch 8/10, Batch 711/883, Training Loss: 0.5263\n",
      "Epoch 8/10, Batch 712/883, Training Loss: 0.6693\n",
      "Epoch 8/10, Batch 713/883, Training Loss: 0.6018\n",
      "Epoch 8/10, Batch 714/883, Training Loss: 0.8224\n",
      "Epoch 8/10, Batch 715/883, Training Loss: 0.4845\n",
      "Epoch 8/10, Batch 716/883, Training Loss: 1.0287\n",
      "Epoch 8/10, Batch 717/883, Training Loss: 0.7646\n",
      "Epoch 8/10, Batch 718/883, Training Loss: 0.3188\n",
      "Epoch 8/10, Batch 719/883, Training Loss: 0.4157\n",
      "Epoch 8/10, Batch 720/883, Training Loss: 0.5724\n",
      "Epoch 8/10, Batch 721/883, Training Loss: 0.8527\n",
      "Epoch 8/10, Batch 722/883, Training Loss: 0.5150\n",
      "Epoch 8/10, Batch 723/883, Training Loss: 0.4855\n",
      "Epoch 8/10, Batch 724/883, Training Loss: 0.5893\n",
      "Epoch 8/10, Batch 725/883, Training Loss: 0.9386\n",
      "Epoch 8/10, Batch 726/883, Training Loss: 0.6774\n",
      "Epoch 8/10, Batch 727/883, Training Loss: 0.9418\n",
      "Epoch 8/10, Batch 728/883, Training Loss: 1.0740\n",
      "Epoch 8/10, Batch 729/883, Training Loss: 0.8016\n",
      "Epoch 8/10, Batch 730/883, Training Loss: 0.5505\n",
      "Epoch 8/10, Batch 731/883, Training Loss: 0.4319\n",
      "Epoch 8/10, Batch 732/883, Training Loss: 0.6982\n",
      "Epoch 8/10, Batch 733/883, Training Loss: 0.8005\n",
      "Epoch 8/10, Batch 734/883, Training Loss: 0.5627\n",
      "Epoch 8/10, Batch 735/883, Training Loss: 0.4993\n",
      "Epoch 8/10, Batch 736/883, Training Loss: 0.5082\n",
      "Epoch 8/10, Batch 737/883, Training Loss: 0.4959\n",
      "Epoch 8/10, Batch 738/883, Training Loss: 0.6279\n",
      "Epoch 8/10, Batch 739/883, Training Loss: 0.9007\n",
      "Epoch 8/10, Batch 740/883, Training Loss: 0.4541\n",
      "Epoch 8/10, Batch 741/883, Training Loss: 0.6317\n",
      "Epoch 8/10, Batch 742/883, Training Loss: 0.7329\n",
      "Epoch 8/10, Batch 743/883, Training Loss: 0.9197\n",
      "Epoch 8/10, Batch 744/883, Training Loss: 0.6763\n",
      "Epoch 8/10, Batch 745/883, Training Loss: 0.5943\n",
      "Epoch 8/10, Batch 746/883, Training Loss: 0.4787\n",
      "Epoch 8/10, Batch 747/883, Training Loss: 0.4470\n",
      "Epoch 8/10, Batch 748/883, Training Loss: 0.7760\n",
      "Epoch 8/10, Batch 749/883, Training Loss: 0.4240\n",
      "Epoch 8/10, Batch 750/883, Training Loss: 0.6328\n",
      "Epoch 8/10, Batch 751/883, Training Loss: 0.9767\n",
      "Epoch 8/10, Batch 752/883, Training Loss: 0.3929\n",
      "Epoch 8/10, Batch 753/883, Training Loss: 0.5487\n",
      "Epoch 8/10, Batch 754/883, Training Loss: 0.6139\n",
      "Epoch 8/10, Batch 755/883, Training Loss: 0.5298\n",
      "Epoch 8/10, Batch 756/883, Training Loss: 0.5519\n",
      "Epoch 8/10, Batch 757/883, Training Loss: 0.5406\n",
      "Epoch 8/10, Batch 758/883, Training Loss: 0.7226\n",
      "Epoch 8/10, Batch 759/883, Training Loss: 0.7413\n",
      "Epoch 8/10, Batch 760/883, Training Loss: 0.4663\n",
      "Epoch 8/10, Batch 761/883, Training Loss: 0.5950\n",
      "Epoch 8/10, Batch 762/883, Training Loss: 0.3874\n",
      "Epoch 8/10, Batch 763/883, Training Loss: 0.4864\n",
      "Epoch 8/10, Batch 764/883, Training Loss: 0.5915\n",
      "Epoch 8/10, Batch 765/883, Training Loss: 0.4177\n",
      "Epoch 8/10, Batch 766/883, Training Loss: 0.7618\n",
      "Epoch 8/10, Batch 767/883, Training Loss: 0.9749\n",
      "Epoch 8/10, Batch 768/883, Training Loss: 0.4863\n",
      "Epoch 8/10, Batch 769/883, Training Loss: 0.9739\n",
      "Epoch 8/10, Batch 770/883, Training Loss: 0.5545\n",
      "Epoch 8/10, Batch 771/883, Training Loss: 0.6945\n",
      "Epoch 8/10, Batch 772/883, Training Loss: 0.3898\n",
      "Epoch 8/10, Batch 773/883, Training Loss: 0.6186\n",
      "Epoch 8/10, Batch 774/883, Training Loss: 0.4552\n",
      "Epoch 8/10, Batch 775/883, Training Loss: 0.6357\n",
      "Epoch 8/10, Batch 776/883, Training Loss: 0.4147\n",
      "Epoch 8/10, Batch 777/883, Training Loss: 0.3417\n",
      "Epoch 8/10, Batch 778/883, Training Loss: 0.5403\n",
      "Epoch 8/10, Batch 779/883, Training Loss: 0.8604\n",
      "Epoch 8/10, Batch 780/883, Training Loss: 0.5974\n",
      "Epoch 8/10, Batch 781/883, Training Loss: 0.5048\n",
      "Epoch 8/10, Batch 782/883, Training Loss: 0.9949\n",
      "Epoch 8/10, Batch 783/883, Training Loss: 1.0072\n",
      "Epoch 8/10, Batch 784/883, Training Loss: 0.8119\n",
      "Epoch 8/10, Batch 785/883, Training Loss: 0.5682\n",
      "Epoch 8/10, Batch 786/883, Training Loss: 0.4514\n",
      "Epoch 8/10, Batch 787/883, Training Loss: 0.6316\n",
      "Epoch 8/10, Batch 788/883, Training Loss: 0.6754\n",
      "Epoch 8/10, Batch 789/883, Training Loss: 0.6094\n",
      "Epoch 8/10, Batch 790/883, Training Loss: 0.8527\n",
      "Epoch 8/10, Batch 791/883, Training Loss: 0.4801\n",
      "Epoch 8/10, Batch 792/883, Training Loss: 0.5947\n",
      "Epoch 8/10, Batch 793/883, Training Loss: 0.6863\n",
      "Epoch 8/10, Batch 794/883, Training Loss: 0.6555\n",
      "Epoch 8/10, Batch 795/883, Training Loss: 0.7726\n",
      "Epoch 8/10, Batch 796/883, Training Loss: 0.6096\n",
      "Epoch 8/10, Batch 797/883, Training Loss: 0.7946\n",
      "Epoch 8/10, Batch 798/883, Training Loss: 0.6182\n",
      "Epoch 8/10, Batch 799/883, Training Loss: 0.5126\n",
      "Epoch 8/10, Batch 800/883, Training Loss: 0.5102\n",
      "Epoch 8/10, Batch 801/883, Training Loss: 0.6568\n",
      "Epoch 8/10, Batch 802/883, Training Loss: 0.8917\n",
      "Epoch 8/10, Batch 803/883, Training Loss: 0.9372\n",
      "Epoch 8/10, Batch 804/883, Training Loss: 0.4790\n",
      "Epoch 8/10, Batch 805/883, Training Loss: 0.5955\n",
      "Epoch 8/10, Batch 806/883, Training Loss: 0.5389\n",
      "Epoch 8/10, Batch 807/883, Training Loss: 0.6978\n",
      "Epoch 8/10, Batch 808/883, Training Loss: 0.6562\n",
      "Epoch 8/10, Batch 809/883, Training Loss: 0.3927\n",
      "Epoch 8/10, Batch 810/883, Training Loss: 0.5974\n",
      "Epoch 8/10, Batch 811/883, Training Loss: 0.8985\n",
      "Epoch 8/10, Batch 812/883, Training Loss: 0.5843\n",
      "Epoch 8/10, Batch 813/883, Training Loss: 1.2293\n",
      "Epoch 8/10, Batch 814/883, Training Loss: 0.4620\n",
      "Epoch 8/10, Batch 815/883, Training Loss: 0.8136\n",
      "Epoch 8/10, Batch 816/883, Training Loss: 0.6993\n",
      "Epoch 8/10, Batch 817/883, Training Loss: 0.7526\n",
      "Epoch 8/10, Batch 818/883, Training Loss: 0.6921\n",
      "Epoch 8/10, Batch 819/883, Training Loss: 0.9254\n",
      "Epoch 8/10, Batch 820/883, Training Loss: 0.4645\n",
      "Epoch 8/10, Batch 821/883, Training Loss: 0.5927\n",
      "Epoch 8/10, Batch 822/883, Training Loss: 0.7877\n",
      "Epoch 8/10, Batch 823/883, Training Loss: 0.5517\n",
      "Epoch 8/10, Batch 824/883, Training Loss: 0.6949\n",
      "Epoch 8/10, Batch 825/883, Training Loss: 0.8833\n",
      "Epoch 8/10, Batch 826/883, Training Loss: 0.6374\n",
      "Epoch 8/10, Batch 827/883, Training Loss: 0.6230\n",
      "Epoch 8/10, Batch 828/883, Training Loss: 0.4852\n",
      "Epoch 8/10, Batch 829/883, Training Loss: 0.8689\n",
      "Epoch 8/10, Batch 830/883, Training Loss: 0.7252\n",
      "Epoch 8/10, Batch 831/883, Training Loss: 0.6364\n",
      "Epoch 8/10, Batch 832/883, Training Loss: 0.5081\n",
      "Epoch 8/10, Batch 833/883, Training Loss: 0.5460\n",
      "Epoch 8/10, Batch 834/883, Training Loss: 0.7756\n",
      "Epoch 8/10, Batch 835/883, Training Loss: 0.6327\n",
      "Epoch 8/10, Batch 836/883, Training Loss: 0.4690\n",
      "Epoch 8/10, Batch 837/883, Training Loss: 0.5847\n",
      "Epoch 8/10, Batch 838/883, Training Loss: 0.4206\n",
      "Epoch 8/10, Batch 839/883, Training Loss: 0.8420\n",
      "Epoch 8/10, Batch 840/883, Training Loss: 0.7334\n",
      "Epoch 8/10, Batch 841/883, Training Loss: 0.5024\n",
      "Epoch 8/10, Batch 842/883, Training Loss: 0.8122\n",
      "Epoch 8/10, Batch 843/883, Training Loss: 0.4605\n",
      "Epoch 8/10, Batch 844/883, Training Loss: 0.4799\n",
      "Epoch 8/10, Batch 845/883, Training Loss: 0.7081\n",
      "Epoch 8/10, Batch 846/883, Training Loss: 0.4535\n",
      "Epoch 8/10, Batch 847/883, Training Loss: 0.5159\n",
      "Epoch 8/10, Batch 848/883, Training Loss: 0.5288\n",
      "Epoch 8/10, Batch 849/883, Training Loss: 0.9744\n",
      "Epoch 8/10, Batch 850/883, Training Loss: 0.7756\n",
      "Epoch 8/10, Batch 851/883, Training Loss: 0.9389\n",
      "Epoch 8/10, Batch 852/883, Training Loss: 0.4313\n",
      "Epoch 8/10, Batch 853/883, Training Loss: 0.3916\n",
      "Epoch 8/10, Batch 854/883, Training Loss: 0.8032\n",
      "Epoch 8/10, Batch 855/883, Training Loss: 0.3596\n",
      "Epoch 8/10, Batch 856/883, Training Loss: 0.7289\n",
      "Epoch 8/10, Batch 857/883, Training Loss: 0.5964\n",
      "Epoch 8/10, Batch 858/883, Training Loss: 0.5792\n",
      "Epoch 8/10, Batch 859/883, Training Loss: 0.8254\n",
      "Epoch 8/10, Batch 860/883, Training Loss: 0.4829\n",
      "Epoch 8/10, Batch 861/883, Training Loss: 0.5473\n",
      "Epoch 8/10, Batch 862/883, Training Loss: 0.4923\n",
      "Epoch 8/10, Batch 863/883, Training Loss: 0.5077\n",
      "Epoch 8/10, Batch 864/883, Training Loss: 0.5084\n",
      "Epoch 8/10, Batch 865/883, Training Loss: 0.5409\n",
      "Epoch 8/10, Batch 866/883, Training Loss: 0.3638\n",
      "Epoch 8/10, Batch 867/883, Training Loss: 0.6439\n",
      "Epoch 8/10, Batch 868/883, Training Loss: 0.4763\n",
      "Epoch 8/10, Batch 869/883, Training Loss: 0.7088\n",
      "Epoch 8/10, Batch 870/883, Training Loss: 0.7370\n",
      "Epoch 8/10, Batch 871/883, Training Loss: 0.8877\n",
      "Epoch 8/10, Batch 872/883, Training Loss: 0.6737\n",
      "Epoch 8/10, Batch 873/883, Training Loss: 0.6876\n",
      "Epoch 8/10, Batch 874/883, Training Loss: 0.7655\n",
      "Epoch 8/10, Batch 875/883, Training Loss: 0.7251\n",
      "Epoch 8/10, Batch 876/883, Training Loss: 0.7109\n",
      "Epoch 8/10, Batch 877/883, Training Loss: 0.6345\n",
      "Epoch 8/10, Batch 878/883, Training Loss: 0.5741\n",
      "Epoch 8/10, Batch 879/883, Training Loss: 1.4903\n",
      "Epoch 8/10, Batch 880/883, Training Loss: 0.6993\n",
      "Epoch 8/10, Batch 881/883, Training Loss: 0.4970\n",
      "Epoch 8/10, Batch 882/883, Training Loss: 0.4289\n",
      "Epoch 8/10, Batch 883/883, Training Loss: 0.5877\n",
      "Epoch 8/10, Training Loss: 0.6273, Validation Loss: 0.5671, Validation Accuracy: 0.7493\n",
      "Epoch 9/10, Batch 1/883, Training Loss: 0.6198\n",
      "Epoch 9/10, Batch 2/883, Training Loss: 0.7614\n",
      "Epoch 9/10, Batch 3/883, Training Loss: 0.5280\n",
      "Epoch 9/10, Batch 4/883, Training Loss: 0.7488\n",
      "Epoch 9/10, Batch 5/883, Training Loss: 0.6349\n",
      "Epoch 9/10, Batch 6/883, Training Loss: 0.6473\n",
      "Epoch 9/10, Batch 7/883, Training Loss: 0.5576\n",
      "Epoch 9/10, Batch 8/883, Training Loss: 0.6161\n",
      "Epoch 9/10, Batch 9/883, Training Loss: 0.6577\n",
      "Epoch 9/10, Batch 10/883, Training Loss: 0.3237\n",
      "Epoch 9/10, Batch 11/883, Training Loss: 0.7323\n",
      "Epoch 9/10, Batch 12/883, Training Loss: 0.6753\n",
      "Epoch 9/10, Batch 13/883, Training Loss: 0.5904\n",
      "Epoch 9/10, Batch 14/883, Training Loss: 0.4870\n",
      "Epoch 9/10, Batch 15/883, Training Loss: 0.6149\n",
      "Epoch 9/10, Batch 16/883, Training Loss: 0.7980\n",
      "Epoch 9/10, Batch 17/883, Training Loss: 0.5662\n",
      "Epoch 9/10, Batch 18/883, Training Loss: 0.4703\n",
      "Epoch 9/10, Batch 19/883, Training Loss: 0.8006\n",
      "Epoch 9/10, Batch 20/883, Training Loss: 0.5249\n",
      "Epoch 9/10, Batch 21/883, Training Loss: 0.5384\n",
      "Epoch 9/10, Batch 22/883, Training Loss: 1.3574\n",
      "Epoch 9/10, Batch 23/883, Training Loss: 0.4863\n",
      "Epoch 9/10, Batch 24/883, Training Loss: 0.8399\n",
      "Epoch 9/10, Batch 25/883, Training Loss: 0.4624\n",
      "Epoch 9/10, Batch 26/883, Training Loss: 1.1728\n",
      "Epoch 9/10, Batch 27/883, Training Loss: 0.5938\n",
      "Epoch 9/10, Batch 28/883, Training Loss: 0.3865\n",
      "Epoch 9/10, Batch 29/883, Training Loss: 0.6544\n",
      "Epoch 9/10, Batch 30/883, Training Loss: 0.4354\n",
      "Epoch 9/10, Batch 31/883, Training Loss: 0.6459\n",
      "Epoch 9/10, Batch 32/883, Training Loss: 0.5069\n",
      "Epoch 9/10, Batch 33/883, Training Loss: 0.4908\n",
      "Epoch 9/10, Batch 34/883, Training Loss: 0.6686\n",
      "Epoch 9/10, Batch 35/883, Training Loss: 0.5391\n",
      "Epoch 9/10, Batch 36/883, Training Loss: 0.4186\n",
      "Epoch 9/10, Batch 37/883, Training Loss: 0.5338\n",
      "Epoch 9/10, Batch 38/883, Training Loss: 0.4562\n",
      "Epoch 9/10, Batch 39/883, Training Loss: 0.6172\n",
      "Epoch 9/10, Batch 40/883, Training Loss: 0.6438\n",
      "Epoch 9/10, Batch 41/883, Training Loss: 0.5917\n",
      "Epoch 9/10, Batch 42/883, Training Loss: 0.8021\n",
      "Epoch 9/10, Batch 43/883, Training Loss: 0.7443\n",
      "Epoch 9/10, Batch 44/883, Training Loss: 0.9797\n",
      "Epoch 9/10, Batch 45/883, Training Loss: 0.5817\n",
      "Epoch 9/10, Batch 46/883, Training Loss: 0.5313\n",
      "Epoch 9/10, Batch 47/883, Training Loss: 0.5339\n",
      "Epoch 9/10, Batch 48/883, Training Loss: 0.6824\n",
      "Epoch 9/10, Batch 49/883, Training Loss: 0.5469\n",
      "Epoch 9/10, Batch 50/883, Training Loss: 0.3598\n",
      "Epoch 9/10, Batch 51/883, Training Loss: 0.7250\n",
      "Epoch 9/10, Batch 52/883, Training Loss: 0.5383\n",
      "Epoch 9/10, Batch 53/883, Training Loss: 0.3488\n",
      "Epoch 9/10, Batch 54/883, Training Loss: 0.6341\n",
      "Epoch 9/10, Batch 55/883, Training Loss: 0.7123\n",
      "Epoch 9/10, Batch 56/883, Training Loss: 0.4340\n",
      "Epoch 9/10, Batch 57/883, Training Loss: 1.0845\n",
      "Epoch 9/10, Batch 58/883, Training Loss: 0.6686\n",
      "Epoch 9/10, Batch 59/883, Training Loss: 0.4365\n",
      "Epoch 9/10, Batch 60/883, Training Loss: 0.6838\n",
      "Epoch 9/10, Batch 61/883, Training Loss: 0.7343\n",
      "Epoch 9/10, Batch 62/883, Training Loss: 0.6270\n",
      "Epoch 9/10, Batch 63/883, Training Loss: 0.4173\n",
      "Epoch 9/10, Batch 64/883, Training Loss: 0.5987\n",
      "Epoch 9/10, Batch 65/883, Training Loss: 0.7538\n",
      "Epoch 9/10, Batch 66/883, Training Loss: 0.4890\n",
      "Epoch 9/10, Batch 67/883, Training Loss: 0.6518\n",
      "Epoch 9/10, Batch 68/883, Training Loss: 0.5659\n",
      "Epoch 9/10, Batch 69/883, Training Loss: 0.5707\n",
      "Epoch 9/10, Batch 70/883, Training Loss: 0.4515\n",
      "Epoch 9/10, Batch 71/883, Training Loss: 0.7056\n",
      "Epoch 9/10, Batch 72/883, Training Loss: 0.4208\n",
      "Epoch 9/10, Batch 73/883, Training Loss: 0.7914\n",
      "Epoch 9/10, Batch 74/883, Training Loss: 0.5609\n",
      "Epoch 9/10, Batch 75/883, Training Loss: 0.4598\n",
      "Epoch 9/10, Batch 76/883, Training Loss: 0.5841\n",
      "Epoch 9/10, Batch 77/883, Training Loss: 0.3763\n",
      "Epoch 9/10, Batch 78/883, Training Loss: 0.6571\n",
      "Epoch 9/10, Batch 79/883, Training Loss: 0.9196\n",
      "Epoch 9/10, Batch 80/883, Training Loss: 0.8867\n",
      "Epoch 9/10, Batch 81/883, Training Loss: 0.7206\n",
      "Epoch 9/10, Batch 82/883, Training Loss: 0.5955\n",
      "Epoch 9/10, Batch 83/883, Training Loss: 0.4837\n",
      "Epoch 9/10, Batch 84/883, Training Loss: 0.5322\n",
      "Epoch 9/10, Batch 85/883, Training Loss: 0.6658\n",
      "Epoch 9/10, Batch 86/883, Training Loss: 0.3871\n",
      "Epoch 9/10, Batch 87/883, Training Loss: 0.5495\n",
      "Epoch 9/10, Batch 88/883, Training Loss: 0.7990\n",
      "Epoch 9/10, Batch 89/883, Training Loss: 0.4373\n",
      "Epoch 9/10, Batch 90/883, Training Loss: 0.2965\n",
      "Epoch 9/10, Batch 91/883, Training Loss: 0.4520\n",
      "Epoch 9/10, Batch 92/883, Training Loss: 0.5366\n",
      "Epoch 9/10, Batch 93/883, Training Loss: 0.3036\n",
      "Epoch 9/10, Batch 94/883, Training Loss: 0.8817\n",
      "Epoch 9/10, Batch 95/883, Training Loss: 0.7056\n",
      "Epoch 9/10, Batch 96/883, Training Loss: 0.5883\n",
      "Epoch 9/10, Batch 97/883, Training Loss: 0.6616\n",
      "Epoch 9/10, Batch 98/883, Training Loss: 0.5608\n",
      "Epoch 9/10, Batch 99/883, Training Loss: 0.4863\n",
      "Epoch 9/10, Batch 100/883, Training Loss: 0.5063\n",
      "Epoch 9/10, Batch 101/883, Training Loss: 0.4950\n",
      "Epoch 9/10, Batch 102/883, Training Loss: 0.4408\n",
      "Epoch 9/10, Batch 103/883, Training Loss: 0.4981\n",
      "Epoch 9/10, Batch 104/883, Training Loss: 0.4276\n",
      "Epoch 9/10, Batch 105/883, Training Loss: 0.7875\n",
      "Epoch 9/10, Batch 106/883, Training Loss: 1.0030\n",
      "Epoch 9/10, Batch 107/883, Training Loss: 0.5210\n",
      "Epoch 9/10, Batch 108/883, Training Loss: 0.7347\n",
      "Epoch 9/10, Batch 109/883, Training Loss: 0.3576\n",
      "Epoch 9/10, Batch 110/883, Training Loss: 0.6535\n",
      "Epoch 9/10, Batch 111/883, Training Loss: 0.5725\n",
      "Epoch 9/10, Batch 112/883, Training Loss: 0.5414\n",
      "Epoch 9/10, Batch 113/883, Training Loss: 0.4524\n",
      "Epoch 9/10, Batch 114/883, Training Loss: 0.8220\n",
      "Epoch 9/10, Batch 115/883, Training Loss: 0.6853\n",
      "Epoch 9/10, Batch 116/883, Training Loss: 0.3568\n",
      "Epoch 9/10, Batch 117/883, Training Loss: 0.4884\n",
      "Epoch 9/10, Batch 118/883, Training Loss: 0.7151\n",
      "Epoch 9/10, Batch 119/883, Training Loss: 0.4922\n",
      "Epoch 9/10, Batch 120/883, Training Loss: 0.5392\n",
      "Epoch 9/10, Batch 121/883, Training Loss: 0.5034\n",
      "Epoch 9/10, Batch 122/883, Training Loss: 0.5843\n",
      "Epoch 9/10, Batch 123/883, Training Loss: 0.4197\n",
      "Epoch 9/10, Batch 124/883, Training Loss: 0.3071\n",
      "Epoch 9/10, Batch 125/883, Training Loss: 0.6865\n",
      "Epoch 9/10, Batch 126/883, Training Loss: 0.7831\n",
      "Epoch 9/10, Batch 127/883, Training Loss: 0.6086\n",
      "Epoch 9/10, Batch 128/883, Training Loss: 0.5159\n",
      "Epoch 9/10, Batch 129/883, Training Loss: 0.4317\n",
      "Epoch 9/10, Batch 130/883, Training Loss: 0.5251\n",
      "Epoch 9/10, Batch 131/883, Training Loss: 0.6650\n",
      "Epoch 9/10, Batch 132/883, Training Loss: 0.3361\n",
      "Epoch 9/10, Batch 133/883, Training Loss: 0.3150\n",
      "Epoch 9/10, Batch 134/883, Training Loss: 0.3836\n",
      "Epoch 9/10, Batch 135/883, Training Loss: 0.5511\n",
      "Epoch 9/10, Batch 136/883, Training Loss: 0.4380\n",
      "Epoch 9/10, Batch 137/883, Training Loss: 0.8857\n",
      "Epoch 9/10, Batch 138/883, Training Loss: 0.6189\n",
      "Epoch 9/10, Batch 139/883, Training Loss: 0.8103\n",
      "Epoch 9/10, Batch 140/883, Training Loss: 1.1660\n",
      "Epoch 9/10, Batch 141/883, Training Loss: 0.5674\n",
      "Epoch 9/10, Batch 142/883, Training Loss: 0.6943\n",
      "Epoch 9/10, Batch 143/883, Training Loss: 0.8023\n",
      "Epoch 9/10, Batch 144/883, Training Loss: 0.6913\n",
      "Epoch 9/10, Batch 145/883, Training Loss: 0.6148\n",
      "Epoch 9/10, Batch 146/883, Training Loss: 0.5784\n",
      "Epoch 9/10, Batch 147/883, Training Loss: 0.4781\n",
      "Epoch 9/10, Batch 148/883, Training Loss: 0.4244\n",
      "Epoch 9/10, Batch 149/883, Training Loss: 0.5503\n",
      "Epoch 9/10, Batch 150/883, Training Loss: 0.7468\n",
      "Epoch 9/10, Batch 151/883, Training Loss: 0.4254\n",
      "Epoch 9/10, Batch 152/883, Training Loss: 0.8106\n",
      "Epoch 9/10, Batch 153/883, Training Loss: 0.3208\n",
      "Epoch 9/10, Batch 154/883, Training Loss: 0.6652\n",
      "Epoch 9/10, Batch 155/883, Training Loss: 0.3604\n",
      "Epoch 9/10, Batch 156/883, Training Loss: 0.4064\n",
      "Epoch 9/10, Batch 157/883, Training Loss: 0.5993\n",
      "Epoch 9/10, Batch 158/883, Training Loss: 0.6779\n",
      "Epoch 9/10, Batch 159/883, Training Loss: 0.5902\n",
      "Epoch 9/10, Batch 160/883, Training Loss: 0.4959\n",
      "Epoch 9/10, Batch 161/883, Training Loss: 0.5544\n",
      "Epoch 9/10, Batch 162/883, Training Loss: 0.9490\n",
      "Epoch 9/10, Batch 163/883, Training Loss: 0.4809\n",
      "Epoch 9/10, Batch 164/883, Training Loss: 0.5257\n",
      "Epoch 9/10, Batch 165/883, Training Loss: 0.3593\n",
      "Epoch 9/10, Batch 166/883, Training Loss: 0.4122\n",
      "Epoch 9/10, Batch 167/883, Training Loss: 0.7837\n",
      "Epoch 9/10, Batch 168/883, Training Loss: 0.5314\n",
      "Epoch 9/10, Batch 169/883, Training Loss: 0.4565\n",
      "Epoch 9/10, Batch 170/883, Training Loss: 0.6143\n",
      "Epoch 9/10, Batch 171/883, Training Loss: 0.6511\n",
      "Epoch 9/10, Batch 172/883, Training Loss: 0.6011\n",
      "Epoch 9/10, Batch 173/883, Training Loss: 0.6059\n",
      "Epoch 9/10, Batch 174/883, Training Loss: 0.6736\n",
      "Epoch 9/10, Batch 175/883, Training Loss: 0.5219\n",
      "Epoch 9/10, Batch 176/883, Training Loss: 0.4521\n",
      "Epoch 9/10, Batch 177/883, Training Loss: 0.7826\n",
      "Epoch 9/10, Batch 178/883, Training Loss: 0.5744\n",
      "Epoch 9/10, Batch 179/883, Training Loss: 0.6847\n",
      "Epoch 9/10, Batch 180/883, Training Loss: 0.6791\n",
      "Epoch 9/10, Batch 181/883, Training Loss: 0.4675\n",
      "Epoch 9/10, Batch 182/883, Training Loss: 0.4409\n",
      "Epoch 9/10, Batch 183/883, Training Loss: 0.3929\n",
      "Epoch 9/10, Batch 184/883, Training Loss: 0.5464\n",
      "Epoch 9/10, Batch 185/883, Training Loss: 1.0658\n",
      "Epoch 9/10, Batch 186/883, Training Loss: 0.4364\n",
      "Epoch 9/10, Batch 187/883, Training Loss: 1.3069\n",
      "Epoch 9/10, Batch 188/883, Training Loss: 0.8228\n",
      "Epoch 9/10, Batch 189/883, Training Loss: 0.4616\n",
      "Epoch 9/10, Batch 190/883, Training Loss: 0.5685\n",
      "Epoch 9/10, Batch 191/883, Training Loss: 0.4436\n",
      "Epoch 9/10, Batch 192/883, Training Loss: 0.6482\n",
      "Epoch 9/10, Batch 193/883, Training Loss: 0.4693\n",
      "Epoch 9/10, Batch 194/883, Training Loss: 0.6738\n",
      "Epoch 9/10, Batch 195/883, Training Loss: 0.4535\n",
      "Epoch 9/10, Batch 196/883, Training Loss: 0.5102\n",
      "Epoch 9/10, Batch 197/883, Training Loss: 0.6558\n",
      "Epoch 9/10, Batch 198/883, Training Loss: 0.6011\n",
      "Epoch 9/10, Batch 199/883, Training Loss: 0.3332\n",
      "Epoch 9/10, Batch 200/883, Training Loss: 0.6711\n",
      "Epoch 9/10, Batch 201/883, Training Loss: 0.3972\n",
      "Epoch 9/10, Batch 202/883, Training Loss: 0.7022\n",
      "Epoch 9/10, Batch 203/883, Training Loss: 0.4887\n",
      "Epoch 9/10, Batch 204/883, Training Loss: 0.7701\n",
      "Epoch 9/10, Batch 205/883, Training Loss: 0.6227\n",
      "Epoch 9/10, Batch 206/883, Training Loss: 0.7961\n",
      "Epoch 9/10, Batch 207/883, Training Loss: 0.4277\n",
      "Epoch 9/10, Batch 208/883, Training Loss: 0.4465\n",
      "Epoch 9/10, Batch 209/883, Training Loss: 0.4588\n",
      "Epoch 9/10, Batch 210/883, Training Loss: 0.6501\n",
      "Epoch 9/10, Batch 211/883, Training Loss: 1.0628\n",
      "Epoch 9/10, Batch 212/883, Training Loss: 0.7513\n",
      "Epoch 9/10, Batch 213/883, Training Loss: 0.5839\n",
      "Epoch 9/10, Batch 214/883, Training Loss: 0.6774\n",
      "Epoch 9/10, Batch 215/883, Training Loss: 0.5975\n",
      "Epoch 9/10, Batch 216/883, Training Loss: 0.5213\n",
      "Epoch 9/10, Batch 217/883, Training Loss: 0.5420\n",
      "Epoch 9/10, Batch 218/883, Training Loss: 0.6976\n",
      "Epoch 9/10, Batch 219/883, Training Loss: 0.6400\n",
      "Epoch 9/10, Batch 220/883, Training Loss: 0.7362\n",
      "Epoch 9/10, Batch 221/883, Training Loss: 0.9052\n",
      "Epoch 9/10, Batch 222/883, Training Loss: 0.5328\n",
      "Epoch 9/10, Batch 223/883, Training Loss: 0.4959\n",
      "Epoch 9/10, Batch 224/883, Training Loss: 0.4433\n",
      "Epoch 9/10, Batch 225/883, Training Loss: 0.4027\n",
      "Epoch 9/10, Batch 226/883, Training Loss: 0.5484\n",
      "Epoch 9/10, Batch 227/883, Training Loss: 0.5660\n",
      "Epoch 9/10, Batch 228/883, Training Loss: 0.4775\n",
      "Epoch 9/10, Batch 229/883, Training Loss: 0.4572\n",
      "Epoch 9/10, Batch 230/883, Training Loss: 0.6958\n",
      "Epoch 9/10, Batch 231/883, Training Loss: 0.5008\n",
      "Epoch 9/10, Batch 232/883, Training Loss: 0.3762\n",
      "Epoch 9/10, Batch 233/883, Training Loss: 0.5638\n",
      "Epoch 9/10, Batch 234/883, Training Loss: 0.5539\n",
      "Epoch 9/10, Batch 235/883, Training Loss: 0.3741\n",
      "Epoch 9/10, Batch 236/883, Training Loss: 0.7670\n",
      "Epoch 9/10, Batch 237/883, Training Loss: 0.4749\n",
      "Epoch 9/10, Batch 238/883, Training Loss: 0.6327\n",
      "Epoch 9/10, Batch 239/883, Training Loss: 0.5556\n",
      "Epoch 9/10, Batch 240/883, Training Loss: 0.4399\n",
      "Epoch 9/10, Batch 241/883, Training Loss: 0.4501\n",
      "Epoch 9/10, Batch 242/883, Training Loss: 1.1540\n",
      "Epoch 9/10, Batch 243/883, Training Loss: 0.4954\n",
      "Epoch 9/10, Batch 244/883, Training Loss: 0.4961\n",
      "Epoch 9/10, Batch 245/883, Training Loss: 0.3747\n",
      "Epoch 9/10, Batch 246/883, Training Loss: 0.5307\n",
      "Epoch 9/10, Batch 247/883, Training Loss: 0.3966\n",
      "Epoch 9/10, Batch 248/883, Training Loss: 0.6119\n",
      "Epoch 9/10, Batch 249/883, Training Loss: 0.6450\n",
      "Epoch 9/10, Batch 250/883, Training Loss: 0.6070\n",
      "Epoch 9/10, Batch 251/883, Training Loss: 0.5119\n",
      "Epoch 9/10, Batch 252/883, Training Loss: 0.5615\n",
      "Epoch 9/10, Batch 253/883, Training Loss: 0.9572\n",
      "Epoch 9/10, Batch 254/883, Training Loss: 0.4970\n",
      "Epoch 9/10, Batch 255/883, Training Loss: 0.4463\n",
      "Epoch 9/10, Batch 256/883, Training Loss: 0.6930\n",
      "Epoch 9/10, Batch 257/883, Training Loss: 0.5216\n",
      "Epoch 9/10, Batch 258/883, Training Loss: 0.8207\n",
      "Epoch 9/10, Batch 259/883, Training Loss: 0.3616\n",
      "Epoch 9/10, Batch 260/883, Training Loss: 0.4134\n",
      "Epoch 9/10, Batch 261/883, Training Loss: 0.5185\n",
      "Epoch 9/10, Batch 262/883, Training Loss: 0.4949\n",
      "Epoch 9/10, Batch 263/883, Training Loss: 0.6286\n",
      "Epoch 9/10, Batch 264/883, Training Loss: 0.7681\n",
      "Epoch 9/10, Batch 265/883, Training Loss: 0.5853\n",
      "Epoch 9/10, Batch 266/883, Training Loss: 0.5664\n",
      "Epoch 9/10, Batch 267/883, Training Loss: 0.5252\n",
      "Epoch 9/10, Batch 268/883, Training Loss: 0.7272\n",
      "Epoch 9/10, Batch 269/883, Training Loss: 0.8560\n",
      "Epoch 9/10, Batch 270/883, Training Loss: 0.5533\n",
      "Epoch 9/10, Batch 271/883, Training Loss: 0.6003\n",
      "Epoch 9/10, Batch 272/883, Training Loss: 0.7802\n",
      "Epoch 9/10, Batch 273/883, Training Loss: 0.3708\n",
      "Epoch 9/10, Batch 274/883, Training Loss: 0.5279\n",
      "Epoch 9/10, Batch 275/883, Training Loss: 0.7818\n",
      "Epoch 9/10, Batch 276/883, Training Loss: 0.6981\n",
      "Epoch 9/10, Batch 277/883, Training Loss: 0.3445\n",
      "Epoch 9/10, Batch 278/883, Training Loss: 0.4671\n",
      "Epoch 9/10, Batch 279/883, Training Loss: 0.5188\n",
      "Epoch 9/10, Batch 280/883, Training Loss: 0.7436\n",
      "Epoch 9/10, Batch 281/883, Training Loss: 0.5251\n",
      "Epoch 9/10, Batch 282/883, Training Loss: 0.4904\n",
      "Epoch 9/10, Batch 283/883, Training Loss: 0.3312\n",
      "Epoch 9/10, Batch 284/883, Training Loss: 0.9731\n",
      "Epoch 9/10, Batch 285/883, Training Loss: 0.5684\n",
      "Epoch 9/10, Batch 286/883, Training Loss: 0.3919\n",
      "Epoch 9/10, Batch 287/883, Training Loss: 0.7669\n",
      "Epoch 9/10, Batch 288/883, Training Loss: 0.5877\n",
      "Epoch 9/10, Batch 289/883, Training Loss: 0.4158\n",
      "Epoch 9/10, Batch 290/883, Training Loss: 0.6931\n",
      "Epoch 9/10, Batch 291/883, Training Loss: 0.5335\n",
      "Epoch 9/10, Batch 292/883, Training Loss: 0.2388\n",
      "Epoch 9/10, Batch 293/883, Training Loss: 0.4767\n",
      "Epoch 9/10, Batch 294/883, Training Loss: 0.5419\n",
      "Epoch 9/10, Batch 295/883, Training Loss: 0.4855\n",
      "Epoch 9/10, Batch 296/883, Training Loss: 0.6041\n",
      "Epoch 9/10, Batch 297/883, Training Loss: 1.3066\n",
      "Epoch 9/10, Batch 298/883, Training Loss: 0.5850\n",
      "Epoch 9/10, Batch 299/883, Training Loss: 0.6016\n",
      "Epoch 9/10, Batch 300/883, Training Loss: 0.5727\n",
      "Epoch 9/10, Batch 301/883, Training Loss: 0.8216\n",
      "Epoch 9/10, Batch 302/883, Training Loss: 0.8416\n",
      "Epoch 9/10, Batch 303/883, Training Loss: 0.7900\n",
      "Epoch 9/10, Batch 304/883, Training Loss: 0.5405\n",
      "Epoch 9/10, Batch 305/883, Training Loss: 0.5262\n",
      "Epoch 9/10, Batch 306/883, Training Loss: 0.8432\n",
      "Epoch 9/10, Batch 307/883, Training Loss: 0.7572\n",
      "Epoch 9/10, Batch 308/883, Training Loss: 0.5240\n",
      "Epoch 9/10, Batch 309/883, Training Loss: 0.6691\n",
      "Epoch 9/10, Batch 310/883, Training Loss: 0.6013\n",
      "Epoch 9/10, Batch 311/883, Training Loss: 0.3051\n",
      "Epoch 9/10, Batch 312/883, Training Loss: 0.4820\n",
      "Epoch 9/10, Batch 313/883, Training Loss: 0.7502\n",
      "Epoch 9/10, Batch 314/883, Training Loss: 0.6282\n",
      "Epoch 9/10, Batch 315/883, Training Loss: 0.8634\n",
      "Epoch 9/10, Batch 316/883, Training Loss: 0.5830\n",
      "Epoch 9/10, Batch 317/883, Training Loss: 0.3709\n",
      "Epoch 9/10, Batch 318/883, Training Loss: 1.0000\n",
      "Epoch 9/10, Batch 319/883, Training Loss: 0.4493\n",
      "Epoch 9/10, Batch 320/883, Training Loss: 0.8051\n",
      "Epoch 9/10, Batch 321/883, Training Loss: 0.7104\n",
      "Epoch 9/10, Batch 322/883, Training Loss: 0.6566\n",
      "Epoch 9/10, Batch 323/883, Training Loss: 0.5588\n",
      "Epoch 9/10, Batch 324/883, Training Loss: 0.4147\n",
      "Epoch 9/10, Batch 325/883, Training Loss: 0.5888\n",
      "Epoch 9/10, Batch 326/883, Training Loss: 0.2801\n",
      "Epoch 9/10, Batch 327/883, Training Loss: 0.4071\n",
      "Epoch 9/10, Batch 328/883, Training Loss: 0.6259\n",
      "Epoch 9/10, Batch 329/883, Training Loss: 0.4621\n",
      "Epoch 9/10, Batch 330/883, Training Loss: 0.5329\n",
      "Epoch 9/10, Batch 331/883, Training Loss: 0.5794\n",
      "Epoch 9/10, Batch 332/883, Training Loss: 0.5195\n",
      "Epoch 9/10, Batch 333/883, Training Loss: 0.5230\n",
      "Epoch 9/10, Batch 334/883, Training Loss: 1.0025\n",
      "Epoch 9/10, Batch 335/883, Training Loss: 0.3443\n",
      "Epoch 9/10, Batch 336/883, Training Loss: 0.8155\n",
      "Epoch 9/10, Batch 337/883, Training Loss: 0.4167\n",
      "Epoch 9/10, Batch 338/883, Training Loss: 0.5052\n",
      "Epoch 9/10, Batch 339/883, Training Loss: 0.8386\n",
      "Epoch 9/10, Batch 340/883, Training Loss: 0.7053\n",
      "Epoch 9/10, Batch 341/883, Training Loss: 0.6065\n",
      "Epoch 9/10, Batch 342/883, Training Loss: 0.4996\n",
      "Epoch 9/10, Batch 343/883, Training Loss: 0.4118\n",
      "Epoch 9/10, Batch 344/883, Training Loss: 0.7467\n",
      "Epoch 9/10, Batch 345/883, Training Loss: 0.3760\n",
      "Epoch 9/10, Batch 346/883, Training Loss: 0.5697\n",
      "Epoch 9/10, Batch 347/883, Training Loss: 0.3987\n",
      "Epoch 9/10, Batch 348/883, Training Loss: 0.4773\n",
      "Epoch 9/10, Batch 349/883, Training Loss: 0.5129\n",
      "Epoch 9/10, Batch 350/883, Training Loss: 0.8203\n",
      "Epoch 9/10, Batch 351/883, Training Loss: 0.4747\n",
      "Epoch 9/10, Batch 352/883, Training Loss: 0.8836\n",
      "Epoch 9/10, Batch 353/883, Training Loss: 0.8483\n",
      "Epoch 9/10, Batch 354/883, Training Loss: 0.5407\n",
      "Epoch 9/10, Batch 355/883, Training Loss: 0.8874\n",
      "Epoch 9/10, Batch 356/883, Training Loss: 0.6413\n",
      "Epoch 9/10, Batch 357/883, Training Loss: 0.3886\n",
      "Epoch 9/10, Batch 358/883, Training Loss: 0.6152\n",
      "Epoch 9/10, Batch 359/883, Training Loss: 0.5591\n",
      "Epoch 9/10, Batch 360/883, Training Loss: 0.4984\n",
      "Epoch 9/10, Batch 361/883, Training Loss: 0.5297\n",
      "Epoch 9/10, Batch 362/883, Training Loss: 0.4397\n",
      "Epoch 9/10, Batch 363/883, Training Loss: 0.4239\n",
      "Epoch 9/10, Batch 364/883, Training Loss: 0.5370\n",
      "Epoch 9/10, Batch 365/883, Training Loss: 0.4995\n",
      "Epoch 9/10, Batch 366/883, Training Loss: 0.5525\n",
      "Epoch 9/10, Batch 367/883, Training Loss: 0.8951\n",
      "Epoch 9/10, Batch 368/883, Training Loss: 0.3164\n",
      "Epoch 9/10, Batch 369/883, Training Loss: 0.8067\n",
      "Epoch 9/10, Batch 370/883, Training Loss: 0.3794\n",
      "Epoch 9/10, Batch 371/883, Training Loss: 0.5785\n",
      "Epoch 9/10, Batch 372/883, Training Loss: 0.6269\n",
      "Epoch 9/10, Batch 373/883, Training Loss: 0.5645\n",
      "Epoch 9/10, Batch 374/883, Training Loss: 0.6994\n",
      "Epoch 9/10, Batch 375/883, Training Loss: 0.3998\n",
      "Epoch 9/10, Batch 376/883, Training Loss: 0.2893\n",
      "Epoch 9/10, Batch 377/883, Training Loss: 0.6501\n",
      "Epoch 9/10, Batch 378/883, Training Loss: 0.8029\n",
      "Epoch 9/10, Batch 379/883, Training Loss: 0.6617\n",
      "Epoch 9/10, Batch 380/883, Training Loss: 0.8497\n",
      "Epoch 9/10, Batch 381/883, Training Loss: 0.7573\n",
      "Epoch 9/10, Batch 382/883, Training Loss: 0.6341\n",
      "Epoch 9/10, Batch 383/883, Training Loss: 0.7264\n",
      "Epoch 9/10, Batch 384/883, Training Loss: 0.4783\n",
      "Epoch 9/10, Batch 385/883, Training Loss: 0.8274\n",
      "Epoch 9/10, Batch 386/883, Training Loss: 0.7873\n",
      "Epoch 9/10, Batch 387/883, Training Loss: 0.5271\n",
      "Epoch 9/10, Batch 388/883, Training Loss: 0.6589\n",
      "Epoch 9/10, Batch 389/883, Training Loss: 0.5655\n",
      "Epoch 9/10, Batch 390/883, Training Loss: 0.6952\n",
      "Epoch 9/10, Batch 391/883, Training Loss: 0.2707\n",
      "Epoch 9/10, Batch 392/883, Training Loss: 0.5068\n",
      "Epoch 9/10, Batch 393/883, Training Loss: 0.6417\n",
      "Epoch 9/10, Batch 394/883, Training Loss: 0.6503\n",
      "Epoch 9/10, Batch 395/883, Training Loss: 0.4004\n",
      "Epoch 9/10, Batch 396/883, Training Loss: 0.5029\n",
      "Epoch 9/10, Batch 397/883, Training Loss: 0.7076\n",
      "Epoch 9/10, Batch 398/883, Training Loss: 0.5086\n",
      "Epoch 9/10, Batch 399/883, Training Loss: 0.5785\n",
      "Epoch 9/10, Batch 400/883, Training Loss: 0.4496\n",
      "Epoch 9/10, Batch 401/883, Training Loss: 0.6353\n",
      "Epoch 9/10, Batch 402/883, Training Loss: 0.7802\n",
      "Epoch 9/10, Batch 403/883, Training Loss: 0.4032\n",
      "Epoch 9/10, Batch 404/883, Training Loss: 0.6319\n",
      "Epoch 9/10, Batch 405/883, Training Loss: 0.4095\n",
      "Epoch 9/10, Batch 406/883, Training Loss: 0.7371\n",
      "Epoch 9/10, Batch 407/883, Training Loss: 0.6090\n",
      "Epoch 9/10, Batch 408/883, Training Loss: 0.5026\n",
      "Epoch 9/10, Batch 409/883, Training Loss: 0.4533\n",
      "Epoch 9/10, Batch 410/883, Training Loss: 0.7327\n",
      "Epoch 9/10, Batch 411/883, Training Loss: 0.8227\n",
      "Epoch 9/10, Batch 412/883, Training Loss: 0.4719\n",
      "Epoch 9/10, Batch 413/883, Training Loss: 0.5214\n",
      "Epoch 9/10, Batch 414/883, Training Loss: 0.7834\n",
      "Epoch 9/10, Batch 415/883, Training Loss: 0.4527\n",
      "Epoch 9/10, Batch 416/883, Training Loss: 0.3445\n",
      "Epoch 9/10, Batch 417/883, Training Loss: 0.3142\n",
      "Epoch 9/10, Batch 418/883, Training Loss: 0.3851\n",
      "Epoch 9/10, Batch 419/883, Training Loss: 0.6068\n",
      "Epoch 9/10, Batch 420/883, Training Loss: 0.8790\n",
      "Epoch 9/10, Batch 421/883, Training Loss: 0.5298\n",
      "Epoch 9/10, Batch 422/883, Training Loss: 0.3302\n",
      "Epoch 9/10, Batch 423/883, Training Loss: 0.4746\n",
      "Epoch 9/10, Batch 424/883, Training Loss: 0.5014\n",
      "Epoch 9/10, Batch 425/883, Training Loss: 0.5633\n",
      "Epoch 9/10, Batch 426/883, Training Loss: 0.6487\n",
      "Epoch 9/10, Batch 427/883, Training Loss: 0.5998\n",
      "Epoch 9/10, Batch 428/883, Training Loss: 0.8766\n",
      "Epoch 9/10, Batch 429/883, Training Loss: 0.7302\n",
      "Epoch 9/10, Batch 430/883, Training Loss: 0.6133\n",
      "Epoch 9/10, Batch 431/883, Training Loss: 0.3447\n",
      "Epoch 9/10, Batch 432/883, Training Loss: 0.6171\n",
      "Epoch 9/10, Batch 433/883, Training Loss: 0.5355\n",
      "Epoch 9/10, Batch 434/883, Training Loss: 0.4732\n",
      "Epoch 9/10, Batch 435/883, Training Loss: 0.5832\n",
      "Epoch 9/10, Batch 436/883, Training Loss: 0.7486\n",
      "Epoch 9/10, Batch 437/883, Training Loss: 0.4685\n",
      "Epoch 9/10, Batch 438/883, Training Loss: 1.2550\n",
      "Epoch 9/10, Batch 439/883, Training Loss: 0.4571\n",
      "Epoch 9/10, Batch 440/883, Training Loss: 0.6841\n",
      "Epoch 9/10, Batch 441/883, Training Loss: 0.7006\n",
      "Epoch 9/10, Batch 442/883, Training Loss: 0.4890\n",
      "Epoch 9/10, Batch 443/883, Training Loss: 0.5705\n",
      "Epoch 9/10, Batch 444/883, Training Loss: 0.9121\n",
      "Epoch 9/10, Batch 445/883, Training Loss: 0.5146\n",
      "Epoch 9/10, Batch 446/883, Training Loss: 0.6614\n",
      "Epoch 9/10, Batch 447/883, Training Loss: 0.8139\n",
      "Epoch 9/10, Batch 448/883, Training Loss: 0.5343\n",
      "Epoch 9/10, Batch 449/883, Training Loss: 0.5248\n",
      "Epoch 9/10, Batch 450/883, Training Loss: 0.4700\n",
      "Epoch 9/10, Batch 451/883, Training Loss: 0.6290\n",
      "Epoch 9/10, Batch 452/883, Training Loss: 0.5132\n",
      "Epoch 9/10, Batch 453/883, Training Loss: 0.8747\n",
      "Epoch 9/10, Batch 454/883, Training Loss: 0.6388\n",
      "Epoch 9/10, Batch 455/883, Training Loss: 0.3620\n",
      "Epoch 9/10, Batch 456/883, Training Loss: 0.5437\n",
      "Epoch 9/10, Batch 457/883, Training Loss: 1.1173\n",
      "Epoch 9/10, Batch 458/883, Training Loss: 0.6970\n",
      "Epoch 9/10, Batch 459/883, Training Loss: 0.5615\n",
      "Epoch 9/10, Batch 460/883, Training Loss: 0.5298\n",
      "Epoch 9/10, Batch 461/883, Training Loss: 0.5267\n",
      "Epoch 9/10, Batch 462/883, Training Loss: 0.5784\n",
      "Epoch 9/10, Batch 463/883, Training Loss: 0.7704\n",
      "Epoch 9/10, Batch 464/883, Training Loss: 0.5070\n",
      "Epoch 9/10, Batch 465/883, Training Loss: 0.4645\n",
      "Epoch 9/10, Batch 466/883, Training Loss: 0.4299\n",
      "Epoch 9/10, Batch 467/883, Training Loss: 0.5233\n",
      "Epoch 9/10, Batch 468/883, Training Loss: 0.4879\n",
      "Epoch 9/10, Batch 469/883, Training Loss: 0.7154\n",
      "Epoch 9/10, Batch 470/883, Training Loss: 0.7444\n",
      "Epoch 9/10, Batch 471/883, Training Loss: 0.4874\n",
      "Epoch 9/10, Batch 472/883, Training Loss: 0.3660\n",
      "Epoch 9/10, Batch 473/883, Training Loss: 0.7534\n",
      "Epoch 9/10, Batch 474/883, Training Loss: 0.4685\n",
      "Epoch 9/10, Batch 475/883, Training Loss: 0.3147\n",
      "Epoch 9/10, Batch 476/883, Training Loss: 0.4616\n",
      "Epoch 9/10, Batch 477/883, Training Loss: 0.6922\n",
      "Epoch 9/10, Batch 478/883, Training Loss: 0.4040\n",
      "Epoch 9/10, Batch 479/883, Training Loss: 0.5722\n",
      "Epoch 9/10, Batch 480/883, Training Loss: 0.4796\n",
      "Epoch 9/10, Batch 481/883, Training Loss: 0.4874\n",
      "Epoch 9/10, Batch 482/883, Training Loss: 0.7147\n",
      "Epoch 9/10, Batch 483/883, Training Loss: 0.3621\n",
      "Epoch 9/10, Batch 484/883, Training Loss: 0.4987\n",
      "Epoch 9/10, Batch 485/883, Training Loss: 0.5426\n",
      "Epoch 9/10, Batch 486/883, Training Loss: 0.6252\n",
      "Epoch 9/10, Batch 487/883, Training Loss: 0.4719\n",
      "Epoch 9/10, Batch 488/883, Training Loss: 0.5933\n",
      "Epoch 9/10, Batch 489/883, Training Loss: 0.4788\n",
      "Epoch 9/10, Batch 490/883, Training Loss: 0.6100\n",
      "Epoch 9/10, Batch 491/883, Training Loss: 0.5522\n",
      "Epoch 9/10, Batch 492/883, Training Loss: 0.6749\n",
      "Epoch 9/10, Batch 493/883, Training Loss: 0.4178\n",
      "Epoch 9/10, Batch 494/883, Training Loss: 0.4289\n",
      "Epoch 9/10, Batch 495/883, Training Loss: 0.3432\n",
      "Epoch 9/10, Batch 496/883, Training Loss: 0.6768\n",
      "Epoch 9/10, Batch 497/883, Training Loss: 0.4374\n",
      "Epoch 9/10, Batch 498/883, Training Loss: 0.9504\n",
      "Epoch 9/10, Batch 499/883, Training Loss: 0.8683\n",
      "Epoch 9/10, Batch 500/883, Training Loss: 0.5996\n",
      "Epoch 9/10, Batch 501/883, Training Loss: 0.6485\n",
      "Epoch 9/10, Batch 502/883, Training Loss: 0.4316\n",
      "Epoch 9/10, Batch 503/883, Training Loss: 0.4845\n",
      "Epoch 9/10, Batch 504/883, Training Loss: 0.4648\n",
      "Epoch 9/10, Batch 505/883, Training Loss: 0.4975\n",
      "Epoch 9/10, Batch 506/883, Training Loss: 0.5015\n",
      "Epoch 9/10, Batch 507/883, Training Loss: 0.5730\n",
      "Epoch 9/10, Batch 508/883, Training Loss: 0.5155\n",
      "Epoch 9/10, Batch 509/883, Training Loss: 0.9360\n",
      "Epoch 9/10, Batch 510/883, Training Loss: 0.5974\n",
      "Epoch 9/10, Batch 511/883, Training Loss: 0.3360\n",
      "Epoch 9/10, Batch 512/883, Training Loss: 0.6215\n",
      "Epoch 9/10, Batch 513/883, Training Loss: 0.7455\n",
      "Epoch 9/10, Batch 514/883, Training Loss: 0.6195\n",
      "Epoch 9/10, Batch 515/883, Training Loss: 0.5443\n",
      "Epoch 9/10, Batch 516/883, Training Loss: 0.2648\n",
      "Epoch 9/10, Batch 517/883, Training Loss: 0.4001\n",
      "Epoch 9/10, Batch 518/883, Training Loss: 0.3399\n",
      "Epoch 9/10, Batch 519/883, Training Loss: 0.4710\n",
      "Epoch 9/10, Batch 520/883, Training Loss: 0.9475\n",
      "Epoch 9/10, Batch 521/883, Training Loss: 0.5806\n",
      "Epoch 9/10, Batch 522/883, Training Loss: 0.8505\n",
      "Epoch 9/10, Batch 523/883, Training Loss: 1.0066\n",
      "Epoch 9/10, Batch 524/883, Training Loss: 0.5668\n",
      "Epoch 9/10, Batch 525/883, Training Loss: 0.3357\n",
      "Epoch 9/10, Batch 526/883, Training Loss: 0.3883\n",
      "Epoch 9/10, Batch 527/883, Training Loss: 0.8424\n",
      "Epoch 9/10, Batch 528/883, Training Loss: 0.4974\n",
      "Epoch 9/10, Batch 529/883, Training Loss: 0.3773\n",
      "Epoch 9/10, Batch 530/883, Training Loss: 0.7557\n",
      "Epoch 9/10, Batch 531/883, Training Loss: 0.4371\n",
      "Epoch 9/10, Batch 532/883, Training Loss: 0.4941\n",
      "Epoch 9/10, Batch 533/883, Training Loss: 0.7792\n",
      "Epoch 9/10, Batch 534/883, Training Loss: 0.4573\n",
      "Epoch 9/10, Batch 535/883, Training Loss: 0.3879\n",
      "Epoch 9/10, Batch 536/883, Training Loss: 0.5636\n",
      "Epoch 9/10, Batch 537/883, Training Loss: 0.4290\n",
      "Epoch 9/10, Batch 538/883, Training Loss: 0.2591\n",
      "Epoch 9/10, Batch 539/883, Training Loss: 0.8175\n",
      "Epoch 9/10, Batch 540/883, Training Loss: 0.5769\n",
      "Epoch 9/10, Batch 541/883, Training Loss: 0.6109\n",
      "Epoch 9/10, Batch 542/883, Training Loss: 0.2947\n",
      "Epoch 9/10, Batch 543/883, Training Loss: 0.5780\n",
      "Epoch 9/10, Batch 544/883, Training Loss: 0.3780\n",
      "Epoch 9/10, Batch 545/883, Training Loss: 0.4690\n",
      "Epoch 9/10, Batch 546/883, Training Loss: 0.4312\n",
      "Epoch 9/10, Batch 547/883, Training Loss: 0.5152\n",
      "Epoch 9/10, Batch 548/883, Training Loss: 0.4201\n",
      "Epoch 9/10, Batch 549/883, Training Loss: 0.7370\n",
      "Epoch 9/10, Batch 550/883, Training Loss: 0.5797\n",
      "Epoch 9/10, Batch 551/883, Training Loss: 0.4195\n",
      "Epoch 9/10, Batch 552/883, Training Loss: 0.8319\n",
      "Epoch 9/10, Batch 553/883, Training Loss: 0.3688\n",
      "Epoch 9/10, Batch 554/883, Training Loss: 0.5552\n",
      "Epoch 9/10, Batch 555/883, Training Loss: 0.5606\n",
      "Epoch 9/10, Batch 556/883, Training Loss: 0.7814\n",
      "Epoch 9/10, Batch 557/883, Training Loss: 1.0357\n",
      "Epoch 9/10, Batch 558/883, Training Loss: 1.1770\n",
      "Epoch 9/10, Batch 559/883, Training Loss: 0.5371\n",
      "Epoch 9/10, Batch 560/883, Training Loss: 0.8070\n",
      "Epoch 9/10, Batch 561/883, Training Loss: 0.5175\n",
      "Epoch 9/10, Batch 562/883, Training Loss: 0.2698\n",
      "Epoch 9/10, Batch 563/883, Training Loss: 0.7846\n",
      "Epoch 9/10, Batch 564/883, Training Loss: 0.6146\n",
      "Epoch 9/10, Batch 565/883, Training Loss: 1.0076\n",
      "Epoch 9/10, Batch 566/883, Training Loss: 0.2633\n",
      "Epoch 9/10, Batch 567/883, Training Loss: 0.4474\n",
      "Epoch 9/10, Batch 568/883, Training Loss: 0.7526\n",
      "Epoch 9/10, Batch 569/883, Training Loss: 0.7194\n",
      "Epoch 9/10, Batch 570/883, Training Loss: 0.7899\n",
      "Epoch 9/10, Batch 571/883, Training Loss: 0.6242\n",
      "Epoch 9/10, Batch 572/883, Training Loss: 0.6424\n",
      "Epoch 9/10, Batch 573/883, Training Loss: 0.4708\n",
      "Epoch 9/10, Batch 574/883, Training Loss: 0.9003\n",
      "Epoch 9/10, Batch 575/883, Training Loss: 0.5714\n",
      "Epoch 9/10, Batch 576/883, Training Loss: 1.0302\n",
      "Epoch 9/10, Batch 577/883, Training Loss: 0.6981\n",
      "Epoch 9/10, Batch 578/883, Training Loss: 0.7851\n",
      "Epoch 9/10, Batch 579/883, Training Loss: 0.5170\n",
      "Epoch 9/10, Batch 580/883, Training Loss: 0.4755\n",
      "Epoch 9/10, Batch 581/883, Training Loss: 0.5734\n",
      "Epoch 9/10, Batch 582/883, Training Loss: 0.7327\n",
      "Epoch 9/10, Batch 583/883, Training Loss: 0.4693\n",
      "Epoch 9/10, Batch 584/883, Training Loss: 0.6273\n",
      "Epoch 9/10, Batch 585/883, Training Loss: 0.5544\n",
      "Epoch 9/10, Batch 586/883, Training Loss: 0.3508\n",
      "Epoch 9/10, Batch 587/883, Training Loss: 0.4915\n",
      "Epoch 9/10, Batch 588/883, Training Loss: 0.6328\n",
      "Epoch 9/10, Batch 589/883, Training Loss: 0.4640\n",
      "Epoch 9/10, Batch 590/883, Training Loss: 0.5927\n",
      "Epoch 9/10, Batch 591/883, Training Loss: 0.4532\n",
      "Epoch 9/10, Batch 592/883, Training Loss: 0.7061\n",
      "Epoch 9/10, Batch 593/883, Training Loss: 0.6037\n",
      "Epoch 9/10, Batch 594/883, Training Loss: 0.4553\n",
      "Epoch 9/10, Batch 595/883, Training Loss: 0.6797\n",
      "Epoch 9/10, Batch 596/883, Training Loss: 1.1140\n",
      "Epoch 9/10, Batch 597/883, Training Loss: 0.5015\n",
      "Epoch 9/10, Batch 598/883, Training Loss: 0.9080\n",
      "Epoch 9/10, Batch 599/883, Training Loss: 0.4486\n",
      "Epoch 9/10, Batch 600/883, Training Loss: 0.6017\n",
      "Epoch 9/10, Batch 601/883, Training Loss: 0.9742\n",
      "Epoch 9/10, Batch 602/883, Training Loss: 0.4400\n",
      "Epoch 9/10, Batch 603/883, Training Loss: 0.5196\n",
      "Epoch 9/10, Batch 604/883, Training Loss: 0.4047\n",
      "Epoch 9/10, Batch 605/883, Training Loss: 0.6157\n",
      "Epoch 9/10, Batch 606/883, Training Loss: 0.7161\n",
      "Epoch 9/10, Batch 607/883, Training Loss: 0.7243\n",
      "Epoch 9/10, Batch 608/883, Training Loss: 0.5865\n",
      "Epoch 9/10, Batch 609/883, Training Loss: 0.4092\n",
      "Epoch 9/10, Batch 610/883, Training Loss: 0.4890\n",
      "Epoch 9/10, Batch 611/883, Training Loss: 0.3866\n",
      "Epoch 9/10, Batch 612/883, Training Loss: 0.5832\n",
      "Epoch 9/10, Batch 613/883, Training Loss: 0.4397\n",
      "Epoch 9/10, Batch 614/883, Training Loss: 0.5298\n",
      "Epoch 9/10, Batch 615/883, Training Loss: 0.5446\n",
      "Epoch 9/10, Batch 616/883, Training Loss: 0.3640\n",
      "Epoch 9/10, Batch 617/883, Training Loss: 0.7293\n",
      "Epoch 9/10, Batch 618/883, Training Loss: 0.7355\n",
      "Epoch 9/10, Batch 619/883, Training Loss: 0.5351\n",
      "Epoch 9/10, Batch 620/883, Training Loss: 0.8780\n",
      "Epoch 9/10, Batch 621/883, Training Loss: 0.5818\n",
      "Epoch 9/10, Batch 622/883, Training Loss: 0.5731\n",
      "Epoch 9/10, Batch 623/883, Training Loss: 0.6249\n",
      "Epoch 9/10, Batch 624/883, Training Loss: 0.5893\n",
      "Epoch 9/10, Batch 625/883, Training Loss: 1.0214\n",
      "Epoch 9/10, Batch 626/883, Training Loss: 0.9880\n",
      "Epoch 9/10, Batch 627/883, Training Loss: 0.5158\n",
      "Epoch 9/10, Batch 628/883, Training Loss: 0.4745\n",
      "Epoch 9/10, Batch 629/883, Training Loss: 0.7877\n",
      "Epoch 9/10, Batch 630/883, Training Loss: 0.5112\n",
      "Epoch 9/10, Batch 631/883, Training Loss: 0.6591\n",
      "Epoch 9/10, Batch 632/883, Training Loss: 0.3038\n",
      "Epoch 9/10, Batch 633/883, Training Loss: 0.4749\n",
      "Epoch 9/10, Batch 634/883, Training Loss: 0.5453\n",
      "Epoch 9/10, Batch 635/883, Training Loss: 0.9227\n",
      "Epoch 9/10, Batch 636/883, Training Loss: 0.5324\n",
      "Epoch 9/10, Batch 637/883, Training Loss: 0.4045\n",
      "Epoch 9/10, Batch 638/883, Training Loss: 0.9544\n",
      "Epoch 9/10, Batch 639/883, Training Loss: 0.5551\n",
      "Epoch 9/10, Batch 640/883, Training Loss: 0.6103\n",
      "Epoch 9/10, Batch 641/883, Training Loss: 0.5248\n",
      "Epoch 9/10, Batch 642/883, Training Loss: 0.5912\n",
      "Epoch 9/10, Batch 643/883, Training Loss: 0.6009\n",
      "Epoch 9/10, Batch 644/883, Training Loss: 0.5275\n",
      "Epoch 9/10, Batch 645/883, Training Loss: 0.6293\n",
      "Epoch 9/10, Batch 646/883, Training Loss: 0.6543\n",
      "Epoch 9/10, Batch 647/883, Training Loss: 0.7247\n",
      "Epoch 9/10, Batch 648/883, Training Loss: 0.6406\n",
      "Epoch 9/10, Batch 649/883, Training Loss: 0.9730\n",
      "Epoch 9/10, Batch 650/883, Training Loss: 0.5407\n",
      "Epoch 9/10, Batch 651/883, Training Loss: 0.5454\n",
      "Epoch 9/10, Batch 652/883, Training Loss: 0.4941\n",
      "Epoch 9/10, Batch 653/883, Training Loss: 0.5979\n",
      "Epoch 9/10, Batch 654/883, Training Loss: 0.3982\n",
      "Epoch 9/10, Batch 655/883, Training Loss: 0.3791\n",
      "Epoch 9/10, Batch 656/883, Training Loss: 0.5996\n",
      "Epoch 9/10, Batch 657/883, Training Loss: 0.4425\n",
      "Epoch 9/10, Batch 658/883, Training Loss: 0.6695\n",
      "Epoch 9/10, Batch 659/883, Training Loss: 0.4930\n",
      "Epoch 9/10, Batch 660/883, Training Loss: 0.3933\n",
      "Epoch 9/10, Batch 661/883, Training Loss: 0.4126\n",
      "Epoch 9/10, Batch 662/883, Training Loss: 0.7372\n",
      "Epoch 9/10, Batch 663/883, Training Loss: 0.9199\n",
      "Epoch 9/10, Batch 664/883, Training Loss: 0.2832\n",
      "Epoch 9/10, Batch 665/883, Training Loss: 0.6960\n",
      "Epoch 9/10, Batch 666/883, Training Loss: 0.7413\n",
      "Epoch 9/10, Batch 667/883, Training Loss: 0.5991\n",
      "Epoch 9/10, Batch 668/883, Training Loss: 0.7279\n",
      "Epoch 9/10, Batch 669/883, Training Loss: 0.8503\n",
      "Epoch 9/10, Batch 670/883, Training Loss: 0.6574\n",
      "Epoch 9/10, Batch 671/883, Training Loss: 0.5957\n",
      "Epoch 9/10, Batch 672/883, Training Loss: 0.5980\n",
      "Epoch 9/10, Batch 673/883, Training Loss: 0.6681\n",
      "Epoch 9/10, Batch 674/883, Training Loss: 0.6920\n",
      "Epoch 9/10, Batch 675/883, Training Loss: 0.3028\n",
      "Epoch 9/10, Batch 676/883, Training Loss: 0.6333\n",
      "Epoch 9/10, Batch 677/883, Training Loss: 0.3698\n",
      "Epoch 9/10, Batch 678/883, Training Loss: 0.7758\n",
      "Epoch 9/10, Batch 679/883, Training Loss: 0.5851\n",
      "Epoch 9/10, Batch 680/883, Training Loss: 0.8769\n",
      "Epoch 9/10, Batch 681/883, Training Loss: 0.5858\n",
      "Epoch 9/10, Batch 682/883, Training Loss: 0.3313\n",
      "Epoch 9/10, Batch 683/883, Training Loss: 0.6303\n",
      "Epoch 9/10, Batch 684/883, Training Loss: 0.7988\n",
      "Epoch 9/10, Batch 685/883, Training Loss: 0.6002\n",
      "Epoch 9/10, Batch 686/883, Training Loss: 0.4622\n",
      "Epoch 9/10, Batch 687/883, Training Loss: 0.3779\n",
      "Epoch 9/10, Batch 688/883, Training Loss: 0.6379\n",
      "Epoch 9/10, Batch 689/883, Training Loss: 0.7008\n",
      "Epoch 9/10, Batch 690/883, Training Loss: 0.5732\n",
      "Epoch 9/10, Batch 691/883, Training Loss: 0.4767\n",
      "Epoch 9/10, Batch 692/883, Training Loss: 0.6255\n",
      "Epoch 9/10, Batch 693/883, Training Loss: 0.5674\n",
      "Epoch 9/10, Batch 694/883, Training Loss: 0.4087\n",
      "Epoch 9/10, Batch 695/883, Training Loss: 0.3302\n",
      "Epoch 9/10, Batch 696/883, Training Loss: 0.7209\n",
      "Epoch 9/10, Batch 697/883, Training Loss: 0.4527\n",
      "Epoch 9/10, Batch 698/883, Training Loss: 0.6194\n",
      "Epoch 9/10, Batch 699/883, Training Loss: 0.4397\n",
      "Epoch 9/10, Batch 700/883, Training Loss: 0.3758\n",
      "Epoch 9/10, Batch 701/883, Training Loss: 0.5566\n",
      "Epoch 9/10, Batch 702/883, Training Loss: 0.6172\n",
      "Epoch 9/10, Batch 703/883, Training Loss: 0.7695\n",
      "Epoch 9/10, Batch 704/883, Training Loss: 0.6742\n",
      "Epoch 9/10, Batch 705/883, Training Loss: 0.5791\n",
      "Epoch 9/10, Batch 706/883, Training Loss: 0.4300\n",
      "Epoch 9/10, Batch 707/883, Training Loss: 0.6237\n",
      "Epoch 9/10, Batch 708/883, Training Loss: 0.6373\n",
      "Epoch 9/10, Batch 709/883, Training Loss: 0.5968\n",
      "Epoch 9/10, Batch 710/883, Training Loss: 0.5789\n",
      "Epoch 9/10, Batch 711/883, Training Loss: 0.4494\n",
      "Epoch 9/10, Batch 712/883, Training Loss: 0.7567\n",
      "Epoch 9/10, Batch 713/883, Training Loss: 0.4200\n",
      "Epoch 9/10, Batch 714/883, Training Loss: 0.4443\n",
      "Epoch 9/10, Batch 715/883, Training Loss: 0.4488\n",
      "Epoch 9/10, Batch 716/883, Training Loss: 0.6458\n",
      "Epoch 9/10, Batch 717/883, Training Loss: 0.6883\n",
      "Epoch 9/10, Batch 718/883, Training Loss: 0.4732\n",
      "Epoch 9/10, Batch 719/883, Training Loss: 0.4997\n",
      "Epoch 9/10, Batch 720/883, Training Loss: 0.8972\n",
      "Epoch 9/10, Batch 721/883, Training Loss: 0.5664\n",
      "Epoch 9/10, Batch 722/883, Training Loss: 0.8285\n",
      "Epoch 9/10, Batch 723/883, Training Loss: 0.4284\n",
      "Epoch 9/10, Batch 724/883, Training Loss: 0.7163\n",
      "Epoch 9/10, Batch 725/883, Training Loss: 0.8115\n",
      "Epoch 9/10, Batch 726/883, Training Loss: 0.8012\n",
      "Epoch 9/10, Batch 727/883, Training Loss: 0.3920\n",
      "Epoch 9/10, Batch 728/883, Training Loss: 1.0088\n",
      "Epoch 9/10, Batch 729/883, Training Loss: 0.8352\n",
      "Epoch 9/10, Batch 730/883, Training Loss: 0.5944\n",
      "Epoch 9/10, Batch 731/883, Training Loss: 0.3924\n",
      "Epoch 9/10, Batch 732/883, Training Loss: 0.6071\n",
      "Epoch 9/10, Batch 733/883, Training Loss: 0.6117\n",
      "Epoch 9/10, Batch 734/883, Training Loss: 0.5528\n",
      "Epoch 9/10, Batch 735/883, Training Loss: 0.4360\n",
      "Epoch 9/10, Batch 736/883, Training Loss: 0.3609\n",
      "Epoch 9/10, Batch 737/883, Training Loss: 0.5007\n",
      "Epoch 9/10, Batch 738/883, Training Loss: 0.8719\n",
      "Epoch 9/10, Batch 739/883, Training Loss: 0.3222\n",
      "Epoch 9/10, Batch 740/883, Training Loss: 1.0175\n",
      "Epoch 9/10, Batch 741/883, Training Loss: 0.3599\n",
      "Epoch 9/10, Batch 742/883, Training Loss: 0.6223\n",
      "Epoch 9/10, Batch 743/883, Training Loss: 0.4520\n",
      "Epoch 9/10, Batch 744/883, Training Loss: 0.4553\n",
      "Epoch 9/10, Batch 745/883, Training Loss: 0.5920\n",
      "Epoch 9/10, Batch 746/883, Training Loss: 0.4594\n",
      "Epoch 9/10, Batch 747/883, Training Loss: 0.5986\n",
      "Epoch 9/10, Batch 748/883, Training Loss: 0.9490\n",
      "Epoch 9/10, Batch 749/883, Training Loss: 0.8704\n",
      "Epoch 9/10, Batch 750/883, Training Loss: 0.5251\n",
      "Epoch 9/10, Batch 751/883, Training Loss: 0.3910\n",
      "Epoch 9/10, Batch 752/883, Training Loss: 0.6105\n",
      "Epoch 9/10, Batch 753/883, Training Loss: 0.5795\n",
      "Epoch 9/10, Batch 754/883, Training Loss: 0.7962\n",
      "Epoch 9/10, Batch 755/883, Training Loss: 0.6275\n",
      "Epoch 9/10, Batch 756/883, Training Loss: 0.4179\n",
      "Epoch 9/10, Batch 757/883, Training Loss: 0.5388\n",
      "Epoch 9/10, Batch 758/883, Training Loss: 0.4228\n",
      "Epoch 9/10, Batch 759/883, Training Loss: 0.7484\n",
      "Epoch 9/10, Batch 760/883, Training Loss: 0.2796\n",
      "Epoch 9/10, Batch 761/883, Training Loss: 0.5832\n",
      "Epoch 9/10, Batch 762/883, Training Loss: 0.4011\n",
      "Epoch 9/10, Batch 763/883, Training Loss: 0.6220\n",
      "Epoch 9/10, Batch 764/883, Training Loss: 0.5275\n",
      "Epoch 9/10, Batch 765/883, Training Loss: 0.7786\n",
      "Epoch 9/10, Batch 766/883, Training Loss: 0.5184\n",
      "Epoch 9/10, Batch 767/883, Training Loss: 0.5634\n",
      "Epoch 9/10, Batch 768/883, Training Loss: 0.6700\n",
      "Epoch 9/10, Batch 769/883, Training Loss: 0.4786\n",
      "Epoch 9/10, Batch 770/883, Training Loss: 0.8945\n",
      "Epoch 9/10, Batch 771/883, Training Loss: 0.3961\n",
      "Epoch 9/10, Batch 772/883, Training Loss: 0.6386\n",
      "Epoch 9/10, Batch 773/883, Training Loss: 0.5278\n",
      "Epoch 9/10, Batch 774/883, Training Loss: 0.6312\n",
      "Epoch 9/10, Batch 775/883, Training Loss: 0.5484\n",
      "Epoch 9/10, Batch 776/883, Training Loss: 0.4063\n",
      "Epoch 9/10, Batch 777/883, Training Loss: 0.3392\n",
      "Epoch 9/10, Batch 778/883, Training Loss: 0.5037\n",
      "Epoch 9/10, Batch 779/883, Training Loss: 0.3295\n",
      "Epoch 9/10, Batch 780/883, Training Loss: 0.3277\n",
      "Epoch 9/10, Batch 781/883, Training Loss: 0.5344\n",
      "Epoch 9/10, Batch 782/883, Training Loss: 0.6977\n",
      "Epoch 9/10, Batch 783/883, Training Loss: 0.5485\n",
      "Epoch 9/10, Batch 784/883, Training Loss: 0.6822\n",
      "Epoch 9/10, Batch 785/883, Training Loss: 0.5904\n",
      "Epoch 9/10, Batch 786/883, Training Loss: 0.3987\n",
      "Epoch 9/10, Batch 787/883, Training Loss: 0.8477\n",
      "Epoch 9/10, Batch 788/883, Training Loss: 0.7011\n",
      "Epoch 9/10, Batch 789/883, Training Loss: 0.3850\n",
      "Epoch 9/10, Batch 790/883, Training Loss: 0.5936\n",
      "Epoch 9/10, Batch 791/883, Training Loss: 0.4731\n",
      "Epoch 9/10, Batch 792/883, Training Loss: 0.2868\n",
      "Epoch 9/10, Batch 793/883, Training Loss: 0.5230\n",
      "Epoch 9/10, Batch 794/883, Training Loss: 0.4477\n",
      "Epoch 9/10, Batch 795/883, Training Loss: 0.5804\n",
      "Epoch 9/10, Batch 796/883, Training Loss: 0.8663\n",
      "Epoch 9/10, Batch 797/883, Training Loss: 0.5835\n",
      "Epoch 9/10, Batch 798/883, Training Loss: 0.8857\n",
      "Epoch 9/10, Batch 799/883, Training Loss: 0.5269\n",
      "Epoch 9/10, Batch 800/883, Training Loss: 0.6724\n",
      "Epoch 9/10, Batch 801/883, Training Loss: 0.4916\n",
      "Epoch 9/10, Batch 802/883, Training Loss: 0.8130\n",
      "Epoch 9/10, Batch 803/883, Training Loss: 0.5185\n",
      "Epoch 9/10, Batch 804/883, Training Loss: 0.4860\n",
      "Epoch 9/10, Batch 805/883, Training Loss: 0.3837\n",
      "Epoch 9/10, Batch 806/883, Training Loss: 0.6577\n",
      "Epoch 9/10, Batch 807/883, Training Loss: 0.5665\n",
      "Epoch 9/10, Batch 808/883, Training Loss: 0.6875\n",
      "Epoch 9/10, Batch 809/883, Training Loss: 0.8937\n",
      "Epoch 9/10, Batch 810/883, Training Loss: 0.4354\n",
      "Epoch 9/10, Batch 811/883, Training Loss: 0.5265\n",
      "Epoch 9/10, Batch 812/883, Training Loss: 0.7377\n",
      "Epoch 9/10, Batch 813/883, Training Loss: 0.4423\n",
      "Epoch 9/10, Batch 814/883, Training Loss: 0.5031\n",
      "Epoch 9/10, Batch 815/883, Training Loss: 0.5203\n",
      "Epoch 9/10, Batch 816/883, Training Loss: 0.4917\n",
      "Epoch 9/10, Batch 817/883, Training Loss: 0.4262\n",
      "Epoch 9/10, Batch 818/883, Training Loss: 0.4743\n",
      "Epoch 9/10, Batch 819/883, Training Loss: 1.0761\n",
      "Epoch 9/10, Batch 820/883, Training Loss: 0.4769\n",
      "Epoch 9/10, Batch 821/883, Training Loss: 1.0472\n",
      "Epoch 9/10, Batch 822/883, Training Loss: 0.6374\n",
      "Epoch 9/10, Batch 823/883, Training Loss: 0.5926\n",
      "Epoch 9/10, Batch 824/883, Training Loss: 0.2881\n",
      "Epoch 9/10, Batch 825/883, Training Loss: 0.6305\n",
      "Epoch 9/10, Batch 826/883, Training Loss: 0.5809\n",
      "Epoch 9/10, Batch 827/883, Training Loss: 0.4856\n",
      "Epoch 9/10, Batch 828/883, Training Loss: 0.7638\n",
      "Epoch 9/10, Batch 829/883, Training Loss: 0.4993\n",
      "Epoch 9/10, Batch 830/883, Training Loss: 0.6431\n",
      "Epoch 9/10, Batch 831/883, Training Loss: 0.7674\n",
      "Epoch 9/10, Batch 832/883, Training Loss: 0.5664\n",
      "Epoch 9/10, Batch 833/883, Training Loss: 0.6309\n",
      "Epoch 9/10, Batch 834/883, Training Loss: 0.8614\n",
      "Epoch 9/10, Batch 835/883, Training Loss: 0.4702\n",
      "Epoch 9/10, Batch 836/883, Training Loss: 0.4753\n",
      "Epoch 9/10, Batch 837/883, Training Loss: 0.5817\n",
      "Epoch 9/10, Batch 838/883, Training Loss: 0.5215\n",
      "Epoch 9/10, Batch 839/883, Training Loss: 0.6146\n",
      "Epoch 9/10, Batch 840/883, Training Loss: 0.4495\n",
      "Epoch 9/10, Batch 841/883, Training Loss: 0.6122\n",
      "Epoch 9/10, Batch 842/883, Training Loss: 0.6308\n",
      "Epoch 9/10, Batch 843/883, Training Loss: 0.6962\n",
      "Epoch 9/10, Batch 844/883, Training Loss: 0.5299\n",
      "Epoch 9/10, Batch 845/883, Training Loss: 0.5587\n",
      "Epoch 9/10, Batch 846/883, Training Loss: 0.5478\n",
      "Epoch 9/10, Batch 847/883, Training Loss: 0.5571\n",
      "Epoch 9/10, Batch 848/883, Training Loss: 0.5256\n",
      "Epoch 9/10, Batch 849/883, Training Loss: 0.4364\n",
      "Epoch 9/10, Batch 850/883, Training Loss: 0.4133\n",
      "Epoch 9/10, Batch 851/883, Training Loss: 0.4575\n",
      "Epoch 9/10, Batch 852/883, Training Loss: 0.4208\n",
      "Epoch 9/10, Batch 853/883, Training Loss: 0.6532\n",
      "Epoch 9/10, Batch 854/883, Training Loss: 0.5293\n",
      "Epoch 9/10, Batch 855/883, Training Loss: 0.3993\n",
      "Epoch 9/10, Batch 856/883, Training Loss: 0.8331\n",
      "Epoch 9/10, Batch 857/883, Training Loss: 0.7395\n",
      "Epoch 9/10, Batch 858/883, Training Loss: 0.5195\n",
      "Epoch 9/10, Batch 859/883, Training Loss: 0.6498\n",
      "Epoch 9/10, Batch 860/883, Training Loss: 0.5551\n",
      "Epoch 9/10, Batch 861/883, Training Loss: 0.5569\n",
      "Epoch 9/10, Batch 862/883, Training Loss: 0.5497\n",
      "Epoch 9/10, Batch 863/883, Training Loss: 0.6730\n",
      "Epoch 9/10, Batch 864/883, Training Loss: 0.4929\n",
      "Epoch 9/10, Batch 865/883, Training Loss: 0.6260\n",
      "Epoch 9/10, Batch 866/883, Training Loss: 0.5963\n",
      "Epoch 9/10, Batch 867/883, Training Loss: 0.6587\n",
      "Epoch 9/10, Batch 868/883, Training Loss: 0.3214\n",
      "Epoch 9/10, Batch 869/883, Training Loss: 0.5948\n",
      "Epoch 9/10, Batch 870/883, Training Loss: 0.7235\n",
      "Epoch 9/10, Batch 871/883, Training Loss: 0.5681\n",
      "Epoch 9/10, Batch 872/883, Training Loss: 0.5133\n",
      "Epoch 9/10, Batch 873/883, Training Loss: 0.5211\n",
      "Epoch 9/10, Batch 874/883, Training Loss: 0.8064\n",
      "Epoch 9/10, Batch 875/883, Training Loss: 0.6154\n",
      "Epoch 9/10, Batch 876/883, Training Loss: 0.6659\n",
      "Epoch 9/10, Batch 877/883, Training Loss: 0.5476\n",
      "Epoch 9/10, Batch 878/883, Training Loss: 0.6021\n",
      "Epoch 9/10, Batch 879/883, Training Loss: 0.5399\n",
      "Epoch 9/10, Batch 880/883, Training Loss: 0.5827\n",
      "Epoch 9/10, Batch 881/883, Training Loss: 0.8807\n",
      "Epoch 9/10, Batch 882/883, Training Loss: 0.5403\n",
      "Epoch 9/10, Batch 883/883, Training Loss: 0.2738\n",
      "Epoch 9/10, Training Loss: 0.5900, Validation Loss: 0.6287, Validation Accuracy: 0.7231\n",
      "Epoch 10/10, Batch 1/883, Training Loss: 0.6240\n",
      "Epoch 10/10, Batch 2/883, Training Loss: 0.4735\n",
      "Epoch 10/10, Batch 3/883, Training Loss: 0.2241\n",
      "Epoch 10/10, Batch 4/883, Training Loss: 0.6712\n",
      "Epoch 10/10, Batch 5/883, Training Loss: 1.0383\n",
      "Epoch 10/10, Batch 6/883, Training Loss: 0.7660\n",
      "Epoch 10/10, Batch 7/883, Training Loss: 0.3729\n",
      "Epoch 10/10, Batch 8/883, Training Loss: 0.4360\n",
      "Epoch 10/10, Batch 9/883, Training Loss: 0.4184\n",
      "Epoch 10/10, Batch 10/883, Training Loss: 0.5972\n",
      "Epoch 10/10, Batch 11/883, Training Loss: 0.7105\n",
      "Epoch 10/10, Batch 12/883, Training Loss: 0.3054\n",
      "Epoch 10/10, Batch 13/883, Training Loss: 0.3728\n",
      "Epoch 10/10, Batch 14/883, Training Loss: 0.4981\n",
      "Epoch 10/10, Batch 15/883, Training Loss: 0.3469\n",
      "Epoch 10/10, Batch 16/883, Training Loss: 0.7559\n",
      "Epoch 10/10, Batch 17/883, Training Loss: 0.5314\n",
      "Epoch 10/10, Batch 18/883, Training Loss: 0.5823\n",
      "Epoch 10/10, Batch 19/883, Training Loss: 0.6707\n",
      "Epoch 10/10, Batch 20/883, Training Loss: 0.5794\n",
      "Epoch 10/10, Batch 21/883, Training Loss: 0.2938\n",
      "Epoch 10/10, Batch 22/883, Training Loss: 0.3439\n",
      "Epoch 10/10, Batch 23/883, Training Loss: 0.3645\n",
      "Epoch 10/10, Batch 24/883, Training Loss: 0.4282\n",
      "Epoch 10/10, Batch 25/883, Training Loss: 0.3655\n",
      "Epoch 10/10, Batch 26/883, Training Loss: 0.7874\n",
      "Epoch 10/10, Batch 27/883, Training Loss: 0.9791\n",
      "Epoch 10/10, Batch 28/883, Training Loss: 0.3328\n",
      "Epoch 10/10, Batch 29/883, Training Loss: 0.3341\n",
      "Epoch 10/10, Batch 30/883, Training Loss: 0.5028\n",
      "Epoch 10/10, Batch 31/883, Training Loss: 0.7525\n",
      "Epoch 10/10, Batch 32/883, Training Loss: 0.3341\n",
      "Epoch 10/10, Batch 33/883, Training Loss: 0.3502\n",
      "Epoch 10/10, Batch 34/883, Training Loss: 0.5901\n",
      "Epoch 10/10, Batch 35/883, Training Loss: 1.0314\n",
      "Epoch 10/10, Batch 36/883, Training Loss: 0.7518\n",
      "Epoch 10/10, Batch 37/883, Training Loss: 0.5358\n",
      "Epoch 10/10, Batch 38/883, Training Loss: 0.5130\n",
      "Epoch 10/10, Batch 39/883, Training Loss: 0.8251\n",
      "Epoch 10/10, Batch 40/883, Training Loss: 0.6608\n",
      "Epoch 10/10, Batch 41/883, Training Loss: 0.8492\n",
      "Epoch 10/10, Batch 42/883, Training Loss: 0.6008\n",
      "Epoch 10/10, Batch 43/883, Training Loss: 0.8441\n",
      "Epoch 10/10, Batch 44/883, Training Loss: 0.5850\n",
      "Epoch 10/10, Batch 45/883, Training Loss: 0.5046\n",
      "Epoch 10/10, Batch 46/883, Training Loss: 0.9936\n",
      "Epoch 10/10, Batch 47/883, Training Loss: 0.3160\n",
      "Epoch 10/10, Batch 48/883, Training Loss: 0.4496\n",
      "Epoch 10/10, Batch 49/883, Training Loss: 0.6704\n",
      "Epoch 10/10, Batch 50/883, Training Loss: 0.3850\n",
      "Epoch 10/10, Batch 51/883, Training Loss: 0.4674\n",
      "Epoch 10/10, Batch 52/883, Training Loss: 0.6108\n",
      "Epoch 10/10, Batch 53/883, Training Loss: 0.4979\n",
      "Epoch 10/10, Batch 54/883, Training Loss: 0.6087\n",
      "Epoch 10/10, Batch 55/883, Training Loss: 0.3773\n",
      "Epoch 10/10, Batch 56/883, Training Loss: 0.6621\n",
      "Epoch 10/10, Batch 57/883, Training Loss: 0.6906\n",
      "Epoch 10/10, Batch 58/883, Training Loss: 0.4545\n",
      "Epoch 10/10, Batch 59/883, Training Loss: 0.4562\n",
      "Epoch 10/10, Batch 60/883, Training Loss: 0.5311\n",
      "Epoch 10/10, Batch 61/883, Training Loss: 0.6384\n",
      "Epoch 10/10, Batch 62/883, Training Loss: 0.6775\n",
      "Epoch 10/10, Batch 63/883, Training Loss: 0.5486\n",
      "Epoch 10/10, Batch 64/883, Training Loss: 1.1085\n",
      "Epoch 10/10, Batch 65/883, Training Loss: 0.4527\n",
      "Epoch 10/10, Batch 66/883, Training Loss: 0.3812\n",
      "Epoch 10/10, Batch 67/883, Training Loss: 0.5349\n",
      "Epoch 10/10, Batch 68/883, Training Loss: 0.7439\n",
      "Epoch 10/10, Batch 69/883, Training Loss: 0.3744\n",
      "Epoch 10/10, Batch 70/883, Training Loss: 0.4045\n",
      "Epoch 10/10, Batch 71/883, Training Loss: 0.6542\n",
      "Epoch 10/10, Batch 72/883, Training Loss: 1.0114\n",
      "Epoch 10/10, Batch 73/883, Training Loss: 0.3830\n",
      "Epoch 10/10, Batch 74/883, Training Loss: 0.4723\n",
      "Epoch 10/10, Batch 75/883, Training Loss: 0.8818\n",
      "Epoch 10/10, Batch 76/883, Training Loss: 0.6051\n",
      "Epoch 10/10, Batch 77/883, Training Loss: 0.3969\n",
      "Epoch 10/10, Batch 78/883, Training Loss: 0.9818\n",
      "Epoch 10/10, Batch 79/883, Training Loss: 0.5339\n",
      "Epoch 10/10, Batch 80/883, Training Loss: 0.5491\n",
      "Epoch 10/10, Batch 81/883, Training Loss: 0.6062\n",
      "Epoch 10/10, Batch 82/883, Training Loss: 0.5369\n",
      "Epoch 10/10, Batch 83/883, Training Loss: 0.5555\n",
      "Epoch 10/10, Batch 84/883, Training Loss: 0.5357\n",
      "Epoch 10/10, Batch 85/883, Training Loss: 0.6862\n",
      "Epoch 10/10, Batch 86/883, Training Loss: 0.4780\n",
      "Epoch 10/10, Batch 87/883, Training Loss: 0.3595\n",
      "Epoch 10/10, Batch 88/883, Training Loss: 0.3333\n",
      "Epoch 10/10, Batch 89/883, Training Loss: 0.4589\n",
      "Epoch 10/10, Batch 90/883, Training Loss: 0.5397\n",
      "Epoch 10/10, Batch 91/883, Training Loss: 0.7051\n",
      "Epoch 10/10, Batch 92/883, Training Loss: 0.4619\n",
      "Epoch 10/10, Batch 93/883, Training Loss: 0.5735\n",
      "Epoch 10/10, Batch 94/883, Training Loss: 0.5457\n",
      "Epoch 10/10, Batch 95/883, Training Loss: 0.6686\n",
      "Epoch 10/10, Batch 96/883, Training Loss: 0.4108\n",
      "Epoch 10/10, Batch 97/883, Training Loss: 0.6979\n",
      "Epoch 10/10, Batch 98/883, Training Loss: 0.9157\n",
      "Epoch 10/10, Batch 99/883, Training Loss: 0.7073\n",
      "Epoch 10/10, Batch 100/883, Training Loss: 0.7539\n",
      "Epoch 10/10, Batch 101/883, Training Loss: 0.5726\n",
      "Epoch 10/10, Batch 102/883, Training Loss: 0.3210\n",
      "Epoch 10/10, Batch 103/883, Training Loss: 0.5920\n",
      "Epoch 10/10, Batch 104/883, Training Loss: 0.8051\n",
      "Epoch 10/10, Batch 105/883, Training Loss: 0.4490\n",
      "Epoch 10/10, Batch 106/883, Training Loss: 0.3193\n",
      "Epoch 10/10, Batch 107/883, Training Loss: 0.5502\n",
      "Epoch 10/10, Batch 108/883, Training Loss: 0.7785\n",
      "Epoch 10/10, Batch 109/883, Training Loss: 0.6921\n",
      "Epoch 10/10, Batch 110/883, Training Loss: 0.4038\n",
      "Epoch 10/10, Batch 111/883, Training Loss: 0.7942\n",
      "Epoch 10/10, Batch 112/883, Training Loss: 0.3377\n",
      "Epoch 10/10, Batch 113/883, Training Loss: 0.5740\n",
      "Epoch 10/10, Batch 114/883, Training Loss: 0.6541\n",
      "Epoch 10/10, Batch 115/883, Training Loss: 0.3057\n",
      "Epoch 10/10, Batch 116/883, Training Loss: 0.7962\n",
      "Epoch 10/10, Batch 117/883, Training Loss: 0.3908\n",
      "Epoch 10/10, Batch 118/883, Training Loss: 0.5764\n",
      "Epoch 10/10, Batch 119/883, Training Loss: 0.4056\n",
      "Epoch 10/10, Batch 120/883, Training Loss: 0.4304\n",
      "Epoch 10/10, Batch 121/883, Training Loss: 0.5083\n",
      "Epoch 10/10, Batch 122/883, Training Loss: 0.3738\n",
      "Epoch 10/10, Batch 123/883, Training Loss: 0.3417\n",
      "Epoch 10/10, Batch 124/883, Training Loss: 0.4090\n",
      "Epoch 10/10, Batch 125/883, Training Loss: 0.6893\n",
      "Epoch 10/10, Batch 126/883, Training Loss: 0.4172\n",
      "Epoch 10/10, Batch 127/883, Training Loss: 0.7535\n",
      "Epoch 10/10, Batch 128/883, Training Loss: 0.7560\n",
      "Epoch 10/10, Batch 129/883, Training Loss: 0.3090\n",
      "Epoch 10/10, Batch 130/883, Training Loss: 0.5878\n",
      "Epoch 10/10, Batch 131/883, Training Loss: 0.3059\n",
      "Epoch 10/10, Batch 132/883, Training Loss: 0.4494\n",
      "Epoch 10/10, Batch 133/883, Training Loss: 0.4681\n",
      "Epoch 10/10, Batch 134/883, Training Loss: 0.9895\n",
      "Epoch 10/10, Batch 135/883, Training Loss: 0.6166\n",
      "Epoch 10/10, Batch 136/883, Training Loss: 0.9257\n",
      "Epoch 10/10, Batch 137/883, Training Loss: 0.6441\n",
      "Epoch 10/10, Batch 138/883, Training Loss: 0.7610\n",
      "Epoch 10/10, Batch 139/883, Training Loss: 0.4132\n",
      "Epoch 10/10, Batch 140/883, Training Loss: 0.6923\n",
      "Epoch 10/10, Batch 141/883, Training Loss: 0.4372\n",
      "Epoch 10/10, Batch 142/883, Training Loss: 0.5805\n",
      "Epoch 10/10, Batch 143/883, Training Loss: 0.8757\n",
      "Epoch 10/10, Batch 144/883, Training Loss: 0.4830\n",
      "Epoch 10/10, Batch 145/883, Training Loss: 0.7331\n",
      "Epoch 10/10, Batch 146/883, Training Loss: 0.4488\n",
      "Epoch 10/10, Batch 147/883, Training Loss: 0.7804\n",
      "Epoch 10/10, Batch 148/883, Training Loss: 0.4218\n",
      "Epoch 10/10, Batch 149/883, Training Loss: 0.6026\n",
      "Epoch 10/10, Batch 150/883, Training Loss: 0.2711\n",
      "Epoch 10/10, Batch 151/883, Training Loss: 0.5569\n",
      "Epoch 10/10, Batch 152/883, Training Loss: 0.4819\n",
      "Epoch 10/10, Batch 153/883, Training Loss: 0.6330\n",
      "Epoch 10/10, Batch 154/883, Training Loss: 0.2952\n",
      "Epoch 10/10, Batch 155/883, Training Loss: 0.6366\n",
      "Epoch 10/10, Batch 156/883, Training Loss: 0.3596\n",
      "Epoch 10/10, Batch 157/883, Training Loss: 0.4480\n",
      "Epoch 10/10, Batch 158/883, Training Loss: 0.5328\n",
      "Epoch 10/10, Batch 159/883, Training Loss: 0.7224\n",
      "Epoch 10/10, Batch 160/883, Training Loss: 0.5116\n",
      "Epoch 10/10, Batch 161/883, Training Loss: 0.3272\n",
      "Epoch 10/10, Batch 162/883, Training Loss: 0.3804\n",
      "Epoch 10/10, Batch 163/883, Training Loss: 0.6879\n",
      "Epoch 10/10, Batch 164/883, Training Loss: 0.3432\n",
      "Epoch 10/10, Batch 165/883, Training Loss: 0.6134\n",
      "Epoch 10/10, Batch 166/883, Training Loss: 0.6492\n",
      "Epoch 10/10, Batch 167/883, Training Loss: 0.5416\n",
      "Epoch 10/10, Batch 168/883, Training Loss: 0.7012\n",
      "Epoch 10/10, Batch 169/883, Training Loss: 0.5183\n",
      "Epoch 10/10, Batch 170/883, Training Loss: 1.0603\n",
      "Epoch 10/10, Batch 171/883, Training Loss: 0.8786\n",
      "Epoch 10/10, Batch 172/883, Training Loss: 0.3757\n",
      "Epoch 10/10, Batch 173/883, Training Loss: 0.3951\n",
      "Epoch 10/10, Batch 174/883, Training Loss: 0.6112\n",
      "Epoch 10/10, Batch 175/883, Training Loss: 0.6030\n",
      "Epoch 10/10, Batch 176/883, Training Loss: 0.5545\n",
      "Epoch 10/10, Batch 177/883, Training Loss: 0.6217\n",
      "Epoch 10/10, Batch 178/883, Training Loss: 0.9193\n",
      "Epoch 10/10, Batch 179/883, Training Loss: 0.5266\n",
      "Epoch 10/10, Batch 180/883, Training Loss: 0.5750\n",
      "Epoch 10/10, Batch 181/883, Training Loss: 0.9063\n",
      "Epoch 10/10, Batch 182/883, Training Loss: 0.7849\n",
      "Epoch 10/10, Batch 183/883, Training Loss: 0.4699\n",
      "Epoch 10/10, Batch 184/883, Training Loss: 0.4376\n",
      "Epoch 10/10, Batch 185/883, Training Loss: 0.4344\n",
      "Epoch 10/10, Batch 186/883, Training Loss: 0.3895\n",
      "Epoch 10/10, Batch 187/883, Training Loss: 0.5619\n",
      "Epoch 10/10, Batch 188/883, Training Loss: 0.3228\n",
      "Epoch 10/10, Batch 189/883, Training Loss: 0.5420\n",
      "Epoch 10/10, Batch 190/883, Training Loss: 0.4588\n",
      "Epoch 10/10, Batch 191/883, Training Loss: 0.7016\n",
      "Epoch 10/10, Batch 192/883, Training Loss: 0.4148\n",
      "Epoch 10/10, Batch 193/883, Training Loss: 1.0204\n",
      "Epoch 10/10, Batch 194/883, Training Loss: 0.3748\n",
      "Epoch 10/10, Batch 195/883, Training Loss: 0.7856\n",
      "Epoch 10/10, Batch 196/883, Training Loss: 0.6861\n",
      "Epoch 10/10, Batch 197/883, Training Loss: 0.5473\n",
      "Epoch 10/10, Batch 198/883, Training Loss: 0.4082\n",
      "Epoch 10/10, Batch 199/883, Training Loss: 0.8828\n",
      "Epoch 10/10, Batch 200/883, Training Loss: 0.4734\n",
      "Epoch 10/10, Batch 201/883, Training Loss: 0.5436\n",
      "Epoch 10/10, Batch 202/883, Training Loss: 0.5708\n",
      "Epoch 10/10, Batch 203/883, Training Loss: 0.5544\n",
      "Epoch 10/10, Batch 204/883, Training Loss: 0.4208\n",
      "Epoch 10/10, Batch 205/883, Training Loss: 0.8018\n",
      "Epoch 10/10, Batch 206/883, Training Loss: 0.4065\n",
      "Epoch 10/10, Batch 207/883, Training Loss: 0.6679\n",
      "Epoch 10/10, Batch 208/883, Training Loss: 0.3236\n",
      "Epoch 10/10, Batch 209/883, Training Loss: 0.6684\n",
      "Epoch 10/10, Batch 210/883, Training Loss: 0.4536\n",
      "Epoch 10/10, Batch 211/883, Training Loss: 0.5684\n",
      "Epoch 10/10, Batch 212/883, Training Loss: 0.7308\n",
      "Epoch 10/10, Batch 213/883, Training Loss: 0.2888\n",
      "Epoch 10/10, Batch 214/883, Training Loss: 0.5289\n",
      "Epoch 10/10, Batch 215/883, Training Loss: 0.4907\n",
      "Epoch 10/10, Batch 216/883, Training Loss: 0.5897\n",
      "Epoch 10/10, Batch 217/883, Training Loss: 0.7474\n",
      "Epoch 10/10, Batch 218/883, Training Loss: 0.5321\n",
      "Epoch 10/10, Batch 219/883, Training Loss: 0.7247\n",
      "Epoch 10/10, Batch 220/883, Training Loss: 0.3181\n",
      "Epoch 10/10, Batch 221/883, Training Loss: 0.4009\n",
      "Epoch 10/10, Batch 222/883, Training Loss: 0.4236\n",
      "Epoch 10/10, Batch 223/883, Training Loss: 0.5563\n",
      "Epoch 10/10, Batch 224/883, Training Loss: 0.2307\n",
      "Epoch 10/10, Batch 225/883, Training Loss: 0.5722\n",
      "Epoch 10/10, Batch 226/883, Training Loss: 0.5306\n",
      "Epoch 10/10, Batch 227/883, Training Loss: 0.6988\n",
      "Epoch 10/10, Batch 228/883, Training Loss: 0.5859\n",
      "Epoch 10/10, Batch 229/883, Training Loss: 0.4230\n",
      "Epoch 10/10, Batch 230/883, Training Loss: 0.7917\n",
      "Epoch 10/10, Batch 231/883, Training Loss: 0.7911\n",
      "Epoch 10/10, Batch 232/883, Training Loss: 0.3446\n",
      "Epoch 10/10, Batch 233/883, Training Loss: 0.4573\n",
      "Epoch 10/10, Batch 234/883, Training Loss: 0.3327\n",
      "Epoch 10/10, Batch 235/883, Training Loss: 0.5163\n",
      "Epoch 10/10, Batch 236/883, Training Loss: 0.4918\n",
      "Epoch 10/10, Batch 237/883, Training Loss: 0.3978\n",
      "Epoch 10/10, Batch 238/883, Training Loss: 0.7800\n",
      "Epoch 10/10, Batch 239/883, Training Loss: 0.5179\n",
      "Epoch 10/10, Batch 240/883, Training Loss: 0.4316\n",
      "Epoch 10/10, Batch 241/883, Training Loss: 0.5343\n",
      "Epoch 10/10, Batch 242/883, Training Loss: 0.4583\n",
      "Epoch 10/10, Batch 243/883, Training Loss: 0.3936\n",
      "Epoch 10/10, Batch 244/883, Training Loss: 0.5257\n",
      "Epoch 10/10, Batch 245/883, Training Loss: 0.4103\n",
      "Epoch 10/10, Batch 246/883, Training Loss: 0.6660\n",
      "Epoch 10/10, Batch 247/883, Training Loss: 0.5818\n",
      "Epoch 10/10, Batch 248/883, Training Loss: 0.5227\n",
      "Epoch 10/10, Batch 249/883, Training Loss: 0.3392\n",
      "Epoch 10/10, Batch 250/883, Training Loss: 0.5823\n",
      "Epoch 10/10, Batch 251/883, Training Loss: 0.4556\n",
      "Epoch 10/10, Batch 252/883, Training Loss: 0.7252\n",
      "Epoch 10/10, Batch 253/883, Training Loss: 0.4026\n",
      "Epoch 10/10, Batch 254/883, Training Loss: 0.4778\n",
      "Epoch 10/10, Batch 255/883, Training Loss: 0.4410\n",
      "Epoch 10/10, Batch 256/883, Training Loss: 0.3611\n",
      "Epoch 10/10, Batch 257/883, Training Loss: 0.7339\n",
      "Epoch 10/10, Batch 258/883, Training Loss: 0.4829\n",
      "Epoch 10/10, Batch 259/883, Training Loss: 0.3768\n",
      "Epoch 10/10, Batch 260/883, Training Loss: 0.4485\n",
      "Epoch 10/10, Batch 261/883, Training Loss: 0.9187\n",
      "Epoch 10/10, Batch 262/883, Training Loss: 0.4133\n",
      "Epoch 10/10, Batch 263/883, Training Loss: 0.4734\n",
      "Epoch 10/10, Batch 264/883, Training Loss: 0.4909\n",
      "Epoch 10/10, Batch 265/883, Training Loss: 0.7438\n",
      "Epoch 10/10, Batch 266/883, Training Loss: 0.6287\n",
      "Epoch 10/10, Batch 267/883, Training Loss: 0.3102\n",
      "Epoch 10/10, Batch 268/883, Training Loss: 0.4300\n",
      "Epoch 10/10, Batch 269/883, Training Loss: 1.2345\n",
      "Epoch 10/10, Batch 270/883, Training Loss: 0.4912\n",
      "Epoch 10/10, Batch 271/883, Training Loss: 0.5186\n",
      "Epoch 10/10, Batch 272/883, Training Loss: 0.2896\n",
      "Epoch 10/10, Batch 273/883, Training Loss: 0.3075\n",
      "Epoch 10/10, Batch 274/883, Training Loss: 0.4969\n",
      "Epoch 10/10, Batch 275/883, Training Loss: 0.3723\n",
      "Epoch 10/10, Batch 276/883, Training Loss: 0.4941\n",
      "Epoch 10/10, Batch 277/883, Training Loss: 0.6559\n",
      "Epoch 10/10, Batch 278/883, Training Loss: 0.6214\n",
      "Epoch 10/10, Batch 279/883, Training Loss: 0.4864\n",
      "Epoch 10/10, Batch 280/883, Training Loss: 0.3490\n",
      "Epoch 10/10, Batch 281/883, Training Loss: 0.4930\n",
      "Epoch 10/10, Batch 282/883, Training Loss: 0.6113\n",
      "Epoch 10/10, Batch 283/883, Training Loss: 0.8208\n",
      "Epoch 10/10, Batch 284/883, Training Loss: 0.6140\n",
      "Epoch 10/10, Batch 285/883, Training Loss: 0.4776\n",
      "Epoch 10/10, Batch 286/883, Training Loss: 0.7100\n",
      "Epoch 10/10, Batch 287/883, Training Loss: 0.6046\n",
      "Epoch 10/10, Batch 288/883, Training Loss: 0.5261\n",
      "Epoch 10/10, Batch 289/883, Training Loss: 0.4112\n",
      "Epoch 10/10, Batch 290/883, Training Loss: 0.4615\n",
      "Epoch 10/10, Batch 291/883, Training Loss: 0.8461\n",
      "Epoch 10/10, Batch 292/883, Training Loss: 0.7447\n",
      "Epoch 10/10, Batch 293/883, Training Loss: 0.8066\n",
      "Epoch 10/10, Batch 294/883, Training Loss: 0.6058\n",
      "Epoch 10/10, Batch 295/883, Training Loss: 0.3475\n",
      "Epoch 10/10, Batch 296/883, Training Loss: 0.3719\n",
      "Epoch 10/10, Batch 297/883, Training Loss: 0.6752\n",
      "Epoch 10/10, Batch 298/883, Training Loss: 0.8159\n",
      "Epoch 10/10, Batch 299/883, Training Loss: 0.4928\n",
      "Epoch 10/10, Batch 300/883, Training Loss: 0.4508\n",
      "Epoch 10/10, Batch 301/883, Training Loss: 0.6688\n",
      "Epoch 10/10, Batch 302/883, Training Loss: 0.5816\n",
      "Epoch 10/10, Batch 303/883, Training Loss: 0.8436\n",
      "Epoch 10/10, Batch 304/883, Training Loss: 0.2880\n",
      "Epoch 10/10, Batch 305/883, Training Loss: 1.0022\n",
      "Epoch 10/10, Batch 306/883, Training Loss: 0.4739\n",
      "Epoch 10/10, Batch 307/883, Training Loss: 0.7408\n",
      "Epoch 10/10, Batch 308/883, Training Loss: 0.2706\n",
      "Epoch 10/10, Batch 309/883, Training Loss: 0.4279\n",
      "Epoch 10/10, Batch 310/883, Training Loss: 0.7360\n",
      "Epoch 10/10, Batch 311/883, Training Loss: 0.8282\n",
      "Epoch 10/10, Batch 312/883, Training Loss: 0.8115\n",
      "Epoch 10/10, Batch 313/883, Training Loss: 0.5255\n",
      "Epoch 10/10, Batch 314/883, Training Loss: 0.5521\n",
      "Epoch 10/10, Batch 315/883, Training Loss: 0.6750\n",
      "Epoch 10/10, Batch 316/883, Training Loss: 0.5336\n",
      "Epoch 10/10, Batch 317/883, Training Loss: 0.5755\n",
      "Epoch 10/10, Batch 318/883, Training Loss: 0.7310\n",
      "Epoch 10/10, Batch 319/883, Training Loss: 0.5387\n",
      "Epoch 10/10, Batch 320/883, Training Loss: 0.5011\n",
      "Epoch 10/10, Batch 321/883, Training Loss: 0.4246\n",
      "Epoch 10/10, Batch 322/883, Training Loss: 0.3329\n",
      "Epoch 10/10, Batch 323/883, Training Loss: 1.1538\n",
      "Epoch 10/10, Batch 324/883, Training Loss: 0.6143\n",
      "Epoch 10/10, Batch 325/883, Training Loss: 0.9604\n",
      "Epoch 10/10, Batch 326/883, Training Loss: 0.5466\n",
      "Epoch 10/10, Batch 327/883, Training Loss: 0.4788\n",
      "Epoch 10/10, Batch 328/883, Training Loss: 0.5300\n",
      "Epoch 10/10, Batch 329/883, Training Loss: 0.6130\n",
      "Epoch 10/10, Batch 330/883, Training Loss: 0.3973\n",
      "Epoch 10/10, Batch 331/883, Training Loss: 0.5375\n",
      "Epoch 10/10, Batch 332/883, Training Loss: 0.6495\n",
      "Epoch 10/10, Batch 333/883, Training Loss: 0.6631\n",
      "Epoch 10/10, Batch 334/883, Training Loss: 0.2638\n",
      "Epoch 10/10, Batch 335/883, Training Loss: 0.5757\n",
      "Epoch 10/10, Batch 336/883, Training Loss: 0.7301\n",
      "Epoch 10/10, Batch 337/883, Training Loss: 0.4455\n",
      "Epoch 10/10, Batch 338/883, Training Loss: 0.6135\n",
      "Epoch 10/10, Batch 339/883, Training Loss: 0.2677\n",
      "Epoch 10/10, Batch 340/883, Training Loss: 0.4310\n",
      "Epoch 10/10, Batch 341/883, Training Loss: 0.5176\n",
      "Epoch 10/10, Batch 342/883, Training Loss: 0.4085\n",
      "Epoch 10/10, Batch 343/883, Training Loss: 0.2494\n",
      "Epoch 10/10, Batch 344/883, Training Loss: 0.6148\n",
      "Epoch 10/10, Batch 345/883, Training Loss: 0.6263\n",
      "Epoch 10/10, Batch 346/883, Training Loss: 0.3994\n",
      "Epoch 10/10, Batch 347/883, Training Loss: 0.5900\n",
      "Epoch 10/10, Batch 348/883, Training Loss: 0.5133\n",
      "Epoch 10/10, Batch 349/883, Training Loss: 0.6803\n",
      "Epoch 10/10, Batch 350/883, Training Loss: 0.4769\n",
      "Epoch 10/10, Batch 351/883, Training Loss: 0.3895\n",
      "Epoch 10/10, Batch 352/883, Training Loss: 1.0600\n",
      "Epoch 10/10, Batch 353/883, Training Loss: 0.4793\n",
      "Epoch 10/10, Batch 354/883, Training Loss: 0.3525\n",
      "Epoch 10/10, Batch 355/883, Training Loss: 0.4289\n",
      "Epoch 10/10, Batch 356/883, Training Loss: 0.6788\n",
      "Epoch 10/10, Batch 357/883, Training Loss: 0.5529\n",
      "Epoch 10/10, Batch 358/883, Training Loss: 0.4789\n",
      "Epoch 10/10, Batch 359/883, Training Loss: 0.4557\n",
      "Epoch 10/10, Batch 360/883, Training Loss: 0.3588\n",
      "Epoch 10/10, Batch 361/883, Training Loss: 0.9141\n",
      "Epoch 10/10, Batch 362/883, Training Loss: 0.7019\n",
      "Epoch 10/10, Batch 363/883, Training Loss: 0.5933\n",
      "Epoch 10/10, Batch 364/883, Training Loss: 0.5809\n",
      "Epoch 10/10, Batch 365/883, Training Loss: 0.5280\n",
      "Epoch 10/10, Batch 366/883, Training Loss: 0.6216\n",
      "Epoch 10/10, Batch 367/883, Training Loss: 0.5707\n",
      "Epoch 10/10, Batch 368/883, Training Loss: 0.4794\n",
      "Epoch 10/10, Batch 369/883, Training Loss: 0.5367\n",
      "Epoch 10/10, Batch 370/883, Training Loss: 0.8389\n",
      "Epoch 10/10, Batch 371/883, Training Loss: 0.4764\n",
      "Epoch 10/10, Batch 372/883, Training Loss: 0.5106\n",
      "Epoch 10/10, Batch 373/883, Training Loss: 0.3076\n",
      "Epoch 10/10, Batch 374/883, Training Loss: 0.5040\n",
      "Epoch 10/10, Batch 375/883, Training Loss: 0.4333\n",
      "Epoch 10/10, Batch 376/883, Training Loss: 0.8947\n",
      "Epoch 10/10, Batch 377/883, Training Loss: 1.0009\n",
      "Epoch 10/10, Batch 378/883, Training Loss: 0.6308\n",
      "Epoch 10/10, Batch 379/883, Training Loss: 0.3440\n",
      "Epoch 10/10, Batch 380/883, Training Loss: 1.1171\n",
      "Epoch 10/10, Batch 381/883, Training Loss: 0.6165\n",
      "Epoch 10/10, Batch 382/883, Training Loss: 0.7008\n",
      "Epoch 10/10, Batch 383/883, Training Loss: 0.5678\n",
      "Epoch 10/10, Batch 384/883, Training Loss: 0.8009\n",
      "Epoch 10/10, Batch 385/883, Training Loss: 0.8217\n",
      "Epoch 10/10, Batch 386/883, Training Loss: 0.6869\n",
      "Epoch 10/10, Batch 387/883, Training Loss: 0.7656\n",
      "Epoch 10/10, Batch 388/883, Training Loss: 0.5296\n",
      "Epoch 10/10, Batch 389/883, Training Loss: 0.8158\n",
      "Epoch 10/10, Batch 390/883, Training Loss: 0.2836\n",
      "Epoch 10/10, Batch 391/883, Training Loss: 0.3570\n",
      "Epoch 10/10, Batch 392/883, Training Loss: 0.4616\n",
      "Epoch 10/10, Batch 393/883, Training Loss: 0.4475\n",
      "Epoch 10/10, Batch 394/883, Training Loss: 0.3793\n",
      "Epoch 10/10, Batch 395/883, Training Loss: 0.4728\n",
      "Epoch 10/10, Batch 396/883, Training Loss: 0.7776\n",
      "Epoch 10/10, Batch 397/883, Training Loss: 0.4237\n",
      "Epoch 10/10, Batch 398/883, Training Loss: 0.4483\n",
      "Epoch 10/10, Batch 399/883, Training Loss: 0.4098\n",
      "Epoch 10/10, Batch 400/883, Training Loss: 0.8922\n",
      "Epoch 10/10, Batch 401/883, Training Loss: 0.5917\n",
      "Epoch 10/10, Batch 402/883, Training Loss: 0.6459\n",
      "Epoch 10/10, Batch 403/883, Training Loss: 0.3704\n",
      "Epoch 10/10, Batch 404/883, Training Loss: 0.9582\n",
      "Epoch 10/10, Batch 405/883, Training Loss: 0.4544\n",
      "Epoch 10/10, Batch 406/883, Training Loss: 0.7722\n",
      "Epoch 10/10, Batch 407/883, Training Loss: 0.4723\n",
      "Epoch 10/10, Batch 408/883, Training Loss: 0.4428\n",
      "Epoch 10/10, Batch 409/883, Training Loss: 0.6344\n",
      "Epoch 10/10, Batch 410/883, Training Loss: 0.5829\n",
      "Epoch 10/10, Batch 411/883, Training Loss: 0.4156\n",
      "Epoch 10/10, Batch 412/883, Training Loss: 0.5519\n",
      "Epoch 10/10, Batch 413/883, Training Loss: 0.8302\n",
      "Epoch 10/10, Batch 414/883, Training Loss: 0.7731\n",
      "Epoch 10/10, Batch 415/883, Training Loss: 0.4005\n",
      "Epoch 10/10, Batch 416/883, Training Loss: 0.8053\n",
      "Epoch 10/10, Batch 417/883, Training Loss: 0.7513\n",
      "Epoch 10/10, Batch 418/883, Training Loss: 0.4943\n",
      "Epoch 10/10, Batch 419/883, Training Loss: 0.5869\n",
      "Epoch 10/10, Batch 420/883, Training Loss: 0.5274\n",
      "Epoch 10/10, Batch 421/883, Training Loss: 0.7243\n",
      "Epoch 10/10, Batch 422/883, Training Loss: 0.4415\n",
      "Epoch 10/10, Batch 423/883, Training Loss: 0.6189\n",
      "Epoch 10/10, Batch 424/883, Training Loss: 0.5131\n",
      "Epoch 10/10, Batch 425/883, Training Loss: 0.3584\n",
      "Epoch 10/10, Batch 426/883, Training Loss: 0.7802\n",
      "Epoch 10/10, Batch 427/883, Training Loss: 0.2164\n",
      "Epoch 10/10, Batch 428/883, Training Loss: 0.9241\n",
      "Epoch 10/10, Batch 429/883, Training Loss: 0.4448\n",
      "Epoch 10/10, Batch 430/883, Training Loss: 0.5943\n",
      "Epoch 10/10, Batch 431/883, Training Loss: 0.5580\n",
      "Epoch 10/10, Batch 432/883, Training Loss: 0.6506\n",
      "Epoch 10/10, Batch 433/883, Training Loss: 0.7244\n",
      "Epoch 10/10, Batch 434/883, Training Loss: 0.4429\n",
      "Epoch 10/10, Batch 435/883, Training Loss: 0.4181\n",
      "Epoch 10/10, Batch 436/883, Training Loss: 0.6330\n",
      "Epoch 10/10, Batch 437/883, Training Loss: 0.3282\n",
      "Epoch 10/10, Batch 438/883, Training Loss: 0.7814\n",
      "Epoch 10/10, Batch 439/883, Training Loss: 0.6818\n",
      "Epoch 10/10, Batch 440/883, Training Loss: 0.5901\n",
      "Epoch 10/10, Batch 441/883, Training Loss: 0.6826\n",
      "Epoch 10/10, Batch 442/883, Training Loss: 0.5125\n",
      "Epoch 10/10, Batch 443/883, Training Loss: 0.3608\n",
      "Epoch 10/10, Batch 444/883, Training Loss: 0.7026\n",
      "Epoch 10/10, Batch 445/883, Training Loss: 0.8154\n",
      "Epoch 10/10, Batch 446/883, Training Loss: 0.3843\n",
      "Epoch 10/10, Batch 447/883, Training Loss: 0.5710\n",
      "Epoch 10/10, Batch 448/883, Training Loss: 0.5513\n",
      "Epoch 10/10, Batch 449/883, Training Loss: 0.6192\n",
      "Epoch 10/10, Batch 450/883, Training Loss: 0.3868\n",
      "Epoch 10/10, Batch 451/883, Training Loss: 0.4483\n",
      "Epoch 10/10, Batch 452/883, Training Loss: 0.5686\n",
      "Epoch 10/10, Batch 453/883, Training Loss: 0.8148\n",
      "Epoch 10/10, Batch 454/883, Training Loss: 0.4865\n",
      "Epoch 10/10, Batch 455/883, Training Loss: 0.5074\n",
      "Epoch 10/10, Batch 456/883, Training Loss: 0.6746\n",
      "Epoch 10/10, Batch 457/883, Training Loss: 0.4950\n",
      "Epoch 10/10, Batch 458/883, Training Loss: 0.7405\n",
      "Epoch 10/10, Batch 459/883, Training Loss: 0.6159\n",
      "Epoch 10/10, Batch 460/883, Training Loss: 0.7860\n",
      "Epoch 10/10, Batch 461/883, Training Loss: 0.3978\n",
      "Epoch 10/10, Batch 462/883, Training Loss: 0.4978\n",
      "Epoch 10/10, Batch 463/883, Training Loss: 0.5331\n",
      "Epoch 10/10, Batch 464/883, Training Loss: 0.4923\n",
      "Epoch 10/10, Batch 465/883, Training Loss: 0.7044\n",
      "Epoch 10/10, Batch 466/883, Training Loss: 0.7622\n",
      "Epoch 10/10, Batch 467/883, Training Loss: 0.5294\n",
      "Epoch 10/10, Batch 468/883, Training Loss: 0.8228\n",
      "Epoch 10/10, Batch 469/883, Training Loss: 0.5349\n",
      "Epoch 10/10, Batch 470/883, Training Loss: 0.8934\n",
      "Epoch 10/10, Batch 471/883, Training Loss: 0.8416\n",
      "Epoch 10/10, Batch 472/883, Training Loss: 0.5146\n",
      "Epoch 10/10, Batch 473/883, Training Loss: 0.6079\n",
      "Epoch 10/10, Batch 474/883, Training Loss: 0.8318\n",
      "Epoch 10/10, Batch 475/883, Training Loss: 0.9952\n",
      "Epoch 10/10, Batch 476/883, Training Loss: 0.6752\n",
      "Epoch 10/10, Batch 477/883, Training Loss: 0.5141\n",
      "Epoch 10/10, Batch 478/883, Training Loss: 0.5242\n",
      "Epoch 10/10, Batch 479/883, Training Loss: 0.9643\n",
      "Epoch 10/10, Batch 480/883, Training Loss: 0.6905\n",
      "Epoch 10/10, Batch 481/883, Training Loss: 0.3050\n",
      "Epoch 10/10, Batch 482/883, Training Loss: 0.4240\n",
      "Epoch 10/10, Batch 483/883, Training Loss: 0.4004\n",
      "Epoch 10/10, Batch 484/883, Training Loss: 0.7103\n",
      "Epoch 10/10, Batch 485/883, Training Loss: 0.6304\n",
      "Epoch 10/10, Batch 486/883, Training Loss: 0.4309\n",
      "Epoch 10/10, Batch 487/883, Training Loss: 0.5013\n",
      "Epoch 10/10, Batch 488/883, Training Loss: 0.5325\n",
      "Epoch 10/10, Batch 489/883, Training Loss: 0.7678\n",
      "Epoch 10/10, Batch 490/883, Training Loss: 0.3999\n",
      "Epoch 10/10, Batch 491/883, Training Loss: 0.5860\n",
      "Epoch 10/10, Batch 492/883, Training Loss: 0.3638\n",
      "Epoch 10/10, Batch 493/883, Training Loss: 0.6015\n",
      "Epoch 10/10, Batch 494/883, Training Loss: 0.5134\n",
      "Epoch 10/10, Batch 495/883, Training Loss: 0.6263\n",
      "Epoch 10/10, Batch 496/883, Training Loss: 0.7612\n",
      "Epoch 10/10, Batch 497/883, Training Loss: 0.6224\n",
      "Epoch 10/10, Batch 498/883, Training Loss: 0.2990\n",
      "Epoch 10/10, Batch 499/883, Training Loss: 0.4766\n",
      "Epoch 10/10, Batch 500/883, Training Loss: 0.5644\n",
      "Epoch 10/10, Batch 501/883, Training Loss: 0.4017\n",
      "Epoch 10/10, Batch 502/883, Training Loss: 0.5433\n",
      "Epoch 10/10, Batch 503/883, Training Loss: 0.4501\n",
      "Epoch 10/10, Batch 504/883, Training Loss: 0.5500\n",
      "Epoch 10/10, Batch 505/883, Training Loss: 0.5367\n",
      "Epoch 10/10, Batch 506/883, Training Loss: 0.4580\n",
      "Epoch 10/10, Batch 507/883, Training Loss: 0.6118\n",
      "Epoch 10/10, Batch 508/883, Training Loss: 0.6533\n",
      "Epoch 10/10, Batch 509/883, Training Loss: 0.5455\n",
      "Epoch 10/10, Batch 510/883, Training Loss: 0.3636\n",
      "Epoch 10/10, Batch 511/883, Training Loss: 0.6189\n",
      "Epoch 10/10, Batch 512/883, Training Loss: 0.6819\n",
      "Epoch 10/10, Batch 513/883, Training Loss: 0.6485\n",
      "Epoch 10/10, Batch 514/883, Training Loss: 0.5515\n",
      "Epoch 10/10, Batch 515/883, Training Loss: 0.3373\n",
      "Epoch 10/10, Batch 516/883, Training Loss: 0.5770\n",
      "Epoch 10/10, Batch 517/883, Training Loss: 0.4192\n",
      "Epoch 10/10, Batch 518/883, Training Loss: 0.5655\n",
      "Epoch 10/10, Batch 519/883, Training Loss: 0.5569\n",
      "Epoch 10/10, Batch 520/883, Training Loss: 0.5095\n",
      "Epoch 10/10, Batch 521/883, Training Loss: 0.3077\n",
      "Epoch 10/10, Batch 522/883, Training Loss: 0.5345\n",
      "Epoch 10/10, Batch 523/883, Training Loss: 0.6400\n",
      "Epoch 10/10, Batch 524/883, Training Loss: 0.3080\n",
      "Epoch 10/10, Batch 525/883, Training Loss: 0.7088\n",
      "Epoch 10/10, Batch 526/883, Training Loss: 0.5812\n",
      "Epoch 10/10, Batch 527/883, Training Loss: 0.6522\n",
      "Epoch 10/10, Batch 528/883, Training Loss: 0.4693\n",
      "Epoch 10/10, Batch 529/883, Training Loss: 0.4813\n",
      "Epoch 10/10, Batch 530/883, Training Loss: 0.8451\n",
      "Epoch 10/10, Batch 531/883, Training Loss: 0.4642\n",
      "Epoch 10/10, Batch 532/883, Training Loss: 0.5742\n",
      "Epoch 10/10, Batch 533/883, Training Loss: 0.3833\n",
      "Epoch 10/10, Batch 534/883, Training Loss: 0.4711\n",
      "Epoch 10/10, Batch 535/883, Training Loss: 0.7128\n",
      "Epoch 10/10, Batch 536/883, Training Loss: 0.5882\n",
      "Epoch 10/10, Batch 537/883, Training Loss: 0.7085\n",
      "Epoch 10/10, Batch 538/883, Training Loss: 0.6364\n",
      "Epoch 10/10, Batch 539/883, Training Loss: 0.3986\n",
      "Epoch 10/10, Batch 540/883, Training Loss: 0.4137\n",
      "Epoch 10/10, Batch 541/883, Training Loss: 0.7521\n",
      "Epoch 10/10, Batch 542/883, Training Loss: 0.6066\n",
      "Epoch 10/10, Batch 543/883, Training Loss: 0.7837\n",
      "Epoch 10/10, Batch 544/883, Training Loss: 0.6006\n",
      "Epoch 10/10, Batch 545/883, Training Loss: 0.2719\n",
      "Epoch 10/10, Batch 546/883, Training Loss: 0.5276\n",
      "Epoch 10/10, Batch 547/883, Training Loss: 0.4381\n",
      "Epoch 10/10, Batch 548/883, Training Loss: 0.7832\n",
      "Epoch 10/10, Batch 549/883, Training Loss: 0.3467\n",
      "Epoch 10/10, Batch 550/883, Training Loss: 0.4537\n",
      "Epoch 10/10, Batch 551/883, Training Loss: 0.2933\n",
      "Epoch 10/10, Batch 552/883, Training Loss: 0.5666\n",
      "Epoch 10/10, Batch 553/883, Training Loss: 0.7307\n",
      "Epoch 10/10, Batch 554/883, Training Loss: 0.3570\n",
      "Epoch 10/10, Batch 555/883, Training Loss: 0.4550\n",
      "Epoch 10/10, Batch 556/883, Training Loss: 0.5646\n",
      "Epoch 10/10, Batch 557/883, Training Loss: 0.3145\n",
      "Epoch 10/10, Batch 558/883, Training Loss: 0.2182\n",
      "Epoch 10/10, Batch 559/883, Training Loss: 0.7311\n",
      "Epoch 10/10, Batch 560/883, Training Loss: 0.6580\n",
      "Epoch 10/10, Batch 561/883, Training Loss: 0.4739\n",
      "Epoch 10/10, Batch 562/883, Training Loss: 0.4348\n",
      "Epoch 10/10, Batch 563/883, Training Loss: 0.4006\n",
      "Epoch 10/10, Batch 564/883, Training Loss: 0.7286\n",
      "Epoch 10/10, Batch 565/883, Training Loss: 0.4052\n",
      "Epoch 10/10, Batch 566/883, Training Loss: 0.4913\n",
      "Epoch 10/10, Batch 567/883, Training Loss: 0.5684\n",
      "Epoch 10/10, Batch 568/883, Training Loss: 0.4678\n",
      "Epoch 10/10, Batch 569/883, Training Loss: 0.4527\n",
      "Epoch 10/10, Batch 570/883, Training Loss: 0.3821\n",
      "Epoch 10/10, Batch 571/883, Training Loss: 0.5995\n",
      "Epoch 10/10, Batch 572/883, Training Loss: 0.5294\n",
      "Epoch 10/10, Batch 573/883, Training Loss: 0.3021\n",
      "Epoch 10/10, Batch 574/883, Training Loss: 0.4869\n",
      "Epoch 10/10, Batch 575/883, Training Loss: 0.5061\n",
      "Epoch 10/10, Batch 576/883, Training Loss: 0.2746\n",
      "Epoch 10/10, Batch 577/883, Training Loss: 0.6826\n",
      "Epoch 10/10, Batch 578/883, Training Loss: 0.8499\n",
      "Epoch 10/10, Batch 579/883, Training Loss: 0.4515\n",
      "Epoch 10/10, Batch 580/883, Training Loss: 0.4264\n",
      "Epoch 10/10, Batch 581/883, Training Loss: 0.9442\n",
      "Epoch 10/10, Batch 582/883, Training Loss: 0.4656\n",
      "Epoch 10/10, Batch 583/883, Training Loss: 0.8969\n",
      "Epoch 10/10, Batch 584/883, Training Loss: 1.1452\n",
      "Epoch 10/10, Batch 585/883, Training Loss: 0.5072\n",
      "Epoch 10/10, Batch 586/883, Training Loss: 0.4283\n",
      "Epoch 10/10, Batch 587/883, Training Loss: 0.9660\n",
      "Epoch 10/10, Batch 588/883, Training Loss: 0.3667\n",
      "Epoch 10/10, Batch 589/883, Training Loss: 0.3512\n",
      "Epoch 10/10, Batch 590/883, Training Loss: 0.5352\n",
      "Epoch 10/10, Batch 591/883, Training Loss: 0.7875\n",
      "Epoch 10/10, Batch 592/883, Training Loss: 0.2398\n",
      "Epoch 10/10, Batch 593/883, Training Loss: 0.5310\n",
      "Epoch 10/10, Batch 594/883, Training Loss: 0.5154\n",
      "Epoch 10/10, Batch 595/883, Training Loss: 0.5113\n",
      "Epoch 10/10, Batch 596/883, Training Loss: 0.4469\n",
      "Epoch 10/10, Batch 597/883, Training Loss: 0.5552\n",
      "Epoch 10/10, Batch 598/883, Training Loss: 0.7253\n",
      "Epoch 10/10, Batch 599/883, Training Loss: 0.5145\n",
      "Epoch 10/10, Batch 600/883, Training Loss: 0.7154\n",
      "Epoch 10/10, Batch 601/883, Training Loss: 0.6636\n",
      "Epoch 10/10, Batch 602/883, Training Loss: 0.6686\n",
      "Epoch 10/10, Batch 603/883, Training Loss: 0.5668\n",
      "Epoch 10/10, Batch 604/883, Training Loss: 0.4552\n",
      "Epoch 10/10, Batch 605/883, Training Loss: 0.5056\n",
      "Epoch 10/10, Batch 606/883, Training Loss: 0.5972\n",
      "Epoch 10/10, Batch 607/883, Training Loss: 0.3704\n",
      "Epoch 10/10, Batch 608/883, Training Loss: 0.8764\n",
      "Epoch 10/10, Batch 609/883, Training Loss: 0.7954\n",
      "Epoch 10/10, Batch 610/883, Training Loss: 0.6175\n",
      "Epoch 10/10, Batch 611/883, Training Loss: 0.4719\n",
      "Epoch 10/10, Batch 612/883, Training Loss: 0.6490\n",
      "Epoch 10/10, Batch 613/883, Training Loss: 0.6157\n",
      "Epoch 10/10, Batch 614/883, Training Loss: 0.3352\n",
      "Epoch 10/10, Batch 615/883, Training Loss: 0.4934\n",
      "Epoch 10/10, Batch 616/883, Training Loss: 0.4197\n",
      "Epoch 10/10, Batch 617/883, Training Loss: 0.4220\n",
      "Epoch 10/10, Batch 618/883, Training Loss: 0.4873\n",
      "Epoch 10/10, Batch 619/883, Training Loss: 0.4127\n",
      "Epoch 10/10, Batch 620/883, Training Loss: 0.5797\n",
      "Epoch 10/10, Batch 621/883, Training Loss: 0.3965\n",
      "Epoch 10/10, Batch 622/883, Training Loss: 0.6901\n",
      "Epoch 10/10, Batch 623/883, Training Loss: 0.7086\n",
      "Epoch 10/10, Batch 624/883, Training Loss: 0.4115\n",
      "Epoch 10/10, Batch 625/883, Training Loss: 0.6631\n",
      "Epoch 10/10, Batch 626/883, Training Loss: 0.4230\n",
      "Epoch 10/10, Batch 627/883, Training Loss: 0.4767\n",
      "Epoch 10/10, Batch 628/883, Training Loss: 0.6391\n",
      "Epoch 10/10, Batch 629/883, Training Loss: 0.6156\n",
      "Epoch 10/10, Batch 630/883, Training Loss: 0.7146\n",
      "Epoch 10/10, Batch 631/883, Training Loss: 0.4630\n",
      "Epoch 10/10, Batch 632/883, Training Loss: 0.4491\n",
      "Epoch 10/10, Batch 633/883, Training Loss: 0.4285\n",
      "Epoch 10/10, Batch 634/883, Training Loss: 0.5872\n",
      "Epoch 10/10, Batch 635/883, Training Loss: 0.7487\n",
      "Epoch 10/10, Batch 636/883, Training Loss: 0.7394\n",
      "Epoch 10/10, Batch 637/883, Training Loss: 0.3881\n",
      "Epoch 10/10, Batch 638/883, Training Loss: 0.4980\n",
      "Epoch 10/10, Batch 639/883, Training Loss: 0.9965\n",
      "Epoch 10/10, Batch 640/883, Training Loss: 0.5519\n",
      "Epoch 10/10, Batch 641/883, Training Loss: 0.3351\n",
      "Epoch 10/10, Batch 642/883, Training Loss: 0.7063\n",
      "Epoch 10/10, Batch 643/883, Training Loss: 0.7410\n",
      "Epoch 10/10, Batch 644/883, Training Loss: 0.6786\n",
      "Epoch 10/10, Batch 645/883, Training Loss: 0.7070\n",
      "Epoch 10/10, Batch 646/883, Training Loss: 0.6096\n",
      "Epoch 10/10, Batch 647/883, Training Loss: 0.7031\n",
      "Epoch 10/10, Batch 648/883, Training Loss: 0.7035\n",
      "Epoch 10/10, Batch 649/883, Training Loss: 0.2588\n",
      "Epoch 10/10, Batch 650/883, Training Loss: 0.4152\n",
      "Epoch 10/10, Batch 651/883, Training Loss: 0.3860\n",
      "Epoch 10/10, Batch 652/883, Training Loss: 0.5517\n",
      "Epoch 10/10, Batch 653/883, Training Loss: 0.4765\n",
      "Epoch 10/10, Batch 654/883, Training Loss: 0.4298\n",
      "Epoch 10/10, Batch 655/883, Training Loss: 0.4351\n",
      "Epoch 10/10, Batch 656/883, Training Loss: 0.4953\n",
      "Epoch 10/10, Batch 657/883, Training Loss: 0.6376\n",
      "Epoch 10/10, Batch 658/883, Training Loss: 0.4998\n",
      "Epoch 10/10, Batch 659/883, Training Loss: 0.6712\n",
      "Epoch 10/10, Batch 660/883, Training Loss: 0.3797\n",
      "Epoch 10/10, Batch 661/883, Training Loss: 0.4081\n",
      "Epoch 10/10, Batch 662/883, Training Loss: 0.7939\n",
      "Epoch 10/10, Batch 663/883, Training Loss: 0.4039\n",
      "Epoch 10/10, Batch 664/883, Training Loss: 0.5238\n",
      "Epoch 10/10, Batch 665/883, Training Loss: 0.3050\n",
      "Epoch 10/10, Batch 666/883, Training Loss: 0.4211\n",
      "Epoch 10/10, Batch 667/883, Training Loss: 0.4555\n",
      "Epoch 10/10, Batch 668/883, Training Loss: 0.6870\n",
      "Epoch 10/10, Batch 669/883, Training Loss: 0.4878\n",
      "Epoch 10/10, Batch 670/883, Training Loss: 0.4488\n",
      "Epoch 10/10, Batch 671/883, Training Loss: 0.5197\n",
      "Epoch 10/10, Batch 672/883, Training Loss: 0.3424\n",
      "Epoch 10/10, Batch 673/883, Training Loss: 0.4079\n",
      "Epoch 10/10, Batch 674/883, Training Loss: 0.5257\n",
      "Epoch 10/10, Batch 675/883, Training Loss: 0.4950\n",
      "Epoch 10/10, Batch 676/883, Training Loss: 0.3198\n",
      "Epoch 10/10, Batch 677/883, Training Loss: 0.6193\n",
      "Epoch 10/10, Batch 678/883, Training Loss: 0.7077\n",
      "Epoch 10/10, Batch 679/883, Training Loss: 0.4539\n",
      "Epoch 10/10, Batch 680/883, Training Loss: 0.6270\n",
      "Epoch 10/10, Batch 681/883, Training Loss: 0.7623\n",
      "Epoch 10/10, Batch 682/883, Training Loss: 0.3442\n",
      "Epoch 10/10, Batch 683/883, Training Loss: 0.3190\n",
      "Epoch 10/10, Batch 684/883, Training Loss: 0.5938\n",
      "Epoch 10/10, Batch 685/883, Training Loss: 0.3055\n",
      "Epoch 10/10, Batch 686/883, Training Loss: 0.3526\n",
      "Epoch 10/10, Batch 687/883, Training Loss: 0.6773\n",
      "Epoch 10/10, Batch 688/883, Training Loss: 0.5236\n",
      "Epoch 10/10, Batch 689/883, Training Loss: 0.3609\n",
      "Epoch 10/10, Batch 690/883, Training Loss: 0.4735\n",
      "Epoch 10/10, Batch 691/883, Training Loss: 0.6674\n",
      "Epoch 10/10, Batch 692/883, Training Loss: 0.3899\n",
      "Epoch 10/10, Batch 693/883, Training Loss: 0.8222\n",
      "Epoch 10/10, Batch 694/883, Training Loss: 0.5824\n",
      "Epoch 10/10, Batch 695/883, Training Loss: 1.1878\n",
      "Epoch 10/10, Batch 696/883, Training Loss: 0.6941\n",
      "Epoch 10/10, Batch 697/883, Training Loss: 0.6817\n",
      "Epoch 10/10, Batch 698/883, Training Loss: 0.6300\n",
      "Epoch 10/10, Batch 699/883, Training Loss: 0.4405\n",
      "Epoch 10/10, Batch 700/883, Training Loss: 0.3731\n",
      "Epoch 10/10, Batch 701/883, Training Loss: 0.4872\n",
      "Epoch 10/10, Batch 702/883, Training Loss: 0.3077\n",
      "Epoch 10/10, Batch 703/883, Training Loss: 0.3576\n",
      "Epoch 10/10, Batch 704/883, Training Loss: 0.5375\n",
      "Epoch 10/10, Batch 705/883, Training Loss: 0.6186\n",
      "Epoch 10/10, Batch 706/883, Training Loss: 0.9203\n",
      "Epoch 10/10, Batch 707/883, Training Loss: 0.5941\n",
      "Epoch 10/10, Batch 708/883, Training Loss: 0.6354\n",
      "Epoch 10/10, Batch 709/883, Training Loss: 0.7959\n",
      "Epoch 10/10, Batch 710/883, Training Loss: 0.6241\n",
      "Epoch 10/10, Batch 711/883, Training Loss: 0.4817\n",
      "Epoch 10/10, Batch 712/883, Training Loss: 0.5104\n",
      "Epoch 10/10, Batch 713/883, Training Loss: 0.4610\n",
      "Epoch 10/10, Batch 714/883, Training Loss: 0.4287\n",
      "Epoch 10/10, Batch 715/883, Training Loss: 0.4934\n",
      "Epoch 10/10, Batch 716/883, Training Loss: 0.5898\n",
      "Epoch 10/10, Batch 717/883, Training Loss: 0.4768\n",
      "Epoch 10/10, Batch 718/883, Training Loss: 0.6260\n",
      "Epoch 10/10, Batch 719/883, Training Loss: 0.8885\n",
      "Epoch 10/10, Batch 720/883, Training Loss: 0.7647\n",
      "Epoch 10/10, Batch 721/883, Training Loss: 0.5445\n",
      "Epoch 10/10, Batch 722/883, Training Loss: 0.2735\n",
      "Epoch 10/10, Batch 723/883, Training Loss: 0.6239\n",
      "Epoch 10/10, Batch 724/883, Training Loss: 0.6453\n",
      "Epoch 10/10, Batch 725/883, Training Loss: 0.5892\n",
      "Epoch 10/10, Batch 726/883, Training Loss: 0.5489\n",
      "Epoch 10/10, Batch 727/883, Training Loss: 0.6430\n",
      "Epoch 10/10, Batch 728/883, Training Loss: 0.7455\n",
      "Epoch 10/10, Batch 729/883, Training Loss: 0.7133\n",
      "Epoch 10/10, Batch 730/883, Training Loss: 0.9908\n",
      "Epoch 10/10, Batch 731/883, Training Loss: 0.7269\n",
      "Epoch 10/10, Batch 732/883, Training Loss: 0.7690\n",
      "Epoch 10/10, Batch 733/883, Training Loss: 0.3844\n",
      "Epoch 10/10, Batch 734/883, Training Loss: 0.4303\n",
      "Epoch 10/10, Batch 735/883, Training Loss: 0.7469\n",
      "Epoch 10/10, Batch 736/883, Training Loss: 0.5945\n",
      "Epoch 10/10, Batch 737/883, Training Loss: 0.3059\n",
      "Epoch 10/10, Batch 738/883, Training Loss: 0.7030\n",
      "Epoch 10/10, Batch 739/883, Training Loss: 0.6148\n",
      "Epoch 10/10, Batch 740/883, Training Loss: 0.7337\n",
      "Epoch 10/10, Batch 741/883, Training Loss: 0.5116\n",
      "Epoch 10/10, Batch 742/883, Training Loss: 0.4462\n",
      "Epoch 10/10, Batch 743/883, Training Loss: 0.2223\n",
      "Epoch 10/10, Batch 744/883, Training Loss: 0.6558\n",
      "Epoch 10/10, Batch 745/883, Training Loss: 0.3318\n",
      "Epoch 10/10, Batch 746/883, Training Loss: 1.0872\n",
      "Epoch 10/10, Batch 747/883, Training Loss: 0.3982\n",
      "Epoch 10/10, Batch 748/883, Training Loss: 0.5153\n",
      "Epoch 10/10, Batch 749/883, Training Loss: 0.5791\n",
      "Epoch 10/10, Batch 750/883, Training Loss: 0.4046\n",
      "Epoch 10/10, Batch 751/883, Training Loss: 0.3639\n",
      "Epoch 10/10, Batch 752/883, Training Loss: 0.5885\n",
      "Epoch 10/10, Batch 753/883, Training Loss: 0.6854\n",
      "Epoch 10/10, Batch 754/883, Training Loss: 0.4528\n",
      "Epoch 10/10, Batch 755/883, Training Loss: 0.7779\n",
      "Epoch 10/10, Batch 756/883, Training Loss: 0.7185\n",
      "Epoch 10/10, Batch 757/883, Training Loss: 0.4702\n",
      "Epoch 10/10, Batch 758/883, Training Loss: 0.5570\n",
      "Epoch 10/10, Batch 759/883, Training Loss: 0.4835\n",
      "Epoch 10/10, Batch 760/883, Training Loss: 0.7386\n",
      "Epoch 10/10, Batch 761/883, Training Loss: 0.8283\n",
      "Epoch 10/10, Batch 762/883, Training Loss: 0.5022\n",
      "Epoch 10/10, Batch 763/883, Training Loss: 0.7111\n",
      "Epoch 10/10, Batch 764/883, Training Loss: 0.4030\n",
      "Epoch 10/10, Batch 765/883, Training Loss: 0.6331\n",
      "Epoch 10/10, Batch 766/883, Training Loss: 0.4555\n",
      "Epoch 10/10, Batch 767/883, Training Loss: 0.8919\n",
      "Epoch 10/10, Batch 768/883, Training Loss: 0.5243\n",
      "Epoch 10/10, Batch 769/883, Training Loss: 0.5231\n",
      "Epoch 10/10, Batch 770/883, Training Loss: 0.4632\n",
      "Epoch 10/10, Batch 771/883, Training Loss: 0.4837\n",
      "Epoch 10/10, Batch 772/883, Training Loss: 0.2982\n",
      "Epoch 10/10, Batch 773/883, Training Loss: 1.0535\n",
      "Epoch 10/10, Batch 774/883, Training Loss: 0.5509\n",
      "Epoch 10/10, Batch 775/883, Training Loss: 0.5019\n",
      "Epoch 10/10, Batch 776/883, Training Loss: 0.7489\n",
      "Epoch 10/10, Batch 777/883, Training Loss: 0.6287\n",
      "Epoch 10/10, Batch 778/883, Training Loss: 0.6286\n",
      "Epoch 10/10, Batch 779/883, Training Loss: 0.4047\n",
      "Epoch 10/10, Batch 780/883, Training Loss: 0.2514\n",
      "Epoch 10/10, Batch 781/883, Training Loss: 0.3421\n",
      "Epoch 10/10, Batch 782/883, Training Loss: 0.7166\n",
      "Epoch 10/10, Batch 783/883, Training Loss: 0.5366\n",
      "Epoch 10/10, Batch 784/883, Training Loss: 0.6520\n",
      "Epoch 10/10, Batch 785/883, Training Loss: 0.3310\n",
      "Epoch 10/10, Batch 786/883, Training Loss: 0.4093\n",
      "Epoch 10/10, Batch 787/883, Training Loss: 0.6283\n",
      "Epoch 10/10, Batch 788/883, Training Loss: 0.6649\n",
      "Epoch 10/10, Batch 789/883, Training Loss: 0.6768\n",
      "Epoch 10/10, Batch 790/883, Training Loss: 0.4315\n",
      "Epoch 10/10, Batch 791/883, Training Loss: 0.5442\n",
      "Epoch 10/10, Batch 792/883, Training Loss: 0.5318\n",
      "Epoch 10/10, Batch 793/883, Training Loss: 0.9691\n",
      "Epoch 10/10, Batch 794/883, Training Loss: 0.2756\n",
      "Epoch 10/10, Batch 795/883, Training Loss: 0.6957\n",
      "Epoch 10/10, Batch 796/883, Training Loss: 0.6433\n",
      "Epoch 10/10, Batch 797/883, Training Loss: 0.6186\n",
      "Epoch 10/10, Batch 798/883, Training Loss: 0.5261\n",
      "Epoch 10/10, Batch 799/883, Training Loss: 0.2933\n",
      "Epoch 10/10, Batch 800/883, Training Loss: 0.3674\n",
      "Epoch 10/10, Batch 801/883, Training Loss: 0.4656\n",
      "Epoch 10/10, Batch 802/883, Training Loss: 0.3734\n",
      "Epoch 10/10, Batch 803/883, Training Loss: 0.5665\n",
      "Epoch 10/10, Batch 804/883, Training Loss: 0.4253\n",
      "Epoch 10/10, Batch 805/883, Training Loss: 0.3017\n",
      "Epoch 10/10, Batch 806/883, Training Loss: 0.5178\n",
      "Epoch 10/10, Batch 807/883, Training Loss: 0.4035\n",
      "Epoch 10/10, Batch 808/883, Training Loss: 0.5578\n",
      "Epoch 10/10, Batch 809/883, Training Loss: 0.5972\n",
      "Epoch 10/10, Batch 810/883, Training Loss: 0.5489\n",
      "Epoch 10/10, Batch 811/883, Training Loss: 0.7326\n",
      "Epoch 10/10, Batch 812/883, Training Loss: 0.3552\n",
      "Epoch 10/10, Batch 813/883, Training Loss: 0.6118\n",
      "Epoch 10/10, Batch 814/883, Training Loss: 0.5136\n",
      "Epoch 10/10, Batch 815/883, Training Loss: 0.4945\n",
      "Epoch 10/10, Batch 816/883, Training Loss: 0.7491\n",
      "Epoch 10/10, Batch 817/883, Training Loss: 0.8410\n",
      "Epoch 10/10, Batch 818/883, Training Loss: 0.5901\n",
      "Epoch 10/10, Batch 819/883, Training Loss: 0.4054\n",
      "Epoch 10/10, Batch 820/883, Training Loss: 0.5855\n",
      "Epoch 10/10, Batch 821/883, Training Loss: 1.1260\n",
      "Epoch 10/10, Batch 822/883, Training Loss: 0.4885\n",
      "Epoch 10/10, Batch 823/883, Training Loss: 0.2747\n",
      "Epoch 10/10, Batch 824/883, Training Loss: 0.5616\n",
      "Epoch 10/10, Batch 825/883, Training Loss: 0.5385\n",
      "Epoch 10/10, Batch 826/883, Training Loss: 0.4360\n",
      "Epoch 10/10, Batch 827/883, Training Loss: 0.5841\n",
      "Epoch 10/10, Batch 828/883, Training Loss: 0.6365\n",
      "Epoch 10/10, Batch 829/883, Training Loss: 0.4148\n",
      "Epoch 10/10, Batch 830/883, Training Loss: 0.6781\n",
      "Epoch 10/10, Batch 831/883, Training Loss: 0.4333\n",
      "Epoch 10/10, Batch 832/883, Training Loss: 0.3643\n",
      "Epoch 10/10, Batch 833/883, Training Loss: 0.7822\n",
      "Epoch 10/10, Batch 834/883, Training Loss: 0.5884\n",
      "Epoch 10/10, Batch 835/883, Training Loss: 0.2604\n",
      "Epoch 10/10, Batch 836/883, Training Loss: 0.2766\n",
      "Epoch 10/10, Batch 837/883, Training Loss: 0.4847\n",
      "Epoch 10/10, Batch 838/883, Training Loss: 0.7503\n",
      "Epoch 10/10, Batch 839/883, Training Loss: 0.4268\n",
      "Epoch 10/10, Batch 840/883, Training Loss: 0.8207\n",
      "Epoch 10/10, Batch 841/883, Training Loss: 0.3368\n",
      "Epoch 10/10, Batch 842/883, Training Loss: 0.5906\n",
      "Epoch 10/10, Batch 843/883, Training Loss: 0.3693\n",
      "Epoch 10/10, Batch 844/883, Training Loss: 0.5013\n",
      "Epoch 10/10, Batch 845/883, Training Loss: 0.3522\n",
      "Epoch 10/10, Batch 846/883, Training Loss: 0.3556\n",
      "Epoch 10/10, Batch 847/883, Training Loss: 0.3063\n",
      "Epoch 10/10, Batch 848/883, Training Loss: 0.6377\n",
      "Epoch 10/10, Batch 849/883, Training Loss: 0.3906\n",
      "Epoch 10/10, Batch 850/883, Training Loss: 0.4396\n",
      "Epoch 10/10, Batch 851/883, Training Loss: 0.5368\n",
      "Epoch 10/10, Batch 852/883, Training Loss: 0.6438\n",
      "Epoch 10/10, Batch 853/883, Training Loss: 0.7532\n",
      "Epoch 10/10, Batch 854/883, Training Loss: 0.4696\n",
      "Epoch 10/10, Batch 855/883, Training Loss: 0.5150\n",
      "Epoch 10/10, Batch 856/883, Training Loss: 0.4939\n",
      "Epoch 10/10, Batch 857/883, Training Loss: 0.8613\n",
      "Epoch 10/10, Batch 858/883, Training Loss: 0.3512\n",
      "Epoch 10/10, Batch 859/883, Training Loss: 0.5679\n",
      "Epoch 10/10, Batch 860/883, Training Loss: 0.3063\n",
      "Epoch 10/10, Batch 861/883, Training Loss: 0.1906\n",
      "Epoch 10/10, Batch 862/883, Training Loss: 0.5612\n",
      "Epoch 10/10, Batch 863/883, Training Loss: 0.4729\n",
      "Epoch 10/10, Batch 864/883, Training Loss: 0.4619\n",
      "Epoch 10/10, Batch 865/883, Training Loss: 0.3508\n",
      "Epoch 10/10, Batch 866/883, Training Loss: 0.3925\n",
      "Epoch 10/10, Batch 867/883, Training Loss: 0.4888\n",
      "Epoch 10/10, Batch 868/883, Training Loss: 0.7779\n",
      "Epoch 10/10, Batch 869/883, Training Loss: 0.7374\n",
      "Epoch 10/10, Batch 870/883, Training Loss: 0.9886\n",
      "Epoch 10/10, Batch 871/883, Training Loss: 0.5130\n",
      "Epoch 10/10, Batch 872/883, Training Loss: 0.5847\n",
      "Epoch 10/10, Batch 873/883, Training Loss: 0.3595\n",
      "Epoch 10/10, Batch 874/883, Training Loss: 0.4627\n",
      "Epoch 10/10, Batch 875/883, Training Loss: 0.6702\n",
      "Epoch 10/10, Batch 876/883, Training Loss: 0.4420\n",
      "Epoch 10/10, Batch 877/883, Training Loss: 0.5930\n",
      "Epoch 10/10, Batch 878/883, Training Loss: 0.3837\n",
      "Epoch 10/10, Batch 879/883, Training Loss: 0.4606\n",
      "Epoch 10/10, Batch 880/883, Training Loss: 0.4659\n",
      "Epoch 10/10, Batch 881/883, Training Loss: 0.8512\n",
      "Epoch 10/10, Batch 882/883, Training Loss: 0.4614\n",
      "Epoch 10/10, Batch 883/883, Training Loss: 1.0104\n",
      "Epoch 10/10, Training Loss: 0.5602, Validation Loss: 0.5373, Validation Accuracy: 0.7686\n",
      "Test Loss: 0.5260, Test Accuracy: 0.7717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-27 20:03:40,605] Trial 1 finished with value: 0.5372937212316511 and parameters: {'batch_size': 16, 'learning_rate': 0.0046246250285297795, 'weight_decay': 7.14177291770463e-05}. Best is trial 0 with value: 0.2196991708036512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/442, Training Loss: 1.6141\n",
      "Epoch 1/10, Batch 2/442, Training Loss: 1.6847\n",
      "Epoch 1/10, Batch 3/442, Training Loss: 3.2437\n",
      "Epoch 1/10, Batch 4/442, Training Loss: 3.2642\n",
      "Epoch 1/10, Batch 5/442, Training Loss: 1.8986\n",
      "Epoch 1/10, Batch 6/442, Training Loss: 1.4026\n",
      "Epoch 1/10, Batch 7/442, Training Loss: 1.2354\n",
      "Epoch 1/10, Batch 8/442, Training Loss: 1.2611\n",
      "Epoch 1/10, Batch 9/442, Training Loss: 1.3275\n",
      "Epoch 1/10, Batch 10/442, Training Loss: 1.0095\n",
      "Epoch 1/10, Batch 11/442, Training Loss: 0.9113\n",
      "Epoch 1/10, Batch 12/442, Training Loss: 0.9545\n",
      "Epoch 1/10, Batch 13/442, Training Loss: 2.0563\n",
      "Epoch 1/10, Batch 14/442, Training Loss: 1.0994\n",
      "Epoch 1/10, Batch 15/442, Training Loss: 1.0934\n",
      "Epoch 1/10, Batch 16/442, Training Loss: 1.1653\n",
      "Epoch 1/10, Batch 17/442, Training Loss: 0.9365\n",
      "Epoch 1/10, Batch 18/442, Training Loss: 0.8920\n",
      "Epoch 1/10, Batch 19/442, Training Loss: 0.9190\n",
      "Epoch 1/10, Batch 20/442, Training Loss: 1.0503\n",
      "Epoch 1/10, Batch 21/442, Training Loss: 1.3044\n",
      "Epoch 1/10, Batch 22/442, Training Loss: 1.0709\n",
      "Epoch 1/10, Batch 23/442, Training Loss: 1.5335\n",
      "Epoch 1/10, Batch 24/442, Training Loss: 1.5156\n",
      "Epoch 1/10, Batch 25/442, Training Loss: 0.9535\n",
      "Epoch 1/10, Batch 26/442, Training Loss: 0.9843\n",
      "Epoch 1/10, Batch 27/442, Training Loss: 1.0997\n",
      "Epoch 1/10, Batch 28/442, Training Loss: 1.0088\n",
      "Epoch 1/10, Batch 29/442, Training Loss: 1.0316\n",
      "Epoch 1/10, Batch 30/442, Training Loss: 0.9378\n",
      "Epoch 1/10, Batch 31/442, Training Loss: 1.0750\n",
      "Epoch 1/10, Batch 32/442, Training Loss: 0.9394\n",
      "Epoch 1/10, Batch 33/442, Training Loss: 1.0874\n",
      "Epoch 1/10, Batch 34/442, Training Loss: 0.9284\n",
      "Epoch 1/10, Batch 35/442, Training Loss: 1.0061\n",
      "Epoch 1/10, Batch 36/442, Training Loss: 1.0783\n",
      "Epoch 1/10, Batch 37/442, Training Loss: 0.9585\n",
      "Epoch 1/10, Batch 38/442, Training Loss: 0.8482\n",
      "Epoch 1/10, Batch 39/442, Training Loss: 1.0267\n",
      "Epoch 1/10, Batch 40/442, Training Loss: 0.9252\n",
      "Epoch 1/10, Batch 41/442, Training Loss: 0.8799\n",
      "Epoch 1/10, Batch 42/442, Training Loss: 1.0258\n",
      "Epoch 1/10, Batch 43/442, Training Loss: 1.0001\n",
      "Epoch 1/10, Batch 44/442, Training Loss: 0.9105\n",
      "Epoch 1/10, Batch 45/442, Training Loss: 0.9832\n",
      "Epoch 1/10, Batch 46/442, Training Loss: 0.8908\n",
      "Epoch 1/10, Batch 47/442, Training Loss: 1.0136\n",
      "Epoch 1/10, Batch 48/442, Training Loss: 0.9155\n",
      "Epoch 1/10, Batch 49/442, Training Loss: 1.1048\n",
      "Epoch 1/10, Batch 50/442, Training Loss: 0.8250\n",
      "Epoch 1/10, Batch 51/442, Training Loss: 0.9238\n",
      "Epoch 1/10, Batch 52/442, Training Loss: 0.9713\n",
      "Epoch 1/10, Batch 53/442, Training Loss: 0.8905\n",
      "Epoch 1/10, Batch 54/442, Training Loss: 0.9555\n",
      "Epoch 1/10, Batch 55/442, Training Loss: 0.9973\n",
      "Epoch 1/10, Batch 56/442, Training Loss: 0.8741\n",
      "Epoch 1/10, Batch 57/442, Training Loss: 1.1115\n",
      "Epoch 1/10, Batch 58/442, Training Loss: 1.0069\n",
      "Epoch 1/10, Batch 59/442, Training Loss: 0.9310\n",
      "Epoch 1/10, Batch 60/442, Training Loss: 1.0039\n",
      "Epoch 1/10, Batch 61/442, Training Loss: 1.0219\n",
      "Epoch 1/10, Batch 62/442, Training Loss: 0.8988\n",
      "Epoch 1/10, Batch 63/442, Training Loss: 0.8602\n",
      "Epoch 1/10, Batch 64/442, Training Loss: 0.9306\n",
      "Epoch 1/10, Batch 65/442, Training Loss: 1.0152\n",
      "Epoch 1/10, Batch 66/442, Training Loss: 0.8527\n",
      "Epoch 1/10, Batch 67/442, Training Loss: 1.0064\n",
      "Epoch 1/10, Batch 68/442, Training Loss: 0.9437\n",
      "Epoch 1/10, Batch 69/442, Training Loss: 1.2317\n",
      "Epoch 1/10, Batch 70/442, Training Loss: 0.9385\n",
      "Epoch 1/10, Batch 71/442, Training Loss: 1.0396\n",
      "Epoch 1/10, Batch 72/442, Training Loss: 1.1045\n",
      "Epoch 1/10, Batch 73/442, Training Loss: 0.9798\n",
      "Epoch 1/10, Batch 74/442, Training Loss: 0.8359\n",
      "Epoch 1/10, Batch 75/442, Training Loss: 0.9139\n",
      "Epoch 1/10, Batch 76/442, Training Loss: 0.9561\n",
      "Epoch 1/10, Batch 77/442, Training Loss: 1.1251\n",
      "Epoch 1/10, Batch 78/442, Training Loss: 0.8281\n",
      "Epoch 1/10, Batch 79/442, Training Loss: 0.9249\n",
      "Epoch 1/10, Batch 80/442, Training Loss: 1.1945\n",
      "Epoch 1/10, Batch 81/442, Training Loss: 0.9844\n",
      "Epoch 1/10, Batch 82/442, Training Loss: 0.9195\n",
      "Epoch 1/10, Batch 83/442, Training Loss: 1.0093\n",
      "Epoch 1/10, Batch 84/442, Training Loss: 0.8087\n",
      "Epoch 1/10, Batch 85/442, Training Loss: 0.8572\n",
      "Epoch 1/10, Batch 86/442, Training Loss: 0.9221\n",
      "Epoch 1/10, Batch 87/442, Training Loss: 0.9279\n",
      "Epoch 1/10, Batch 88/442, Training Loss: 1.0327\n",
      "Epoch 1/10, Batch 89/442, Training Loss: 1.0601\n",
      "Epoch 1/10, Batch 90/442, Training Loss: 0.7921\n",
      "Epoch 1/10, Batch 91/442, Training Loss: 0.6932\n",
      "Epoch 1/10, Batch 92/442, Training Loss: 1.0914\n",
      "Epoch 1/10, Batch 93/442, Training Loss: 0.7180\n",
      "Epoch 1/10, Batch 94/442, Training Loss: 0.8808\n",
      "Epoch 1/10, Batch 95/442, Training Loss: 0.7793\n",
      "Epoch 1/10, Batch 96/442, Training Loss: 0.7359\n",
      "Epoch 1/10, Batch 97/442, Training Loss: 0.8786\n",
      "Epoch 1/10, Batch 98/442, Training Loss: 0.9214\n",
      "Epoch 1/10, Batch 99/442, Training Loss: 0.6963\n",
      "Epoch 1/10, Batch 100/442, Training Loss: 1.0258\n",
      "Epoch 1/10, Batch 101/442, Training Loss: 0.9713\n",
      "Epoch 1/10, Batch 102/442, Training Loss: 1.2159\n",
      "Epoch 1/10, Batch 103/442, Training Loss: 0.9202\n",
      "Epoch 1/10, Batch 104/442, Training Loss: 0.8087\n",
      "Epoch 1/10, Batch 105/442, Training Loss: 1.0966\n",
      "Epoch 1/10, Batch 106/442, Training Loss: 0.9024\n",
      "Epoch 1/10, Batch 107/442, Training Loss: 1.0824\n",
      "Epoch 1/10, Batch 108/442, Training Loss: 0.9420\n",
      "Epoch 1/10, Batch 109/442, Training Loss: 0.9500\n",
      "Epoch 1/10, Batch 110/442, Training Loss: 1.0085\n",
      "Epoch 1/10, Batch 111/442, Training Loss: 0.8904\n",
      "Epoch 1/10, Batch 112/442, Training Loss: 1.0996\n",
      "Epoch 1/10, Batch 113/442, Training Loss: 0.9096\n",
      "Epoch 1/10, Batch 114/442, Training Loss: 0.8072\n",
      "Epoch 1/10, Batch 115/442, Training Loss: 0.7718\n",
      "Epoch 1/10, Batch 116/442, Training Loss: 0.8260\n",
      "Epoch 1/10, Batch 117/442, Training Loss: 0.9157\n",
      "Epoch 1/10, Batch 118/442, Training Loss: 0.9165\n",
      "Epoch 1/10, Batch 119/442, Training Loss: 0.9075\n",
      "Epoch 1/10, Batch 120/442, Training Loss: 0.9271\n",
      "Epoch 1/10, Batch 121/442, Training Loss: 0.8025\n",
      "Epoch 1/10, Batch 122/442, Training Loss: 1.3242\n",
      "Epoch 1/10, Batch 123/442, Training Loss: 0.7553\n",
      "Epoch 1/10, Batch 124/442, Training Loss: 0.9223\n",
      "Epoch 1/10, Batch 125/442, Training Loss: 0.9098\n",
      "Epoch 1/10, Batch 126/442, Training Loss: 0.9364\n",
      "Epoch 1/10, Batch 127/442, Training Loss: 1.0927\n",
      "Epoch 1/10, Batch 128/442, Training Loss: 1.1065\n",
      "Epoch 1/10, Batch 129/442, Training Loss: 1.0671\n",
      "Epoch 1/10, Batch 130/442, Training Loss: 0.9036\n",
      "Epoch 1/10, Batch 131/442, Training Loss: 0.8370\n",
      "Epoch 1/10, Batch 132/442, Training Loss: 0.9841\n",
      "Epoch 1/10, Batch 133/442, Training Loss: 1.2272\n",
      "Epoch 1/10, Batch 134/442, Training Loss: 0.7912\n",
      "Epoch 1/10, Batch 135/442, Training Loss: 0.8597\n",
      "Epoch 1/10, Batch 136/442, Training Loss: 0.9419\n",
      "Epoch 1/10, Batch 137/442, Training Loss: 0.8772\n",
      "Epoch 1/10, Batch 138/442, Training Loss: 0.8460\n",
      "Epoch 1/10, Batch 139/442, Training Loss: 0.8987\n",
      "Epoch 1/10, Batch 140/442, Training Loss: 0.8006\n",
      "Epoch 1/10, Batch 141/442, Training Loss: 0.7886\n",
      "Epoch 1/10, Batch 142/442, Training Loss: 1.1847\n",
      "Epoch 1/10, Batch 143/442, Training Loss: 0.8716\n",
      "Epoch 1/10, Batch 144/442, Training Loss: 0.8081\n",
      "Epoch 1/10, Batch 145/442, Training Loss: 0.9330\n",
      "Epoch 1/10, Batch 146/442, Training Loss: 1.0640\n",
      "Epoch 1/10, Batch 147/442, Training Loss: 1.1691\n",
      "Epoch 1/10, Batch 148/442, Training Loss: 0.9188\n",
      "Epoch 1/10, Batch 149/442, Training Loss: 1.0492\n",
      "Epoch 1/10, Batch 150/442, Training Loss: 0.8155\n",
      "Epoch 1/10, Batch 151/442, Training Loss: 0.8585\n",
      "Epoch 1/10, Batch 152/442, Training Loss: 0.7749\n",
      "Epoch 1/10, Batch 153/442, Training Loss: 0.8937\n",
      "Epoch 1/10, Batch 154/442, Training Loss: 0.9255\n",
      "Epoch 1/10, Batch 155/442, Training Loss: 0.7568\n",
      "Epoch 1/10, Batch 156/442, Training Loss: 1.3156\n",
      "Epoch 1/10, Batch 157/442, Training Loss: 0.8736\n",
      "Epoch 1/10, Batch 158/442, Training Loss: 0.8220\n",
      "Epoch 1/10, Batch 159/442, Training Loss: 0.7849\n",
      "Epoch 1/10, Batch 160/442, Training Loss: 1.0268\n",
      "Epoch 1/10, Batch 161/442, Training Loss: 0.6738\n",
      "Epoch 1/10, Batch 162/442, Training Loss: 0.8845\n",
      "Epoch 1/10, Batch 163/442, Training Loss: 0.7064\n",
      "Epoch 1/10, Batch 164/442, Training Loss: 1.0893\n",
      "Epoch 1/10, Batch 165/442, Training Loss: 0.9912\n",
      "Epoch 1/10, Batch 166/442, Training Loss: 0.7168\n",
      "Epoch 1/10, Batch 167/442, Training Loss: 1.0312\n",
      "Epoch 1/10, Batch 168/442, Training Loss: 0.8647\n",
      "Epoch 1/10, Batch 169/442, Training Loss: 0.8456\n",
      "Epoch 1/10, Batch 170/442, Training Loss: 0.9118\n",
      "Epoch 1/10, Batch 171/442, Training Loss: 0.8324\n",
      "Epoch 1/10, Batch 172/442, Training Loss: 1.2015\n",
      "Epoch 1/10, Batch 173/442, Training Loss: 1.1394\n",
      "Epoch 1/10, Batch 174/442, Training Loss: 0.9428\n",
      "Epoch 1/10, Batch 175/442, Training Loss: 1.1825\n",
      "Epoch 1/10, Batch 176/442, Training Loss: 1.0333\n",
      "Epoch 1/10, Batch 177/442, Training Loss: 0.7752\n",
      "Epoch 1/10, Batch 178/442, Training Loss: 1.0296\n",
      "Epoch 1/10, Batch 179/442, Training Loss: 0.8854\n",
      "Epoch 1/10, Batch 180/442, Training Loss: 0.9450\n",
      "Epoch 1/10, Batch 181/442, Training Loss: 0.9970\n",
      "Epoch 1/10, Batch 182/442, Training Loss: 0.6984\n",
      "Epoch 1/10, Batch 183/442, Training Loss: 1.0906\n",
      "Epoch 1/10, Batch 184/442, Training Loss: 0.9780\n",
      "Epoch 1/10, Batch 185/442, Training Loss: 1.0066\n",
      "Epoch 1/10, Batch 186/442, Training Loss: 0.9840\n",
      "Epoch 1/10, Batch 187/442, Training Loss: 0.9417\n",
      "Epoch 1/10, Batch 188/442, Training Loss: 0.8934\n",
      "Epoch 1/10, Batch 189/442, Training Loss: 1.0464\n",
      "Epoch 1/10, Batch 190/442, Training Loss: 0.9468\n",
      "Epoch 1/10, Batch 191/442, Training Loss: 0.7686\n",
      "Epoch 1/10, Batch 192/442, Training Loss: 0.9448\n",
      "Epoch 1/10, Batch 193/442, Training Loss: 0.9392\n",
      "Epoch 1/10, Batch 194/442, Training Loss: 0.8968\n",
      "Epoch 1/10, Batch 195/442, Training Loss: 1.0113\n",
      "Epoch 1/10, Batch 196/442, Training Loss: 1.1572\n",
      "Epoch 1/10, Batch 197/442, Training Loss: 0.8582\n",
      "Epoch 1/10, Batch 198/442, Training Loss: 0.9378\n",
      "Epoch 1/10, Batch 199/442, Training Loss: 0.7898\n",
      "Epoch 1/10, Batch 200/442, Training Loss: 0.9203\n",
      "Epoch 1/10, Batch 201/442, Training Loss: 0.7316\n",
      "Epoch 1/10, Batch 202/442, Training Loss: 0.9780\n",
      "Epoch 1/10, Batch 203/442, Training Loss: 1.2684\n",
      "Epoch 1/10, Batch 204/442, Training Loss: 0.9229\n",
      "Epoch 1/10, Batch 205/442, Training Loss: 1.0733\n",
      "Epoch 1/10, Batch 206/442, Training Loss: 0.6933\n",
      "Epoch 1/10, Batch 207/442, Training Loss: 0.9399\n",
      "Epoch 1/10, Batch 208/442, Training Loss: 0.8538\n",
      "Epoch 1/10, Batch 209/442, Training Loss: 0.7023\n",
      "Epoch 1/10, Batch 210/442, Training Loss: 0.9537\n",
      "Epoch 1/10, Batch 211/442, Training Loss: 1.3217\n",
      "Epoch 1/10, Batch 212/442, Training Loss: 0.9007\n",
      "Epoch 1/10, Batch 213/442, Training Loss: 0.8571\n",
      "Epoch 1/10, Batch 214/442, Training Loss: 1.0015\n",
      "Epoch 1/10, Batch 215/442, Training Loss: 0.7680\n",
      "Epoch 1/10, Batch 216/442, Training Loss: 0.7758\n",
      "Epoch 1/10, Batch 217/442, Training Loss: 0.8984\n",
      "Epoch 1/10, Batch 218/442, Training Loss: 1.0530\n",
      "Epoch 1/10, Batch 219/442, Training Loss: 1.0593\n",
      "Epoch 1/10, Batch 220/442, Training Loss: 0.8765\n",
      "Epoch 1/10, Batch 221/442, Training Loss: 0.7834\n",
      "Epoch 1/10, Batch 222/442, Training Loss: 1.0751\n",
      "Epoch 1/10, Batch 223/442, Training Loss: 0.9062\n",
      "Epoch 1/10, Batch 224/442, Training Loss: 1.0383\n",
      "Epoch 1/10, Batch 225/442, Training Loss: 0.7950\n",
      "Epoch 1/10, Batch 226/442, Training Loss: 0.9868\n",
      "Epoch 1/10, Batch 227/442, Training Loss: 0.9006\n",
      "Epoch 1/10, Batch 228/442, Training Loss: 0.8361\n",
      "Epoch 1/10, Batch 229/442, Training Loss: 0.8069\n",
      "Epoch 1/10, Batch 230/442, Training Loss: 1.0500\n",
      "Epoch 1/10, Batch 231/442, Training Loss: 1.2173\n",
      "Epoch 1/10, Batch 232/442, Training Loss: 0.9482\n",
      "Epoch 1/10, Batch 233/442, Training Loss: 0.8888\n",
      "Epoch 1/10, Batch 234/442, Training Loss: 0.8262\n",
      "Epoch 1/10, Batch 235/442, Training Loss: 0.9457\n",
      "Epoch 1/10, Batch 236/442, Training Loss: 0.7976\n",
      "Epoch 1/10, Batch 237/442, Training Loss: 0.7658\n",
      "Epoch 1/10, Batch 238/442, Training Loss: 0.8312\n",
      "Epoch 1/10, Batch 239/442, Training Loss: 1.0753\n",
      "Epoch 1/10, Batch 240/442, Training Loss: 1.0423\n",
      "Epoch 1/10, Batch 241/442, Training Loss: 0.7546\n",
      "Epoch 1/10, Batch 242/442, Training Loss: 1.0230\n",
      "Epoch 1/10, Batch 243/442, Training Loss: 0.6918\n",
      "Epoch 1/10, Batch 244/442, Training Loss: 1.0483\n",
      "Epoch 1/10, Batch 245/442, Training Loss: 0.7594\n",
      "Epoch 1/10, Batch 246/442, Training Loss: 0.8478\n",
      "Epoch 1/10, Batch 247/442, Training Loss: 0.8366\n",
      "Epoch 1/10, Batch 248/442, Training Loss: 0.6911\n",
      "Epoch 1/10, Batch 249/442, Training Loss: 0.9316\n",
      "Epoch 1/10, Batch 250/442, Training Loss: 0.9520\n",
      "Epoch 1/10, Batch 251/442, Training Loss: 0.8532\n",
      "Epoch 1/10, Batch 252/442, Training Loss: 0.9050\n",
      "Epoch 1/10, Batch 253/442, Training Loss: 0.7459\n",
      "Epoch 1/10, Batch 254/442, Training Loss: 0.9833\n",
      "Epoch 1/10, Batch 255/442, Training Loss: 0.8472\n",
      "Epoch 1/10, Batch 256/442, Training Loss: 0.6133\n",
      "Epoch 1/10, Batch 257/442, Training Loss: 0.8353\n",
      "Epoch 1/10, Batch 258/442, Training Loss: 0.8656\n",
      "Epoch 1/10, Batch 259/442, Training Loss: 0.7169\n",
      "Epoch 1/10, Batch 260/442, Training Loss: 0.7752\n",
      "Epoch 1/10, Batch 261/442, Training Loss: 0.9018\n",
      "Epoch 1/10, Batch 262/442, Training Loss: 0.6876\n",
      "Epoch 1/10, Batch 263/442, Training Loss: 0.8842\n",
      "Epoch 1/10, Batch 264/442, Training Loss: 0.7161\n",
      "Epoch 1/10, Batch 265/442, Training Loss: 0.9161\n",
      "Epoch 1/10, Batch 266/442, Training Loss: 0.7061\n",
      "Epoch 1/10, Batch 267/442, Training Loss: 0.6886\n",
      "Epoch 1/10, Batch 268/442, Training Loss: 0.6741\n",
      "Epoch 1/10, Batch 269/442, Training Loss: 1.1240\n",
      "Epoch 1/10, Batch 270/442, Training Loss: 0.7343\n",
      "Epoch 1/10, Batch 271/442, Training Loss: 0.7881\n",
      "Epoch 1/10, Batch 272/442, Training Loss: 0.6348\n",
      "Epoch 1/10, Batch 273/442, Training Loss: 0.9960\n",
      "Epoch 1/10, Batch 274/442, Training Loss: 1.0281\n",
      "Epoch 1/10, Batch 275/442, Training Loss: 0.8152\n",
      "Epoch 1/10, Batch 276/442, Training Loss: 0.9330\n",
      "Epoch 1/10, Batch 277/442, Training Loss: 1.1350\n",
      "Epoch 1/10, Batch 278/442, Training Loss: 0.7810\n",
      "Epoch 1/10, Batch 279/442, Training Loss: 0.8951\n",
      "Epoch 1/10, Batch 280/442, Training Loss: 0.7843\n",
      "Epoch 1/10, Batch 281/442, Training Loss: 0.6751\n",
      "Epoch 1/10, Batch 282/442, Training Loss: 0.9098\n",
      "Epoch 1/10, Batch 283/442, Training Loss: 1.0440\n",
      "Epoch 1/10, Batch 284/442, Training Loss: 0.8355\n",
      "Epoch 1/10, Batch 285/442, Training Loss: 0.7971\n",
      "Epoch 1/10, Batch 286/442, Training Loss: 0.8992\n",
      "Epoch 1/10, Batch 287/442, Training Loss: 0.8893\n",
      "Epoch 1/10, Batch 288/442, Training Loss: 0.9209\n",
      "Epoch 1/10, Batch 289/442, Training Loss: 0.8738\n",
      "Epoch 1/10, Batch 290/442, Training Loss: 0.9515\n",
      "Epoch 1/10, Batch 291/442, Training Loss: 0.7162\n",
      "Epoch 1/10, Batch 292/442, Training Loss: 0.9776\n",
      "Epoch 1/10, Batch 293/442, Training Loss: 1.0944\n",
      "Epoch 1/10, Batch 294/442, Training Loss: 0.8974\n",
      "Epoch 1/10, Batch 295/442, Training Loss: 0.8009\n",
      "Epoch 1/10, Batch 296/442, Training Loss: 0.9177\n",
      "Epoch 1/10, Batch 297/442, Training Loss: 0.8626\n",
      "Epoch 1/10, Batch 298/442, Training Loss: 0.9144\n",
      "Epoch 1/10, Batch 299/442, Training Loss: 1.0004\n",
      "Epoch 1/10, Batch 300/442, Training Loss: 0.9402\n",
      "Epoch 1/10, Batch 301/442, Training Loss: 0.8297\n",
      "Epoch 1/10, Batch 302/442, Training Loss: 0.7811\n",
      "Epoch 1/10, Batch 303/442, Training Loss: 0.7971\n",
      "Epoch 1/10, Batch 304/442, Training Loss: 0.8433\n",
      "Epoch 1/10, Batch 305/442, Training Loss: 0.8738\n",
      "Epoch 1/10, Batch 306/442, Training Loss: 0.7342\n",
      "Epoch 1/10, Batch 307/442, Training Loss: 0.8388\n",
      "Epoch 1/10, Batch 308/442, Training Loss: 0.8257\n",
      "Epoch 1/10, Batch 309/442, Training Loss: 0.8661\n",
      "Epoch 1/10, Batch 310/442, Training Loss: 0.6699\n",
      "Epoch 1/10, Batch 311/442, Training Loss: 0.8466\n",
      "Epoch 1/10, Batch 312/442, Training Loss: 1.0668\n",
      "Epoch 1/10, Batch 313/442, Training Loss: 0.6740\n",
      "Epoch 1/10, Batch 314/442, Training Loss: 0.7586\n",
      "Epoch 1/10, Batch 315/442, Training Loss: 0.8929\n",
      "Epoch 1/10, Batch 316/442, Training Loss: 1.0379\n",
      "Epoch 1/10, Batch 317/442, Training Loss: 0.9811\n",
      "Epoch 1/10, Batch 318/442, Training Loss: 0.8545\n",
      "Epoch 1/10, Batch 319/442, Training Loss: 0.7009\n",
      "Epoch 1/10, Batch 320/442, Training Loss: 0.8540\n",
      "Epoch 1/10, Batch 321/442, Training Loss: 0.7496\n",
      "Epoch 1/10, Batch 322/442, Training Loss: 0.7512\n",
      "Epoch 1/10, Batch 323/442, Training Loss: 0.7514\n",
      "Epoch 1/10, Batch 324/442, Training Loss: 0.7330\n",
      "Epoch 1/10, Batch 325/442, Training Loss: 0.9573\n",
      "Epoch 1/10, Batch 326/442, Training Loss: 0.8544\n",
      "Epoch 1/10, Batch 327/442, Training Loss: 0.8360\n",
      "Epoch 1/10, Batch 328/442, Training Loss: 0.7280\n",
      "Epoch 1/10, Batch 329/442, Training Loss: 0.8636\n",
      "Epoch 1/10, Batch 330/442, Training Loss: 0.9193\n",
      "Epoch 1/10, Batch 331/442, Training Loss: 0.7036\n",
      "Epoch 1/10, Batch 332/442, Training Loss: 0.8415\n",
      "Epoch 1/10, Batch 333/442, Training Loss: 0.9321\n",
      "Epoch 1/10, Batch 334/442, Training Loss: 0.7681\n",
      "Epoch 1/10, Batch 335/442, Training Loss: 0.7646\n",
      "Epoch 1/10, Batch 336/442, Training Loss: 0.9969\n",
      "Epoch 1/10, Batch 337/442, Training Loss: 0.7596\n",
      "Epoch 1/10, Batch 338/442, Training Loss: 0.7310\n",
      "Epoch 1/10, Batch 339/442, Training Loss: 0.6719\n",
      "Epoch 1/10, Batch 340/442, Training Loss: 1.0159\n",
      "Epoch 1/10, Batch 341/442, Training Loss: 0.8644\n",
      "Epoch 1/10, Batch 342/442, Training Loss: 0.7800\n",
      "Epoch 1/10, Batch 343/442, Training Loss: 0.8486\n",
      "Epoch 1/10, Batch 344/442, Training Loss: 0.6907\n",
      "Epoch 1/10, Batch 345/442, Training Loss: 0.6128\n",
      "Epoch 1/10, Batch 346/442, Training Loss: 0.6782\n",
      "Epoch 1/10, Batch 347/442, Training Loss: 0.8507\n",
      "Epoch 1/10, Batch 348/442, Training Loss: 0.8646\n",
      "Epoch 1/10, Batch 349/442, Training Loss: 0.6285\n",
      "Epoch 1/10, Batch 350/442, Training Loss: 0.7068\n",
      "Epoch 1/10, Batch 351/442, Training Loss: 0.7965\n",
      "Epoch 1/10, Batch 352/442, Training Loss: 0.5715\n",
      "Epoch 1/10, Batch 353/442, Training Loss: 0.6865\n",
      "Epoch 1/10, Batch 354/442, Training Loss: 0.7433\n",
      "Epoch 1/10, Batch 355/442, Training Loss: 0.8620\n",
      "Epoch 1/10, Batch 356/442, Training Loss: 1.1180\n",
      "Epoch 1/10, Batch 357/442, Training Loss: 0.6016\n",
      "Epoch 1/10, Batch 358/442, Training Loss: 0.6719\n",
      "Epoch 1/10, Batch 359/442, Training Loss: 0.9059\n",
      "Epoch 1/10, Batch 360/442, Training Loss: 0.6413\n",
      "Epoch 1/10, Batch 361/442, Training Loss: 1.0464\n",
      "Epoch 1/10, Batch 362/442, Training Loss: 0.7732\n",
      "Epoch 1/10, Batch 363/442, Training Loss: 0.8028\n",
      "Epoch 1/10, Batch 364/442, Training Loss: 0.6050\n",
      "Epoch 1/10, Batch 365/442, Training Loss: 0.7126\n",
      "Epoch 1/10, Batch 366/442, Training Loss: 0.9334\n",
      "Epoch 1/10, Batch 367/442, Training Loss: 0.7972\n",
      "Epoch 1/10, Batch 368/442, Training Loss: 0.6904\n",
      "Epoch 1/10, Batch 369/442, Training Loss: 0.9149\n",
      "Epoch 1/10, Batch 370/442, Training Loss: 0.7112\n",
      "Epoch 1/10, Batch 371/442, Training Loss: 0.9239\n",
      "Epoch 1/10, Batch 372/442, Training Loss: 0.8495\n",
      "Epoch 1/10, Batch 373/442, Training Loss: 0.8443\n",
      "Epoch 1/10, Batch 374/442, Training Loss: 1.2212\n",
      "Epoch 1/10, Batch 375/442, Training Loss: 1.0795\n",
      "Epoch 1/10, Batch 376/442, Training Loss: 0.8893\n",
      "Epoch 1/10, Batch 377/442, Training Loss: 0.8637\n",
      "Epoch 1/10, Batch 378/442, Training Loss: 0.7956\n",
      "Epoch 1/10, Batch 379/442, Training Loss: 0.8360\n",
      "Epoch 1/10, Batch 380/442, Training Loss: 0.7838\n",
      "Epoch 1/10, Batch 381/442, Training Loss: 0.8496\n",
      "Epoch 1/10, Batch 382/442, Training Loss: 0.9984\n",
      "Epoch 1/10, Batch 383/442, Training Loss: 0.9254\n",
      "Epoch 1/10, Batch 384/442, Training Loss: 0.8249\n",
      "Epoch 1/10, Batch 385/442, Training Loss: 0.8397\n",
      "Epoch 1/10, Batch 386/442, Training Loss: 0.8076\n",
      "Epoch 1/10, Batch 387/442, Training Loss: 0.7941\n",
      "Epoch 1/10, Batch 388/442, Training Loss: 1.0047\n",
      "Epoch 1/10, Batch 389/442, Training Loss: 0.8408\n",
      "Epoch 1/10, Batch 390/442, Training Loss: 0.6914\n",
      "Epoch 1/10, Batch 391/442, Training Loss: 0.7592\n",
      "Epoch 1/10, Batch 392/442, Training Loss: 0.8793\n",
      "Epoch 1/10, Batch 393/442, Training Loss: 0.7357\n",
      "Epoch 1/10, Batch 394/442, Training Loss: 0.7842\n",
      "Epoch 1/10, Batch 395/442, Training Loss: 0.7070\n",
      "Epoch 1/10, Batch 396/442, Training Loss: 0.8922\n",
      "Epoch 1/10, Batch 397/442, Training Loss: 0.9338\n",
      "Epoch 1/10, Batch 398/442, Training Loss: 0.7705\n",
      "Epoch 1/10, Batch 399/442, Training Loss: 1.1013\n",
      "Epoch 1/10, Batch 400/442, Training Loss: 0.8369\n",
      "Epoch 1/10, Batch 401/442, Training Loss: 1.1397\n",
      "Epoch 1/10, Batch 402/442, Training Loss: 0.8993\n",
      "Epoch 1/10, Batch 403/442, Training Loss: 0.6893\n",
      "Epoch 1/10, Batch 404/442, Training Loss: 0.8672\n",
      "Epoch 1/10, Batch 405/442, Training Loss: 0.7578\n",
      "Epoch 1/10, Batch 406/442, Training Loss: 0.8377\n",
      "Epoch 1/10, Batch 407/442, Training Loss: 1.1116\n",
      "Epoch 1/10, Batch 408/442, Training Loss: 1.0100\n",
      "Epoch 1/10, Batch 409/442, Training Loss: 1.0405\n",
      "Epoch 1/10, Batch 410/442, Training Loss: 0.8009\n",
      "Epoch 1/10, Batch 411/442, Training Loss: 0.6920\n",
      "Epoch 1/10, Batch 412/442, Training Loss: 0.7652\n",
      "Epoch 1/10, Batch 413/442, Training Loss: 0.7297\n",
      "Epoch 1/10, Batch 414/442, Training Loss: 0.7933\n",
      "Epoch 1/10, Batch 415/442, Training Loss: 0.6763\n",
      "Epoch 1/10, Batch 416/442, Training Loss: 0.7297\n",
      "Epoch 1/10, Batch 417/442, Training Loss: 0.7056\n",
      "Epoch 1/10, Batch 418/442, Training Loss: 0.9086\n",
      "Epoch 1/10, Batch 419/442, Training Loss: 0.8825\n",
      "Epoch 1/10, Batch 420/442, Training Loss: 0.7834\n",
      "Epoch 1/10, Batch 421/442, Training Loss: 0.8503\n",
      "Epoch 1/10, Batch 422/442, Training Loss: 0.7315\n",
      "Epoch 1/10, Batch 423/442, Training Loss: 0.8058\n",
      "Epoch 1/10, Batch 424/442, Training Loss: 0.8695\n",
      "Epoch 1/10, Batch 425/442, Training Loss: 1.1311\n",
      "Epoch 1/10, Batch 426/442, Training Loss: 0.6683\n",
      "Epoch 1/10, Batch 427/442, Training Loss: 0.8351\n",
      "Epoch 1/10, Batch 428/442, Training Loss: 0.9033\n",
      "Epoch 1/10, Batch 429/442, Training Loss: 0.7196\n",
      "Epoch 1/10, Batch 430/442, Training Loss: 0.7840\n",
      "Epoch 1/10, Batch 431/442, Training Loss: 0.7513\n",
      "Epoch 1/10, Batch 432/442, Training Loss: 0.6958\n",
      "Epoch 1/10, Batch 433/442, Training Loss: 0.9514\n",
      "Epoch 1/10, Batch 434/442, Training Loss: 0.9757\n",
      "Epoch 1/10, Batch 435/442, Training Loss: 0.7024\n",
      "Epoch 1/10, Batch 436/442, Training Loss: 1.0181\n",
      "Epoch 1/10, Batch 437/442, Training Loss: 0.6202\n",
      "Epoch 1/10, Batch 438/442, Training Loss: 0.7650\n",
      "Epoch 1/10, Batch 439/442, Training Loss: 0.8360\n",
      "Epoch 1/10, Batch 440/442, Training Loss: 0.8520\n",
      "Epoch 1/10, Batch 441/442, Training Loss: 0.9178\n",
      "Epoch 1/10, Batch 442/442, Training Loss: 1.3174\n",
      "Epoch 1/10, Training Loss: 0.9210, Validation Loss: 0.8317, Validation Accuracy: 0.5518\n",
      "Epoch 2/10, Batch 1/442, Training Loss: 0.9133\n",
      "Epoch 2/10, Batch 2/442, Training Loss: 0.7818\n",
      "Epoch 2/10, Batch 3/442, Training Loss: 0.8304\n",
      "Epoch 2/10, Batch 4/442, Training Loss: 0.9337\n",
      "Epoch 2/10, Batch 5/442, Training Loss: 0.8344\n",
      "Epoch 2/10, Batch 6/442, Training Loss: 0.9821\n",
      "Epoch 2/10, Batch 7/442, Training Loss: 0.7633\n",
      "Epoch 2/10, Batch 8/442, Training Loss: 1.0239\n",
      "Epoch 2/10, Batch 9/442, Training Loss: 0.7443\n",
      "Epoch 2/10, Batch 10/442, Training Loss: 0.8492\n",
      "Epoch 2/10, Batch 11/442, Training Loss: 0.7284\n",
      "Epoch 2/10, Batch 12/442, Training Loss: 0.6960\n",
      "Epoch 2/10, Batch 13/442, Training Loss: 0.6820\n",
      "Epoch 2/10, Batch 14/442, Training Loss: 0.9620\n",
      "Epoch 2/10, Batch 15/442, Training Loss: 0.8949\n",
      "Epoch 2/10, Batch 16/442, Training Loss: 0.9468\n",
      "Epoch 2/10, Batch 17/442, Training Loss: 0.6371\n",
      "Epoch 2/10, Batch 18/442, Training Loss: 0.7666\n",
      "Epoch 2/10, Batch 19/442, Training Loss: 0.7520\n",
      "Epoch 2/10, Batch 20/442, Training Loss: 0.7574\n",
      "Epoch 2/10, Batch 21/442, Training Loss: 0.6912\n",
      "Epoch 2/10, Batch 22/442, Training Loss: 0.7495\n",
      "Epoch 2/10, Batch 23/442, Training Loss: 0.7843\n",
      "Epoch 2/10, Batch 24/442, Training Loss: 0.6365\n",
      "Epoch 2/10, Batch 25/442, Training Loss: 0.5333\n",
      "Epoch 2/10, Batch 26/442, Training Loss: 1.0186\n",
      "Epoch 2/10, Batch 27/442, Training Loss: 0.7244\n",
      "Epoch 2/10, Batch 28/442, Training Loss: 0.7743\n",
      "Epoch 2/10, Batch 29/442, Training Loss: 1.1262\n",
      "Epoch 2/10, Batch 30/442, Training Loss: 0.7627\n",
      "Epoch 2/10, Batch 31/442, Training Loss: 0.9262\n",
      "Epoch 2/10, Batch 32/442, Training Loss: 1.1060\n",
      "Epoch 2/10, Batch 33/442, Training Loss: 0.8529\n",
      "Epoch 2/10, Batch 34/442, Training Loss: 0.7090\n",
      "Epoch 2/10, Batch 35/442, Training Loss: 1.0524\n",
      "Epoch 2/10, Batch 36/442, Training Loss: 0.7821\n",
      "Epoch 2/10, Batch 37/442, Training Loss: 0.6832\n",
      "Epoch 2/10, Batch 38/442, Training Loss: 0.9406\n",
      "Epoch 2/10, Batch 39/442, Training Loss: 0.9073\n",
      "Epoch 2/10, Batch 40/442, Training Loss: 0.7476\n",
      "Epoch 2/10, Batch 41/442, Training Loss: 0.7295\n",
      "Epoch 2/10, Batch 42/442, Training Loss: 0.8248\n",
      "Epoch 2/10, Batch 43/442, Training Loss: 1.2464\n",
      "Epoch 2/10, Batch 44/442, Training Loss: 0.7715\n",
      "Epoch 2/10, Batch 45/442, Training Loss: 0.9199\n",
      "Epoch 2/10, Batch 46/442, Training Loss: 0.7262\n",
      "Epoch 2/10, Batch 47/442, Training Loss: 0.9499\n",
      "Epoch 2/10, Batch 48/442, Training Loss: 0.8733\n",
      "Epoch 2/10, Batch 49/442, Training Loss: 0.8816\n",
      "Epoch 2/10, Batch 50/442, Training Loss: 0.8258\n",
      "Epoch 2/10, Batch 51/442, Training Loss: 0.9522\n",
      "Epoch 2/10, Batch 52/442, Training Loss: 1.0115\n",
      "Epoch 2/10, Batch 53/442, Training Loss: 0.9351\n",
      "Epoch 2/10, Batch 54/442, Training Loss: 0.6867\n",
      "Epoch 2/10, Batch 55/442, Training Loss: 0.8836\n",
      "Epoch 2/10, Batch 56/442, Training Loss: 0.8662\n",
      "Epoch 2/10, Batch 57/442, Training Loss: 0.8199\n",
      "Epoch 2/10, Batch 58/442, Training Loss: 0.6430\n",
      "Epoch 2/10, Batch 59/442, Training Loss: 0.8036\n",
      "Epoch 2/10, Batch 60/442, Training Loss: 0.7541\n",
      "Epoch 2/10, Batch 61/442, Training Loss: 0.7675\n",
      "Epoch 2/10, Batch 62/442, Training Loss: 0.9963\n",
      "Epoch 2/10, Batch 63/442, Training Loss: 0.8852\n",
      "Epoch 2/10, Batch 64/442, Training Loss: 0.8678\n",
      "Epoch 2/10, Batch 65/442, Training Loss: 0.9426\n",
      "Epoch 2/10, Batch 66/442, Training Loss: 0.8906\n",
      "Epoch 2/10, Batch 67/442, Training Loss: 0.7075\n",
      "Epoch 2/10, Batch 68/442, Training Loss: 0.7230\n",
      "Epoch 2/10, Batch 69/442, Training Loss: 0.8533\n",
      "Epoch 2/10, Batch 70/442, Training Loss: 1.0928\n",
      "Epoch 2/10, Batch 71/442, Training Loss: 0.7850\n",
      "Epoch 2/10, Batch 72/442, Training Loss: 0.8952\n",
      "Epoch 2/10, Batch 73/442, Training Loss: 0.8585\n",
      "Epoch 2/10, Batch 74/442, Training Loss: 0.7499\n",
      "Epoch 2/10, Batch 75/442, Training Loss: 0.6966\n",
      "Epoch 2/10, Batch 76/442, Training Loss: 0.9324\n",
      "Epoch 2/10, Batch 77/442, Training Loss: 0.9369\n",
      "Epoch 2/10, Batch 78/442, Training Loss: 0.8408\n",
      "Epoch 2/10, Batch 79/442, Training Loss: 0.7979\n",
      "Epoch 2/10, Batch 80/442, Training Loss: 0.7176\n",
      "Epoch 2/10, Batch 81/442, Training Loss: 0.9951\n",
      "Epoch 2/10, Batch 82/442, Training Loss: 0.8273\n",
      "Epoch 2/10, Batch 83/442, Training Loss: 0.7124\n",
      "Epoch 2/10, Batch 84/442, Training Loss: 0.7350\n",
      "Epoch 2/10, Batch 85/442, Training Loss: 0.8148\n",
      "Epoch 2/10, Batch 86/442, Training Loss: 0.7367\n",
      "Epoch 2/10, Batch 87/442, Training Loss: 0.8893\n",
      "Epoch 2/10, Batch 88/442, Training Loss: 0.7828\n",
      "Epoch 2/10, Batch 89/442, Training Loss: 0.8723\n",
      "Epoch 2/10, Batch 90/442, Training Loss: 0.8681\n",
      "Epoch 2/10, Batch 91/442, Training Loss: 0.8336\n",
      "Epoch 2/10, Batch 92/442, Training Loss: 0.9016\n",
      "Epoch 2/10, Batch 93/442, Training Loss: 0.7749\n",
      "Epoch 2/10, Batch 94/442, Training Loss: 0.8555\n",
      "Epoch 2/10, Batch 95/442, Training Loss: 1.0815\n",
      "Epoch 2/10, Batch 96/442, Training Loss: 0.8991\n",
      "Epoch 2/10, Batch 97/442, Training Loss: 0.6622\n",
      "Epoch 2/10, Batch 98/442, Training Loss: 0.6593\n",
      "Epoch 2/10, Batch 99/442, Training Loss: 0.7920\n",
      "Epoch 2/10, Batch 100/442, Training Loss: 0.7762\n",
      "Epoch 2/10, Batch 101/442, Training Loss: 0.6940\n",
      "Epoch 2/10, Batch 102/442, Training Loss: 0.8896\n",
      "Epoch 2/10, Batch 103/442, Training Loss: 0.8789\n",
      "Epoch 2/10, Batch 104/442, Training Loss: 0.7323\n",
      "Epoch 2/10, Batch 105/442, Training Loss: 1.0989\n",
      "Epoch 2/10, Batch 106/442, Training Loss: 0.6912\n",
      "Epoch 2/10, Batch 107/442, Training Loss: 0.9629\n",
      "Epoch 2/10, Batch 108/442, Training Loss: 0.9794\n",
      "Epoch 2/10, Batch 109/442, Training Loss: 0.7698\n",
      "Epoch 2/10, Batch 110/442, Training Loss: 0.7719\n",
      "Epoch 2/10, Batch 111/442, Training Loss: 0.7569\n",
      "Epoch 2/10, Batch 112/442, Training Loss: 0.8079\n",
      "Epoch 2/10, Batch 113/442, Training Loss: 0.7179\n",
      "Epoch 2/10, Batch 114/442, Training Loss: 0.7376\n",
      "Epoch 2/10, Batch 115/442, Training Loss: 0.7754\n",
      "Epoch 2/10, Batch 116/442, Training Loss: 0.7112\n",
      "Epoch 2/10, Batch 117/442, Training Loss: 0.7408\n",
      "Epoch 2/10, Batch 118/442, Training Loss: 0.9593\n",
      "Epoch 2/10, Batch 119/442, Training Loss: 1.2291\n",
      "Epoch 2/10, Batch 120/442, Training Loss: 0.8752\n",
      "Epoch 2/10, Batch 121/442, Training Loss: 0.8528\n",
      "Epoch 2/10, Batch 122/442, Training Loss: 0.7582\n",
      "Epoch 2/10, Batch 123/442, Training Loss: 0.8490\n",
      "Epoch 2/10, Batch 124/442, Training Loss: 0.7658\n",
      "Epoch 2/10, Batch 125/442, Training Loss: 0.6393\n",
      "Epoch 2/10, Batch 126/442, Training Loss: 0.5815\n",
      "Epoch 2/10, Batch 127/442, Training Loss: 0.7668\n",
      "Epoch 2/10, Batch 128/442, Training Loss: 0.6573\n",
      "Epoch 2/10, Batch 129/442, Training Loss: 0.8465\n",
      "Epoch 2/10, Batch 130/442, Training Loss: 0.8473\n",
      "Epoch 2/10, Batch 131/442, Training Loss: 0.6509\n",
      "Epoch 2/10, Batch 132/442, Training Loss: 0.6359\n",
      "Epoch 2/10, Batch 133/442, Training Loss: 0.6321\n",
      "Epoch 2/10, Batch 134/442, Training Loss: 0.8138\n",
      "Epoch 2/10, Batch 135/442, Training Loss: 0.8544\n",
      "Epoch 2/10, Batch 136/442, Training Loss: 0.9862\n",
      "Epoch 2/10, Batch 137/442, Training Loss: 0.7991\n",
      "Epoch 2/10, Batch 138/442, Training Loss: 0.6264\n",
      "Epoch 2/10, Batch 139/442, Training Loss: 0.9546\n",
      "Epoch 2/10, Batch 140/442, Training Loss: 0.6654\n",
      "Epoch 2/10, Batch 141/442, Training Loss: 0.8419\n",
      "Epoch 2/10, Batch 142/442, Training Loss: 0.8159\n",
      "Epoch 2/10, Batch 143/442, Training Loss: 0.7023\n",
      "Epoch 2/10, Batch 144/442, Training Loss: 0.6842\n",
      "Epoch 2/10, Batch 145/442, Training Loss: 0.8413\n",
      "Epoch 2/10, Batch 146/442, Training Loss: 0.7857\n",
      "Epoch 2/10, Batch 147/442, Training Loss: 0.7393\n",
      "Epoch 2/10, Batch 148/442, Training Loss: 0.7959\n",
      "Epoch 2/10, Batch 149/442, Training Loss: 0.6970\n",
      "Epoch 2/10, Batch 150/442, Training Loss: 0.8869\n",
      "Epoch 2/10, Batch 151/442, Training Loss: 0.7765\n",
      "Epoch 2/10, Batch 152/442, Training Loss: 0.6177\n",
      "Epoch 2/10, Batch 153/442, Training Loss: 0.8100\n",
      "Epoch 2/10, Batch 154/442, Training Loss: 0.8985\n",
      "Epoch 2/10, Batch 155/442, Training Loss: 1.2621\n",
      "Epoch 2/10, Batch 156/442, Training Loss: 0.6208\n",
      "Epoch 2/10, Batch 157/442, Training Loss: 0.8954\n",
      "Epoch 2/10, Batch 158/442, Training Loss: 0.7055\n",
      "Epoch 2/10, Batch 159/442, Training Loss: 0.8635\n",
      "Epoch 2/10, Batch 160/442, Training Loss: 0.7468\n",
      "Epoch 2/10, Batch 161/442, Training Loss: 0.8320\n",
      "Epoch 2/10, Batch 162/442, Training Loss: 0.8123\n",
      "Epoch 2/10, Batch 163/442, Training Loss: 0.6074\n",
      "Epoch 2/10, Batch 164/442, Training Loss: 0.8251\n",
      "Epoch 2/10, Batch 165/442, Training Loss: 0.8273\n",
      "Epoch 2/10, Batch 166/442, Training Loss: 0.9798\n",
      "Epoch 2/10, Batch 167/442, Training Loss: 0.6609\n",
      "Epoch 2/10, Batch 168/442, Training Loss: 0.6772\n",
      "Epoch 2/10, Batch 169/442, Training Loss: 0.9718\n",
      "Epoch 2/10, Batch 170/442, Training Loss: 0.8913\n",
      "Epoch 2/10, Batch 171/442, Training Loss: 0.8331\n",
      "Epoch 2/10, Batch 172/442, Training Loss: 0.7318\n",
      "Epoch 2/10, Batch 173/442, Training Loss: 0.8910\n",
      "Epoch 2/10, Batch 174/442, Training Loss: 0.7965\n",
      "Epoch 2/10, Batch 175/442, Training Loss: 0.7584\n",
      "Epoch 2/10, Batch 176/442, Training Loss: 0.6969\n",
      "Epoch 2/10, Batch 177/442, Training Loss: 0.6842\n",
      "Epoch 2/10, Batch 178/442, Training Loss: 0.6330\n",
      "Epoch 2/10, Batch 179/442, Training Loss: 0.6983\n",
      "Epoch 2/10, Batch 180/442, Training Loss: 0.7657\n",
      "Epoch 2/10, Batch 181/442, Training Loss: 0.7586\n",
      "Epoch 2/10, Batch 182/442, Training Loss: 0.7968\n",
      "Epoch 2/10, Batch 183/442, Training Loss: 0.8881\n",
      "Epoch 2/10, Batch 184/442, Training Loss: 0.6952\n",
      "Epoch 2/10, Batch 185/442, Training Loss: 0.7049\n",
      "Epoch 2/10, Batch 186/442, Training Loss: 0.8109\n",
      "Epoch 2/10, Batch 187/442, Training Loss: 0.8481\n",
      "Epoch 2/10, Batch 188/442, Training Loss: 0.7385\n",
      "Epoch 2/10, Batch 189/442, Training Loss: 0.7324\n",
      "Epoch 2/10, Batch 190/442, Training Loss: 0.7702\n",
      "Epoch 2/10, Batch 191/442, Training Loss: 0.6424\n",
      "Epoch 2/10, Batch 192/442, Training Loss: 0.6518\n",
      "Epoch 2/10, Batch 193/442, Training Loss: 0.7178\n",
      "Epoch 2/10, Batch 194/442, Training Loss: 0.7488\n",
      "Epoch 2/10, Batch 195/442, Training Loss: 0.7311\n",
      "Epoch 2/10, Batch 196/442, Training Loss: 0.7954\n",
      "Epoch 2/10, Batch 197/442, Training Loss: 0.8057\n",
      "Epoch 2/10, Batch 198/442, Training Loss: 0.6899\n",
      "Epoch 2/10, Batch 199/442, Training Loss: 0.6824\n",
      "Epoch 2/10, Batch 200/442, Training Loss: 0.8675\n",
      "Epoch 2/10, Batch 201/442, Training Loss: 0.7137\n",
      "Epoch 2/10, Batch 202/442, Training Loss: 0.6347\n",
      "Epoch 2/10, Batch 203/442, Training Loss: 0.6307\n",
      "Epoch 2/10, Batch 204/442, Training Loss: 1.1331\n",
      "Epoch 2/10, Batch 205/442, Training Loss: 0.8470\n",
      "Epoch 2/10, Batch 206/442, Training Loss: 0.7769\n",
      "Epoch 2/10, Batch 207/442, Training Loss: 1.1680\n",
      "Epoch 2/10, Batch 208/442, Training Loss: 0.7773\n",
      "Epoch 2/10, Batch 209/442, Training Loss: 0.6247\n",
      "Epoch 2/10, Batch 210/442, Training Loss: 0.5414\n",
      "Epoch 2/10, Batch 211/442, Training Loss: 0.7185\n",
      "Epoch 2/10, Batch 212/442, Training Loss: 0.8365\n",
      "Epoch 2/10, Batch 213/442, Training Loss: 0.9998\n",
      "Epoch 2/10, Batch 214/442, Training Loss: 0.6293\n",
      "Epoch 2/10, Batch 215/442, Training Loss: 0.7347\n",
      "Epoch 2/10, Batch 216/442, Training Loss: 0.7366\n",
      "Epoch 2/10, Batch 217/442, Training Loss: 0.6919\n",
      "Epoch 2/10, Batch 218/442, Training Loss: 0.7944\n",
      "Epoch 2/10, Batch 219/442, Training Loss: 0.7387\n",
      "Epoch 2/10, Batch 220/442, Training Loss: 0.7279\n",
      "Epoch 2/10, Batch 221/442, Training Loss: 0.7438\n",
      "Epoch 2/10, Batch 222/442, Training Loss: 0.7098\n",
      "Epoch 2/10, Batch 223/442, Training Loss: 0.8273\n",
      "Epoch 2/10, Batch 224/442, Training Loss: 0.6267\n",
      "Epoch 2/10, Batch 225/442, Training Loss: 1.0414\n",
      "Epoch 2/10, Batch 226/442, Training Loss: 0.8077\n",
      "Epoch 2/10, Batch 227/442, Training Loss: 0.6625\n",
      "Epoch 2/10, Batch 228/442, Training Loss: 0.9698\n",
      "Epoch 2/10, Batch 229/442, Training Loss: 0.7618\n",
      "Epoch 2/10, Batch 230/442, Training Loss: 0.7487\n",
      "Epoch 2/10, Batch 231/442, Training Loss: 0.8115\n",
      "Epoch 2/10, Batch 232/442, Training Loss: 0.8081\n",
      "Epoch 2/10, Batch 233/442, Training Loss: 0.8504\n",
      "Epoch 2/10, Batch 234/442, Training Loss: 0.8338\n",
      "Epoch 2/10, Batch 235/442, Training Loss: 0.6366\n",
      "Epoch 2/10, Batch 236/442, Training Loss: 0.7359\n",
      "Epoch 2/10, Batch 237/442, Training Loss: 0.7340\n",
      "Epoch 2/10, Batch 238/442, Training Loss: 0.7716\n",
      "Epoch 2/10, Batch 239/442, Training Loss: 0.7678\n",
      "Epoch 2/10, Batch 240/442, Training Loss: 0.8633\n",
      "Epoch 2/10, Batch 241/442, Training Loss: 0.7414\n",
      "Epoch 2/10, Batch 242/442, Training Loss: 0.7201\n",
      "Epoch 2/10, Batch 243/442, Training Loss: 0.6982\n",
      "Epoch 2/10, Batch 244/442, Training Loss: 0.8092\n",
      "Epoch 2/10, Batch 245/442, Training Loss: 0.8951\n",
      "Epoch 2/10, Batch 246/442, Training Loss: 0.7914\n",
      "Epoch 2/10, Batch 247/442, Training Loss: 0.7508\n",
      "Epoch 2/10, Batch 248/442, Training Loss: 0.7719\n",
      "Epoch 2/10, Batch 249/442, Training Loss: 0.7901\n",
      "Epoch 2/10, Batch 250/442, Training Loss: 0.9288\n",
      "Epoch 2/10, Batch 251/442, Training Loss: 0.9282\n",
      "Epoch 2/10, Batch 252/442, Training Loss: 0.6913\n",
      "Epoch 2/10, Batch 253/442, Training Loss: 0.6960\n",
      "Epoch 2/10, Batch 254/442, Training Loss: 0.7800\n",
      "Epoch 2/10, Batch 255/442, Training Loss: 0.6134\n",
      "Epoch 2/10, Batch 256/442, Training Loss: 0.8936\n",
      "Epoch 2/10, Batch 257/442, Training Loss: 0.8941\n",
      "Epoch 2/10, Batch 258/442, Training Loss: 0.8029\n",
      "Epoch 2/10, Batch 259/442, Training Loss: 0.7989\n",
      "Epoch 2/10, Batch 260/442, Training Loss: 1.0929\n",
      "Epoch 2/10, Batch 261/442, Training Loss: 0.8217\n",
      "Epoch 2/10, Batch 262/442, Training Loss: 0.6647\n",
      "Epoch 2/10, Batch 263/442, Training Loss: 0.6503\n",
      "Epoch 2/10, Batch 264/442, Training Loss: 1.0231\n",
      "Epoch 2/10, Batch 265/442, Training Loss: 0.7331\n",
      "Epoch 2/10, Batch 266/442, Training Loss: 0.8142\n",
      "Epoch 2/10, Batch 267/442, Training Loss: 0.8886\n",
      "Epoch 2/10, Batch 268/442, Training Loss: 0.9008\n",
      "Epoch 2/10, Batch 269/442, Training Loss: 0.9072\n",
      "Epoch 2/10, Batch 270/442, Training Loss: 0.8583\n",
      "Epoch 2/10, Batch 271/442, Training Loss: 0.8664\n",
      "Epoch 2/10, Batch 272/442, Training Loss: 0.6693\n",
      "Epoch 2/10, Batch 273/442, Training Loss: 0.8713\n",
      "Epoch 2/10, Batch 274/442, Training Loss: 0.7800\n",
      "Epoch 2/10, Batch 275/442, Training Loss: 0.6848\n",
      "Epoch 2/10, Batch 276/442, Training Loss: 0.7008\n",
      "Epoch 2/10, Batch 277/442, Training Loss: 0.6930\n",
      "Epoch 2/10, Batch 278/442, Training Loss: 1.0636\n",
      "Epoch 2/10, Batch 279/442, Training Loss: 0.7693\n",
      "Epoch 2/10, Batch 280/442, Training Loss: 0.7790\n",
      "Epoch 2/10, Batch 281/442, Training Loss: 0.7561\n",
      "Epoch 2/10, Batch 282/442, Training Loss: 0.7245\n",
      "Epoch 2/10, Batch 283/442, Training Loss: 0.6161\n",
      "Epoch 2/10, Batch 284/442, Training Loss: 0.6885\n",
      "Epoch 2/10, Batch 285/442, Training Loss: 0.7208\n",
      "Epoch 2/10, Batch 286/442, Training Loss: 0.7811\n",
      "Epoch 2/10, Batch 287/442, Training Loss: 0.7697\n",
      "Epoch 2/10, Batch 288/442, Training Loss: 0.7638\n",
      "Epoch 2/10, Batch 289/442, Training Loss: 0.6427\n",
      "Epoch 2/10, Batch 290/442, Training Loss: 0.6129\n",
      "Epoch 2/10, Batch 291/442, Training Loss: 0.7398\n",
      "Epoch 2/10, Batch 292/442, Training Loss: 0.9842\n",
      "Epoch 2/10, Batch 293/442, Training Loss: 0.6637\n",
      "Epoch 2/10, Batch 294/442, Training Loss: 0.6364\n",
      "Epoch 2/10, Batch 295/442, Training Loss: 0.7661\n",
      "Epoch 2/10, Batch 296/442, Training Loss: 0.8556\n",
      "Epoch 2/10, Batch 297/442, Training Loss: 0.6267\n",
      "Epoch 2/10, Batch 298/442, Training Loss: 0.7410\n",
      "Epoch 2/10, Batch 299/442, Training Loss: 0.9640\n",
      "Epoch 2/10, Batch 300/442, Training Loss: 1.0110\n",
      "Epoch 2/10, Batch 301/442, Training Loss: 0.8477\n",
      "Epoch 2/10, Batch 302/442, Training Loss: 0.6247\n",
      "Epoch 2/10, Batch 303/442, Training Loss: 0.6060\n",
      "Epoch 2/10, Batch 304/442, Training Loss: 0.9087\n",
      "Epoch 2/10, Batch 305/442, Training Loss: 0.7992\n",
      "Epoch 2/10, Batch 306/442, Training Loss: 0.8597\n",
      "Epoch 2/10, Batch 307/442, Training Loss: 0.8002\n",
      "Epoch 2/10, Batch 308/442, Training Loss: 0.7043\n",
      "Epoch 2/10, Batch 309/442, Training Loss: 0.6828\n",
      "Epoch 2/10, Batch 310/442, Training Loss: 0.8442\n",
      "Epoch 2/10, Batch 311/442, Training Loss: 0.8354\n",
      "Epoch 2/10, Batch 312/442, Training Loss: 0.7515\n",
      "Epoch 2/10, Batch 313/442, Training Loss: 0.6224\n",
      "Epoch 2/10, Batch 314/442, Training Loss: 0.7865\n",
      "Epoch 2/10, Batch 315/442, Training Loss: 0.7520\n",
      "Epoch 2/10, Batch 316/442, Training Loss: 0.8026\n",
      "Epoch 2/10, Batch 317/442, Training Loss: 0.8331\n",
      "Epoch 2/10, Batch 318/442, Training Loss: 0.8605\n",
      "Epoch 2/10, Batch 319/442, Training Loss: 0.8922\n",
      "Epoch 2/10, Batch 320/442, Training Loss: 0.8309\n",
      "Epoch 2/10, Batch 321/442, Training Loss: 1.0973\n",
      "Epoch 2/10, Batch 322/442, Training Loss: 0.7363\n",
      "Epoch 2/10, Batch 323/442, Training Loss: 0.6506\n",
      "Epoch 2/10, Batch 324/442, Training Loss: 0.5823\n",
      "Epoch 2/10, Batch 325/442, Training Loss: 0.7836\n",
      "Epoch 2/10, Batch 326/442, Training Loss: 0.8425\n",
      "Epoch 2/10, Batch 327/442, Training Loss: 0.9142\n",
      "Epoch 2/10, Batch 328/442, Training Loss: 0.7797\n",
      "Epoch 2/10, Batch 329/442, Training Loss: 1.0497\n",
      "Epoch 2/10, Batch 330/442, Training Loss: 0.5926\n",
      "Epoch 2/10, Batch 331/442, Training Loss: 0.9629\n",
      "Epoch 2/10, Batch 332/442, Training Loss: 1.0199\n",
      "Epoch 2/10, Batch 333/442, Training Loss: 0.7282\n",
      "Epoch 2/10, Batch 334/442, Training Loss: 0.7310\n",
      "Epoch 2/10, Batch 335/442, Training Loss: 0.6686\n",
      "Epoch 2/10, Batch 336/442, Training Loss: 0.8877\n",
      "Epoch 2/10, Batch 337/442, Training Loss: 0.9668\n",
      "Epoch 2/10, Batch 338/442, Training Loss: 0.7635\n",
      "Epoch 2/10, Batch 339/442, Training Loss: 0.7443\n",
      "Epoch 2/10, Batch 340/442, Training Loss: 0.9197\n",
      "Epoch 2/10, Batch 341/442, Training Loss: 0.6984\n",
      "Epoch 2/10, Batch 342/442, Training Loss: 0.8039\n",
      "Epoch 2/10, Batch 343/442, Training Loss: 0.7736\n",
      "Epoch 2/10, Batch 344/442, Training Loss: 0.7398\n",
      "Epoch 2/10, Batch 345/442, Training Loss: 0.7405\n",
      "Epoch 2/10, Batch 346/442, Training Loss: 0.7398\n",
      "Epoch 2/10, Batch 347/442, Training Loss: 0.7574\n",
      "Epoch 2/10, Batch 348/442, Training Loss: 0.9346\n",
      "Epoch 2/10, Batch 349/442, Training Loss: 0.7445\n",
      "Epoch 2/10, Batch 350/442, Training Loss: 0.7706\n",
      "Epoch 2/10, Batch 351/442, Training Loss: 0.6312\n",
      "Epoch 2/10, Batch 352/442, Training Loss: 0.7009\n",
      "Epoch 2/10, Batch 353/442, Training Loss: 0.9198\n",
      "Epoch 2/10, Batch 354/442, Training Loss: 0.6140\n",
      "Epoch 2/10, Batch 355/442, Training Loss: 0.9084\n",
      "Epoch 2/10, Batch 356/442, Training Loss: 0.7515\n",
      "Epoch 2/10, Batch 357/442, Training Loss: 0.8872\n",
      "Epoch 2/10, Batch 358/442, Training Loss: 0.8176\n",
      "Epoch 2/10, Batch 359/442, Training Loss: 0.9831\n",
      "Epoch 2/10, Batch 360/442, Training Loss: 0.8776\n",
      "Epoch 2/10, Batch 361/442, Training Loss: 0.6764\n",
      "Epoch 2/10, Batch 362/442, Training Loss: 0.8683\n",
      "Epoch 2/10, Batch 363/442, Training Loss: 0.7663\n",
      "Epoch 2/10, Batch 364/442, Training Loss: 1.0261\n",
      "Epoch 2/10, Batch 365/442, Training Loss: 0.9379\n",
      "Epoch 2/10, Batch 366/442, Training Loss: 0.7821\n",
      "Epoch 2/10, Batch 367/442, Training Loss: 0.7530\n",
      "Epoch 2/10, Batch 368/442, Training Loss: 0.7115\n",
      "Epoch 2/10, Batch 369/442, Training Loss: 0.7492\n",
      "Epoch 2/10, Batch 370/442, Training Loss: 0.7614\n",
      "Epoch 2/10, Batch 371/442, Training Loss: 0.7830\n",
      "Epoch 2/10, Batch 372/442, Training Loss: 0.6869\n",
      "Epoch 2/10, Batch 373/442, Training Loss: 0.7788\n",
      "Epoch 2/10, Batch 374/442, Training Loss: 0.8440\n",
      "Epoch 2/10, Batch 375/442, Training Loss: 0.7428\n",
      "Epoch 2/10, Batch 376/442, Training Loss: 0.7028\n",
      "Epoch 2/10, Batch 377/442, Training Loss: 1.1939\n",
      "Epoch 2/10, Batch 378/442, Training Loss: 0.8743\n",
      "Epoch 2/10, Batch 379/442, Training Loss: 0.6171\n",
      "Epoch 2/10, Batch 380/442, Training Loss: 0.4662\n",
      "Epoch 2/10, Batch 381/442, Training Loss: 0.7460\n",
      "Epoch 2/10, Batch 382/442, Training Loss: 0.6073\n",
      "Epoch 2/10, Batch 383/442, Training Loss: 1.0070\n",
      "Epoch 2/10, Batch 384/442, Training Loss: 0.7641\n",
      "Epoch 2/10, Batch 385/442, Training Loss: 0.6476\n",
      "Epoch 2/10, Batch 386/442, Training Loss: 0.7639\n",
      "Epoch 2/10, Batch 387/442, Training Loss: 0.8647\n",
      "Epoch 2/10, Batch 388/442, Training Loss: 0.6726\n",
      "Epoch 2/10, Batch 389/442, Training Loss: 0.6911\n",
      "Epoch 2/10, Batch 390/442, Training Loss: 0.8355\n",
      "Epoch 2/10, Batch 391/442, Training Loss: 0.7925\n",
      "Epoch 2/10, Batch 392/442, Training Loss: 0.8150\n",
      "Epoch 2/10, Batch 393/442, Training Loss: 0.6848\n",
      "Epoch 2/10, Batch 394/442, Training Loss: 0.7828\n",
      "Epoch 2/10, Batch 395/442, Training Loss: 0.5941\n",
      "Epoch 2/10, Batch 396/442, Training Loss: 0.9885\n",
      "Epoch 2/10, Batch 397/442, Training Loss: 0.6403\n",
      "Epoch 2/10, Batch 398/442, Training Loss: 0.7518\n",
      "Epoch 2/10, Batch 399/442, Training Loss: 0.6913\n",
      "Epoch 2/10, Batch 400/442, Training Loss: 0.8264\n",
      "Epoch 2/10, Batch 401/442, Training Loss: 0.8667\n",
      "Epoch 2/10, Batch 402/442, Training Loss: 0.6076\n",
      "Epoch 2/10, Batch 403/442, Training Loss: 0.7418\n",
      "Epoch 2/10, Batch 404/442, Training Loss: 0.6531\n",
      "Epoch 2/10, Batch 405/442, Training Loss: 0.7061\n",
      "Epoch 2/10, Batch 406/442, Training Loss: 0.6846\n",
      "Epoch 2/10, Batch 407/442, Training Loss: 0.6137\n",
      "Epoch 2/10, Batch 408/442, Training Loss: 0.5945\n",
      "Epoch 2/10, Batch 409/442, Training Loss: 0.7248\n",
      "Epoch 2/10, Batch 410/442, Training Loss: 0.7607\n",
      "Epoch 2/10, Batch 411/442, Training Loss: 0.6495\n",
      "Epoch 2/10, Batch 412/442, Training Loss: 0.7124\n",
      "Epoch 2/10, Batch 413/442, Training Loss: 0.4752\n",
      "Epoch 2/10, Batch 414/442, Training Loss: 0.8327\n",
      "Epoch 2/10, Batch 415/442, Training Loss: 0.5972\n",
      "Epoch 2/10, Batch 416/442, Training Loss: 0.7602\n",
      "Epoch 2/10, Batch 417/442, Training Loss: 0.9128\n",
      "Epoch 2/10, Batch 418/442, Training Loss: 0.7838\n",
      "Epoch 2/10, Batch 419/442, Training Loss: 0.8562\n",
      "Epoch 2/10, Batch 420/442, Training Loss: 0.9577\n",
      "Epoch 2/10, Batch 421/442, Training Loss: 0.8617\n",
      "Epoch 2/10, Batch 422/442, Training Loss: 0.9165\n",
      "Epoch 2/10, Batch 423/442, Training Loss: 0.7704\n",
      "Epoch 2/10, Batch 424/442, Training Loss: 0.7781\n",
      "Epoch 2/10, Batch 425/442, Training Loss: 1.0820\n",
      "Epoch 2/10, Batch 426/442, Training Loss: 1.1871\n",
      "Epoch 2/10, Batch 427/442, Training Loss: 0.6953\n",
      "Epoch 2/10, Batch 428/442, Training Loss: 0.7638\n",
      "Epoch 2/10, Batch 429/442, Training Loss: 0.6249\n",
      "Epoch 2/10, Batch 430/442, Training Loss: 0.7947\n",
      "Epoch 2/10, Batch 431/442, Training Loss: 0.8329\n",
      "Epoch 2/10, Batch 432/442, Training Loss: 0.7645\n",
      "Epoch 2/10, Batch 433/442, Training Loss: 0.7287\n",
      "Epoch 2/10, Batch 434/442, Training Loss: 0.7360\n",
      "Epoch 2/10, Batch 435/442, Training Loss: 0.7155\n",
      "Epoch 2/10, Batch 436/442, Training Loss: 0.7055\n",
      "Epoch 2/10, Batch 437/442, Training Loss: 0.7060\n",
      "Epoch 2/10, Batch 438/442, Training Loss: 0.7305\n",
      "Epoch 2/10, Batch 439/442, Training Loss: 0.6326\n",
      "Epoch 2/10, Batch 440/442, Training Loss: 0.7407\n",
      "Epoch 2/10, Batch 441/442, Training Loss: 0.6195\n",
      "Epoch 2/10, Batch 442/442, Training Loss: 0.6693\n",
      "Epoch 2/10, Training Loss: 0.7938, Validation Loss: 0.8418, Validation Accuracy: 0.5928\n",
      "Epoch 3/10, Batch 1/442, Training Loss: 0.6087\n",
      "Epoch 3/10, Batch 2/442, Training Loss: 0.7386\n",
      "Epoch 3/10, Batch 3/442, Training Loss: 0.9439\n",
      "Epoch 3/10, Batch 4/442, Training Loss: 0.5941\n",
      "Epoch 3/10, Batch 5/442, Training Loss: 0.8641\n",
      "Epoch 3/10, Batch 6/442, Training Loss: 0.6638\n",
      "Epoch 3/10, Batch 7/442, Training Loss: 0.6504\n",
      "Epoch 3/10, Batch 8/442, Training Loss: 0.6212\n",
      "Epoch 3/10, Batch 9/442, Training Loss: 0.6849\n",
      "Epoch 3/10, Batch 10/442, Training Loss: 0.8329\n",
      "Epoch 3/10, Batch 11/442, Training Loss: 0.9664\n",
      "Epoch 3/10, Batch 12/442, Training Loss: 0.6791\n",
      "Epoch 3/10, Batch 13/442, Training Loss: 0.7668\n",
      "Epoch 3/10, Batch 14/442, Training Loss: 0.6900\n",
      "Epoch 3/10, Batch 15/442, Training Loss: 0.6926\n",
      "Epoch 3/10, Batch 16/442, Training Loss: 0.7789\n",
      "Epoch 3/10, Batch 17/442, Training Loss: 0.6916\n",
      "Epoch 3/10, Batch 18/442, Training Loss: 0.6402\n",
      "Epoch 3/10, Batch 19/442, Training Loss: 0.6393\n",
      "Epoch 3/10, Batch 20/442, Training Loss: 0.8088\n",
      "Epoch 3/10, Batch 21/442, Training Loss: 0.6168\n",
      "Epoch 3/10, Batch 22/442, Training Loss: 0.7513\n",
      "Epoch 3/10, Batch 23/442, Training Loss: 0.6920\n",
      "Epoch 3/10, Batch 24/442, Training Loss: 0.8803\n",
      "Epoch 3/10, Batch 25/442, Training Loss: 0.5613\n",
      "Epoch 3/10, Batch 26/442, Training Loss: 0.9292\n",
      "Epoch 3/10, Batch 27/442, Training Loss: 0.7460\n",
      "Epoch 3/10, Batch 28/442, Training Loss: 0.6413\n",
      "Epoch 3/10, Batch 29/442, Training Loss: 0.5682\n",
      "Epoch 3/10, Batch 30/442, Training Loss: 0.7147\n",
      "Epoch 3/10, Batch 31/442, Training Loss: 0.8656\n",
      "Epoch 3/10, Batch 32/442, Training Loss: 0.7650\n",
      "Epoch 3/10, Batch 33/442, Training Loss: 0.6906\n",
      "Epoch 3/10, Batch 34/442, Training Loss: 0.7564\n",
      "Epoch 3/10, Batch 35/442, Training Loss: 0.7735\n",
      "Epoch 3/10, Batch 36/442, Training Loss: 0.7306\n",
      "Epoch 3/10, Batch 37/442, Training Loss: 1.1436\n",
      "Epoch 3/10, Batch 38/442, Training Loss: 1.1473\n",
      "Epoch 3/10, Batch 39/442, Training Loss: 0.9309\n",
      "Epoch 3/10, Batch 40/442, Training Loss: 0.6039\n",
      "Epoch 3/10, Batch 41/442, Training Loss: 0.6316\n",
      "Epoch 3/10, Batch 42/442, Training Loss: 0.7670\n",
      "Epoch 3/10, Batch 43/442, Training Loss: 0.8338\n",
      "Epoch 3/10, Batch 44/442, Training Loss: 0.7045\n",
      "Epoch 3/10, Batch 45/442, Training Loss: 0.7103\n",
      "Epoch 3/10, Batch 46/442, Training Loss: 0.7153\n",
      "Epoch 3/10, Batch 47/442, Training Loss: 0.8067\n",
      "Epoch 3/10, Batch 48/442, Training Loss: 0.6844\n",
      "Epoch 3/10, Batch 49/442, Training Loss: 0.8679\n",
      "Epoch 3/10, Batch 50/442, Training Loss: 0.8337\n",
      "Epoch 3/10, Batch 51/442, Training Loss: 0.7122\n",
      "Epoch 3/10, Batch 52/442, Training Loss: 0.7178\n",
      "Epoch 3/10, Batch 53/442, Training Loss: 1.1834\n",
      "Epoch 3/10, Batch 54/442, Training Loss: 0.8707\n",
      "Epoch 3/10, Batch 55/442, Training Loss: 0.7413\n",
      "Epoch 3/10, Batch 56/442, Training Loss: 0.7748\n",
      "Epoch 3/10, Batch 57/442, Training Loss: 0.7671\n",
      "Epoch 3/10, Batch 58/442, Training Loss: 0.7985\n",
      "Epoch 3/10, Batch 59/442, Training Loss: 0.8122\n",
      "Epoch 3/10, Batch 60/442, Training Loss: 0.8448\n",
      "Epoch 3/10, Batch 61/442, Training Loss: 0.6584\n",
      "Epoch 3/10, Batch 62/442, Training Loss: 0.8284\n",
      "Epoch 3/10, Batch 63/442, Training Loss: 0.7620\n",
      "Epoch 3/10, Batch 64/442, Training Loss: 0.7394\n",
      "Epoch 3/10, Batch 65/442, Training Loss: 0.7699\n",
      "Epoch 3/10, Batch 66/442, Training Loss: 0.6575\n",
      "Epoch 3/10, Batch 67/442, Training Loss: 0.7998\n",
      "Epoch 3/10, Batch 68/442, Training Loss: 0.6463\n",
      "Epoch 3/10, Batch 69/442, Training Loss: 0.7260\n",
      "Epoch 3/10, Batch 70/442, Training Loss: 0.9352\n",
      "Epoch 3/10, Batch 71/442, Training Loss: 0.7639\n",
      "Epoch 3/10, Batch 72/442, Training Loss: 0.5737\n",
      "Epoch 3/10, Batch 73/442, Training Loss: 0.8559\n",
      "Epoch 3/10, Batch 74/442, Training Loss: 0.7620\n",
      "Epoch 3/10, Batch 75/442, Training Loss: 0.6658\n",
      "Epoch 3/10, Batch 76/442, Training Loss: 0.7229\n",
      "Epoch 3/10, Batch 77/442, Training Loss: 0.6471\n",
      "Epoch 3/10, Batch 78/442, Training Loss: 0.9821\n",
      "Epoch 3/10, Batch 79/442, Training Loss: 0.8081\n",
      "Epoch 3/10, Batch 80/442, Training Loss: 0.7509\n",
      "Epoch 3/10, Batch 81/442, Training Loss: 0.6297\n",
      "Epoch 3/10, Batch 82/442, Training Loss: 0.5720\n",
      "Epoch 3/10, Batch 83/442, Training Loss: 0.8822\n",
      "Epoch 3/10, Batch 84/442, Training Loss: 1.0378\n",
      "Epoch 3/10, Batch 85/442, Training Loss: 0.6896\n",
      "Epoch 3/10, Batch 86/442, Training Loss: 0.6367\n",
      "Epoch 3/10, Batch 87/442, Training Loss: 0.6230\n",
      "Epoch 3/10, Batch 88/442, Training Loss: 0.7229\n",
      "Epoch 3/10, Batch 89/442, Training Loss: 0.9465\n",
      "Epoch 3/10, Batch 90/442, Training Loss: 0.6948\n",
      "Epoch 3/10, Batch 91/442, Training Loss: 0.8000\n",
      "Epoch 3/10, Batch 92/442, Training Loss: 0.9123\n",
      "Epoch 3/10, Batch 93/442, Training Loss: 0.9285\n",
      "Epoch 3/10, Batch 94/442, Training Loss: 0.9416\n",
      "Epoch 3/10, Batch 95/442, Training Loss: 0.7296\n",
      "Epoch 3/10, Batch 96/442, Training Loss: 0.7035\n",
      "Epoch 3/10, Batch 97/442, Training Loss: 0.6875\n",
      "Epoch 3/10, Batch 98/442, Training Loss: 0.8493\n",
      "Epoch 3/10, Batch 99/442, Training Loss: 0.7436\n",
      "Epoch 3/10, Batch 100/442, Training Loss: 0.6275\n",
      "Epoch 3/10, Batch 101/442, Training Loss: 0.8526\n",
      "Epoch 3/10, Batch 102/442, Training Loss: 0.7593\n",
      "Epoch 3/10, Batch 103/442, Training Loss: 0.8066\n",
      "Epoch 3/10, Batch 104/442, Training Loss: 0.6981\n",
      "Epoch 3/10, Batch 105/442, Training Loss: 0.5344\n",
      "Epoch 3/10, Batch 106/442, Training Loss: 0.7560\n",
      "Epoch 3/10, Batch 107/442, Training Loss: 1.0011\n",
      "Epoch 3/10, Batch 108/442, Training Loss: 0.6794\n",
      "Epoch 3/10, Batch 109/442, Training Loss: 0.9357\n",
      "Epoch 3/10, Batch 110/442, Training Loss: 0.6706\n",
      "Epoch 3/10, Batch 111/442, Training Loss: 0.9968\n",
      "Epoch 3/10, Batch 112/442, Training Loss: 0.8075\n",
      "Epoch 3/10, Batch 113/442, Training Loss: 0.7952\n",
      "Epoch 3/10, Batch 114/442, Training Loss: 0.7475\n",
      "Epoch 3/10, Batch 115/442, Training Loss: 0.4829\n",
      "Epoch 3/10, Batch 116/442, Training Loss: 0.5942\n",
      "Epoch 3/10, Batch 117/442, Training Loss: 0.7215\n",
      "Epoch 3/10, Batch 118/442, Training Loss: 0.8540\n",
      "Epoch 3/10, Batch 119/442, Training Loss: 0.8804\n",
      "Epoch 3/10, Batch 120/442, Training Loss: 0.5864\n",
      "Epoch 3/10, Batch 121/442, Training Loss: 0.6991\n",
      "Epoch 3/10, Batch 122/442, Training Loss: 0.7366\n",
      "Epoch 3/10, Batch 123/442, Training Loss: 0.7243\n",
      "Epoch 3/10, Batch 124/442, Training Loss: 0.7204\n",
      "Epoch 3/10, Batch 125/442, Training Loss: 0.7354\n",
      "Epoch 3/10, Batch 126/442, Training Loss: 0.6971\n",
      "Epoch 3/10, Batch 127/442, Training Loss: 0.6584\n",
      "Epoch 3/10, Batch 128/442, Training Loss: 0.8484\n",
      "Epoch 3/10, Batch 129/442, Training Loss: 0.8448\n",
      "Epoch 3/10, Batch 130/442, Training Loss: 0.5965\n",
      "Epoch 3/10, Batch 131/442, Training Loss: 0.6194\n",
      "Epoch 3/10, Batch 132/442, Training Loss: 0.6429\n",
      "Epoch 3/10, Batch 133/442, Training Loss: 0.9156\n",
      "Epoch 3/10, Batch 134/442, Training Loss: 0.8078\n",
      "Epoch 3/10, Batch 135/442, Training Loss: 0.6372\n",
      "Epoch 3/10, Batch 136/442, Training Loss: 0.7003\n",
      "Epoch 3/10, Batch 137/442, Training Loss: 0.8565\n",
      "Epoch 3/10, Batch 138/442, Training Loss: 0.7934\n",
      "Epoch 3/10, Batch 139/442, Training Loss: 0.9947\n",
      "Epoch 3/10, Batch 140/442, Training Loss: 0.9106\n",
      "Epoch 3/10, Batch 141/442, Training Loss: 0.7775\n",
      "Epoch 3/10, Batch 142/442, Training Loss: 0.8967\n",
      "Epoch 3/10, Batch 143/442, Training Loss: 0.6253\n",
      "Epoch 3/10, Batch 144/442, Training Loss: 0.5647\n",
      "Epoch 3/10, Batch 145/442, Training Loss: 0.9120\n",
      "Epoch 3/10, Batch 146/442, Training Loss: 0.7283\n",
      "Epoch 3/10, Batch 147/442, Training Loss: 0.6270\n",
      "Epoch 3/10, Batch 148/442, Training Loss: 0.6443\n",
      "Epoch 3/10, Batch 149/442, Training Loss: 0.6581\n",
      "Epoch 3/10, Batch 150/442, Training Loss: 0.7538\n",
      "Epoch 3/10, Batch 151/442, Training Loss: 0.8441\n",
      "Epoch 3/10, Batch 152/442, Training Loss: 0.6631\n",
      "Epoch 3/10, Batch 153/442, Training Loss: 0.5120\n",
      "Epoch 3/10, Batch 154/442, Training Loss: 0.6396\n",
      "Epoch 3/10, Batch 155/442, Training Loss: 0.8028\n",
      "Epoch 3/10, Batch 156/442, Training Loss: 0.7097\n",
      "Epoch 3/10, Batch 157/442, Training Loss: 0.6967\n",
      "Epoch 3/10, Batch 158/442, Training Loss: 0.6583\n",
      "Epoch 3/10, Batch 159/442, Training Loss: 0.8855\n",
      "Epoch 3/10, Batch 160/442, Training Loss: 0.6684\n",
      "Epoch 3/10, Batch 161/442, Training Loss: 0.7838\n",
      "Epoch 3/10, Batch 162/442, Training Loss: 0.6175\n",
      "Epoch 3/10, Batch 163/442, Training Loss: 0.6757\n",
      "Epoch 3/10, Batch 164/442, Training Loss: 0.7292\n",
      "Epoch 3/10, Batch 165/442, Training Loss: 0.6049\n",
      "Epoch 3/10, Batch 166/442, Training Loss: 0.9969\n",
      "Epoch 3/10, Batch 167/442, Training Loss: 0.8174\n",
      "Epoch 3/10, Batch 168/442, Training Loss: 0.8211\n",
      "Epoch 3/10, Batch 169/442, Training Loss: 0.7628\n",
      "Epoch 3/10, Batch 170/442, Training Loss: 0.6233\n",
      "Epoch 3/10, Batch 171/442, Training Loss: 0.7034\n",
      "Epoch 3/10, Batch 172/442, Training Loss: 0.7318\n",
      "Epoch 3/10, Batch 173/442, Training Loss: 0.7944\n",
      "Epoch 3/10, Batch 174/442, Training Loss: 0.6382\n",
      "Epoch 3/10, Batch 175/442, Training Loss: 0.6870\n",
      "Epoch 3/10, Batch 176/442, Training Loss: 0.7114\n",
      "Epoch 3/10, Batch 177/442, Training Loss: 0.7443\n",
      "Epoch 3/10, Batch 178/442, Training Loss: 0.6237\n",
      "Epoch 3/10, Batch 179/442, Training Loss: 0.7131\n",
      "Epoch 3/10, Batch 180/442, Training Loss: 0.7745\n",
      "Epoch 3/10, Batch 181/442, Training Loss: 0.6451\n",
      "Epoch 3/10, Batch 182/442, Training Loss: 0.6971\n",
      "Epoch 3/10, Batch 183/442, Training Loss: 0.8763\n",
      "Epoch 3/10, Batch 184/442, Training Loss: 0.8060\n",
      "Epoch 3/10, Batch 185/442, Training Loss: 0.5609\n",
      "Epoch 3/10, Batch 186/442, Training Loss: 0.8951\n",
      "Epoch 3/10, Batch 187/442, Training Loss: 0.6633\n",
      "Epoch 3/10, Batch 188/442, Training Loss: 0.6966\n",
      "Epoch 3/10, Batch 189/442, Training Loss: 0.6453\n",
      "Epoch 3/10, Batch 190/442, Training Loss: 0.6734\n",
      "Epoch 3/10, Batch 191/442, Training Loss: 0.5755\n",
      "Epoch 3/10, Batch 192/442, Training Loss: 0.6670\n",
      "Epoch 3/10, Batch 193/442, Training Loss: 0.7825\n",
      "Epoch 3/10, Batch 194/442, Training Loss: 0.9132\n",
      "Epoch 3/10, Batch 195/442, Training Loss: 0.8252\n",
      "Epoch 3/10, Batch 196/442, Training Loss: 0.6243\n",
      "Epoch 3/10, Batch 197/442, Training Loss: 0.5783\n",
      "Epoch 3/10, Batch 198/442, Training Loss: 0.7315\n",
      "Epoch 3/10, Batch 199/442, Training Loss: 0.9327\n",
      "Epoch 3/10, Batch 200/442, Training Loss: 0.6373\n",
      "Epoch 3/10, Batch 201/442, Training Loss: 0.6016\n",
      "Epoch 3/10, Batch 202/442, Training Loss: 0.9971\n",
      "Epoch 3/10, Batch 203/442, Training Loss: 0.6810\n",
      "Epoch 3/10, Batch 204/442, Training Loss: 0.7982\n",
      "Epoch 3/10, Batch 205/442, Training Loss: 0.6157\n",
      "Epoch 3/10, Batch 206/442, Training Loss: 0.5196\n",
      "Epoch 3/10, Batch 207/442, Training Loss: 0.7784\n",
      "Epoch 3/10, Batch 208/442, Training Loss: 0.7327\n",
      "Epoch 3/10, Batch 209/442, Training Loss: 0.7032\n",
      "Epoch 3/10, Batch 210/442, Training Loss: 0.6469\n",
      "Epoch 3/10, Batch 211/442, Training Loss: 0.7180\n",
      "Epoch 3/10, Batch 212/442, Training Loss: 0.5969\n",
      "Epoch 3/10, Batch 213/442, Training Loss: 0.5295\n",
      "Epoch 3/10, Batch 214/442, Training Loss: 0.6933\n",
      "Epoch 3/10, Batch 215/442, Training Loss: 0.8433\n",
      "Epoch 3/10, Batch 216/442, Training Loss: 0.9274\n",
      "Epoch 3/10, Batch 217/442, Training Loss: 0.9487\n",
      "Epoch 3/10, Batch 218/442, Training Loss: 0.7358\n",
      "Epoch 3/10, Batch 219/442, Training Loss: 0.8772\n",
      "Epoch 3/10, Batch 220/442, Training Loss: 0.8425\n",
      "Epoch 3/10, Batch 221/442, Training Loss: 0.7614\n",
      "Epoch 3/10, Batch 222/442, Training Loss: 0.6710\n",
      "Epoch 3/10, Batch 223/442, Training Loss: 0.5632\n",
      "Epoch 3/10, Batch 224/442, Training Loss: 0.5614\n",
      "Epoch 3/10, Batch 225/442, Training Loss: 0.6883\n",
      "Epoch 3/10, Batch 226/442, Training Loss: 0.6966\n",
      "Epoch 3/10, Batch 227/442, Training Loss: 0.7674\n",
      "Epoch 3/10, Batch 228/442, Training Loss: 0.6127\n",
      "Epoch 3/10, Batch 229/442, Training Loss: 0.8263\n",
      "Epoch 3/10, Batch 230/442, Training Loss: 0.7569\n",
      "Epoch 3/10, Batch 231/442, Training Loss: 0.8754\n",
      "Epoch 3/10, Batch 232/442, Training Loss: 0.7413\n",
      "Epoch 3/10, Batch 233/442, Training Loss: 0.9177\n",
      "Epoch 3/10, Batch 234/442, Training Loss: 0.6619\n",
      "Epoch 3/10, Batch 235/442, Training Loss: 0.7246\n",
      "Epoch 3/10, Batch 236/442, Training Loss: 0.7309\n",
      "Epoch 3/10, Batch 237/442, Training Loss: 0.7550\n",
      "Epoch 3/10, Batch 238/442, Training Loss: 0.6416\n",
      "Epoch 3/10, Batch 239/442, Training Loss: 0.5763\n",
      "Epoch 3/10, Batch 240/442, Training Loss: 0.6597\n",
      "Epoch 3/10, Batch 241/442, Training Loss: 0.6281\n",
      "Epoch 3/10, Batch 242/442, Training Loss: 0.7121\n",
      "Epoch 3/10, Batch 243/442, Training Loss: 0.9987\n",
      "Epoch 3/10, Batch 244/442, Training Loss: 0.7227\n",
      "Epoch 3/10, Batch 245/442, Training Loss: 0.6117\n",
      "Epoch 3/10, Batch 246/442, Training Loss: 0.7042\n",
      "Epoch 3/10, Batch 247/442, Training Loss: 0.5950\n",
      "Epoch 3/10, Batch 248/442, Training Loss: 0.6500\n",
      "Epoch 3/10, Batch 249/442, Training Loss: 0.5078\n",
      "Epoch 3/10, Batch 250/442, Training Loss: 0.7885\n",
      "Epoch 3/10, Batch 251/442, Training Loss: 0.5955\n",
      "Epoch 3/10, Batch 252/442, Training Loss: 0.8195\n",
      "Epoch 3/10, Batch 253/442, Training Loss: 0.8605\n",
      "Epoch 3/10, Batch 254/442, Training Loss: 0.8051\n",
      "Epoch 3/10, Batch 255/442, Training Loss: 0.7235\n",
      "Epoch 3/10, Batch 256/442, Training Loss: 0.7214\n",
      "Epoch 3/10, Batch 257/442, Training Loss: 0.8110\n",
      "Epoch 3/10, Batch 258/442, Training Loss: 1.0230\n",
      "Epoch 3/10, Batch 259/442, Training Loss: 0.7456\n",
      "Epoch 3/10, Batch 260/442, Training Loss: 0.5868\n",
      "Epoch 3/10, Batch 261/442, Training Loss: 0.6649\n",
      "Epoch 3/10, Batch 262/442, Training Loss: 0.6075\n",
      "Epoch 3/10, Batch 263/442, Training Loss: 0.5700\n",
      "Epoch 3/10, Batch 264/442, Training Loss: 0.5805\n",
      "Epoch 3/10, Batch 265/442, Training Loss: 0.5893\n",
      "Epoch 3/10, Batch 266/442, Training Loss: 0.6150\n",
      "Epoch 3/10, Batch 267/442, Training Loss: 0.6967\n",
      "Epoch 3/10, Batch 268/442, Training Loss: 0.7184\n",
      "Epoch 3/10, Batch 269/442, Training Loss: 0.7863\n",
      "Epoch 3/10, Batch 270/442, Training Loss: 0.6689\n",
      "Epoch 3/10, Batch 271/442, Training Loss: 0.8196\n",
      "Epoch 3/10, Batch 272/442, Training Loss: 0.8102\n",
      "Epoch 3/10, Batch 273/442, Training Loss: 0.6103\n",
      "Epoch 3/10, Batch 274/442, Training Loss: 0.8313\n",
      "Epoch 3/10, Batch 275/442, Training Loss: 0.5931\n",
      "Epoch 3/10, Batch 276/442, Training Loss: 0.8261\n",
      "Epoch 3/10, Batch 277/442, Training Loss: 0.6971\n",
      "Epoch 3/10, Batch 278/442, Training Loss: 0.5143\n",
      "Epoch 3/10, Batch 279/442, Training Loss: 0.6069\n",
      "Epoch 3/10, Batch 280/442, Training Loss: 0.7482\n",
      "Epoch 3/10, Batch 281/442, Training Loss: 0.7349\n",
      "Epoch 3/10, Batch 282/442, Training Loss: 0.6577\n",
      "Epoch 3/10, Batch 283/442, Training Loss: 0.6662\n",
      "Epoch 3/10, Batch 284/442, Training Loss: 0.7075\n",
      "Epoch 3/10, Batch 285/442, Training Loss: 0.7803\n",
      "Epoch 3/10, Batch 286/442, Training Loss: 0.7525\n",
      "Epoch 3/10, Batch 287/442, Training Loss: 0.6990\n",
      "Epoch 3/10, Batch 288/442, Training Loss: 0.6787\n",
      "Epoch 3/10, Batch 289/442, Training Loss: 0.5926\n",
      "Epoch 3/10, Batch 290/442, Training Loss: 0.6712\n",
      "Epoch 3/10, Batch 291/442, Training Loss: 0.4991\n",
      "Epoch 3/10, Batch 292/442, Training Loss: 0.7731\n",
      "Epoch 3/10, Batch 293/442, Training Loss: 0.4393\n",
      "Epoch 3/10, Batch 294/442, Training Loss: 0.7607\n",
      "Epoch 3/10, Batch 295/442, Training Loss: 0.7593\n",
      "Epoch 3/10, Batch 296/442, Training Loss: 0.7684\n",
      "Epoch 3/10, Batch 297/442, Training Loss: 0.6866\n",
      "Epoch 3/10, Batch 298/442, Training Loss: 0.9611\n",
      "Epoch 3/10, Batch 299/442, Training Loss: 0.7122\n",
      "Epoch 3/10, Batch 300/442, Training Loss: 0.7939\n",
      "Epoch 3/10, Batch 301/442, Training Loss: 0.7826\n",
      "Epoch 3/10, Batch 302/442, Training Loss: 0.7022\n",
      "Epoch 3/10, Batch 303/442, Training Loss: 0.6717\n",
      "Epoch 3/10, Batch 304/442, Training Loss: 0.7152\n",
      "Epoch 3/10, Batch 305/442, Training Loss: 0.5268\n",
      "Epoch 3/10, Batch 306/442, Training Loss: 0.6635\n",
      "Epoch 3/10, Batch 307/442, Training Loss: 0.7456\n",
      "Epoch 3/10, Batch 308/442, Training Loss: 0.7673\n",
      "Epoch 3/10, Batch 309/442, Training Loss: 0.7128\n",
      "Epoch 3/10, Batch 310/442, Training Loss: 0.6121\n",
      "Epoch 3/10, Batch 311/442, Training Loss: 0.8237\n",
      "Epoch 3/10, Batch 312/442, Training Loss: 0.6740\n",
      "Epoch 3/10, Batch 313/442, Training Loss: 0.7942\n",
      "Epoch 3/10, Batch 314/442, Training Loss: 0.8468\n",
      "Epoch 3/10, Batch 315/442, Training Loss: 0.6566\n",
      "Epoch 3/10, Batch 316/442, Training Loss: 0.8730\n",
      "Epoch 3/10, Batch 317/442, Training Loss: 0.8917\n",
      "Epoch 3/10, Batch 318/442, Training Loss: 0.6597\n",
      "Epoch 3/10, Batch 319/442, Training Loss: 0.7840\n",
      "Epoch 3/10, Batch 320/442, Training Loss: 0.5241\n",
      "Epoch 3/10, Batch 321/442, Training Loss: 0.5123\n",
      "Epoch 3/10, Batch 322/442, Training Loss: 0.7439\n",
      "Epoch 3/10, Batch 323/442, Training Loss: 0.7045\n",
      "Epoch 3/10, Batch 324/442, Training Loss: 0.8606\n",
      "Epoch 3/10, Batch 325/442, Training Loss: 0.7258\n",
      "Epoch 3/10, Batch 326/442, Training Loss: 0.6918\n",
      "Epoch 3/10, Batch 327/442, Training Loss: 0.8292\n",
      "Epoch 3/10, Batch 328/442, Training Loss: 1.0255\n",
      "Epoch 3/10, Batch 329/442, Training Loss: 0.6804\n",
      "Epoch 3/10, Batch 330/442, Training Loss: 0.7545\n",
      "Epoch 3/10, Batch 331/442, Training Loss: 0.6406\n",
      "Epoch 3/10, Batch 332/442, Training Loss: 0.6633\n",
      "Epoch 3/10, Batch 333/442, Training Loss: 0.7925\n",
      "Epoch 3/10, Batch 334/442, Training Loss: 0.8474\n",
      "Epoch 3/10, Batch 335/442, Training Loss: 0.6335\n",
      "Epoch 3/10, Batch 336/442, Training Loss: 0.9522\n",
      "Epoch 3/10, Batch 337/442, Training Loss: 0.6754\n",
      "Epoch 3/10, Batch 338/442, Training Loss: 0.5225\n",
      "Epoch 3/10, Batch 339/442, Training Loss: 0.7629\n",
      "Epoch 3/10, Batch 340/442, Training Loss: 0.6060\n",
      "Epoch 3/10, Batch 341/442, Training Loss: 0.8017\n",
      "Epoch 3/10, Batch 342/442, Training Loss: 0.6282\n",
      "Epoch 3/10, Batch 343/442, Training Loss: 0.6831\n",
      "Epoch 3/10, Batch 344/442, Training Loss: 0.6916\n",
      "Epoch 3/10, Batch 345/442, Training Loss: 0.8840\n",
      "Epoch 3/10, Batch 346/442, Training Loss: 0.6964\n",
      "Epoch 3/10, Batch 347/442, Training Loss: 0.7014\n",
      "Epoch 3/10, Batch 348/442, Training Loss: 0.6334\n",
      "Epoch 3/10, Batch 349/442, Training Loss: 0.7340\n",
      "Epoch 3/10, Batch 350/442, Training Loss: 0.7111\n",
      "Epoch 3/10, Batch 351/442, Training Loss: 0.7937\n",
      "Epoch 3/10, Batch 352/442, Training Loss: 0.8234\n",
      "Epoch 3/10, Batch 353/442, Training Loss: 0.9198\n",
      "Epoch 3/10, Batch 354/442, Training Loss: 0.7769\n",
      "Epoch 3/10, Batch 355/442, Training Loss: 0.6257\n",
      "Epoch 3/10, Batch 356/442, Training Loss: 0.7625\n",
      "Epoch 3/10, Batch 357/442, Training Loss: 0.7007\n",
      "Epoch 3/10, Batch 358/442, Training Loss: 0.8877\n",
      "Epoch 3/10, Batch 359/442, Training Loss: 0.7405\n",
      "Epoch 3/10, Batch 360/442, Training Loss: 0.6280\n",
      "Epoch 3/10, Batch 361/442, Training Loss: 0.6632\n",
      "Epoch 3/10, Batch 362/442, Training Loss: 0.8876\n",
      "Epoch 3/10, Batch 363/442, Training Loss: 0.8324\n",
      "Epoch 3/10, Batch 364/442, Training Loss: 0.6694\n",
      "Epoch 3/10, Batch 365/442, Training Loss: 0.6559\n",
      "Epoch 3/10, Batch 366/442, Training Loss: 0.6577\n",
      "Epoch 3/10, Batch 367/442, Training Loss: 0.6374\n",
      "Epoch 3/10, Batch 368/442, Training Loss: 0.9726\n",
      "Epoch 3/10, Batch 369/442, Training Loss: 0.6348\n",
      "Epoch 3/10, Batch 370/442, Training Loss: 0.7614\n",
      "Epoch 3/10, Batch 371/442, Training Loss: 0.6754\n",
      "Epoch 3/10, Batch 372/442, Training Loss: 0.7251\n",
      "Epoch 3/10, Batch 373/442, Training Loss: 0.7375\n",
      "Epoch 3/10, Batch 374/442, Training Loss: 0.8284\n",
      "Epoch 3/10, Batch 375/442, Training Loss: 0.4556\n",
      "Epoch 3/10, Batch 376/442, Training Loss: 0.8521\n",
      "Epoch 3/10, Batch 377/442, Training Loss: 0.8635\n",
      "Epoch 3/10, Batch 378/442, Training Loss: 0.5924\n",
      "Epoch 3/10, Batch 379/442, Training Loss: 0.6503\n",
      "Epoch 3/10, Batch 380/442, Training Loss: 0.8422\n",
      "Epoch 3/10, Batch 381/442, Training Loss: 0.7324\n",
      "Epoch 3/10, Batch 382/442, Training Loss: 0.6448\n",
      "Epoch 3/10, Batch 383/442, Training Loss: 0.7111\n",
      "Epoch 3/10, Batch 384/442, Training Loss: 0.6775\n",
      "Epoch 3/10, Batch 385/442, Training Loss: 0.7500\n",
      "Epoch 3/10, Batch 386/442, Training Loss: 0.6073\n",
      "Epoch 3/10, Batch 387/442, Training Loss: 0.6432\n",
      "Epoch 3/10, Batch 388/442, Training Loss: 0.8330\n",
      "Epoch 3/10, Batch 389/442, Training Loss: 0.7669\n",
      "Epoch 3/10, Batch 390/442, Training Loss: 0.7137\n",
      "Epoch 3/10, Batch 391/442, Training Loss: 0.5712\n",
      "Epoch 3/10, Batch 392/442, Training Loss: 0.7529\n",
      "Epoch 3/10, Batch 393/442, Training Loss: 0.8317\n",
      "Epoch 3/10, Batch 394/442, Training Loss: 0.6186\n",
      "Epoch 3/10, Batch 395/442, Training Loss: 0.6726\n",
      "Epoch 3/10, Batch 396/442, Training Loss: 0.8455\n",
      "Epoch 3/10, Batch 397/442, Training Loss: 0.5421\n",
      "Epoch 3/10, Batch 398/442, Training Loss: 0.6396\n",
      "Epoch 3/10, Batch 399/442, Training Loss: 0.8170\n",
      "Epoch 3/10, Batch 400/442, Training Loss: 0.8473\n",
      "Epoch 3/10, Batch 401/442, Training Loss: 0.7285\n",
      "Epoch 3/10, Batch 402/442, Training Loss: 0.7434\n",
      "Epoch 3/10, Batch 403/442, Training Loss: 0.7444\n",
      "Epoch 3/10, Batch 404/442, Training Loss: 0.8602\n",
      "Epoch 3/10, Batch 405/442, Training Loss: 0.5719\n",
      "Epoch 3/10, Batch 406/442, Training Loss: 0.7391\n",
      "Epoch 3/10, Batch 407/442, Training Loss: 0.6653\n",
      "Epoch 3/10, Batch 408/442, Training Loss: 0.7439\n",
      "Epoch 3/10, Batch 409/442, Training Loss: 0.5487\n",
      "Epoch 3/10, Batch 410/442, Training Loss: 0.7073\n",
      "Epoch 3/10, Batch 411/442, Training Loss: 0.5290\n",
      "Epoch 3/10, Batch 412/442, Training Loss: 0.7185\n",
      "Epoch 3/10, Batch 413/442, Training Loss: 0.7738\n",
      "Epoch 3/10, Batch 414/442, Training Loss: 0.8158\n",
      "Epoch 3/10, Batch 415/442, Training Loss: 0.7975\n",
      "Epoch 3/10, Batch 416/442, Training Loss: 0.6982\n",
      "Epoch 3/10, Batch 417/442, Training Loss: 0.7806\n",
      "Epoch 3/10, Batch 418/442, Training Loss: 0.7323\n",
      "Epoch 3/10, Batch 419/442, Training Loss: 0.7438\n",
      "Epoch 3/10, Batch 420/442, Training Loss: 0.7935\n",
      "Epoch 3/10, Batch 421/442, Training Loss: 0.7709\n",
      "Epoch 3/10, Batch 422/442, Training Loss: 0.6928\n",
      "Epoch 3/10, Batch 423/442, Training Loss: 1.1493\n",
      "Epoch 3/10, Batch 424/442, Training Loss: 1.0323\n",
      "Epoch 3/10, Batch 425/442, Training Loss: 0.6644\n",
      "Epoch 3/10, Batch 426/442, Training Loss: 0.6997\n",
      "Epoch 3/10, Batch 427/442, Training Loss: 0.6833\n",
      "Epoch 3/10, Batch 428/442, Training Loss: 0.5396\n",
      "Epoch 3/10, Batch 429/442, Training Loss: 0.7106\n",
      "Epoch 3/10, Batch 430/442, Training Loss: 0.5772\n",
      "Epoch 3/10, Batch 431/442, Training Loss: 0.7249\n",
      "Epoch 3/10, Batch 432/442, Training Loss: 0.8079\n",
      "Epoch 3/10, Batch 433/442, Training Loss: 0.7842\n",
      "Epoch 3/10, Batch 434/442, Training Loss: 0.8383\n",
      "Epoch 3/10, Batch 435/442, Training Loss: 0.7279\n",
      "Epoch 3/10, Batch 436/442, Training Loss: 0.6207\n",
      "Epoch 3/10, Batch 437/442, Training Loss: 0.5485\n",
      "Epoch 3/10, Batch 438/442, Training Loss: 0.6911\n",
      "Epoch 3/10, Batch 439/442, Training Loss: 0.7318\n",
      "Epoch 3/10, Batch 440/442, Training Loss: 0.8606\n",
      "Epoch 3/10, Batch 441/442, Training Loss: 0.9629\n",
      "Epoch 3/10, Batch 442/442, Training Loss: 1.0131\n",
      "Epoch 3/10, Training Loss: 0.7353, Validation Loss: 0.7573, Validation Accuracy: 0.6447\n",
      "Epoch 4/10, Batch 1/442, Training Loss: 0.4356\n",
      "Epoch 4/10, Batch 2/442, Training Loss: 0.9002\n",
      "Epoch 4/10, Batch 3/442, Training Loss: 0.6130\n",
      "Epoch 4/10, Batch 4/442, Training Loss: 0.8967\n",
      "Epoch 4/10, Batch 5/442, Training Loss: 0.6382\n",
      "Epoch 4/10, Batch 6/442, Training Loss: 0.8418\n",
      "Epoch 4/10, Batch 7/442, Training Loss: 0.7666\n",
      "Epoch 4/10, Batch 8/442, Training Loss: 0.7183\n",
      "Epoch 4/10, Batch 9/442, Training Loss: 0.7469\n",
      "Epoch 4/10, Batch 10/442, Training Loss: 0.8393\n",
      "Epoch 4/10, Batch 11/442, Training Loss: 0.6679\n",
      "Epoch 4/10, Batch 12/442, Training Loss: 0.5354\n",
      "Epoch 4/10, Batch 13/442, Training Loss: 0.6791\n",
      "Epoch 4/10, Batch 14/442, Training Loss: 0.6659\n",
      "Epoch 4/10, Batch 15/442, Training Loss: 0.5347\n",
      "Epoch 4/10, Batch 16/442, Training Loss: 0.7083\n",
      "Epoch 4/10, Batch 17/442, Training Loss: 0.7762\n",
      "Epoch 4/10, Batch 18/442, Training Loss: 0.5838\n",
      "Epoch 4/10, Batch 19/442, Training Loss: 0.6205\n",
      "Epoch 4/10, Batch 20/442, Training Loss: 0.6693\n",
      "Epoch 4/10, Batch 21/442, Training Loss: 0.7689\n",
      "Epoch 4/10, Batch 22/442, Training Loss: 0.7315\n",
      "Epoch 4/10, Batch 23/442, Training Loss: 0.9370\n",
      "Epoch 4/10, Batch 24/442, Training Loss: 0.6679\n",
      "Epoch 4/10, Batch 25/442, Training Loss: 0.9463\n",
      "Epoch 4/10, Batch 26/442, Training Loss: 0.7334\n",
      "Epoch 4/10, Batch 27/442, Training Loss: 0.6804\n",
      "Epoch 4/10, Batch 28/442, Training Loss: 0.8357\n",
      "Epoch 4/10, Batch 29/442, Training Loss: 0.6388\n",
      "Epoch 4/10, Batch 30/442, Training Loss: 0.6237\n",
      "Epoch 4/10, Batch 31/442, Training Loss: 0.8877\n",
      "Epoch 4/10, Batch 32/442, Training Loss: 0.5586\n",
      "Epoch 4/10, Batch 33/442, Training Loss: 0.6037\n",
      "Epoch 4/10, Batch 34/442, Training Loss: 0.6828\n",
      "Epoch 4/10, Batch 35/442, Training Loss: 0.7772\n",
      "Epoch 4/10, Batch 36/442, Training Loss: 0.6686\n",
      "Epoch 4/10, Batch 37/442, Training Loss: 0.5795\n",
      "Epoch 4/10, Batch 38/442, Training Loss: 0.5940\n",
      "Epoch 4/10, Batch 39/442, Training Loss: 0.7909\n",
      "Epoch 4/10, Batch 40/442, Training Loss: 0.6382\n",
      "Epoch 4/10, Batch 41/442, Training Loss: 0.7420\n",
      "Epoch 4/10, Batch 42/442, Training Loss: 0.7695\n",
      "Epoch 4/10, Batch 43/442, Training Loss: 1.0078\n",
      "Epoch 4/10, Batch 44/442, Training Loss: 0.5074\n",
      "Epoch 4/10, Batch 45/442, Training Loss: 0.6596\n",
      "Epoch 4/10, Batch 46/442, Training Loss: 0.9707\n",
      "Epoch 4/10, Batch 47/442, Training Loss: 0.6318\n",
      "Epoch 4/10, Batch 48/442, Training Loss: 0.9870\n",
      "Epoch 4/10, Batch 49/442, Training Loss: 0.8221\n",
      "Epoch 4/10, Batch 50/442, Training Loss: 0.7362\n",
      "Epoch 4/10, Batch 51/442, Training Loss: 0.7077\n",
      "Epoch 4/10, Batch 52/442, Training Loss: 0.8810\n",
      "Epoch 4/10, Batch 53/442, Training Loss: 0.6748\n",
      "Epoch 4/10, Batch 54/442, Training Loss: 0.7590\n",
      "Epoch 4/10, Batch 55/442, Training Loss: 0.6308\n",
      "Epoch 4/10, Batch 56/442, Training Loss: 0.8616\n",
      "Epoch 4/10, Batch 57/442, Training Loss: 0.7273\n",
      "Epoch 4/10, Batch 58/442, Training Loss: 0.7638\n",
      "Epoch 4/10, Batch 59/442, Training Loss: 0.5813\n",
      "Epoch 4/10, Batch 60/442, Training Loss: 0.6793\n",
      "Epoch 4/10, Batch 61/442, Training Loss: 0.9557\n",
      "Epoch 4/10, Batch 62/442, Training Loss: 0.6826\n",
      "Epoch 4/10, Batch 63/442, Training Loss: 0.4609\n",
      "Epoch 4/10, Batch 64/442, Training Loss: 0.5850\n",
      "Epoch 4/10, Batch 65/442, Training Loss: 0.5764\n",
      "Epoch 4/10, Batch 66/442, Training Loss: 0.5503\n",
      "Epoch 4/10, Batch 67/442, Training Loss: 0.6657\n",
      "Epoch 4/10, Batch 68/442, Training Loss: 0.9678\n",
      "Epoch 4/10, Batch 69/442, Training Loss: 0.5067\n",
      "Epoch 4/10, Batch 70/442, Training Loss: 0.6949\n",
      "Epoch 4/10, Batch 71/442, Training Loss: 0.8099\n",
      "Epoch 4/10, Batch 72/442, Training Loss: 0.6022\n",
      "Epoch 4/10, Batch 73/442, Training Loss: 0.7190\n",
      "Epoch 4/10, Batch 74/442, Training Loss: 0.8216\n",
      "Epoch 4/10, Batch 75/442, Training Loss: 0.9130\n",
      "Epoch 4/10, Batch 76/442, Training Loss: 0.6784\n",
      "Epoch 4/10, Batch 77/442, Training Loss: 0.6725\n",
      "Epoch 4/10, Batch 78/442, Training Loss: 0.6723\n",
      "Epoch 4/10, Batch 79/442, Training Loss: 0.7504\n",
      "Epoch 4/10, Batch 80/442, Training Loss: 0.7339\n",
      "Epoch 4/10, Batch 81/442, Training Loss: 0.6662\n",
      "Epoch 4/10, Batch 82/442, Training Loss: 0.9215\n",
      "Epoch 4/10, Batch 83/442, Training Loss: 0.8332\n",
      "Epoch 4/10, Batch 84/442, Training Loss: 0.7855\n",
      "Epoch 4/10, Batch 85/442, Training Loss: 0.6693\n",
      "Epoch 4/10, Batch 86/442, Training Loss: 0.7468\n",
      "Epoch 4/10, Batch 87/442, Training Loss: 0.6914\n",
      "Epoch 4/10, Batch 88/442, Training Loss: 0.9594\n",
      "Epoch 4/10, Batch 89/442, Training Loss: 0.6682\n",
      "Epoch 4/10, Batch 90/442, Training Loss: 0.5096\n",
      "Epoch 4/10, Batch 91/442, Training Loss: 0.6048\n",
      "Epoch 4/10, Batch 92/442, Training Loss: 0.8229\n",
      "Epoch 4/10, Batch 93/442, Training Loss: 0.6405\n",
      "Epoch 4/10, Batch 94/442, Training Loss: 0.6927\n",
      "Epoch 4/10, Batch 95/442, Training Loss: 0.5809\n",
      "Epoch 4/10, Batch 96/442, Training Loss: 0.6156\n",
      "Epoch 4/10, Batch 97/442, Training Loss: 0.5384\n",
      "Epoch 4/10, Batch 98/442, Training Loss: 0.9686\n",
      "Epoch 4/10, Batch 99/442, Training Loss: 0.6162\n",
      "Epoch 4/10, Batch 100/442, Training Loss: 1.1946\n",
      "Epoch 4/10, Batch 101/442, Training Loss: 0.6030\n",
      "Epoch 4/10, Batch 102/442, Training Loss: 0.7160\n",
      "Epoch 4/10, Batch 103/442, Training Loss: 0.6588\n",
      "Epoch 4/10, Batch 104/442, Training Loss: 0.6943\n",
      "Epoch 4/10, Batch 105/442, Training Loss: 0.7801\n",
      "Epoch 4/10, Batch 106/442, Training Loss: 0.7301\n",
      "Epoch 4/10, Batch 107/442, Training Loss: 0.6093\n",
      "Epoch 4/10, Batch 108/442, Training Loss: 0.5826\n",
      "Epoch 4/10, Batch 109/442, Training Loss: 0.6134\n",
      "Epoch 4/10, Batch 110/442, Training Loss: 0.5319\n",
      "Epoch 4/10, Batch 111/442, Training Loss: 0.7067\n",
      "Epoch 4/10, Batch 112/442, Training Loss: 0.5121\n",
      "Epoch 4/10, Batch 113/442, Training Loss: 0.5558\n",
      "Epoch 4/10, Batch 114/442, Training Loss: 0.7262\n",
      "Epoch 4/10, Batch 115/442, Training Loss: 0.9366\n",
      "Epoch 4/10, Batch 116/442, Training Loss: 0.6293\n",
      "Epoch 4/10, Batch 117/442, Training Loss: 0.5073\n",
      "Epoch 4/10, Batch 118/442, Training Loss: 0.4959\n",
      "Epoch 4/10, Batch 119/442, Training Loss: 0.6452\n",
      "Epoch 4/10, Batch 120/442, Training Loss: 0.5700\n",
      "Epoch 4/10, Batch 121/442, Training Loss: 0.8120\n",
      "Epoch 4/10, Batch 122/442, Training Loss: 1.1154\n",
      "Epoch 4/10, Batch 123/442, Training Loss: 0.4822\n",
      "Epoch 4/10, Batch 124/442, Training Loss: 0.8020\n",
      "Epoch 4/10, Batch 125/442, Training Loss: 0.5933\n",
      "Epoch 4/10, Batch 126/442, Training Loss: 0.8694\n",
      "Epoch 4/10, Batch 127/442, Training Loss: 0.6556\n",
      "Epoch 4/10, Batch 128/442, Training Loss: 0.6961\n",
      "Epoch 4/10, Batch 129/442, Training Loss: 0.7049\n",
      "Epoch 4/10, Batch 130/442, Training Loss: 0.6732\n",
      "Epoch 4/10, Batch 131/442, Training Loss: 0.7328\n",
      "Epoch 4/10, Batch 132/442, Training Loss: 0.8456\n",
      "Epoch 4/10, Batch 133/442, Training Loss: 0.6034\n",
      "Epoch 4/10, Batch 134/442, Training Loss: 0.6066\n",
      "Epoch 4/10, Batch 135/442, Training Loss: 0.7988\n",
      "Epoch 4/10, Batch 136/442, Training Loss: 0.7807\n",
      "Epoch 4/10, Batch 137/442, Training Loss: 0.7364\n",
      "Epoch 4/10, Batch 138/442, Training Loss: 0.7120\n",
      "Epoch 4/10, Batch 139/442, Training Loss: 0.6868\n",
      "Epoch 4/10, Batch 140/442, Training Loss: 0.7025\n",
      "Epoch 4/10, Batch 141/442, Training Loss: 1.2092\n",
      "Epoch 4/10, Batch 142/442, Training Loss: 0.7386\n",
      "Epoch 4/10, Batch 143/442, Training Loss: 0.5818\n",
      "Epoch 4/10, Batch 144/442, Training Loss: 0.8305\n",
      "Epoch 4/10, Batch 145/442, Training Loss: 0.7635\n",
      "Epoch 4/10, Batch 146/442, Training Loss: 0.6839\n",
      "Epoch 4/10, Batch 147/442, Training Loss: 0.6514\n",
      "Epoch 4/10, Batch 148/442, Training Loss: 0.7123\n",
      "Epoch 4/10, Batch 149/442, Training Loss: 0.5599\n",
      "Epoch 4/10, Batch 150/442, Training Loss: 0.8329\n",
      "Epoch 4/10, Batch 151/442, Training Loss: 0.6037\n",
      "Epoch 4/10, Batch 152/442, Training Loss: 0.7382\n",
      "Epoch 4/10, Batch 153/442, Training Loss: 0.6138\n",
      "Epoch 4/10, Batch 154/442, Training Loss: 0.5958\n",
      "Epoch 4/10, Batch 155/442, Training Loss: 0.5538\n",
      "Epoch 4/10, Batch 156/442, Training Loss: 0.8221\n",
      "Epoch 4/10, Batch 157/442, Training Loss: 0.7321\n",
      "Epoch 4/10, Batch 158/442, Training Loss: 0.7596\n",
      "Epoch 4/10, Batch 159/442, Training Loss: 0.7392\n",
      "Epoch 4/10, Batch 160/442, Training Loss: 0.7440\n",
      "Epoch 4/10, Batch 161/442, Training Loss: 0.6790\n",
      "Epoch 4/10, Batch 162/442, Training Loss: 0.6857\n",
      "Epoch 4/10, Batch 163/442, Training Loss: 0.8069\n",
      "Epoch 4/10, Batch 164/442, Training Loss: 0.7819\n",
      "Epoch 4/10, Batch 165/442, Training Loss: 0.6653\n",
      "Epoch 4/10, Batch 166/442, Training Loss: 0.9207\n",
      "Epoch 4/10, Batch 167/442, Training Loss: 0.6806\n",
      "Epoch 4/10, Batch 168/442, Training Loss: 0.5822\n",
      "Epoch 4/10, Batch 169/442, Training Loss: 0.8048\n",
      "Epoch 4/10, Batch 170/442, Training Loss: 0.4748\n",
      "Epoch 4/10, Batch 171/442, Training Loss: 0.8103\n",
      "Epoch 4/10, Batch 172/442, Training Loss: 0.6468\n",
      "Epoch 4/10, Batch 173/442, Training Loss: 0.8240\n",
      "Epoch 4/10, Batch 174/442, Training Loss: 0.5794\n",
      "Epoch 4/10, Batch 175/442, Training Loss: 0.8076\n",
      "Epoch 4/10, Batch 176/442, Training Loss: 0.7342\n",
      "Epoch 4/10, Batch 177/442, Training Loss: 0.5597\n",
      "Epoch 4/10, Batch 178/442, Training Loss: 0.5991\n",
      "Epoch 4/10, Batch 179/442, Training Loss: 0.6720\n",
      "Epoch 4/10, Batch 180/442, Training Loss: 0.6637\n",
      "Epoch 4/10, Batch 181/442, Training Loss: 0.7794\n",
      "Epoch 4/10, Batch 182/442, Training Loss: 0.6518\n",
      "Epoch 4/10, Batch 183/442, Training Loss: 0.6706\n",
      "Epoch 4/10, Batch 184/442, Training Loss: 0.6599\n",
      "Epoch 4/10, Batch 185/442, Training Loss: 0.5681\n",
      "Epoch 4/10, Batch 186/442, Training Loss: 0.8937\n",
      "Epoch 4/10, Batch 187/442, Training Loss: 0.6592\n",
      "Epoch 4/10, Batch 188/442, Training Loss: 0.6816\n",
      "Epoch 4/10, Batch 189/442, Training Loss: 0.7286\n",
      "Epoch 4/10, Batch 190/442, Training Loss: 0.7131\n",
      "Epoch 4/10, Batch 191/442, Training Loss: 0.5904\n",
      "Epoch 4/10, Batch 192/442, Training Loss: 0.6742\n",
      "Epoch 4/10, Batch 193/442, Training Loss: 0.5485\n",
      "Epoch 4/10, Batch 194/442, Training Loss: 0.7448\n",
      "Epoch 4/10, Batch 195/442, Training Loss: 0.8190\n",
      "Epoch 4/10, Batch 196/442, Training Loss: 0.6229\n",
      "Epoch 4/10, Batch 197/442, Training Loss: 0.5829\n",
      "Epoch 4/10, Batch 198/442, Training Loss: 0.5712\n",
      "Epoch 4/10, Batch 199/442, Training Loss: 0.8548\n",
      "Epoch 4/10, Batch 200/442, Training Loss: 0.6446\n",
      "Epoch 4/10, Batch 201/442, Training Loss: 0.6757\n",
      "Epoch 4/10, Batch 202/442, Training Loss: 0.5960\n",
      "Epoch 4/10, Batch 203/442, Training Loss: 0.6131\n",
      "Epoch 4/10, Batch 204/442, Training Loss: 0.7732\n",
      "Epoch 4/10, Batch 205/442, Training Loss: 0.6742\n",
      "Epoch 4/10, Batch 206/442, Training Loss: 0.6425\n",
      "Epoch 4/10, Batch 207/442, Training Loss: 1.0813\n",
      "Epoch 4/10, Batch 208/442, Training Loss: 0.6659\n",
      "Epoch 4/10, Batch 209/442, Training Loss: 0.6513\n",
      "Epoch 4/10, Batch 210/442, Training Loss: 0.7051\n",
      "Epoch 4/10, Batch 211/442, Training Loss: 0.6460\n",
      "Epoch 4/10, Batch 212/442, Training Loss: 0.5176\n",
      "Epoch 4/10, Batch 213/442, Training Loss: 0.9595\n",
      "Epoch 4/10, Batch 214/442, Training Loss: 0.7294\n",
      "Epoch 4/10, Batch 215/442, Training Loss: 0.5762\n",
      "Epoch 4/10, Batch 216/442, Training Loss: 0.5963\n",
      "Epoch 4/10, Batch 217/442, Training Loss: 0.8482\n",
      "Epoch 4/10, Batch 218/442, Training Loss: 0.7046\n",
      "Epoch 4/10, Batch 219/442, Training Loss: 0.5821\n",
      "Epoch 4/10, Batch 220/442, Training Loss: 0.5828\n",
      "Epoch 4/10, Batch 221/442, Training Loss: 0.5841\n",
      "Epoch 4/10, Batch 222/442, Training Loss: 0.8939\n",
      "Epoch 4/10, Batch 223/442, Training Loss: 0.6145\n",
      "Epoch 4/10, Batch 224/442, Training Loss: 0.4189\n",
      "Epoch 4/10, Batch 225/442, Training Loss: 0.9395\n",
      "Epoch 4/10, Batch 226/442, Training Loss: 0.7207\n",
      "Epoch 4/10, Batch 227/442, Training Loss: 0.6261\n",
      "Epoch 4/10, Batch 228/442, Training Loss: 1.0175\n",
      "Epoch 4/10, Batch 229/442, Training Loss: 0.7198\n",
      "Epoch 4/10, Batch 230/442, Training Loss: 0.5829\n",
      "Epoch 4/10, Batch 231/442, Training Loss: 0.7841\n",
      "Epoch 4/10, Batch 232/442, Training Loss: 0.6699\n",
      "Epoch 4/10, Batch 233/442, Training Loss: 0.6758\n",
      "Epoch 4/10, Batch 234/442, Training Loss: 0.5280\n",
      "Epoch 4/10, Batch 235/442, Training Loss: 0.7058\n",
      "Epoch 4/10, Batch 236/442, Training Loss: 0.5375\n",
      "Epoch 4/10, Batch 237/442, Training Loss: 0.6114\n",
      "Epoch 4/10, Batch 238/442, Training Loss: 0.6803\n",
      "Epoch 4/10, Batch 239/442, Training Loss: 0.6825\n",
      "Epoch 4/10, Batch 240/442, Training Loss: 0.5644\n",
      "Epoch 4/10, Batch 241/442, Training Loss: 0.7427\n",
      "Epoch 4/10, Batch 242/442, Training Loss: 0.8121\n",
      "Epoch 4/10, Batch 243/442, Training Loss: 0.6310\n",
      "Epoch 4/10, Batch 244/442, Training Loss: 0.6503\n",
      "Epoch 4/10, Batch 245/442, Training Loss: 0.5097\n",
      "Epoch 4/10, Batch 246/442, Training Loss: 0.5953\n",
      "Epoch 4/10, Batch 247/442, Training Loss: 0.9551\n",
      "Epoch 4/10, Batch 248/442, Training Loss: 0.7686\n",
      "Epoch 4/10, Batch 249/442, Training Loss: 0.5192\n",
      "Epoch 4/10, Batch 250/442, Training Loss: 0.5211\n",
      "Epoch 4/10, Batch 251/442, Training Loss: 0.6101\n",
      "Epoch 4/10, Batch 252/442, Training Loss: 0.6463\n",
      "Epoch 4/10, Batch 253/442, Training Loss: 0.7401\n",
      "Epoch 4/10, Batch 254/442, Training Loss: 0.9636\n",
      "Epoch 4/10, Batch 255/442, Training Loss: 0.7486\n",
      "Epoch 4/10, Batch 256/442, Training Loss: 0.6508\n",
      "Epoch 4/10, Batch 257/442, Training Loss: 0.7914\n",
      "Epoch 4/10, Batch 258/442, Training Loss: 0.7313\n",
      "Epoch 4/10, Batch 259/442, Training Loss: 0.5829\n",
      "Epoch 4/10, Batch 260/442, Training Loss: 0.6910\n",
      "Epoch 4/10, Batch 261/442, Training Loss: 0.7168\n",
      "Epoch 4/10, Batch 262/442, Training Loss: 0.5710\n",
      "Epoch 4/10, Batch 263/442, Training Loss: 0.7117\n",
      "Epoch 4/10, Batch 264/442, Training Loss: 0.8223\n",
      "Epoch 4/10, Batch 265/442, Training Loss: 0.5401\n",
      "Epoch 4/10, Batch 266/442, Training Loss: 0.5998\n",
      "Epoch 4/10, Batch 267/442, Training Loss: 0.7506\n",
      "Epoch 4/10, Batch 268/442, Training Loss: 0.6520\n",
      "Epoch 4/10, Batch 269/442, Training Loss: 0.7186\n",
      "Epoch 4/10, Batch 270/442, Training Loss: 0.5895\n",
      "Epoch 4/10, Batch 271/442, Training Loss: 0.7983\n",
      "Epoch 4/10, Batch 272/442, Training Loss: 0.7590\n",
      "Epoch 4/10, Batch 273/442, Training Loss: 0.6867\n",
      "Epoch 4/10, Batch 274/442, Training Loss: 0.6955\n",
      "Epoch 4/10, Batch 275/442, Training Loss: 0.7547\n",
      "Epoch 4/10, Batch 276/442, Training Loss: 0.6449\n",
      "Epoch 4/10, Batch 277/442, Training Loss: 0.7399\n",
      "Epoch 4/10, Batch 278/442, Training Loss: 0.8155\n",
      "Epoch 4/10, Batch 279/442, Training Loss: 0.5748\n",
      "Epoch 4/10, Batch 280/442, Training Loss: 0.6687\n",
      "Epoch 4/10, Batch 281/442, Training Loss: 0.7440\n",
      "Epoch 4/10, Batch 282/442, Training Loss: 0.6873\n",
      "Epoch 4/10, Batch 283/442, Training Loss: 0.6839\n",
      "Epoch 4/10, Batch 284/442, Training Loss: 0.6708\n",
      "Epoch 4/10, Batch 285/442, Training Loss: 0.9503\n",
      "Epoch 4/10, Batch 286/442, Training Loss: 0.8266\n",
      "Epoch 4/10, Batch 287/442, Training Loss: 0.6087\n",
      "Epoch 4/10, Batch 288/442, Training Loss: 0.5948\n",
      "Epoch 4/10, Batch 289/442, Training Loss: 0.6964\n",
      "Epoch 4/10, Batch 290/442, Training Loss: 0.6146\n",
      "Epoch 4/10, Batch 291/442, Training Loss: 0.6252\n",
      "Epoch 4/10, Batch 292/442, Training Loss: 0.6319\n",
      "Epoch 4/10, Batch 293/442, Training Loss: 0.6141\n",
      "Epoch 4/10, Batch 294/442, Training Loss: 0.5574\n",
      "Epoch 4/10, Batch 295/442, Training Loss: 0.6705\n",
      "Epoch 4/10, Batch 296/442, Training Loss: 0.6396\n",
      "Epoch 4/10, Batch 297/442, Training Loss: 0.6212\n",
      "Epoch 4/10, Batch 298/442, Training Loss: 0.6834\n",
      "Epoch 4/10, Batch 299/442, Training Loss: 0.5916\n",
      "Epoch 4/10, Batch 300/442, Training Loss: 0.8654\n",
      "Epoch 4/10, Batch 301/442, Training Loss: 0.5654\n",
      "Epoch 4/10, Batch 302/442, Training Loss: 0.5945\n",
      "Epoch 4/10, Batch 303/442, Training Loss: 0.6954\n",
      "Epoch 4/10, Batch 304/442, Training Loss: 0.6946\n",
      "Epoch 4/10, Batch 305/442, Training Loss: 0.6826\n",
      "Epoch 4/10, Batch 306/442, Training Loss: 0.9691\n",
      "Epoch 4/10, Batch 307/442, Training Loss: 0.5783\n",
      "Epoch 4/10, Batch 308/442, Training Loss: 0.7201\n",
      "Epoch 4/10, Batch 309/442, Training Loss: 0.8251\n",
      "Epoch 4/10, Batch 310/442, Training Loss: 0.7886\n",
      "Epoch 4/10, Batch 311/442, Training Loss: 0.6643\n",
      "Epoch 4/10, Batch 312/442, Training Loss: 0.5238\n",
      "Epoch 4/10, Batch 313/442, Training Loss: 0.5417\n",
      "Epoch 4/10, Batch 314/442, Training Loss: 0.5864\n",
      "Epoch 4/10, Batch 315/442, Training Loss: 0.7520\n",
      "Epoch 4/10, Batch 316/442, Training Loss: 0.7256\n",
      "Epoch 4/10, Batch 317/442, Training Loss: 0.6001\n",
      "Epoch 4/10, Batch 318/442, Training Loss: 0.6373\n",
      "Epoch 4/10, Batch 319/442, Training Loss: 0.5707\n",
      "Epoch 4/10, Batch 320/442, Training Loss: 0.6141\n",
      "Epoch 4/10, Batch 321/442, Training Loss: 0.4175\n",
      "Epoch 4/10, Batch 322/442, Training Loss: 0.6357\n",
      "Epoch 4/10, Batch 323/442, Training Loss: 0.5815\n",
      "Epoch 4/10, Batch 324/442, Training Loss: 0.5575\n",
      "Epoch 4/10, Batch 325/442, Training Loss: 0.7839\n",
      "Epoch 4/10, Batch 326/442, Training Loss: 0.7374\n",
      "Epoch 4/10, Batch 327/442, Training Loss: 0.5411\n",
      "Epoch 4/10, Batch 328/442, Training Loss: 0.5483\n",
      "Epoch 4/10, Batch 329/442, Training Loss: 0.7858\n",
      "Epoch 4/10, Batch 330/442, Training Loss: 1.0626\n",
      "Epoch 4/10, Batch 331/442, Training Loss: 0.6969\n",
      "Epoch 4/10, Batch 332/442, Training Loss: 0.5921\n",
      "Epoch 4/10, Batch 333/442, Training Loss: 0.7901\n",
      "Epoch 4/10, Batch 334/442, Training Loss: 0.6998\n",
      "Epoch 4/10, Batch 335/442, Training Loss: 0.7621\n",
      "Epoch 4/10, Batch 336/442, Training Loss: 0.6480\n",
      "Epoch 4/10, Batch 337/442, Training Loss: 0.6940\n",
      "Epoch 4/10, Batch 338/442, Training Loss: 0.7087\n",
      "Epoch 4/10, Batch 339/442, Training Loss: 0.6533\n",
      "Epoch 4/10, Batch 340/442, Training Loss: 0.5966\n",
      "Epoch 4/10, Batch 341/442, Training Loss: 0.8261\n",
      "Epoch 4/10, Batch 342/442, Training Loss: 0.6429\n",
      "Epoch 4/10, Batch 343/442, Training Loss: 0.6014\n",
      "Epoch 4/10, Batch 344/442, Training Loss: 0.6796\n",
      "Epoch 4/10, Batch 345/442, Training Loss: 0.7772\n",
      "Epoch 4/10, Batch 346/442, Training Loss: 0.7214\n",
      "Epoch 4/10, Batch 347/442, Training Loss: 0.7961\n",
      "Epoch 4/10, Batch 348/442, Training Loss: 0.4985\n",
      "Epoch 4/10, Batch 349/442, Training Loss: 0.8013\n",
      "Epoch 4/10, Batch 350/442, Training Loss: 0.6972\n",
      "Epoch 4/10, Batch 351/442, Training Loss: 0.7704\n",
      "Epoch 4/10, Batch 352/442, Training Loss: 0.5998\n",
      "Epoch 4/10, Batch 353/442, Training Loss: 0.5501\n",
      "Epoch 4/10, Batch 354/442, Training Loss: 0.5982\n",
      "Epoch 4/10, Batch 355/442, Training Loss: 0.6393\n",
      "Epoch 4/10, Batch 356/442, Training Loss: 0.5585\n",
      "Epoch 4/10, Batch 357/442, Training Loss: 0.9020\n",
      "Epoch 4/10, Batch 358/442, Training Loss: 0.7860\n",
      "Epoch 4/10, Batch 359/442, Training Loss: 0.6727\n",
      "Epoch 4/10, Batch 360/442, Training Loss: 0.5531\n",
      "Epoch 4/10, Batch 361/442, Training Loss: 0.5678\n",
      "Epoch 4/10, Batch 362/442, Training Loss: 0.8065\n",
      "Epoch 4/10, Batch 363/442, Training Loss: 0.5839\n",
      "Epoch 4/10, Batch 364/442, Training Loss: 0.6627\n",
      "Epoch 4/10, Batch 365/442, Training Loss: 0.5227\n",
      "Epoch 4/10, Batch 366/442, Training Loss: 0.8229\n",
      "Epoch 4/10, Batch 367/442, Training Loss: 0.7436\n",
      "Epoch 4/10, Batch 368/442, Training Loss: 0.5987\n",
      "Epoch 4/10, Batch 369/442, Training Loss: 0.6848\n",
      "Epoch 4/10, Batch 370/442, Training Loss: 0.7751\n",
      "Epoch 4/10, Batch 371/442, Training Loss: 0.7741\n",
      "Epoch 4/10, Batch 372/442, Training Loss: 0.7997\n",
      "Epoch 4/10, Batch 373/442, Training Loss: 0.5800\n",
      "Epoch 4/10, Batch 374/442, Training Loss: 0.7213\n",
      "Epoch 4/10, Batch 375/442, Training Loss: 0.5825\n",
      "Epoch 4/10, Batch 376/442, Training Loss: 0.5694\n",
      "Epoch 4/10, Batch 377/442, Training Loss: 0.6951\n",
      "Epoch 4/10, Batch 378/442, Training Loss: 0.6833\n",
      "Epoch 4/10, Batch 379/442, Training Loss: 0.5155\n",
      "Epoch 4/10, Batch 380/442, Training Loss: 0.8879\n",
      "Epoch 4/10, Batch 381/442, Training Loss: 0.5838\n",
      "Epoch 4/10, Batch 382/442, Training Loss: 0.6681\n",
      "Epoch 4/10, Batch 383/442, Training Loss: 0.8591\n",
      "Epoch 4/10, Batch 384/442, Training Loss: 0.5994\n",
      "Epoch 4/10, Batch 385/442, Training Loss: 0.8142\n",
      "Epoch 4/10, Batch 386/442, Training Loss: 0.5674\n",
      "Epoch 4/10, Batch 387/442, Training Loss: 0.5278\n",
      "Epoch 4/10, Batch 388/442, Training Loss: 0.5868\n",
      "Epoch 4/10, Batch 389/442, Training Loss: 0.6924\n",
      "Epoch 4/10, Batch 390/442, Training Loss: 0.6034\n",
      "Epoch 4/10, Batch 391/442, Training Loss: 0.6191\n",
      "Epoch 4/10, Batch 392/442, Training Loss: 0.5932\n",
      "Epoch 4/10, Batch 393/442, Training Loss: 0.6948\n",
      "Epoch 4/10, Batch 394/442, Training Loss: 0.7386\n",
      "Epoch 4/10, Batch 395/442, Training Loss: 0.6976\n",
      "Epoch 4/10, Batch 396/442, Training Loss: 0.6677\n",
      "Epoch 4/10, Batch 397/442, Training Loss: 0.5994\n",
      "Epoch 4/10, Batch 398/442, Training Loss: 0.6482\n",
      "Epoch 4/10, Batch 399/442, Training Loss: 0.6840\n",
      "Epoch 4/10, Batch 400/442, Training Loss: 0.5928\n",
      "Epoch 4/10, Batch 401/442, Training Loss: 0.5693\n",
      "Epoch 4/10, Batch 402/442, Training Loss: 0.5585\n",
      "Epoch 4/10, Batch 403/442, Training Loss: 0.4995\n",
      "Epoch 4/10, Batch 404/442, Training Loss: 0.7727\n",
      "Epoch 4/10, Batch 405/442, Training Loss: 0.6626\n",
      "Epoch 4/10, Batch 406/442, Training Loss: 0.6542\n",
      "Epoch 4/10, Batch 407/442, Training Loss: 0.5887\n",
      "Epoch 4/10, Batch 408/442, Training Loss: 0.6146\n",
      "Epoch 4/10, Batch 409/442, Training Loss: 0.6111\n",
      "Epoch 4/10, Batch 410/442, Training Loss: 0.6087\n",
      "Epoch 4/10, Batch 411/442, Training Loss: 0.6325\n",
      "Epoch 4/10, Batch 412/442, Training Loss: 0.5167\n",
      "Epoch 4/10, Batch 413/442, Training Loss: 0.5588\n",
      "Epoch 4/10, Batch 414/442, Training Loss: 0.7271\n",
      "Epoch 4/10, Batch 415/442, Training Loss: 0.5150\n",
      "Epoch 4/10, Batch 416/442, Training Loss: 0.4314\n",
      "Epoch 4/10, Batch 417/442, Training Loss: 0.6878\n",
      "Epoch 4/10, Batch 418/442, Training Loss: 0.7191\n",
      "Epoch 4/10, Batch 419/442, Training Loss: 0.5706\n",
      "Epoch 4/10, Batch 420/442, Training Loss: 0.6599\n",
      "Epoch 4/10, Batch 421/442, Training Loss: 1.0421\n",
      "Epoch 4/10, Batch 422/442, Training Loss: 0.7097\n",
      "Epoch 4/10, Batch 423/442, Training Loss: 0.8607\n",
      "Epoch 4/10, Batch 424/442, Training Loss: 0.8437\n",
      "Epoch 4/10, Batch 425/442, Training Loss: 0.7433\n",
      "Epoch 4/10, Batch 426/442, Training Loss: 0.6845\n",
      "Epoch 4/10, Batch 427/442, Training Loss: 0.6375\n",
      "Epoch 4/10, Batch 428/442, Training Loss: 0.6048\n",
      "Epoch 4/10, Batch 429/442, Training Loss: 0.5892\n",
      "Epoch 4/10, Batch 430/442, Training Loss: 0.7857\n",
      "Epoch 4/10, Batch 431/442, Training Loss: 0.6341\n",
      "Epoch 4/10, Batch 432/442, Training Loss: 0.6475\n",
      "Epoch 4/10, Batch 433/442, Training Loss: 0.6078\n",
      "Epoch 4/10, Batch 434/442, Training Loss: 0.5990\n",
      "Epoch 4/10, Batch 435/442, Training Loss: 0.8045\n",
      "Epoch 4/10, Batch 436/442, Training Loss: 0.6384\n",
      "Epoch 4/10, Batch 437/442, Training Loss: 0.5419\n",
      "Epoch 4/10, Batch 438/442, Training Loss: 0.7314\n",
      "Epoch 4/10, Batch 439/442, Training Loss: 0.4718\n",
      "Epoch 4/10, Batch 440/442, Training Loss: 0.4057\n",
      "Epoch 4/10, Batch 441/442, Training Loss: 0.5688\n",
      "Epoch 4/10, Batch 442/442, Training Loss: 0.6331\n",
      "Epoch 4/10, Training Loss: 0.6883, Validation Loss: 0.7196, Validation Accuracy: 0.6596\n",
      "Epoch 5/10, Batch 1/442, Training Loss: 0.7063\n",
      "Epoch 5/10, Batch 2/442, Training Loss: 0.7076\n",
      "Epoch 5/10, Batch 3/442, Training Loss: 1.0392\n",
      "Epoch 5/10, Batch 4/442, Training Loss: 0.6561\n",
      "Epoch 5/10, Batch 5/442, Training Loss: 0.7248\n",
      "Epoch 5/10, Batch 6/442, Training Loss: 0.6067\n",
      "Epoch 5/10, Batch 7/442, Training Loss: 0.8377\n",
      "Epoch 5/10, Batch 8/442, Training Loss: 0.6490\n",
      "Epoch 5/10, Batch 9/442, Training Loss: 0.6755\n",
      "Epoch 5/10, Batch 10/442, Training Loss: 0.6928\n",
      "Epoch 5/10, Batch 11/442, Training Loss: 0.5296\n",
      "Epoch 5/10, Batch 12/442, Training Loss: 0.6305\n",
      "Epoch 5/10, Batch 13/442, Training Loss: 0.5074\n",
      "Epoch 5/10, Batch 14/442, Training Loss: 0.5712\n",
      "Epoch 5/10, Batch 15/442, Training Loss: 0.7052\n",
      "Epoch 5/10, Batch 16/442, Training Loss: 0.5793\n",
      "Epoch 5/10, Batch 17/442, Training Loss: 0.6201\n",
      "Epoch 5/10, Batch 18/442, Training Loss: 0.6191\n",
      "Epoch 5/10, Batch 19/442, Training Loss: 0.6233\n",
      "Epoch 5/10, Batch 20/442, Training Loss: 0.5739\n",
      "Epoch 5/10, Batch 21/442, Training Loss: 0.7316\n",
      "Epoch 5/10, Batch 22/442, Training Loss: 0.6129\n",
      "Epoch 5/10, Batch 23/442, Training Loss: 0.4978\n",
      "Epoch 5/10, Batch 24/442, Training Loss: 0.7726\n",
      "Epoch 5/10, Batch 25/442, Training Loss: 0.9008\n",
      "Epoch 5/10, Batch 26/442, Training Loss: 0.7420\n",
      "Epoch 5/10, Batch 27/442, Training Loss: 0.7238\n",
      "Epoch 5/10, Batch 28/442, Training Loss: 0.7049\n",
      "Epoch 5/10, Batch 29/442, Training Loss: 0.6910\n",
      "Epoch 5/10, Batch 30/442, Training Loss: 0.4836\n",
      "Epoch 5/10, Batch 31/442, Training Loss: 0.5983\n",
      "Epoch 5/10, Batch 32/442, Training Loss: 0.4588\n",
      "Epoch 5/10, Batch 33/442, Training Loss: 0.5978\n",
      "Epoch 5/10, Batch 34/442, Training Loss: 0.9106\n",
      "Epoch 5/10, Batch 35/442, Training Loss: 0.5866\n",
      "Epoch 5/10, Batch 36/442, Training Loss: 0.5414\n",
      "Epoch 5/10, Batch 37/442, Training Loss: 0.7490\n",
      "Epoch 5/10, Batch 38/442, Training Loss: 0.9882\n",
      "Epoch 5/10, Batch 39/442, Training Loss: 0.4759\n",
      "Epoch 5/10, Batch 40/442, Training Loss: 0.3681\n",
      "Epoch 5/10, Batch 41/442, Training Loss: 0.5319\n",
      "Epoch 5/10, Batch 42/442, Training Loss: 0.4885\n",
      "Epoch 5/10, Batch 43/442, Training Loss: 0.6099\n",
      "Epoch 5/10, Batch 44/442, Training Loss: 0.7167\n",
      "Epoch 5/10, Batch 45/442, Training Loss: 0.6539\n",
      "Epoch 5/10, Batch 46/442, Training Loss: 0.5102\n",
      "Epoch 5/10, Batch 47/442, Training Loss: 0.6011\n",
      "Epoch 5/10, Batch 48/442, Training Loss: 0.5910\n",
      "Epoch 5/10, Batch 49/442, Training Loss: 0.5814\n",
      "Epoch 5/10, Batch 50/442, Training Loss: 0.6961\n",
      "Epoch 5/10, Batch 51/442, Training Loss: 0.7413\n",
      "Epoch 5/10, Batch 52/442, Training Loss: 0.7400\n",
      "Epoch 5/10, Batch 53/442, Training Loss: 0.6901\n",
      "Epoch 5/10, Batch 54/442, Training Loss: 0.5583\n",
      "Epoch 5/10, Batch 55/442, Training Loss: 0.7800\n",
      "Epoch 5/10, Batch 56/442, Training Loss: 0.5762\n",
      "Epoch 5/10, Batch 57/442, Training Loss: 0.6465\n",
      "Epoch 5/10, Batch 58/442, Training Loss: 0.5298\n",
      "Epoch 5/10, Batch 59/442, Training Loss: 0.5336\n",
      "Epoch 5/10, Batch 60/442, Training Loss: 0.5248\n",
      "Epoch 5/10, Batch 61/442, Training Loss: 0.5557\n",
      "Epoch 5/10, Batch 62/442, Training Loss: 0.5731\n",
      "Epoch 5/10, Batch 63/442, Training Loss: 0.6507\n",
      "Epoch 5/10, Batch 64/442, Training Loss: 0.6139\n",
      "Epoch 5/10, Batch 65/442, Training Loss: 0.6348\n",
      "Epoch 5/10, Batch 66/442, Training Loss: 0.6236\n",
      "Epoch 5/10, Batch 67/442, Training Loss: 0.5747\n",
      "Epoch 5/10, Batch 68/442, Training Loss: 0.4891\n",
      "Epoch 5/10, Batch 69/442, Training Loss: 0.6233\n",
      "Epoch 5/10, Batch 70/442, Training Loss: 0.6713\n",
      "Epoch 5/10, Batch 71/442, Training Loss: 0.5064\n",
      "Epoch 5/10, Batch 72/442, Training Loss: 0.6218\n",
      "Epoch 5/10, Batch 73/442, Training Loss: 0.6824\n",
      "Epoch 5/10, Batch 74/442, Training Loss: 0.6132\n",
      "Epoch 5/10, Batch 75/442, Training Loss: 0.8095\n",
      "Epoch 5/10, Batch 76/442, Training Loss: 0.6306\n",
      "Epoch 5/10, Batch 77/442, Training Loss: 0.6914\n",
      "Epoch 5/10, Batch 78/442, Training Loss: 0.8991\n",
      "Epoch 5/10, Batch 79/442, Training Loss: 0.4455\n",
      "Epoch 5/10, Batch 80/442, Training Loss: 0.6308\n",
      "Epoch 5/10, Batch 81/442, Training Loss: 0.6476\n",
      "Epoch 5/10, Batch 82/442, Training Loss: 0.5882\n",
      "Epoch 5/10, Batch 83/442, Training Loss: 0.6515\n",
      "Epoch 5/10, Batch 84/442, Training Loss: 0.4625\n",
      "Epoch 5/10, Batch 85/442, Training Loss: 0.5503\n",
      "Epoch 5/10, Batch 86/442, Training Loss: 0.6348\n",
      "Epoch 5/10, Batch 87/442, Training Loss: 0.4615\n",
      "Epoch 5/10, Batch 88/442, Training Loss: 0.6753\n",
      "Epoch 5/10, Batch 89/442, Training Loss: 0.7849\n",
      "Epoch 5/10, Batch 90/442, Training Loss: 0.7888\n",
      "Epoch 5/10, Batch 91/442, Training Loss: 0.5402\n",
      "Epoch 5/10, Batch 92/442, Training Loss: 0.6925\n",
      "Epoch 5/10, Batch 93/442, Training Loss: 0.9460\n",
      "Epoch 5/10, Batch 94/442, Training Loss: 1.1690\n",
      "Epoch 5/10, Batch 95/442, Training Loss: 0.4892\n",
      "Epoch 5/10, Batch 96/442, Training Loss: 0.7269\n",
      "Epoch 5/10, Batch 97/442, Training Loss: 0.4637\n",
      "Epoch 5/10, Batch 98/442, Training Loss: 0.5365\n",
      "Epoch 5/10, Batch 99/442, Training Loss: 0.5828\n",
      "Epoch 5/10, Batch 100/442, Training Loss: 0.5806\n",
      "Epoch 5/10, Batch 101/442, Training Loss: 0.4628\n",
      "Epoch 5/10, Batch 102/442, Training Loss: 0.6234\n",
      "Epoch 5/10, Batch 103/442, Training Loss: 0.6007\n",
      "Epoch 5/10, Batch 104/442, Training Loss: 0.7378\n",
      "Epoch 5/10, Batch 105/442, Training Loss: 0.6924\n",
      "Epoch 5/10, Batch 106/442, Training Loss: 0.9728\n",
      "Epoch 5/10, Batch 107/442, Training Loss: 0.6081\n",
      "Epoch 5/10, Batch 108/442, Training Loss: 0.5861\n",
      "Epoch 5/10, Batch 109/442, Training Loss: 0.5452\n",
      "Epoch 5/10, Batch 110/442, Training Loss: 0.7641\n",
      "Epoch 5/10, Batch 111/442, Training Loss: 0.6268\n",
      "Epoch 5/10, Batch 112/442, Training Loss: 0.6992\n",
      "Epoch 5/10, Batch 113/442, Training Loss: 0.6914\n",
      "Epoch 5/10, Batch 114/442, Training Loss: 0.5277\n",
      "Epoch 5/10, Batch 115/442, Training Loss: 0.6961\n",
      "Epoch 5/10, Batch 116/442, Training Loss: 0.6827\n",
      "Epoch 5/10, Batch 117/442, Training Loss: 0.6658\n",
      "Epoch 5/10, Batch 118/442, Training Loss: 0.5939\n",
      "Epoch 5/10, Batch 119/442, Training Loss: 0.4775\n",
      "Epoch 5/10, Batch 120/442, Training Loss: 0.5936\n",
      "Epoch 5/10, Batch 121/442, Training Loss: 0.8646\n",
      "Epoch 5/10, Batch 122/442, Training Loss: 0.5770\n",
      "Epoch 5/10, Batch 123/442, Training Loss: 0.7862\n",
      "Epoch 5/10, Batch 124/442, Training Loss: 0.7284\n",
      "Epoch 5/10, Batch 125/442, Training Loss: 0.4866\n",
      "Epoch 5/10, Batch 126/442, Training Loss: 0.7163\n",
      "Epoch 5/10, Batch 127/442, Training Loss: 0.7556\n",
      "Epoch 5/10, Batch 128/442, Training Loss: 0.6881\n",
      "Epoch 5/10, Batch 129/442, Training Loss: 0.6617\n",
      "Epoch 5/10, Batch 130/442, Training Loss: 0.5928\n",
      "Epoch 5/10, Batch 131/442, Training Loss: 0.6651\n",
      "Epoch 5/10, Batch 132/442, Training Loss: 0.5355\n",
      "Epoch 5/10, Batch 133/442, Training Loss: 0.6021\n",
      "Epoch 5/10, Batch 134/442, Training Loss: 0.5712\n",
      "Epoch 5/10, Batch 135/442, Training Loss: 0.4699\n",
      "Epoch 5/10, Batch 136/442, Training Loss: 0.7387\n",
      "Epoch 5/10, Batch 137/442, Training Loss: 0.7933\n",
      "Epoch 5/10, Batch 138/442, Training Loss: 0.6657\n",
      "Epoch 5/10, Batch 139/442, Training Loss: 0.6119\n",
      "Epoch 5/10, Batch 140/442, Training Loss: 0.5419\n",
      "Epoch 5/10, Batch 141/442, Training Loss: 0.6004\n",
      "Epoch 5/10, Batch 142/442, Training Loss: 0.4782\n",
      "Epoch 5/10, Batch 143/442, Training Loss: 0.7160\n",
      "Epoch 5/10, Batch 144/442, Training Loss: 0.8858\n",
      "Epoch 5/10, Batch 145/442, Training Loss: 0.9923\n",
      "Epoch 5/10, Batch 146/442, Training Loss: 0.8957\n",
      "Epoch 5/10, Batch 147/442, Training Loss: 0.9226\n",
      "Epoch 5/10, Batch 148/442, Training Loss: 0.5354\n",
      "Epoch 5/10, Batch 149/442, Training Loss: 0.7784\n",
      "Epoch 5/10, Batch 150/442, Training Loss: 0.5653\n",
      "Epoch 5/10, Batch 151/442, Training Loss: 0.7015\n",
      "Epoch 5/10, Batch 152/442, Training Loss: 0.5990\n",
      "Epoch 5/10, Batch 153/442, Training Loss: 0.6092\n",
      "Epoch 5/10, Batch 154/442, Training Loss: 0.7066\n",
      "Epoch 5/10, Batch 155/442, Training Loss: 0.7039\n",
      "Epoch 5/10, Batch 156/442, Training Loss: 0.6961\n",
      "Epoch 5/10, Batch 157/442, Training Loss: 0.7465\n",
      "Epoch 5/10, Batch 158/442, Training Loss: 0.5897\n",
      "Epoch 5/10, Batch 159/442, Training Loss: 0.7712\n",
      "Epoch 5/10, Batch 160/442, Training Loss: 0.6623\n",
      "Epoch 5/10, Batch 161/442, Training Loss: 0.6608\n",
      "Epoch 5/10, Batch 162/442, Training Loss: 0.7408\n",
      "Epoch 5/10, Batch 163/442, Training Loss: 0.5999\n",
      "Epoch 5/10, Batch 164/442, Training Loss: 0.5472\n",
      "Epoch 5/10, Batch 165/442, Training Loss: 0.5687\n",
      "Epoch 5/10, Batch 166/442, Training Loss: 0.6906\n",
      "Epoch 5/10, Batch 167/442, Training Loss: 0.5902\n",
      "Epoch 5/10, Batch 168/442, Training Loss: 0.6354\n",
      "Epoch 5/10, Batch 169/442, Training Loss: 0.5417\n",
      "Epoch 5/10, Batch 170/442, Training Loss: 0.6366\n",
      "Epoch 5/10, Batch 171/442, Training Loss: 0.6657\n",
      "Epoch 5/10, Batch 172/442, Training Loss: 0.4935\n",
      "Epoch 5/10, Batch 173/442, Training Loss: 0.5241\n",
      "Epoch 5/10, Batch 174/442, Training Loss: 0.6730\n",
      "Epoch 5/10, Batch 175/442, Training Loss: 0.7508\n",
      "Epoch 5/10, Batch 176/442, Training Loss: 0.6237\n",
      "Epoch 5/10, Batch 177/442, Training Loss: 0.6346\n",
      "Epoch 5/10, Batch 178/442, Training Loss: 0.4361\n",
      "Epoch 5/10, Batch 179/442, Training Loss: 0.5865\n",
      "Epoch 5/10, Batch 180/442, Training Loss: 0.6188\n",
      "Epoch 5/10, Batch 181/442, Training Loss: 0.4911\n",
      "Epoch 5/10, Batch 182/442, Training Loss: 0.6116\n",
      "Epoch 5/10, Batch 183/442, Training Loss: 0.6635\n",
      "Epoch 5/10, Batch 184/442, Training Loss: 0.5677\n",
      "Epoch 5/10, Batch 185/442, Training Loss: 0.4142\n",
      "Epoch 5/10, Batch 186/442, Training Loss: 0.5327\n",
      "Epoch 5/10, Batch 187/442, Training Loss: 0.6616\n",
      "Epoch 5/10, Batch 188/442, Training Loss: 0.5407\n",
      "Epoch 5/10, Batch 189/442, Training Loss: 0.5391\n",
      "Epoch 5/10, Batch 190/442, Training Loss: 0.6793\n",
      "Epoch 5/10, Batch 191/442, Training Loss: 0.6361\n",
      "Epoch 5/10, Batch 192/442, Training Loss: 0.4601\n",
      "Epoch 5/10, Batch 193/442, Training Loss: 0.5957\n",
      "Epoch 5/10, Batch 194/442, Training Loss: 0.5285\n",
      "Epoch 5/10, Batch 195/442, Training Loss: 0.7009\n",
      "Epoch 5/10, Batch 196/442, Training Loss: 0.4488\n",
      "Epoch 5/10, Batch 197/442, Training Loss: 0.5038\n",
      "Epoch 5/10, Batch 198/442, Training Loss: 0.5150\n",
      "Epoch 5/10, Batch 199/442, Training Loss: 0.6446\n",
      "Epoch 5/10, Batch 200/442, Training Loss: 0.5915\n",
      "Epoch 5/10, Batch 201/442, Training Loss: 0.6743\n",
      "Epoch 5/10, Batch 202/442, Training Loss: 0.6400\n",
      "Epoch 5/10, Batch 203/442, Training Loss: 0.4989\n",
      "Epoch 5/10, Batch 204/442, Training Loss: 0.6581\n",
      "Epoch 5/10, Batch 205/442, Training Loss: 0.4835\n",
      "Epoch 5/10, Batch 206/442, Training Loss: 0.4311\n",
      "Epoch 5/10, Batch 207/442, Training Loss: 0.5023\n",
      "Epoch 5/10, Batch 208/442, Training Loss: 0.6493\n",
      "Epoch 5/10, Batch 209/442, Training Loss: 0.4123\n",
      "Epoch 5/10, Batch 210/442, Training Loss: 0.8686\n",
      "Epoch 5/10, Batch 211/442, Training Loss: 0.6187\n",
      "Epoch 5/10, Batch 212/442, Training Loss: 0.7960\n",
      "Epoch 5/10, Batch 213/442, Training Loss: 0.5641\n",
      "Epoch 5/10, Batch 214/442, Training Loss: 0.6618\n",
      "Epoch 5/10, Batch 215/442, Training Loss: 0.5236\n",
      "Epoch 5/10, Batch 216/442, Training Loss: 0.8167\n",
      "Epoch 5/10, Batch 217/442, Training Loss: 0.5883\n",
      "Epoch 5/10, Batch 218/442, Training Loss: 0.5473\n",
      "Epoch 5/10, Batch 219/442, Training Loss: 0.4775\n",
      "Epoch 5/10, Batch 220/442, Training Loss: 0.5737\n",
      "Epoch 5/10, Batch 221/442, Training Loss: 0.5255\n",
      "Epoch 5/10, Batch 222/442, Training Loss: 0.4778\n",
      "Epoch 5/10, Batch 223/442, Training Loss: 0.8411\n",
      "Epoch 5/10, Batch 224/442, Training Loss: 0.5970\n",
      "Epoch 5/10, Batch 225/442, Training Loss: 0.5962\n",
      "Epoch 5/10, Batch 226/442, Training Loss: 0.4827\n",
      "Epoch 5/10, Batch 227/442, Training Loss: 0.8710\n",
      "Epoch 5/10, Batch 228/442, Training Loss: 0.6231\n",
      "Epoch 5/10, Batch 229/442, Training Loss: 0.6060\n",
      "Epoch 5/10, Batch 230/442, Training Loss: 0.5566\n",
      "Epoch 5/10, Batch 231/442, Training Loss: 0.5694\n",
      "Epoch 5/10, Batch 232/442, Training Loss: 0.5187\n",
      "Epoch 5/10, Batch 233/442, Training Loss: 0.5782\n",
      "Epoch 5/10, Batch 234/442, Training Loss: 0.7323\n",
      "Epoch 5/10, Batch 235/442, Training Loss: 0.7864\n",
      "Epoch 5/10, Batch 236/442, Training Loss: 0.6188\n",
      "Epoch 5/10, Batch 237/442, Training Loss: 0.6299\n",
      "Epoch 5/10, Batch 238/442, Training Loss: 0.6136\n",
      "Epoch 5/10, Batch 239/442, Training Loss: 0.4860\n",
      "Epoch 5/10, Batch 240/442, Training Loss: 0.8592\n",
      "Epoch 5/10, Batch 241/442, Training Loss: 0.6557\n",
      "Epoch 5/10, Batch 242/442, Training Loss: 0.5896\n",
      "Epoch 5/10, Batch 243/442, Training Loss: 0.6996\n",
      "Epoch 5/10, Batch 244/442, Training Loss: 0.7104\n",
      "Epoch 5/10, Batch 245/442, Training Loss: 0.6468\n",
      "Epoch 5/10, Batch 246/442, Training Loss: 0.5437\n",
      "Epoch 5/10, Batch 247/442, Training Loss: 0.4760\n",
      "Epoch 5/10, Batch 248/442, Training Loss: 0.9424\n",
      "Epoch 5/10, Batch 249/442, Training Loss: 0.8765\n",
      "Epoch 5/10, Batch 250/442, Training Loss: 0.7266\n",
      "Epoch 5/10, Batch 251/442, Training Loss: 0.5931\n",
      "Epoch 5/10, Batch 252/442, Training Loss: 0.6958\n",
      "Epoch 5/10, Batch 253/442, Training Loss: 0.6026\n",
      "Epoch 5/10, Batch 254/442, Training Loss: 0.7968\n",
      "Epoch 5/10, Batch 255/442, Training Loss: 0.6123\n",
      "Epoch 5/10, Batch 256/442, Training Loss: 0.6387\n",
      "Epoch 5/10, Batch 257/442, Training Loss: 0.4921\n",
      "Epoch 5/10, Batch 258/442, Training Loss: 0.5683\n",
      "Epoch 5/10, Batch 259/442, Training Loss: 0.7128\n",
      "Epoch 5/10, Batch 260/442, Training Loss: 0.5277\n",
      "Epoch 5/10, Batch 261/442, Training Loss: 0.5825\n",
      "Epoch 5/10, Batch 262/442, Training Loss: 0.5694\n",
      "Epoch 5/10, Batch 263/442, Training Loss: 0.6218\n",
      "Epoch 5/10, Batch 264/442, Training Loss: 0.6898\n",
      "Epoch 5/10, Batch 265/442, Training Loss: 0.6515\n",
      "Epoch 5/10, Batch 266/442, Training Loss: 0.8283\n",
      "Epoch 5/10, Batch 267/442, Training Loss: 0.5200\n",
      "Epoch 5/10, Batch 268/442, Training Loss: 0.4039\n",
      "Epoch 5/10, Batch 269/442, Training Loss: 0.8512\n",
      "Epoch 5/10, Batch 270/442, Training Loss: 0.5368\n",
      "Epoch 5/10, Batch 271/442, Training Loss: 0.6680\n",
      "Epoch 5/10, Batch 272/442, Training Loss: 0.7272\n",
      "Epoch 5/10, Batch 273/442, Training Loss: 0.6933\n",
      "Epoch 5/10, Batch 274/442, Training Loss: 0.5757\n",
      "Epoch 5/10, Batch 275/442, Training Loss: 0.6145\n",
      "Epoch 5/10, Batch 276/442, Training Loss: 0.5860\n",
      "Epoch 5/10, Batch 277/442, Training Loss: 0.6431\n",
      "Epoch 5/10, Batch 278/442, Training Loss: 0.8427\n",
      "Epoch 5/10, Batch 279/442, Training Loss: 0.6066\n",
      "Epoch 5/10, Batch 280/442, Training Loss: 0.4494\n",
      "Epoch 5/10, Batch 281/442, Training Loss: 0.5569\n",
      "Epoch 5/10, Batch 282/442, Training Loss: 0.6861\n",
      "Epoch 5/10, Batch 283/442, Training Loss: 0.7366\n",
      "Epoch 5/10, Batch 284/442, Training Loss: 0.7277\n",
      "Epoch 5/10, Batch 285/442, Training Loss: 0.7667\n",
      "Epoch 5/10, Batch 286/442, Training Loss: 0.7961\n",
      "Epoch 5/10, Batch 287/442, Training Loss: 0.5120\n",
      "Epoch 5/10, Batch 288/442, Training Loss: 0.6529\n",
      "Epoch 5/10, Batch 289/442, Training Loss: 0.5158\n",
      "Epoch 5/10, Batch 290/442, Training Loss: 0.5827\n",
      "Epoch 5/10, Batch 291/442, Training Loss: 0.4645\n",
      "Epoch 5/10, Batch 292/442, Training Loss: 0.5653\n",
      "Epoch 5/10, Batch 293/442, Training Loss: 0.5554\n",
      "Epoch 5/10, Batch 294/442, Training Loss: 0.5861\n",
      "Epoch 5/10, Batch 295/442, Training Loss: 0.7023\n",
      "Epoch 5/10, Batch 296/442, Training Loss: 0.5239\n",
      "Epoch 5/10, Batch 297/442, Training Loss: 0.5771\n",
      "Epoch 5/10, Batch 298/442, Training Loss: 0.7239\n",
      "Epoch 5/10, Batch 299/442, Training Loss: 0.5870\n",
      "Epoch 5/10, Batch 300/442, Training Loss: 0.4690\n",
      "Epoch 5/10, Batch 301/442, Training Loss: 0.7381\n",
      "Epoch 5/10, Batch 302/442, Training Loss: 0.5754\n",
      "Epoch 5/10, Batch 303/442, Training Loss: 0.5770\n",
      "Epoch 5/10, Batch 304/442, Training Loss: 0.5357\n",
      "Epoch 5/10, Batch 305/442, Training Loss: 0.5017\n",
      "Epoch 5/10, Batch 306/442, Training Loss: 0.6291\n",
      "Epoch 5/10, Batch 307/442, Training Loss: 0.6999\n",
      "Epoch 5/10, Batch 308/442, Training Loss: 0.4878\n",
      "Epoch 5/10, Batch 309/442, Training Loss: 0.6441\n",
      "Epoch 5/10, Batch 310/442, Training Loss: 0.5607\n",
      "Epoch 5/10, Batch 311/442, Training Loss: 0.4676\n",
      "Epoch 5/10, Batch 312/442, Training Loss: 0.7201\n",
      "Epoch 5/10, Batch 313/442, Training Loss: 0.5262\n",
      "Epoch 5/10, Batch 314/442, Training Loss: 0.4542\n",
      "Epoch 5/10, Batch 315/442, Training Loss: 0.6573\n",
      "Epoch 5/10, Batch 316/442, Training Loss: 0.6756\n",
      "Epoch 5/10, Batch 317/442, Training Loss: 0.6153\n",
      "Epoch 5/10, Batch 318/442, Training Loss: 0.5114\n",
      "Epoch 5/10, Batch 319/442, Training Loss: 0.5154\n",
      "Epoch 5/10, Batch 320/442, Training Loss: 0.5961\n",
      "Epoch 5/10, Batch 321/442, Training Loss: 0.4577\n",
      "Epoch 5/10, Batch 322/442, Training Loss: 0.5541\n",
      "Epoch 5/10, Batch 323/442, Training Loss: 0.6352\n",
      "Epoch 5/10, Batch 324/442, Training Loss: 0.5079\n",
      "Epoch 5/10, Batch 325/442, Training Loss: 0.3717\n",
      "Epoch 5/10, Batch 326/442, Training Loss: 0.3497\n",
      "Epoch 5/10, Batch 327/442, Training Loss: 0.5648\n",
      "Epoch 5/10, Batch 328/442, Training Loss: 0.7920\n",
      "Epoch 5/10, Batch 329/442, Training Loss: 0.5100\n",
      "Epoch 5/10, Batch 330/442, Training Loss: 0.7313\n",
      "Epoch 5/10, Batch 331/442, Training Loss: 0.5763\n",
      "Epoch 5/10, Batch 332/442, Training Loss: 0.6852\n",
      "Epoch 5/10, Batch 333/442, Training Loss: 0.6057\n",
      "Epoch 5/10, Batch 334/442, Training Loss: 0.3824\n",
      "Epoch 5/10, Batch 335/442, Training Loss: 0.6230\n",
      "Epoch 5/10, Batch 336/442, Training Loss: 0.7406\n",
      "Epoch 5/10, Batch 337/442, Training Loss: 0.6057\n",
      "Epoch 5/10, Batch 338/442, Training Loss: 0.8199\n",
      "Epoch 5/10, Batch 339/442, Training Loss: 0.7372\n",
      "Epoch 5/10, Batch 340/442, Training Loss: 0.8218\n",
      "Epoch 5/10, Batch 341/442, Training Loss: 0.4117\n",
      "Epoch 5/10, Batch 342/442, Training Loss: 0.9458\n",
      "Epoch 5/10, Batch 343/442, Training Loss: 0.4232\n",
      "Epoch 5/10, Batch 344/442, Training Loss: 0.5880\n",
      "Epoch 5/10, Batch 345/442, Training Loss: 0.6071\n",
      "Epoch 5/10, Batch 346/442, Training Loss: 0.5183\n",
      "Epoch 5/10, Batch 347/442, Training Loss: 0.3518\n",
      "Epoch 5/10, Batch 348/442, Training Loss: 0.8627\n",
      "Epoch 5/10, Batch 349/442, Training Loss: 0.6715\n",
      "Epoch 5/10, Batch 350/442, Training Loss: 0.5151\n",
      "Epoch 5/10, Batch 351/442, Training Loss: 0.8514\n",
      "Epoch 5/10, Batch 352/442, Training Loss: 0.5603\n",
      "Epoch 5/10, Batch 353/442, Training Loss: 0.5203\n",
      "Epoch 5/10, Batch 354/442, Training Loss: 0.5924\n",
      "Epoch 5/10, Batch 355/442, Training Loss: 0.5511\n",
      "Epoch 5/10, Batch 356/442, Training Loss: 0.5300\n",
      "Epoch 5/10, Batch 357/442, Training Loss: 0.5866\n",
      "Epoch 5/10, Batch 358/442, Training Loss: 0.3935\n",
      "Epoch 5/10, Batch 359/442, Training Loss: 0.6994\n",
      "Epoch 5/10, Batch 360/442, Training Loss: 0.6408\n",
      "Epoch 5/10, Batch 361/442, Training Loss: 0.5543\n",
      "Epoch 5/10, Batch 362/442, Training Loss: 0.4413\n",
      "Epoch 5/10, Batch 363/442, Training Loss: 0.6841\n",
      "Epoch 5/10, Batch 364/442, Training Loss: 0.4269\n",
      "Epoch 5/10, Batch 365/442, Training Loss: 0.5594\n",
      "Epoch 5/10, Batch 366/442, Training Loss: 0.7100\n",
      "Epoch 5/10, Batch 367/442, Training Loss: 0.5702\n",
      "Epoch 5/10, Batch 368/442, Training Loss: 0.5977\n",
      "Epoch 5/10, Batch 369/442, Training Loss: 0.4821\n",
      "Epoch 5/10, Batch 370/442, Training Loss: 0.4879\n",
      "Epoch 5/10, Batch 371/442, Training Loss: 0.5993\n",
      "Epoch 5/10, Batch 372/442, Training Loss: 0.8578\n",
      "Epoch 5/10, Batch 373/442, Training Loss: 0.6433\n",
      "Epoch 5/10, Batch 374/442, Training Loss: 0.4777\n",
      "Epoch 5/10, Batch 375/442, Training Loss: 0.6764\n",
      "Epoch 5/10, Batch 376/442, Training Loss: 0.6531\n",
      "Epoch 5/10, Batch 377/442, Training Loss: 0.6514\n",
      "Epoch 5/10, Batch 378/442, Training Loss: 0.7310\n",
      "Epoch 5/10, Batch 379/442, Training Loss: 0.7184\n",
      "Epoch 5/10, Batch 380/442, Training Loss: 0.5594\n",
      "Epoch 5/10, Batch 381/442, Training Loss: 0.5976\n",
      "Epoch 5/10, Batch 382/442, Training Loss: 0.6447\n",
      "Epoch 5/10, Batch 383/442, Training Loss: 0.5842\n",
      "Epoch 5/10, Batch 384/442, Training Loss: 0.6485\n",
      "Epoch 5/10, Batch 385/442, Training Loss: 0.7044\n",
      "Epoch 5/10, Batch 386/442, Training Loss: 0.6474\n",
      "Epoch 5/10, Batch 387/442, Training Loss: 0.7551\n",
      "Epoch 5/10, Batch 388/442, Training Loss: 0.5185\n",
      "Epoch 5/10, Batch 389/442, Training Loss: 0.5095\n",
      "Epoch 5/10, Batch 390/442, Training Loss: 0.7054\n",
      "Epoch 5/10, Batch 391/442, Training Loss: 0.5193\n",
      "Epoch 5/10, Batch 392/442, Training Loss: 0.5884\n",
      "Epoch 5/10, Batch 393/442, Training Loss: 0.5908\n",
      "Epoch 5/10, Batch 394/442, Training Loss: 0.7081\n",
      "Epoch 5/10, Batch 395/442, Training Loss: 0.6079\n",
      "Epoch 5/10, Batch 396/442, Training Loss: 0.4947\n",
      "Epoch 5/10, Batch 397/442, Training Loss: 0.5538\n",
      "Epoch 5/10, Batch 398/442, Training Loss: 0.7183\n",
      "Epoch 5/10, Batch 399/442, Training Loss: 0.4104\n",
      "Epoch 5/10, Batch 400/442, Training Loss: 0.6104\n",
      "Epoch 5/10, Batch 401/442, Training Loss: 0.4676\n",
      "Epoch 5/10, Batch 402/442, Training Loss: 0.8279\n",
      "Epoch 5/10, Batch 403/442, Training Loss: 0.4827\n",
      "Epoch 5/10, Batch 404/442, Training Loss: 0.4834\n",
      "Epoch 5/10, Batch 405/442, Training Loss: 0.6849\n",
      "Epoch 5/10, Batch 406/442, Training Loss: 0.2949\n",
      "Epoch 5/10, Batch 407/442, Training Loss: 0.5233\n",
      "Epoch 5/10, Batch 408/442, Training Loss: 0.4772\n",
      "Epoch 5/10, Batch 409/442, Training Loss: 0.6148\n",
      "Epoch 5/10, Batch 410/442, Training Loss: 0.3449\n",
      "Epoch 5/10, Batch 411/442, Training Loss: 0.4823\n",
      "Epoch 5/10, Batch 412/442, Training Loss: 0.6498\n",
      "Epoch 5/10, Batch 413/442, Training Loss: 0.6052\n",
      "Epoch 5/10, Batch 414/442, Training Loss: 0.7498\n",
      "Epoch 5/10, Batch 415/442, Training Loss: 0.4899\n",
      "Epoch 5/10, Batch 416/442, Training Loss: 0.8384\n",
      "Epoch 5/10, Batch 417/442, Training Loss: 0.5726\n",
      "Epoch 5/10, Batch 418/442, Training Loss: 0.6290\n",
      "Epoch 5/10, Batch 419/442, Training Loss: 0.4879\n",
      "Epoch 5/10, Batch 420/442, Training Loss: 0.8348\n",
      "Epoch 5/10, Batch 421/442, Training Loss: 0.8176\n",
      "Epoch 5/10, Batch 422/442, Training Loss: 0.5173\n",
      "Epoch 5/10, Batch 423/442, Training Loss: 0.7516\n",
      "Epoch 5/10, Batch 424/442, Training Loss: 0.5573\n",
      "Epoch 5/10, Batch 425/442, Training Loss: 0.6560\n",
      "Epoch 5/10, Batch 426/442, Training Loss: 0.6304\n",
      "Epoch 5/10, Batch 427/442, Training Loss: 0.5260\n",
      "Epoch 5/10, Batch 428/442, Training Loss: 0.5015\n",
      "Epoch 5/10, Batch 429/442, Training Loss: 0.7449\n",
      "Epoch 5/10, Batch 430/442, Training Loss: 0.6453\n",
      "Epoch 5/10, Batch 431/442, Training Loss: 0.7354\n",
      "Epoch 5/10, Batch 432/442, Training Loss: 0.8573\n",
      "Epoch 5/10, Batch 433/442, Training Loss: 0.7463\n",
      "Epoch 5/10, Batch 434/442, Training Loss: 0.6702\n",
      "Epoch 5/10, Batch 435/442, Training Loss: 0.7379\n",
      "Epoch 5/10, Batch 436/442, Training Loss: 0.5584\n",
      "Epoch 5/10, Batch 437/442, Training Loss: 0.7427\n",
      "Epoch 5/10, Batch 438/442, Training Loss: 0.7265\n",
      "Epoch 5/10, Batch 439/442, Training Loss: 0.5717\n",
      "Epoch 5/10, Batch 440/442, Training Loss: 0.5264\n",
      "Epoch 5/10, Batch 441/442, Training Loss: 0.5913\n",
      "Epoch 5/10, Batch 442/442, Training Loss: 0.6981\n",
      "Epoch 5/10, Training Loss: 0.6250, Validation Loss: 0.6956, Validation Accuracy: 0.6638\n",
      "Epoch 6/10, Batch 1/442, Training Loss: 0.5216\n",
      "Epoch 6/10, Batch 2/442, Training Loss: 0.6591\n",
      "Epoch 6/10, Batch 3/442, Training Loss: 0.6672\n",
      "Epoch 6/10, Batch 4/442, Training Loss: 0.5730\n",
      "Epoch 6/10, Batch 5/442, Training Loss: 0.7852\n",
      "Epoch 6/10, Batch 6/442, Training Loss: 0.6387\n",
      "Epoch 6/10, Batch 7/442, Training Loss: 0.7110\n",
      "Epoch 6/10, Batch 8/442, Training Loss: 0.5460\n",
      "Epoch 6/10, Batch 9/442, Training Loss: 0.6345\n",
      "Epoch 6/10, Batch 10/442, Training Loss: 0.5789\n",
      "Epoch 6/10, Batch 11/442, Training Loss: 0.4786\n",
      "Epoch 6/10, Batch 12/442, Training Loss: 0.5137\n",
      "Epoch 6/10, Batch 13/442, Training Loss: 0.5594\n",
      "Epoch 6/10, Batch 14/442, Training Loss: 0.7924\n",
      "Epoch 6/10, Batch 15/442, Training Loss: 0.5636\n",
      "Epoch 6/10, Batch 16/442, Training Loss: 0.4655\n",
      "Epoch 6/10, Batch 17/442, Training Loss: 0.5109\n",
      "Epoch 6/10, Batch 18/442, Training Loss: 0.5451\n",
      "Epoch 6/10, Batch 19/442, Training Loss: 0.6451\n",
      "Epoch 6/10, Batch 20/442, Training Loss: 0.7148\n",
      "Epoch 6/10, Batch 21/442, Training Loss: 0.4955\n",
      "Epoch 6/10, Batch 22/442, Training Loss: 0.6874\n",
      "Epoch 6/10, Batch 23/442, Training Loss: 0.5445\n",
      "Epoch 6/10, Batch 24/442, Training Loss: 0.6814\n",
      "Epoch 6/10, Batch 25/442, Training Loss: 0.4662\n",
      "Epoch 6/10, Batch 26/442, Training Loss: 0.5263\n",
      "Epoch 6/10, Batch 27/442, Training Loss: 0.4416\n",
      "Epoch 6/10, Batch 28/442, Training Loss: 0.4800\n",
      "Epoch 6/10, Batch 29/442, Training Loss: 0.6635\n",
      "Epoch 6/10, Batch 30/442, Training Loss: 0.5621\n",
      "Epoch 6/10, Batch 31/442, Training Loss: 0.3883\n",
      "Epoch 6/10, Batch 32/442, Training Loss: 0.4022\n",
      "Epoch 6/10, Batch 33/442, Training Loss: 0.7358\n",
      "Epoch 6/10, Batch 34/442, Training Loss: 0.6729\n",
      "Epoch 6/10, Batch 35/442, Training Loss: 0.3387\n",
      "Epoch 6/10, Batch 36/442, Training Loss: 0.8091\n",
      "Epoch 6/10, Batch 37/442, Training Loss: 0.8408\n",
      "Epoch 6/10, Batch 38/442, Training Loss: 0.6116\n",
      "Epoch 6/10, Batch 39/442, Training Loss: 0.5952\n",
      "Epoch 6/10, Batch 40/442, Training Loss: 0.4320\n",
      "Epoch 6/10, Batch 41/442, Training Loss: 0.4715\n",
      "Epoch 6/10, Batch 42/442, Training Loss: 0.6356\n",
      "Epoch 6/10, Batch 43/442, Training Loss: 0.4584\n",
      "Epoch 6/10, Batch 44/442, Training Loss: 0.5452\n",
      "Epoch 6/10, Batch 45/442, Training Loss: 0.4445\n",
      "Epoch 6/10, Batch 46/442, Training Loss: 0.5412\n",
      "Epoch 6/10, Batch 47/442, Training Loss: 0.6594\n",
      "Epoch 6/10, Batch 48/442, Training Loss: 0.5176\n",
      "Epoch 6/10, Batch 49/442, Training Loss: 0.8587\n",
      "Epoch 6/10, Batch 50/442, Training Loss: 0.5651\n",
      "Epoch 6/10, Batch 51/442, Training Loss: 0.8634\n",
      "Epoch 6/10, Batch 52/442, Training Loss: 0.6339\n",
      "Epoch 6/10, Batch 53/442, Training Loss: 0.6344\n",
      "Epoch 6/10, Batch 54/442, Training Loss: 0.4329\n",
      "Epoch 6/10, Batch 55/442, Training Loss: 0.5387\n",
      "Epoch 6/10, Batch 56/442, Training Loss: 0.3844\n",
      "Epoch 6/10, Batch 57/442, Training Loss: 0.3794\n",
      "Epoch 6/10, Batch 58/442, Training Loss: 0.4057\n",
      "Epoch 6/10, Batch 59/442, Training Loss: 0.4424\n",
      "Epoch 6/10, Batch 60/442, Training Loss: 0.3979\n",
      "Epoch 6/10, Batch 61/442, Training Loss: 0.6575\n",
      "Epoch 6/10, Batch 62/442, Training Loss: 0.7187\n",
      "Epoch 6/10, Batch 63/442, Training Loss: 0.6945\n",
      "Epoch 6/10, Batch 64/442, Training Loss: 0.7689\n",
      "Epoch 6/10, Batch 65/442, Training Loss: 0.4118\n",
      "Epoch 6/10, Batch 66/442, Training Loss: 0.5898\n",
      "Epoch 6/10, Batch 67/442, Training Loss: 0.5366\n",
      "Epoch 6/10, Batch 68/442, Training Loss: 0.7078\n",
      "Epoch 6/10, Batch 69/442, Training Loss: 0.6021\n",
      "Epoch 6/10, Batch 70/442, Training Loss: 0.5452\n",
      "Epoch 6/10, Batch 71/442, Training Loss: 0.6069\n",
      "Epoch 6/10, Batch 72/442, Training Loss: 0.6812\n",
      "Epoch 6/10, Batch 73/442, Training Loss: 0.8514\n",
      "Epoch 6/10, Batch 74/442, Training Loss: 0.6152\n",
      "Epoch 6/10, Batch 75/442, Training Loss: 0.4670\n",
      "Epoch 6/10, Batch 76/442, Training Loss: 0.5739\n",
      "Epoch 6/10, Batch 77/442, Training Loss: 0.3553\n",
      "Epoch 6/10, Batch 78/442, Training Loss: 0.6712\n",
      "Epoch 6/10, Batch 79/442, Training Loss: 0.4580\n",
      "Epoch 6/10, Batch 80/442, Training Loss: 0.5520\n",
      "Epoch 6/10, Batch 81/442, Training Loss: 0.7885\n",
      "Epoch 6/10, Batch 82/442, Training Loss: 0.4596\n",
      "Epoch 6/10, Batch 83/442, Training Loss: 0.6772\n",
      "Epoch 6/10, Batch 84/442, Training Loss: 0.5546\n",
      "Epoch 6/10, Batch 85/442, Training Loss: 0.6193\n",
      "Epoch 6/10, Batch 86/442, Training Loss: 0.7091\n",
      "Epoch 6/10, Batch 87/442, Training Loss: 0.5284\n",
      "Epoch 6/10, Batch 88/442, Training Loss: 0.6669\n",
      "Epoch 6/10, Batch 89/442, Training Loss: 0.8225\n",
      "Epoch 6/10, Batch 90/442, Training Loss: 0.5011\n",
      "Epoch 6/10, Batch 91/442, Training Loss: 0.7573\n",
      "Epoch 6/10, Batch 92/442, Training Loss: 0.8135\n",
      "Epoch 6/10, Batch 93/442, Training Loss: 0.8184\n",
      "Epoch 6/10, Batch 94/442, Training Loss: 0.4184\n",
      "Epoch 6/10, Batch 95/442, Training Loss: 0.5705\n",
      "Epoch 6/10, Batch 96/442, Training Loss: 0.5164\n",
      "Epoch 6/10, Batch 97/442, Training Loss: 0.6060\n",
      "Epoch 6/10, Batch 98/442, Training Loss: 0.7076\n",
      "Epoch 6/10, Batch 99/442, Training Loss: 0.4682\n",
      "Epoch 6/10, Batch 100/442, Training Loss: 0.6182\n",
      "Epoch 6/10, Batch 101/442, Training Loss: 0.4978\n",
      "Epoch 6/10, Batch 102/442, Training Loss: 0.3636\n",
      "Epoch 6/10, Batch 103/442, Training Loss: 0.6135\n",
      "Epoch 6/10, Batch 104/442, Training Loss: 0.6217\n",
      "Epoch 6/10, Batch 105/442, Training Loss: 0.5334\n",
      "Epoch 6/10, Batch 106/442, Training Loss: 0.4558\n",
      "Epoch 6/10, Batch 107/442, Training Loss: 0.6033\n",
      "Epoch 6/10, Batch 108/442, Training Loss: 0.5899\n",
      "Epoch 6/10, Batch 109/442, Training Loss: 0.5165\n",
      "Epoch 6/10, Batch 110/442, Training Loss: 0.4378\n",
      "Epoch 6/10, Batch 111/442, Training Loss: 0.5816\n",
      "Epoch 6/10, Batch 112/442, Training Loss: 0.4306\n",
      "Epoch 6/10, Batch 113/442, Training Loss: 0.2760\n",
      "Epoch 6/10, Batch 114/442, Training Loss: 0.5039\n",
      "Epoch 6/10, Batch 115/442, Training Loss: 0.5279\n",
      "Epoch 6/10, Batch 116/442, Training Loss: 0.5086\n",
      "Epoch 6/10, Batch 117/442, Training Loss: 0.6895\n",
      "Epoch 6/10, Batch 118/442, Training Loss: 0.5680\n",
      "Epoch 6/10, Batch 119/442, Training Loss: 1.0625\n",
      "Epoch 6/10, Batch 120/442, Training Loss: 0.4301\n",
      "Epoch 6/10, Batch 121/442, Training Loss: 0.7167\n",
      "Epoch 6/10, Batch 122/442, Training Loss: 0.7257\n",
      "Epoch 6/10, Batch 123/442, Training Loss: 0.6446\n",
      "Epoch 6/10, Batch 124/442, Training Loss: 0.6353\n",
      "Epoch 6/10, Batch 125/442, Training Loss: 0.4622\n",
      "Epoch 6/10, Batch 126/442, Training Loss: 0.7445\n",
      "Epoch 6/10, Batch 127/442, Training Loss: 0.7124\n",
      "Epoch 6/10, Batch 128/442, Training Loss: 0.5938\n",
      "Epoch 6/10, Batch 129/442, Training Loss: 0.3894\n",
      "Epoch 6/10, Batch 130/442, Training Loss: 0.5889\n",
      "Epoch 6/10, Batch 131/442, Training Loss: 0.7034\n",
      "Epoch 6/10, Batch 132/442, Training Loss: 0.5272\n",
      "Epoch 6/10, Batch 133/442, Training Loss: 0.5143\n",
      "Epoch 6/10, Batch 134/442, Training Loss: 0.6404\n",
      "Epoch 6/10, Batch 135/442, Training Loss: 0.5511\n",
      "Epoch 6/10, Batch 136/442, Training Loss: 0.3606\n",
      "Epoch 6/10, Batch 137/442, Training Loss: 0.7160\n",
      "Epoch 6/10, Batch 138/442, Training Loss: 0.5075\n",
      "Epoch 6/10, Batch 139/442, Training Loss: 0.5177\n",
      "Epoch 6/10, Batch 140/442, Training Loss: 0.5567\n",
      "Epoch 6/10, Batch 141/442, Training Loss: 0.4887\n",
      "Epoch 6/10, Batch 142/442, Training Loss: 0.5544\n",
      "Epoch 6/10, Batch 143/442, Training Loss: 0.7092\n",
      "Epoch 6/10, Batch 144/442, Training Loss: 0.6160\n",
      "Epoch 6/10, Batch 145/442, Training Loss: 0.3805\n",
      "Epoch 6/10, Batch 146/442, Training Loss: 0.7311\n",
      "Epoch 6/10, Batch 147/442, Training Loss: 0.5302\n",
      "Epoch 6/10, Batch 148/442, Training Loss: 0.5474\n",
      "Epoch 6/10, Batch 149/442, Training Loss: 0.9497\n",
      "Epoch 6/10, Batch 150/442, Training Loss: 0.6692\n",
      "Epoch 6/10, Batch 151/442, Training Loss: 0.7280\n",
      "Epoch 6/10, Batch 152/442, Training Loss: 0.7218\n",
      "Epoch 6/10, Batch 153/442, Training Loss: 0.6925\n",
      "Epoch 6/10, Batch 154/442, Training Loss: 0.4852\n",
      "Epoch 6/10, Batch 155/442, Training Loss: 0.7617\n",
      "Epoch 6/10, Batch 156/442, Training Loss: 0.5990\n",
      "Epoch 6/10, Batch 157/442, Training Loss: 0.6350\n",
      "Epoch 6/10, Batch 158/442, Training Loss: 0.5728\n",
      "Epoch 6/10, Batch 159/442, Training Loss: 0.4407\n",
      "Epoch 6/10, Batch 160/442, Training Loss: 0.4633\n",
      "Epoch 6/10, Batch 161/442, Training Loss: 0.6181\n",
      "Epoch 6/10, Batch 162/442, Training Loss: 0.7202\n",
      "Epoch 6/10, Batch 163/442, Training Loss: 0.5182\n",
      "Epoch 6/10, Batch 164/442, Training Loss: 0.4902\n",
      "Epoch 6/10, Batch 165/442, Training Loss: 0.7092\n",
      "Epoch 6/10, Batch 166/442, Training Loss: 0.6050\n",
      "Epoch 6/10, Batch 167/442, Training Loss: 0.3941\n",
      "Epoch 6/10, Batch 168/442, Training Loss: 0.6477\n",
      "Epoch 6/10, Batch 169/442, Training Loss: 0.4004\n",
      "Epoch 6/10, Batch 170/442, Training Loss: 0.6961\n",
      "Epoch 6/10, Batch 171/442, Training Loss: 0.4771\n",
      "Epoch 6/10, Batch 172/442, Training Loss: 0.7274\n",
      "Epoch 6/10, Batch 173/442, Training Loss: 0.4419\n",
      "Epoch 6/10, Batch 174/442, Training Loss: 0.8354\n",
      "Epoch 6/10, Batch 175/442, Training Loss: 0.6210\n",
      "Epoch 6/10, Batch 176/442, Training Loss: 0.5076\n",
      "Epoch 6/10, Batch 177/442, Training Loss: 0.4670\n",
      "Epoch 6/10, Batch 178/442, Training Loss: 0.9440\n",
      "Epoch 6/10, Batch 179/442, Training Loss: 0.3445\n",
      "Epoch 6/10, Batch 180/442, Training Loss: 0.3864\n",
      "Epoch 6/10, Batch 181/442, Training Loss: 0.5890\n",
      "Epoch 6/10, Batch 182/442, Training Loss: 0.6791\n",
      "Epoch 6/10, Batch 183/442, Training Loss: 0.6041\n",
      "Epoch 6/10, Batch 184/442, Training Loss: 0.6296\n",
      "Epoch 6/10, Batch 185/442, Training Loss: 0.6166\n",
      "Epoch 6/10, Batch 186/442, Training Loss: 0.7452\n",
      "Epoch 6/10, Batch 187/442, Training Loss: 0.7182\n",
      "Epoch 6/10, Batch 188/442, Training Loss: 0.5048\n",
      "Epoch 6/10, Batch 189/442, Training Loss: 0.6870\n",
      "Epoch 6/10, Batch 190/442, Training Loss: 0.4729\n",
      "Epoch 6/10, Batch 191/442, Training Loss: 0.6226\n",
      "Epoch 6/10, Batch 192/442, Training Loss: 0.6009\n",
      "Epoch 6/10, Batch 193/442, Training Loss: 0.6933\n",
      "Epoch 6/10, Batch 194/442, Training Loss: 0.6440\n",
      "Epoch 6/10, Batch 195/442, Training Loss: 0.4808\n",
      "Epoch 6/10, Batch 196/442, Training Loss: 0.5491\n",
      "Epoch 6/10, Batch 197/442, Training Loss: 0.5886\n",
      "Epoch 6/10, Batch 198/442, Training Loss: 0.3810\n",
      "Epoch 6/10, Batch 199/442, Training Loss: 0.6112\n",
      "Epoch 6/10, Batch 200/442, Training Loss: 0.6748\n",
      "Epoch 6/10, Batch 201/442, Training Loss: 1.0364\n",
      "Epoch 6/10, Batch 202/442, Training Loss: 0.7160\n",
      "Epoch 6/10, Batch 203/442, Training Loss: 0.5801\n",
      "Epoch 6/10, Batch 204/442, Training Loss: 0.8123\n",
      "Epoch 6/10, Batch 205/442, Training Loss: 0.6713\n",
      "Epoch 6/10, Batch 206/442, Training Loss: 0.4637\n",
      "Epoch 6/10, Batch 207/442, Training Loss: 0.6273\n",
      "Epoch 6/10, Batch 208/442, Training Loss: 0.4483\n",
      "Epoch 6/10, Batch 209/442, Training Loss: 0.5053\n",
      "Epoch 6/10, Batch 210/442, Training Loss: 0.6967\n",
      "Epoch 6/10, Batch 211/442, Training Loss: 0.7054\n",
      "Epoch 6/10, Batch 212/442, Training Loss: 0.6294\n",
      "Epoch 6/10, Batch 213/442, Training Loss: 0.5601\n",
      "Epoch 6/10, Batch 214/442, Training Loss: 0.5443\n",
      "Epoch 6/10, Batch 215/442, Training Loss: 0.4561\n",
      "Epoch 6/10, Batch 216/442, Training Loss: 0.6629\n",
      "Epoch 6/10, Batch 217/442, Training Loss: 0.5589\n",
      "Epoch 6/10, Batch 218/442, Training Loss: 0.4948\n",
      "Epoch 6/10, Batch 219/442, Training Loss: 0.6562\n",
      "Epoch 6/10, Batch 220/442, Training Loss: 0.6056\n",
      "Epoch 6/10, Batch 221/442, Training Loss: 0.6174\n",
      "Epoch 6/10, Batch 222/442, Training Loss: 0.6421\n",
      "Epoch 6/10, Batch 223/442, Training Loss: 0.3890\n",
      "Epoch 6/10, Batch 224/442, Training Loss: 0.3587\n",
      "Epoch 6/10, Batch 225/442, Training Loss: 0.4722\n",
      "Epoch 6/10, Batch 226/442, Training Loss: 0.6552\n",
      "Epoch 6/10, Batch 227/442, Training Loss: 0.5982\n",
      "Epoch 6/10, Batch 228/442, Training Loss: 0.6300\n",
      "Epoch 6/10, Batch 229/442, Training Loss: 0.5597\n",
      "Epoch 6/10, Batch 230/442, Training Loss: 0.6020\n",
      "Epoch 6/10, Batch 231/442, Training Loss: 0.5934\n",
      "Epoch 6/10, Batch 232/442, Training Loss: 0.6852\n",
      "Epoch 6/10, Batch 233/442, Training Loss: 0.7446\n",
      "Epoch 6/10, Batch 234/442, Training Loss: 0.4630\n",
      "Epoch 6/10, Batch 235/442, Training Loss: 0.3272\n",
      "Epoch 6/10, Batch 236/442, Training Loss: 0.7856\n",
      "Epoch 6/10, Batch 237/442, Training Loss: 0.7903\n",
      "Epoch 6/10, Batch 238/442, Training Loss: 0.4980\n",
      "Epoch 6/10, Batch 239/442, Training Loss: 0.4822\n",
      "Epoch 6/10, Batch 240/442, Training Loss: 0.5860\n",
      "Epoch 6/10, Batch 241/442, Training Loss: 0.5495\n",
      "Epoch 6/10, Batch 242/442, Training Loss: 0.5138\n",
      "Epoch 6/10, Batch 243/442, Training Loss: 0.5706\n",
      "Epoch 6/10, Batch 244/442, Training Loss: 0.3242\n",
      "Epoch 6/10, Batch 245/442, Training Loss: 0.7674\n",
      "Epoch 6/10, Batch 246/442, Training Loss: 0.3680\n",
      "Epoch 6/10, Batch 247/442, Training Loss: 0.5414\n",
      "Epoch 6/10, Batch 248/442, Training Loss: 0.8296\n",
      "Epoch 6/10, Batch 249/442, Training Loss: 0.6416\n",
      "Epoch 6/10, Batch 250/442, Training Loss: 0.6963\n",
      "Epoch 6/10, Batch 251/442, Training Loss: 0.6986\n",
      "Epoch 6/10, Batch 252/442, Training Loss: 0.4774\n",
      "Epoch 6/10, Batch 253/442, Training Loss: 0.6736\n",
      "Epoch 6/10, Batch 254/442, Training Loss: 0.6550\n",
      "Epoch 6/10, Batch 255/442, Training Loss: 0.6301\n",
      "Epoch 6/10, Batch 256/442, Training Loss: 0.5618\n",
      "Epoch 6/10, Batch 257/442, Training Loss: 0.5513\n",
      "Epoch 6/10, Batch 258/442, Training Loss: 0.6035\n",
      "Epoch 6/10, Batch 259/442, Training Loss: 0.6945\n",
      "Epoch 6/10, Batch 260/442, Training Loss: 0.5348\n",
      "Epoch 6/10, Batch 261/442, Training Loss: 0.7790\n",
      "Epoch 6/10, Batch 262/442, Training Loss: 0.4995\n",
      "Epoch 6/10, Batch 263/442, Training Loss: 0.4102\n",
      "Epoch 6/10, Batch 264/442, Training Loss: 0.7359\n",
      "Epoch 6/10, Batch 265/442, Training Loss: 0.5820\n",
      "Epoch 6/10, Batch 266/442, Training Loss: 0.6063\n",
      "Epoch 6/10, Batch 267/442, Training Loss: 0.6884\n",
      "Epoch 6/10, Batch 268/442, Training Loss: 0.5529\n",
      "Epoch 6/10, Batch 269/442, Training Loss: 0.4079\n",
      "Epoch 6/10, Batch 270/442, Training Loss: 0.5673\n",
      "Epoch 6/10, Batch 271/442, Training Loss: 0.5565\n",
      "Epoch 6/10, Batch 272/442, Training Loss: 0.4828\n",
      "Epoch 6/10, Batch 273/442, Training Loss: 0.4261\n",
      "Epoch 6/10, Batch 274/442, Training Loss: 0.4505\n",
      "Epoch 6/10, Batch 275/442, Training Loss: 0.4007\n",
      "Epoch 6/10, Batch 276/442, Training Loss: 0.5710\n",
      "Epoch 6/10, Batch 277/442, Training Loss: 0.7036\n",
      "Epoch 6/10, Batch 278/442, Training Loss: 0.4727\n",
      "Epoch 6/10, Batch 279/442, Training Loss: 0.8257\n",
      "Epoch 6/10, Batch 280/442, Training Loss: 0.2994\n",
      "Epoch 6/10, Batch 281/442, Training Loss: 0.4810\n",
      "Epoch 6/10, Batch 282/442, Training Loss: 0.5697\n",
      "Epoch 6/10, Batch 283/442, Training Loss: 0.5409\n",
      "Epoch 6/10, Batch 284/442, Training Loss: 0.4404\n",
      "Epoch 6/10, Batch 285/442, Training Loss: 0.5599\n",
      "Epoch 6/10, Batch 286/442, Training Loss: 0.5145\n",
      "Epoch 6/10, Batch 287/442, Training Loss: 0.6747\n",
      "Epoch 6/10, Batch 288/442, Training Loss: 0.4258\n",
      "Epoch 6/10, Batch 289/442, Training Loss: 0.8192\n",
      "Epoch 6/10, Batch 290/442, Training Loss: 0.4939\n",
      "Epoch 6/10, Batch 291/442, Training Loss: 0.5943\n",
      "Epoch 6/10, Batch 292/442, Training Loss: 0.7943\n",
      "Epoch 6/10, Batch 293/442, Training Loss: 0.6076\n",
      "Epoch 6/10, Batch 294/442, Training Loss: 0.4438\n",
      "Epoch 6/10, Batch 295/442, Training Loss: 0.4784\n",
      "Epoch 6/10, Batch 296/442, Training Loss: 0.5228\n",
      "Epoch 6/10, Batch 297/442, Training Loss: 0.5756\n",
      "Epoch 6/10, Batch 298/442, Training Loss: 0.3812\n",
      "Epoch 6/10, Batch 299/442, Training Loss: 0.5073\n",
      "Epoch 6/10, Batch 300/442, Training Loss: 0.5582\n",
      "Epoch 6/10, Batch 301/442, Training Loss: 0.5954\n",
      "Epoch 6/10, Batch 302/442, Training Loss: 0.7763\n",
      "Epoch 6/10, Batch 303/442, Training Loss: 0.5152\n",
      "Epoch 6/10, Batch 304/442, Training Loss: 0.4134\n",
      "Epoch 6/10, Batch 305/442, Training Loss: 0.6537\n",
      "Epoch 6/10, Batch 306/442, Training Loss: 0.4406\n",
      "Epoch 6/10, Batch 307/442, Training Loss: 0.4608\n",
      "Epoch 6/10, Batch 308/442, Training Loss: 0.5082\n",
      "Epoch 6/10, Batch 309/442, Training Loss: 0.7160\n",
      "Epoch 6/10, Batch 310/442, Training Loss: 0.4834\n",
      "Epoch 6/10, Batch 311/442, Training Loss: 0.4900\n",
      "Epoch 6/10, Batch 312/442, Training Loss: 0.6324\n",
      "Epoch 6/10, Batch 313/442, Training Loss: 0.5641\n",
      "Epoch 6/10, Batch 314/442, Training Loss: 0.6568\n",
      "Epoch 6/10, Batch 315/442, Training Loss: 0.6495\n",
      "Epoch 6/10, Batch 316/442, Training Loss: 0.5948\n",
      "Epoch 6/10, Batch 317/442, Training Loss: 0.6286\n",
      "Epoch 6/10, Batch 318/442, Training Loss: 0.4828\n",
      "Epoch 6/10, Batch 319/442, Training Loss: 0.6903\n",
      "Epoch 6/10, Batch 320/442, Training Loss: 0.6336\n",
      "Epoch 6/10, Batch 321/442, Training Loss: 0.3897\n",
      "Epoch 6/10, Batch 322/442, Training Loss: 0.4402\n",
      "Epoch 6/10, Batch 323/442, Training Loss: 0.5828\n",
      "Epoch 6/10, Batch 324/442, Training Loss: 0.5538\n",
      "Epoch 6/10, Batch 325/442, Training Loss: 0.5228\n",
      "Epoch 6/10, Batch 326/442, Training Loss: 0.5306\n",
      "Epoch 6/10, Batch 327/442, Training Loss: 0.6592\n",
      "Epoch 6/10, Batch 328/442, Training Loss: 0.5346\n",
      "Epoch 6/10, Batch 329/442, Training Loss: 0.4929\n",
      "Epoch 6/10, Batch 330/442, Training Loss: 0.5862\n",
      "Epoch 6/10, Batch 331/442, Training Loss: 0.4974\n",
      "Epoch 6/10, Batch 332/442, Training Loss: 0.3886\n",
      "Epoch 6/10, Batch 333/442, Training Loss: 0.3087\n",
      "Epoch 6/10, Batch 334/442, Training Loss: 0.3502\n",
      "Epoch 6/10, Batch 335/442, Training Loss: 0.6374\n",
      "Epoch 6/10, Batch 336/442, Training Loss: 0.3669\n",
      "Epoch 6/10, Batch 337/442, Training Loss: 0.4446\n",
      "Epoch 6/10, Batch 338/442, Training Loss: 0.4635\n",
      "Epoch 6/10, Batch 339/442, Training Loss: 0.6306\n",
      "Epoch 6/10, Batch 340/442, Training Loss: 0.5529\n",
      "Epoch 6/10, Batch 341/442, Training Loss: 0.3565\n",
      "Epoch 6/10, Batch 342/442, Training Loss: 0.5615\n",
      "Epoch 6/10, Batch 343/442, Training Loss: 0.3784\n",
      "Epoch 6/10, Batch 344/442, Training Loss: 0.4088\n",
      "Epoch 6/10, Batch 345/442, Training Loss: 0.6162\n",
      "Epoch 6/10, Batch 346/442, Training Loss: 0.7467\n",
      "Epoch 6/10, Batch 347/442, Training Loss: 0.3689\n",
      "Epoch 6/10, Batch 348/442, Training Loss: 0.6513\n",
      "Epoch 6/10, Batch 349/442, Training Loss: 0.5376\n",
      "Epoch 6/10, Batch 350/442, Training Loss: 0.3766\n",
      "Epoch 6/10, Batch 351/442, Training Loss: 0.5453\n",
      "Epoch 6/10, Batch 352/442, Training Loss: 0.4508\n",
      "Epoch 6/10, Batch 353/442, Training Loss: 0.8444\n",
      "Epoch 6/10, Batch 354/442, Training Loss: 0.5906\n",
      "Epoch 6/10, Batch 355/442, Training Loss: 0.7668\n",
      "Epoch 6/10, Batch 356/442, Training Loss: 0.5539\n",
      "Epoch 6/10, Batch 357/442, Training Loss: 0.5992\n",
      "Epoch 6/10, Batch 358/442, Training Loss: 0.6313\n",
      "Epoch 6/10, Batch 359/442, Training Loss: 0.7954\n",
      "Epoch 6/10, Batch 360/442, Training Loss: 0.5501\n",
      "Epoch 6/10, Batch 361/442, Training Loss: 0.7519\n",
      "Epoch 6/10, Batch 362/442, Training Loss: 0.4191\n",
      "Epoch 6/10, Batch 363/442, Training Loss: 0.3753\n",
      "Epoch 6/10, Batch 364/442, Training Loss: 0.5153\n",
      "Epoch 6/10, Batch 365/442, Training Loss: 0.4292\n",
      "Epoch 6/10, Batch 366/442, Training Loss: 0.4966\n",
      "Epoch 6/10, Batch 367/442, Training Loss: 0.7833\n",
      "Epoch 6/10, Batch 368/442, Training Loss: 0.7271\n",
      "Epoch 6/10, Batch 369/442, Training Loss: 0.5794\n",
      "Epoch 6/10, Batch 370/442, Training Loss: 0.5832\n",
      "Epoch 6/10, Batch 371/442, Training Loss: 0.5660\n",
      "Epoch 6/10, Batch 372/442, Training Loss: 0.4490\n",
      "Epoch 6/10, Batch 373/442, Training Loss: 0.5500\n",
      "Epoch 6/10, Batch 374/442, Training Loss: 0.7413\n",
      "Epoch 6/10, Batch 375/442, Training Loss: 0.7437\n",
      "Epoch 6/10, Batch 376/442, Training Loss: 0.6126\n",
      "Epoch 6/10, Batch 377/442, Training Loss: 0.5765\n",
      "Epoch 6/10, Batch 378/442, Training Loss: 0.4777\n",
      "Epoch 6/10, Batch 379/442, Training Loss: 0.7088\n",
      "Epoch 6/10, Batch 380/442, Training Loss: 0.6214\n",
      "Epoch 6/10, Batch 381/442, Training Loss: 0.4414\n",
      "Epoch 6/10, Batch 382/442, Training Loss: 0.6007\n",
      "Epoch 6/10, Batch 383/442, Training Loss: 0.6273\n",
      "Epoch 6/10, Batch 384/442, Training Loss: 0.5336\n",
      "Epoch 6/10, Batch 385/442, Training Loss: 0.6005\n",
      "Epoch 6/10, Batch 386/442, Training Loss: 0.5883\n",
      "Epoch 6/10, Batch 387/442, Training Loss: 0.4064\n",
      "Epoch 6/10, Batch 388/442, Training Loss: 0.4170\n",
      "Epoch 6/10, Batch 389/442, Training Loss: 0.4782\n",
      "Epoch 6/10, Batch 390/442, Training Loss: 0.5433\n",
      "Epoch 6/10, Batch 391/442, Training Loss: 0.5632\n",
      "Epoch 6/10, Batch 392/442, Training Loss: 0.4057\n",
      "Epoch 6/10, Batch 393/442, Training Loss: 0.5252\n",
      "Epoch 6/10, Batch 394/442, Training Loss: 0.5632\n",
      "Epoch 6/10, Batch 395/442, Training Loss: 0.7543\n",
      "Epoch 6/10, Batch 396/442, Training Loss: 0.7973\n",
      "Epoch 6/10, Batch 397/442, Training Loss: 0.3802\n",
      "Epoch 6/10, Batch 398/442, Training Loss: 0.6758\n",
      "Epoch 6/10, Batch 399/442, Training Loss: 0.4509\n",
      "Epoch 6/10, Batch 400/442, Training Loss: 0.4254\n",
      "Epoch 6/10, Batch 401/442, Training Loss: 0.6748\n",
      "Epoch 6/10, Batch 402/442, Training Loss: 0.6786\n",
      "Epoch 6/10, Batch 403/442, Training Loss: 0.3224\n",
      "Epoch 6/10, Batch 404/442, Training Loss: 0.4057\n",
      "Epoch 6/10, Batch 405/442, Training Loss: 0.6149\n",
      "Epoch 6/10, Batch 406/442, Training Loss: 0.4653\n",
      "Epoch 6/10, Batch 407/442, Training Loss: 0.5898\n",
      "Epoch 6/10, Batch 408/442, Training Loss: 0.5764\n",
      "Epoch 6/10, Batch 409/442, Training Loss: 0.6299\n",
      "Epoch 6/10, Batch 410/442, Training Loss: 0.4687\n",
      "Epoch 6/10, Batch 411/442, Training Loss: 0.4339\n",
      "Epoch 6/10, Batch 412/442, Training Loss: 0.7177\n",
      "Epoch 6/10, Batch 413/442, Training Loss: 0.4381\n",
      "Epoch 6/10, Batch 414/442, Training Loss: 0.7240\n",
      "Epoch 6/10, Batch 415/442, Training Loss: 0.5399\n",
      "Epoch 6/10, Batch 416/442, Training Loss: 0.5859\n",
      "Epoch 6/10, Batch 417/442, Training Loss: 0.3695\n",
      "Epoch 6/10, Batch 418/442, Training Loss: 0.5265\n",
      "Epoch 6/10, Batch 419/442, Training Loss: 0.3920\n",
      "Epoch 6/10, Batch 420/442, Training Loss: 0.5056\n",
      "Epoch 6/10, Batch 421/442, Training Loss: 0.7239\n",
      "Epoch 6/10, Batch 422/442, Training Loss: 0.5407\n",
      "Epoch 6/10, Batch 423/442, Training Loss: 0.6770\n",
      "Epoch 6/10, Batch 424/442, Training Loss: 0.6085\n",
      "Epoch 6/10, Batch 425/442, Training Loss: 0.4328\n",
      "Epoch 6/10, Batch 426/442, Training Loss: 0.5919\n",
      "Epoch 6/10, Batch 427/442, Training Loss: 0.5351\n",
      "Epoch 6/10, Batch 428/442, Training Loss: 0.5230\n",
      "Epoch 6/10, Batch 429/442, Training Loss: 0.5287\n",
      "Epoch 6/10, Batch 430/442, Training Loss: 0.6220\n",
      "Epoch 6/10, Batch 431/442, Training Loss: 0.4566\n",
      "Epoch 6/10, Batch 432/442, Training Loss: 0.7026\n",
      "Epoch 6/10, Batch 433/442, Training Loss: 1.0149\n",
      "Epoch 6/10, Batch 434/442, Training Loss: 0.5606\n",
      "Epoch 6/10, Batch 435/442, Training Loss: 0.5931\n",
      "Epoch 6/10, Batch 436/442, Training Loss: 0.8306\n",
      "Epoch 6/10, Batch 437/442, Training Loss: 0.5811\n",
      "Epoch 6/10, Batch 438/442, Training Loss: 0.4881\n",
      "Epoch 6/10, Batch 439/442, Training Loss: 0.7887\n",
      "Epoch 6/10, Batch 440/442, Training Loss: 0.7546\n",
      "Epoch 6/10, Batch 441/442, Training Loss: 0.6700\n",
      "Epoch 6/10, Batch 442/442, Training Loss: 0.6351\n",
      "Epoch 6/10, Training Loss: 0.5773, Validation Loss: 0.7068, Validation Accuracy: 0.6667\n",
      "Epoch 7/10, Batch 1/442, Training Loss: 0.5462\n",
      "Epoch 7/10, Batch 2/442, Training Loss: 0.6153\n",
      "Epoch 7/10, Batch 3/442, Training Loss: 0.5825\n",
      "Epoch 7/10, Batch 4/442, Training Loss: 0.5077\n",
      "Epoch 7/10, Batch 5/442, Training Loss: 0.3920\n",
      "Epoch 7/10, Batch 6/442, Training Loss: 0.4457\n",
      "Epoch 7/10, Batch 7/442, Training Loss: 0.7257\n",
      "Epoch 7/10, Batch 8/442, Training Loss: 0.4980\n",
      "Epoch 7/10, Batch 9/442, Training Loss: 0.8305\n",
      "Epoch 7/10, Batch 10/442, Training Loss: 0.6720\n",
      "Epoch 7/10, Batch 11/442, Training Loss: 0.5179\n",
      "Epoch 7/10, Batch 12/442, Training Loss: 0.5499\n",
      "Epoch 7/10, Batch 13/442, Training Loss: 0.6077\n",
      "Epoch 7/10, Batch 14/442, Training Loss: 0.5152\n",
      "Epoch 7/10, Batch 15/442, Training Loss: 0.5290\n",
      "Epoch 7/10, Batch 16/442, Training Loss: 0.5417\n",
      "Epoch 7/10, Batch 17/442, Training Loss: 0.5210\n",
      "Epoch 7/10, Batch 18/442, Training Loss: 0.6078\n",
      "Epoch 7/10, Batch 19/442, Training Loss: 0.4913\n",
      "Epoch 7/10, Batch 20/442, Training Loss: 0.6286\n",
      "Epoch 7/10, Batch 21/442, Training Loss: 0.7939\n",
      "Epoch 7/10, Batch 22/442, Training Loss: 0.3922\n",
      "Epoch 7/10, Batch 23/442, Training Loss: 0.7137\n",
      "Epoch 7/10, Batch 24/442, Training Loss: 0.6865\n",
      "Epoch 7/10, Batch 25/442, Training Loss: 0.5697\n",
      "Epoch 7/10, Batch 26/442, Training Loss: 0.5184\n",
      "Epoch 7/10, Batch 27/442, Training Loss: 0.3858\n",
      "Epoch 7/10, Batch 28/442, Training Loss: 0.4743\n",
      "Epoch 7/10, Batch 29/442, Training Loss: 0.4172\n",
      "Epoch 7/10, Batch 30/442, Training Loss: 0.5269\n",
      "Epoch 7/10, Batch 31/442, Training Loss: 0.5719\n",
      "Epoch 7/10, Batch 32/442, Training Loss: 0.4706\n",
      "Epoch 7/10, Batch 33/442, Training Loss: 0.6701\n",
      "Epoch 7/10, Batch 34/442, Training Loss: 0.4710\n",
      "Epoch 7/10, Batch 35/442, Training Loss: 0.3407\n",
      "Epoch 7/10, Batch 36/442, Training Loss: 0.4921\n",
      "Epoch 7/10, Batch 37/442, Training Loss: 0.5579\n",
      "Epoch 7/10, Batch 38/442, Training Loss: 0.5048\n",
      "Epoch 7/10, Batch 39/442, Training Loss: 0.3627\n",
      "Epoch 7/10, Batch 40/442, Training Loss: 0.4851\n",
      "Epoch 7/10, Batch 41/442, Training Loss: 0.6486\n",
      "Epoch 7/10, Batch 42/442, Training Loss: 0.5052\n",
      "Epoch 7/10, Batch 43/442, Training Loss: 0.4030\n",
      "Epoch 7/10, Batch 44/442, Training Loss: 0.6419\n",
      "Epoch 7/10, Batch 45/442, Training Loss: 0.7753\n",
      "Epoch 7/10, Batch 46/442, Training Loss: 0.6358\n",
      "Epoch 7/10, Batch 47/442, Training Loss: 0.5247\n",
      "Epoch 7/10, Batch 48/442, Training Loss: 0.6753\n",
      "Epoch 7/10, Batch 49/442, Training Loss: 0.4155\n",
      "Epoch 7/10, Batch 50/442, Training Loss: 0.4046\n",
      "Epoch 7/10, Batch 51/442, Training Loss: 0.4137\n",
      "Epoch 7/10, Batch 52/442, Training Loss: 0.4958\n",
      "Epoch 7/10, Batch 53/442, Training Loss: 0.3703\n",
      "Epoch 7/10, Batch 54/442, Training Loss: 0.2533\n",
      "Epoch 7/10, Batch 55/442, Training Loss: 0.4444\n",
      "Epoch 7/10, Batch 56/442, Training Loss: 0.7387\n",
      "Epoch 7/10, Batch 57/442, Training Loss: 0.5807\n",
      "Epoch 7/10, Batch 58/442, Training Loss: 0.6185\n",
      "Epoch 7/10, Batch 59/442, Training Loss: 0.3174\n",
      "Epoch 7/10, Batch 60/442, Training Loss: 0.5042\n",
      "Epoch 7/10, Batch 61/442, Training Loss: 0.4318\n",
      "Epoch 7/10, Batch 62/442, Training Loss: 0.4622\n",
      "Epoch 7/10, Batch 63/442, Training Loss: 0.4002\n",
      "Epoch 7/10, Batch 64/442, Training Loss: 0.6931\n",
      "Epoch 7/10, Batch 65/442, Training Loss: 0.3752\n",
      "Epoch 7/10, Batch 66/442, Training Loss: 0.4680\n",
      "Epoch 7/10, Batch 67/442, Training Loss: 0.6155\n",
      "Epoch 7/10, Batch 68/442, Training Loss: 0.6324\n",
      "Epoch 7/10, Batch 69/442, Training Loss: 0.4167\n",
      "Epoch 7/10, Batch 70/442, Training Loss: 0.4409\n",
      "Epoch 7/10, Batch 71/442, Training Loss: 0.5229\n",
      "Epoch 7/10, Batch 72/442, Training Loss: 0.4925\n",
      "Epoch 7/10, Batch 73/442, Training Loss: 0.4664\n",
      "Epoch 7/10, Batch 74/442, Training Loss: 0.4904\n",
      "Epoch 7/10, Batch 75/442, Training Loss: 0.4295\n",
      "Epoch 7/10, Batch 76/442, Training Loss: 0.5574\n",
      "Epoch 7/10, Batch 77/442, Training Loss: 0.5348\n",
      "Epoch 7/10, Batch 78/442, Training Loss: 0.6427\n",
      "Epoch 7/10, Batch 79/442, Training Loss: 0.4423\n",
      "Epoch 7/10, Batch 80/442, Training Loss: 0.4570\n",
      "Epoch 7/10, Batch 81/442, Training Loss: 0.4392\n",
      "Epoch 7/10, Batch 82/442, Training Loss: 0.6014\n",
      "Epoch 7/10, Batch 83/442, Training Loss: 0.7691\n",
      "Epoch 7/10, Batch 84/442, Training Loss: 0.4286\n",
      "Epoch 7/10, Batch 85/442, Training Loss: 0.6976\n",
      "Epoch 7/10, Batch 86/442, Training Loss: 0.5176\n",
      "Epoch 7/10, Batch 87/442, Training Loss: 0.3931\n",
      "Epoch 7/10, Batch 88/442, Training Loss: 0.6912\n",
      "Epoch 7/10, Batch 89/442, Training Loss: 0.3658\n",
      "Epoch 7/10, Batch 90/442, Training Loss: 0.6166\n",
      "Epoch 7/10, Batch 91/442, Training Loss: 0.5501\n",
      "Epoch 7/10, Batch 92/442, Training Loss: 0.7088\n",
      "Epoch 7/10, Batch 93/442, Training Loss: 0.6233\n",
      "Epoch 7/10, Batch 94/442, Training Loss: 0.5700\n",
      "Epoch 7/10, Batch 95/442, Training Loss: 0.4416\n",
      "Epoch 7/10, Batch 96/442, Training Loss: 0.5817\n",
      "Epoch 7/10, Batch 97/442, Training Loss: 0.4448\n",
      "Epoch 7/10, Batch 98/442, Training Loss: 0.4846\n",
      "Epoch 7/10, Batch 99/442, Training Loss: 0.4343\n",
      "Epoch 7/10, Batch 100/442, Training Loss: 0.5328\n",
      "Epoch 7/10, Batch 101/442, Training Loss: 0.5670\n",
      "Epoch 7/10, Batch 102/442, Training Loss: 0.4542\n",
      "Epoch 7/10, Batch 103/442, Training Loss: 0.4958\n",
      "Epoch 7/10, Batch 104/442, Training Loss: 0.4260\n",
      "Epoch 7/10, Batch 105/442, Training Loss: 0.4690\n",
      "Epoch 7/10, Batch 106/442, Training Loss: 0.4084\n",
      "Epoch 7/10, Batch 107/442, Training Loss: 0.3935\n",
      "Epoch 7/10, Batch 108/442, Training Loss: 0.5823\n",
      "Epoch 7/10, Batch 109/442, Training Loss: 0.5429\n",
      "Epoch 7/10, Batch 110/442, Training Loss: 0.5534\n",
      "Epoch 7/10, Batch 111/442, Training Loss: 0.3296\n",
      "Epoch 7/10, Batch 112/442, Training Loss: 0.6860\n",
      "Epoch 7/10, Batch 113/442, Training Loss: 0.4524\n",
      "Epoch 7/10, Batch 114/442, Training Loss: 0.5879\n",
      "Epoch 7/10, Batch 115/442, Training Loss: 0.7942\n",
      "Epoch 7/10, Batch 116/442, Training Loss: 0.4624\n",
      "Epoch 7/10, Batch 117/442, Training Loss: 0.5560\n",
      "Epoch 7/10, Batch 118/442, Training Loss: 0.5028\n",
      "Epoch 7/10, Batch 119/442, Training Loss: 0.4111\n",
      "Epoch 7/10, Batch 120/442, Training Loss: 0.4346\n",
      "Epoch 7/10, Batch 121/442, Training Loss: 0.5519\n",
      "Epoch 7/10, Batch 122/442, Training Loss: 0.5741\n",
      "Epoch 7/10, Batch 123/442, Training Loss: 0.7335\n",
      "Epoch 7/10, Batch 124/442, Training Loss: 0.5251\n",
      "Epoch 7/10, Batch 125/442, Training Loss: 0.5734\n",
      "Epoch 7/10, Batch 126/442, Training Loss: 0.3632\n",
      "Epoch 7/10, Batch 127/442, Training Loss: 0.5215\n",
      "Epoch 7/10, Batch 128/442, Training Loss: 0.4695\n",
      "Epoch 7/10, Batch 129/442, Training Loss: 0.6315\n",
      "Epoch 7/10, Batch 130/442, Training Loss: 0.6764\n",
      "Epoch 7/10, Batch 131/442, Training Loss: 0.4141\n",
      "Epoch 7/10, Batch 132/442, Training Loss: 0.6083\n",
      "Epoch 7/10, Batch 133/442, Training Loss: 0.5888\n",
      "Epoch 7/10, Batch 134/442, Training Loss: 0.7921\n",
      "Epoch 7/10, Batch 135/442, Training Loss: 0.7073\n",
      "Epoch 7/10, Batch 136/442, Training Loss: 0.7123\n",
      "Epoch 7/10, Batch 137/442, Training Loss: 0.4560\n",
      "Epoch 7/10, Batch 138/442, Training Loss: 0.3644\n",
      "Epoch 7/10, Batch 139/442, Training Loss: 0.6174\n",
      "Epoch 7/10, Batch 140/442, Training Loss: 0.4534\n",
      "Epoch 7/10, Batch 141/442, Training Loss: 0.4888\n",
      "Epoch 7/10, Batch 142/442, Training Loss: 0.4862\n",
      "Epoch 7/10, Batch 143/442, Training Loss: 0.4288\n",
      "Epoch 7/10, Batch 144/442, Training Loss: 0.5176\n",
      "Epoch 7/10, Batch 145/442, Training Loss: 0.3811\n",
      "Epoch 7/10, Batch 146/442, Training Loss: 0.5222\n",
      "Epoch 7/10, Batch 147/442, Training Loss: 0.5359\n",
      "Epoch 7/10, Batch 148/442, Training Loss: 0.5963\n",
      "Epoch 7/10, Batch 149/442, Training Loss: 0.3525\n",
      "Epoch 7/10, Batch 150/442, Training Loss: 0.6369\n",
      "Epoch 7/10, Batch 151/442, Training Loss: 0.5735\n",
      "Epoch 7/10, Batch 152/442, Training Loss: 0.6098\n",
      "Epoch 7/10, Batch 153/442, Training Loss: 0.6645\n",
      "Epoch 7/10, Batch 154/442, Training Loss: 0.6243\n",
      "Epoch 7/10, Batch 155/442, Training Loss: 0.4670\n",
      "Epoch 7/10, Batch 156/442, Training Loss: 0.6600\n",
      "Epoch 7/10, Batch 157/442, Training Loss: 0.4871\n",
      "Epoch 7/10, Batch 158/442, Training Loss: 0.9250\n",
      "Epoch 7/10, Batch 159/442, Training Loss: 0.7394\n",
      "Epoch 7/10, Batch 160/442, Training Loss: 0.7113\n",
      "Epoch 7/10, Batch 161/442, Training Loss: 0.5083\n",
      "Epoch 7/10, Batch 162/442, Training Loss: 0.5413\n",
      "Epoch 7/10, Batch 163/442, Training Loss: 0.6068\n",
      "Epoch 7/10, Batch 164/442, Training Loss: 0.5608\n",
      "Epoch 7/10, Batch 165/442, Training Loss: 0.5503\n",
      "Epoch 7/10, Batch 166/442, Training Loss: 0.6303\n",
      "Epoch 7/10, Batch 167/442, Training Loss: 0.4695\n",
      "Epoch 7/10, Batch 168/442, Training Loss: 0.6849\n",
      "Epoch 7/10, Batch 169/442, Training Loss: 0.3829\n",
      "Epoch 7/10, Batch 170/442, Training Loss: 0.5670\n",
      "Epoch 7/10, Batch 171/442, Training Loss: 0.5577\n",
      "Epoch 7/10, Batch 172/442, Training Loss: 0.5261\n",
      "Epoch 7/10, Batch 173/442, Training Loss: 0.4126\n",
      "Epoch 7/10, Batch 174/442, Training Loss: 0.4627\n",
      "Epoch 7/10, Batch 175/442, Training Loss: 0.3728\n",
      "Epoch 7/10, Batch 176/442, Training Loss: 0.5348\n",
      "Epoch 7/10, Batch 177/442, Training Loss: 0.3454\n",
      "Epoch 7/10, Batch 178/442, Training Loss: 0.4979\n",
      "Epoch 7/10, Batch 179/442, Training Loss: 0.4730\n",
      "Epoch 7/10, Batch 180/442, Training Loss: 0.4085\n",
      "Epoch 7/10, Batch 181/442, Training Loss: 0.5340\n",
      "Epoch 7/10, Batch 182/442, Training Loss: 0.5875\n",
      "Epoch 7/10, Batch 183/442, Training Loss: 0.4324\n",
      "Epoch 7/10, Batch 184/442, Training Loss: 0.4000\n",
      "Epoch 7/10, Batch 185/442, Training Loss: 0.5068\n",
      "Epoch 7/10, Batch 186/442, Training Loss: 0.7757\n",
      "Epoch 7/10, Batch 187/442, Training Loss: 0.4637\n",
      "Epoch 7/10, Batch 188/442, Training Loss: 0.4480\n",
      "Epoch 7/10, Batch 189/442, Training Loss: 0.5957\n",
      "Epoch 7/10, Batch 190/442, Training Loss: 0.4177\n",
      "Epoch 7/10, Batch 191/442, Training Loss: 0.3733\n",
      "Epoch 7/10, Batch 192/442, Training Loss: 0.3462\n",
      "Epoch 7/10, Batch 193/442, Training Loss: 0.3993\n",
      "Epoch 7/10, Batch 194/442, Training Loss: 0.4964\n",
      "Epoch 7/10, Batch 195/442, Training Loss: 0.3429\n",
      "Epoch 7/10, Batch 196/442, Training Loss: 0.4377\n",
      "Epoch 7/10, Batch 197/442, Training Loss: 0.4753\n",
      "Epoch 7/10, Batch 198/442, Training Loss: 0.5128\n",
      "Epoch 7/10, Batch 199/442, Training Loss: 0.5278\n",
      "Epoch 7/10, Batch 200/442, Training Loss: 0.6068\n",
      "Epoch 7/10, Batch 201/442, Training Loss: 0.3610\n",
      "Epoch 7/10, Batch 202/442, Training Loss: 0.5331\n",
      "Epoch 7/10, Batch 203/442, Training Loss: 0.6045\n",
      "Epoch 7/10, Batch 204/442, Training Loss: 0.4282\n",
      "Epoch 7/10, Batch 205/442, Training Loss: 0.4078\n",
      "Epoch 7/10, Batch 206/442, Training Loss: 0.2273\n",
      "Epoch 7/10, Batch 207/442, Training Loss: 0.4445\n",
      "Epoch 7/10, Batch 208/442, Training Loss: 0.5278\n",
      "Epoch 7/10, Batch 209/442, Training Loss: 0.4653\n",
      "Epoch 7/10, Batch 210/442, Training Loss: 0.3967\n",
      "Epoch 7/10, Batch 211/442, Training Loss: 0.5385\n",
      "Epoch 7/10, Batch 212/442, Training Loss: 0.6674\n",
      "Epoch 7/10, Batch 213/442, Training Loss: 0.7683\n",
      "Epoch 7/10, Batch 214/442, Training Loss: 0.2871\n",
      "Epoch 7/10, Batch 215/442, Training Loss: 0.6704\n",
      "Epoch 7/10, Batch 216/442, Training Loss: 0.8730\n",
      "Epoch 7/10, Batch 217/442, Training Loss: 0.4222\n",
      "Epoch 7/10, Batch 218/442, Training Loss: 0.6476\n",
      "Epoch 7/10, Batch 219/442, Training Loss: 0.2801\n",
      "Epoch 7/10, Batch 220/442, Training Loss: 0.3729\n",
      "Epoch 7/10, Batch 221/442, Training Loss: 0.4907\n",
      "Epoch 7/10, Batch 222/442, Training Loss: 0.5461\n",
      "Epoch 7/10, Batch 223/442, Training Loss: 0.4843\n",
      "Epoch 7/10, Batch 224/442, Training Loss: 0.4663\n",
      "Epoch 7/10, Batch 225/442, Training Loss: 0.3647\n",
      "Epoch 7/10, Batch 226/442, Training Loss: 0.5329\n",
      "Epoch 7/10, Batch 227/442, Training Loss: 0.6378\n",
      "Epoch 7/10, Batch 228/442, Training Loss: 0.5860\n",
      "Epoch 7/10, Batch 229/442, Training Loss: 0.4272\n",
      "Epoch 7/10, Batch 230/442, Training Loss: 0.4689\n",
      "Epoch 7/10, Batch 231/442, Training Loss: 0.4850\n",
      "Epoch 7/10, Batch 232/442, Training Loss: 0.5432\n",
      "Epoch 7/10, Batch 233/442, Training Loss: 0.5956\n",
      "Epoch 7/10, Batch 234/442, Training Loss: 0.6275\n",
      "Epoch 7/10, Batch 235/442, Training Loss: 0.3859\n",
      "Epoch 7/10, Batch 236/442, Training Loss: 0.7721\n",
      "Epoch 7/10, Batch 237/442, Training Loss: 0.7296\n",
      "Epoch 7/10, Batch 238/442, Training Loss: 0.7393\n",
      "Epoch 7/10, Batch 239/442, Training Loss: 0.3893\n",
      "Epoch 7/10, Batch 240/442, Training Loss: 0.5081\n",
      "Epoch 7/10, Batch 241/442, Training Loss: 0.5507\n",
      "Epoch 7/10, Batch 242/442, Training Loss: 0.3709\n",
      "Epoch 7/10, Batch 243/442, Training Loss: 0.4793\n",
      "Epoch 7/10, Batch 244/442, Training Loss: 0.5553\n",
      "Epoch 7/10, Batch 245/442, Training Loss: 0.5266\n",
      "Epoch 7/10, Batch 246/442, Training Loss: 0.6808\n",
      "Epoch 7/10, Batch 247/442, Training Loss: 0.5292\n",
      "Epoch 7/10, Batch 248/442, Training Loss: 0.4310\n",
      "Epoch 7/10, Batch 249/442, Training Loss: 0.4141\n",
      "Epoch 7/10, Batch 250/442, Training Loss: 0.6995\n",
      "Epoch 7/10, Batch 251/442, Training Loss: 0.5399\n",
      "Epoch 7/10, Batch 252/442, Training Loss: 0.5341\n",
      "Epoch 7/10, Batch 253/442, Training Loss: 0.3793\n",
      "Epoch 7/10, Batch 254/442, Training Loss: 0.6436\n",
      "Epoch 7/10, Batch 255/442, Training Loss: 0.4093\n",
      "Epoch 7/10, Batch 256/442, Training Loss: 0.6279\n",
      "Epoch 7/10, Batch 257/442, Training Loss: 0.5504\n",
      "Epoch 7/10, Batch 258/442, Training Loss: 0.6747\n",
      "Epoch 7/10, Batch 259/442, Training Loss: 0.5000\n",
      "Epoch 7/10, Batch 260/442, Training Loss: 0.5217\n",
      "Epoch 7/10, Batch 261/442, Training Loss: 0.3688\n",
      "Epoch 7/10, Batch 262/442, Training Loss: 0.5663\n",
      "Epoch 7/10, Batch 263/442, Training Loss: 0.6120\n",
      "Epoch 7/10, Batch 264/442, Training Loss: 0.9058\n",
      "Epoch 7/10, Batch 265/442, Training Loss: 0.4737\n",
      "Epoch 7/10, Batch 266/442, Training Loss: 0.4216\n",
      "Epoch 7/10, Batch 267/442, Training Loss: 0.5615\n",
      "Epoch 7/10, Batch 268/442, Training Loss: 0.5336\n",
      "Epoch 7/10, Batch 269/442, Training Loss: 0.5568\n",
      "Epoch 7/10, Batch 270/442, Training Loss: 0.4147\n",
      "Epoch 7/10, Batch 271/442, Training Loss: 0.4978\n",
      "Epoch 7/10, Batch 272/442, Training Loss: 0.7207\n",
      "Epoch 7/10, Batch 273/442, Training Loss: 0.5600\n",
      "Epoch 7/10, Batch 274/442, Training Loss: 0.3488\n",
      "Epoch 7/10, Batch 275/442, Training Loss: 0.4361\n",
      "Epoch 7/10, Batch 276/442, Training Loss: 0.3999\n",
      "Epoch 7/10, Batch 277/442, Training Loss: 0.5564\n",
      "Epoch 7/10, Batch 278/442, Training Loss: 0.4026\n",
      "Epoch 7/10, Batch 279/442, Training Loss: 0.8681\n",
      "Epoch 7/10, Batch 280/442, Training Loss: 0.3533\n",
      "Epoch 7/10, Batch 281/442, Training Loss: 0.7860\n",
      "Epoch 7/10, Batch 282/442, Training Loss: 0.4831\n",
      "Epoch 7/10, Batch 283/442, Training Loss: 0.6933\n",
      "Epoch 7/10, Batch 284/442, Training Loss: 0.5272\n",
      "Epoch 7/10, Batch 285/442, Training Loss: 0.6779\n",
      "Epoch 7/10, Batch 286/442, Training Loss: 0.4219\n",
      "Epoch 7/10, Batch 287/442, Training Loss: 0.3963\n",
      "Epoch 7/10, Batch 288/442, Training Loss: 0.9369\n",
      "Epoch 7/10, Batch 289/442, Training Loss: 0.6493\n",
      "Epoch 7/10, Batch 290/442, Training Loss: 0.3819\n",
      "Epoch 7/10, Batch 291/442, Training Loss: 0.7928\n",
      "Epoch 7/10, Batch 292/442, Training Loss: 0.5814\n",
      "Epoch 7/10, Batch 293/442, Training Loss: 0.5737\n",
      "Epoch 7/10, Batch 294/442, Training Loss: 0.5057\n",
      "Epoch 7/10, Batch 295/442, Training Loss: 0.4959\n",
      "Epoch 7/10, Batch 296/442, Training Loss: 0.4776\n",
      "Epoch 7/10, Batch 297/442, Training Loss: 0.4574\n",
      "Epoch 7/10, Batch 298/442, Training Loss: 0.4944\n",
      "Epoch 7/10, Batch 299/442, Training Loss: 0.6581\n",
      "Epoch 7/10, Batch 300/442, Training Loss: 0.5111\n",
      "Epoch 7/10, Batch 301/442, Training Loss: 0.4296\n",
      "Epoch 7/10, Batch 302/442, Training Loss: 0.7052\n",
      "Epoch 7/10, Batch 303/442, Training Loss: 0.5385\n",
      "Epoch 7/10, Batch 304/442, Training Loss: 0.6736\n",
      "Epoch 7/10, Batch 305/442, Training Loss: 0.3651\n",
      "Epoch 7/10, Batch 306/442, Training Loss: 0.5347\n",
      "Epoch 7/10, Batch 307/442, Training Loss: 0.3794\n",
      "Epoch 7/10, Batch 308/442, Training Loss: 0.5385\n",
      "Epoch 7/10, Batch 309/442, Training Loss: 0.5972\n",
      "Epoch 7/10, Batch 310/442, Training Loss: 0.4606\n",
      "Epoch 7/10, Batch 311/442, Training Loss: 0.3711\n",
      "Epoch 7/10, Batch 312/442, Training Loss: 0.5274\n",
      "Epoch 7/10, Batch 313/442, Training Loss: 0.6860\n",
      "Epoch 7/10, Batch 314/442, Training Loss: 0.7331\n",
      "Epoch 7/10, Batch 315/442, Training Loss: 0.5558\n",
      "Epoch 7/10, Batch 316/442, Training Loss: 0.4604\n",
      "Epoch 7/10, Batch 317/442, Training Loss: 0.6473\n",
      "Epoch 7/10, Batch 318/442, Training Loss: 0.6489\n",
      "Epoch 7/10, Batch 319/442, Training Loss: 0.3812\n",
      "Epoch 7/10, Batch 320/442, Training Loss: 0.4049\n",
      "Epoch 7/10, Batch 321/442, Training Loss: 0.3936\n",
      "Epoch 7/10, Batch 322/442, Training Loss: 0.5099\n",
      "Epoch 7/10, Batch 323/442, Training Loss: 0.7003\n",
      "Epoch 7/10, Batch 324/442, Training Loss: 0.4230\n",
      "Epoch 7/10, Batch 325/442, Training Loss: 0.5328\n",
      "Epoch 7/10, Batch 326/442, Training Loss: 0.5745\n",
      "Epoch 7/10, Batch 327/442, Training Loss: 0.3784\n",
      "Epoch 7/10, Batch 328/442, Training Loss: 0.4384\n",
      "Epoch 7/10, Batch 329/442, Training Loss: 0.6875\n",
      "Epoch 7/10, Batch 330/442, Training Loss: 0.4777\n",
      "Epoch 7/10, Batch 331/442, Training Loss: 0.6131\n",
      "Epoch 7/10, Batch 332/442, Training Loss: 0.5904\n",
      "Epoch 7/10, Batch 333/442, Training Loss: 0.6649\n",
      "Epoch 7/10, Batch 334/442, Training Loss: 0.6739\n",
      "Epoch 7/10, Batch 335/442, Training Loss: 0.5238\n",
      "Epoch 7/10, Batch 336/442, Training Loss: 0.4367\n",
      "Epoch 7/10, Batch 337/442, Training Loss: 0.3763\n",
      "Epoch 7/10, Batch 338/442, Training Loss: 0.5237\n",
      "Epoch 7/10, Batch 339/442, Training Loss: 0.6665\n",
      "Epoch 7/10, Batch 340/442, Training Loss: 0.5182\n",
      "Epoch 7/10, Batch 341/442, Training Loss: 0.5700\n",
      "Epoch 7/10, Batch 342/442, Training Loss: 0.3639\n",
      "Epoch 7/10, Batch 343/442, Training Loss: 0.5703\n",
      "Epoch 7/10, Batch 344/442, Training Loss: 0.6289\n",
      "Epoch 7/10, Batch 345/442, Training Loss: 0.5103\n",
      "Epoch 7/10, Batch 346/442, Training Loss: 0.5906\n",
      "Epoch 7/10, Batch 347/442, Training Loss: 0.5031\n",
      "Epoch 7/10, Batch 348/442, Training Loss: 0.6021\n",
      "Epoch 7/10, Batch 349/442, Training Loss: 0.5762\n",
      "Epoch 7/10, Batch 350/442, Training Loss: 0.5805\n",
      "Epoch 7/10, Batch 351/442, Training Loss: 0.5376\n",
      "Epoch 7/10, Batch 352/442, Training Loss: 0.5215\n",
      "Epoch 7/10, Batch 353/442, Training Loss: 0.4878\n",
      "Epoch 7/10, Batch 354/442, Training Loss: 0.7578\n",
      "Epoch 7/10, Batch 355/442, Training Loss: 0.6820\n",
      "Epoch 7/10, Batch 356/442, Training Loss: 0.3697\n",
      "Epoch 7/10, Batch 357/442, Training Loss: 0.6099\n",
      "Epoch 7/10, Batch 358/442, Training Loss: 0.3846\n",
      "Epoch 7/10, Batch 359/442, Training Loss: 0.4468\n",
      "Epoch 7/10, Batch 360/442, Training Loss: 0.4851\n",
      "Epoch 7/10, Batch 361/442, Training Loss: 0.5600\n",
      "Epoch 7/10, Batch 362/442, Training Loss: 0.3538\n",
      "Epoch 7/10, Batch 363/442, Training Loss: 0.3877\n",
      "Epoch 7/10, Batch 364/442, Training Loss: 0.4447\n",
      "Epoch 7/10, Batch 365/442, Training Loss: 0.5093\n",
      "Epoch 7/10, Batch 366/442, Training Loss: 0.4173\n",
      "Epoch 7/10, Batch 367/442, Training Loss: 0.6029\n",
      "Epoch 7/10, Batch 368/442, Training Loss: 0.5013\n",
      "Epoch 7/10, Batch 369/442, Training Loss: 0.5700\n",
      "Epoch 7/10, Batch 370/442, Training Loss: 0.4910\n",
      "Epoch 7/10, Batch 371/442, Training Loss: 0.8416\n",
      "Epoch 7/10, Batch 372/442, Training Loss: 0.4254\n",
      "Epoch 7/10, Batch 373/442, Training Loss: 0.4592\n",
      "Epoch 7/10, Batch 374/442, Training Loss: 0.6673\n",
      "Epoch 7/10, Batch 375/442, Training Loss: 0.5539\n",
      "Epoch 7/10, Batch 376/442, Training Loss: 0.4347\n",
      "Epoch 7/10, Batch 377/442, Training Loss: 0.3717\n",
      "Epoch 7/10, Batch 378/442, Training Loss: 0.5542\n",
      "Epoch 7/10, Batch 379/442, Training Loss: 0.4754\n",
      "Epoch 7/10, Batch 380/442, Training Loss: 0.3789\n",
      "Epoch 7/10, Batch 381/442, Training Loss: 0.4570\n",
      "Epoch 7/10, Batch 382/442, Training Loss: 0.6088\n",
      "Epoch 7/10, Batch 383/442, Training Loss: 0.3842\n",
      "Epoch 7/10, Batch 384/442, Training Loss: 0.3753\n",
      "Epoch 7/10, Batch 385/442, Training Loss: 0.6831\n",
      "Epoch 7/10, Batch 386/442, Training Loss: 0.2935\n",
      "Epoch 7/10, Batch 387/442, Training Loss: 0.4177\n",
      "Epoch 7/10, Batch 388/442, Training Loss: 0.3512\n",
      "Epoch 7/10, Batch 389/442, Training Loss: 0.5884\n",
      "Epoch 7/10, Batch 390/442, Training Loss: 0.6636\n",
      "Epoch 7/10, Batch 391/442, Training Loss: 0.4691\n",
      "Epoch 7/10, Batch 392/442, Training Loss: 0.3125\n",
      "Epoch 7/10, Batch 393/442, Training Loss: 0.6352\n",
      "Epoch 7/10, Batch 394/442, Training Loss: 0.4907\n",
      "Epoch 7/10, Batch 395/442, Training Loss: 0.4932\n",
      "Epoch 7/10, Batch 396/442, Training Loss: 0.3626\n",
      "Epoch 7/10, Batch 397/442, Training Loss: 0.4878\n",
      "Epoch 7/10, Batch 398/442, Training Loss: 0.5103\n",
      "Epoch 7/10, Batch 399/442, Training Loss: 0.4215\n",
      "Epoch 7/10, Batch 400/442, Training Loss: 0.5490\n",
      "Epoch 7/10, Batch 401/442, Training Loss: 0.4360\n",
      "Epoch 7/10, Batch 402/442, Training Loss: 0.5541\n",
      "Epoch 7/10, Batch 403/442, Training Loss: 0.4545\n",
      "Epoch 7/10, Batch 404/442, Training Loss: 0.3610\n",
      "Epoch 7/10, Batch 405/442, Training Loss: 0.3814\n",
      "Epoch 7/10, Batch 406/442, Training Loss: 0.3411\n",
      "Epoch 7/10, Batch 407/442, Training Loss: 0.4588\n",
      "Epoch 7/10, Batch 408/442, Training Loss: 0.3747\n",
      "Epoch 7/10, Batch 409/442, Training Loss: 0.4552\n",
      "Epoch 7/10, Batch 410/442, Training Loss: 0.5039\n",
      "Epoch 7/10, Batch 411/442, Training Loss: 0.5462\n",
      "Epoch 7/10, Batch 412/442, Training Loss: 0.5019\n",
      "Epoch 7/10, Batch 413/442, Training Loss: 0.8029\n",
      "Epoch 7/10, Batch 414/442, Training Loss: 0.6016\n",
      "Epoch 7/10, Batch 415/442, Training Loss: 0.4095\n",
      "Epoch 7/10, Batch 416/442, Training Loss: 0.4344\n",
      "Epoch 7/10, Batch 417/442, Training Loss: 0.5429\n",
      "Epoch 7/10, Batch 418/442, Training Loss: 0.5374\n",
      "Epoch 7/10, Batch 419/442, Training Loss: 0.4117\n",
      "Epoch 7/10, Batch 420/442, Training Loss: 0.4154\n",
      "Epoch 7/10, Batch 421/442, Training Loss: 0.2529\n",
      "Epoch 7/10, Batch 422/442, Training Loss: 0.2776\n",
      "Epoch 7/10, Batch 423/442, Training Loss: 0.5137\n",
      "Epoch 7/10, Batch 424/442, Training Loss: 0.7942\n",
      "Epoch 7/10, Batch 425/442, Training Loss: 0.5747\n",
      "Epoch 7/10, Batch 426/442, Training Loss: 0.6739\n",
      "Epoch 7/10, Batch 427/442, Training Loss: 0.4873\n",
      "Epoch 7/10, Batch 428/442, Training Loss: 0.5463\n",
      "Epoch 7/10, Batch 429/442, Training Loss: 0.5355\n",
      "Epoch 7/10, Batch 430/442, Training Loss: 0.7453\n",
      "Epoch 7/10, Batch 431/442, Training Loss: 0.4525\n",
      "Epoch 7/10, Batch 432/442, Training Loss: 0.3224\n",
      "Epoch 7/10, Batch 433/442, Training Loss: 0.3990\n",
      "Epoch 7/10, Batch 434/442, Training Loss: 0.4620\n",
      "Epoch 7/10, Batch 435/442, Training Loss: 0.4145\n",
      "Epoch 7/10, Batch 436/442, Training Loss: 0.6230\n",
      "Epoch 7/10, Batch 437/442, Training Loss: 0.5414\n",
      "Epoch 7/10, Batch 438/442, Training Loss: 0.4646\n",
      "Epoch 7/10, Batch 439/442, Training Loss: 0.3615\n",
      "Epoch 7/10, Batch 440/442, Training Loss: 0.7776\n",
      "Epoch 7/10, Batch 441/442, Training Loss: 0.4691\n",
      "Epoch 7/10, Batch 442/442, Training Loss: 0.2351\n",
      "Epoch 7/10, Training Loss: 0.5228, Validation Loss: 0.6343, Validation Accuracy: 0.7470\n",
      "Epoch 8/10, Batch 1/442, Training Loss: 0.4173\n",
      "Epoch 8/10, Batch 2/442, Training Loss: 0.4739\n",
      "Epoch 8/10, Batch 3/442, Training Loss: 0.4697\n",
      "Epoch 8/10, Batch 4/442, Training Loss: 0.4977\n",
      "Epoch 8/10, Batch 5/442, Training Loss: 0.6582\n",
      "Epoch 8/10, Batch 6/442, Training Loss: 0.4649\n",
      "Epoch 8/10, Batch 7/442, Training Loss: 0.5799\n",
      "Epoch 8/10, Batch 8/442, Training Loss: 0.3912\n",
      "Epoch 8/10, Batch 9/442, Training Loss: 0.4677\n",
      "Epoch 8/10, Batch 10/442, Training Loss: 0.5648\n",
      "Epoch 8/10, Batch 11/442, Training Loss: 0.2337\n",
      "Epoch 8/10, Batch 12/442, Training Loss: 0.5566\n",
      "Epoch 8/10, Batch 13/442, Training Loss: 0.6491\n",
      "Epoch 8/10, Batch 14/442, Training Loss: 0.2795\n",
      "Epoch 8/10, Batch 15/442, Training Loss: 0.4301\n",
      "Epoch 8/10, Batch 16/442, Training Loss: 0.5767\n",
      "Epoch 8/10, Batch 17/442, Training Loss: 0.4930\n",
      "Epoch 8/10, Batch 18/442, Training Loss: 0.4402\n",
      "Epoch 8/10, Batch 19/442, Training Loss: 0.5010\n",
      "Epoch 8/10, Batch 20/442, Training Loss: 0.4913\n",
      "Epoch 8/10, Batch 21/442, Training Loss: 0.5018\n",
      "Epoch 8/10, Batch 22/442, Training Loss: 0.7729\n",
      "Epoch 8/10, Batch 23/442, Training Loss: 0.7384\n",
      "Epoch 8/10, Batch 24/442, Training Loss: 0.2767\n",
      "Epoch 8/10, Batch 25/442, Training Loss: 0.4083\n",
      "Epoch 8/10, Batch 26/442, Training Loss: 0.4851\n",
      "Epoch 8/10, Batch 27/442, Training Loss: 0.6342\n",
      "Epoch 8/10, Batch 28/442, Training Loss: 0.6235\n",
      "Epoch 8/10, Batch 29/442, Training Loss: 0.2329\n",
      "Epoch 8/10, Batch 30/442, Training Loss: 0.5237\n",
      "Epoch 8/10, Batch 31/442, Training Loss: 0.4921\n",
      "Epoch 8/10, Batch 32/442, Training Loss: 0.4707\n",
      "Epoch 8/10, Batch 33/442, Training Loss: 0.4205\n",
      "Epoch 8/10, Batch 34/442, Training Loss: 0.4510\n",
      "Epoch 8/10, Batch 35/442, Training Loss: 0.4385\n",
      "Epoch 8/10, Batch 36/442, Training Loss: 0.4465\n",
      "Epoch 8/10, Batch 37/442, Training Loss: 0.5017\n",
      "Epoch 8/10, Batch 38/442, Training Loss: 0.5239\n",
      "Epoch 8/10, Batch 39/442, Training Loss: 0.5027\n",
      "Epoch 8/10, Batch 40/442, Training Loss: 0.5001\n",
      "Epoch 8/10, Batch 41/442, Training Loss: 0.6029\n",
      "Epoch 8/10, Batch 42/442, Training Loss: 0.3549\n",
      "Epoch 8/10, Batch 43/442, Training Loss: 0.5223\n",
      "Epoch 8/10, Batch 44/442, Training Loss: 0.4627\n",
      "Epoch 8/10, Batch 45/442, Training Loss: 0.4619\n",
      "Epoch 8/10, Batch 46/442, Training Loss: 0.4873\n",
      "Epoch 8/10, Batch 47/442, Training Loss: 0.4756\n",
      "Epoch 8/10, Batch 48/442, Training Loss: 0.4787\n",
      "Epoch 8/10, Batch 49/442, Training Loss: 0.8156\n",
      "Epoch 8/10, Batch 50/442, Training Loss: 0.3189\n",
      "Epoch 8/10, Batch 51/442, Training Loss: 0.6134\n",
      "Epoch 8/10, Batch 52/442, Training Loss: 0.4355\n",
      "Epoch 8/10, Batch 53/442, Training Loss: 0.5374\n",
      "Epoch 8/10, Batch 54/442, Training Loss: 0.4063\n",
      "Epoch 8/10, Batch 55/442, Training Loss: 0.4733\n",
      "Epoch 8/10, Batch 56/442, Training Loss: 0.6786\n",
      "Epoch 8/10, Batch 57/442, Training Loss: 0.3779\n",
      "Epoch 8/10, Batch 58/442, Training Loss: 0.4113\n",
      "Epoch 8/10, Batch 59/442, Training Loss: 0.3923\n",
      "Epoch 8/10, Batch 60/442, Training Loss: 0.3502\n",
      "Epoch 8/10, Batch 61/442, Training Loss: 0.4274\n",
      "Epoch 8/10, Batch 62/442, Training Loss: 0.4853\n",
      "Epoch 8/10, Batch 63/442, Training Loss: 0.5495\n",
      "Epoch 8/10, Batch 64/442, Training Loss: 0.4220\n",
      "Epoch 8/10, Batch 65/442, Training Loss: 0.5906\n",
      "Epoch 8/10, Batch 66/442, Training Loss: 0.7098\n",
      "Epoch 8/10, Batch 67/442, Training Loss: 0.4683\n",
      "Epoch 8/10, Batch 68/442, Training Loss: 0.4699\n",
      "Epoch 8/10, Batch 69/442, Training Loss: 0.3883\n",
      "Epoch 8/10, Batch 70/442, Training Loss: 0.5066\n",
      "Epoch 8/10, Batch 71/442, Training Loss: 0.6362\n",
      "Epoch 8/10, Batch 72/442, Training Loss: 0.8141\n",
      "Epoch 8/10, Batch 73/442, Training Loss: 0.5116\n",
      "Epoch 8/10, Batch 74/442, Training Loss: 0.4465\n",
      "Epoch 8/10, Batch 75/442, Training Loss: 0.7372\n",
      "Epoch 8/10, Batch 76/442, Training Loss: 0.4854\n",
      "Epoch 8/10, Batch 77/442, Training Loss: 0.4182\n",
      "Epoch 8/10, Batch 78/442, Training Loss: 0.4311\n",
      "Epoch 8/10, Batch 79/442, Training Loss: 0.3001\n",
      "Epoch 8/10, Batch 80/442, Training Loss: 0.5424\n",
      "Epoch 8/10, Batch 81/442, Training Loss: 0.3553\n",
      "Epoch 8/10, Batch 82/442, Training Loss: 0.4019\n",
      "Epoch 8/10, Batch 83/442, Training Loss: 0.4600\n",
      "Epoch 8/10, Batch 84/442, Training Loss: 0.4107\n",
      "Epoch 8/10, Batch 85/442, Training Loss: 0.6620\n",
      "Epoch 8/10, Batch 86/442, Training Loss: 0.4281\n",
      "Epoch 8/10, Batch 87/442, Training Loss: 0.2419\n",
      "Epoch 8/10, Batch 88/442, Training Loss: 0.3754\n",
      "Epoch 8/10, Batch 89/442, Training Loss: 0.7664\n",
      "Epoch 8/10, Batch 90/442, Training Loss: 0.7085\n",
      "Epoch 8/10, Batch 91/442, Training Loss: 0.5332\n",
      "Epoch 8/10, Batch 92/442, Training Loss: 0.4437\n",
      "Epoch 8/10, Batch 93/442, Training Loss: 0.2752\n",
      "Epoch 8/10, Batch 94/442, Training Loss: 0.6143\n",
      "Epoch 8/10, Batch 95/442, Training Loss: 0.4954\n",
      "Epoch 8/10, Batch 96/442, Training Loss: 0.4102\n",
      "Epoch 8/10, Batch 97/442, Training Loss: 0.4094\n",
      "Epoch 8/10, Batch 98/442, Training Loss: 0.5423\n",
      "Epoch 8/10, Batch 99/442, Training Loss: 0.3470\n",
      "Epoch 8/10, Batch 100/442, Training Loss: 0.3156\n",
      "Epoch 8/10, Batch 101/442, Training Loss: 0.5240\n",
      "Epoch 8/10, Batch 102/442, Training Loss: 0.3923\n",
      "Epoch 8/10, Batch 103/442, Training Loss: 0.4719\n",
      "Epoch 8/10, Batch 104/442, Training Loss: 0.2936\n",
      "Epoch 8/10, Batch 105/442, Training Loss: 0.3042\n",
      "Epoch 8/10, Batch 106/442, Training Loss: 0.2939\n",
      "Epoch 8/10, Batch 107/442, Training Loss: 0.3966\n",
      "Epoch 8/10, Batch 108/442, Training Loss: 0.6801\n",
      "Epoch 8/10, Batch 109/442, Training Loss: 0.3325\n",
      "Epoch 8/10, Batch 110/442, Training Loss: 0.3828\n",
      "Epoch 8/10, Batch 111/442, Training Loss: 0.5223\n",
      "Epoch 8/10, Batch 112/442, Training Loss: 0.4416\n",
      "Epoch 8/10, Batch 113/442, Training Loss: 0.2946\n",
      "Epoch 8/10, Batch 114/442, Training Loss: 0.5050\n",
      "Epoch 8/10, Batch 115/442, Training Loss: 0.4573\n",
      "Epoch 8/10, Batch 116/442, Training Loss: 0.7682\n",
      "Epoch 8/10, Batch 117/442, Training Loss: 0.5628\n",
      "Epoch 8/10, Batch 118/442, Training Loss: 0.2707\n",
      "Epoch 8/10, Batch 119/442, Training Loss: 0.4577\n",
      "Epoch 8/10, Batch 120/442, Training Loss: 0.4724\n",
      "Epoch 8/10, Batch 121/442, Training Loss: 0.4484\n",
      "Epoch 8/10, Batch 122/442, Training Loss: 0.6996\n",
      "Epoch 8/10, Batch 123/442, Training Loss: 0.4222\n",
      "Epoch 8/10, Batch 124/442, Training Loss: 0.4884\n",
      "Epoch 8/10, Batch 125/442, Training Loss: 0.4672\n",
      "Epoch 8/10, Batch 126/442, Training Loss: 0.4124\n",
      "Epoch 8/10, Batch 127/442, Training Loss: 0.4021\n",
      "Epoch 8/10, Batch 128/442, Training Loss: 0.4264\n",
      "Epoch 8/10, Batch 129/442, Training Loss: 0.4177\n",
      "Epoch 8/10, Batch 130/442, Training Loss: 0.4767\n",
      "Epoch 8/10, Batch 131/442, Training Loss: 0.3891\n",
      "Epoch 8/10, Batch 132/442, Training Loss: 0.4312\n",
      "Epoch 8/10, Batch 133/442, Training Loss: 0.3920\n",
      "Epoch 8/10, Batch 134/442, Training Loss: 0.3557\n",
      "Epoch 8/10, Batch 135/442, Training Loss: 0.4592\n",
      "Epoch 8/10, Batch 136/442, Training Loss: 0.4525\n",
      "Epoch 8/10, Batch 137/442, Training Loss: 0.4216\n",
      "Epoch 8/10, Batch 138/442, Training Loss: 0.3612\n",
      "Epoch 8/10, Batch 139/442, Training Loss: 0.8493\n",
      "Epoch 8/10, Batch 140/442, Training Loss: 0.6109\n",
      "Epoch 8/10, Batch 141/442, Training Loss: 0.2776\n",
      "Epoch 8/10, Batch 142/442, Training Loss: 0.4138\n",
      "Epoch 8/10, Batch 143/442, Training Loss: 0.5979\n",
      "Epoch 8/10, Batch 144/442, Training Loss: 0.4353\n",
      "Epoch 8/10, Batch 145/442, Training Loss: 0.4488\n",
      "Epoch 8/10, Batch 146/442, Training Loss: 0.5457\n",
      "Epoch 8/10, Batch 147/442, Training Loss: 0.3645\n",
      "Epoch 8/10, Batch 148/442, Training Loss: 0.5878\n",
      "Epoch 8/10, Batch 149/442, Training Loss: 0.4329\n",
      "Epoch 8/10, Batch 150/442, Training Loss: 0.4954\n",
      "Epoch 8/10, Batch 151/442, Training Loss: 0.4362\n",
      "Epoch 8/10, Batch 152/442, Training Loss: 0.4128\n",
      "Epoch 8/10, Batch 153/442, Training Loss: 0.3603\n",
      "Epoch 8/10, Batch 154/442, Training Loss: 0.3382\n",
      "Epoch 8/10, Batch 155/442, Training Loss: 0.3925\n",
      "Epoch 8/10, Batch 156/442, Training Loss: 0.3644\n",
      "Epoch 8/10, Batch 157/442, Training Loss: 0.5131\n",
      "Epoch 8/10, Batch 158/442, Training Loss: 0.2702\n",
      "Epoch 8/10, Batch 159/442, Training Loss: 0.3827\n",
      "Epoch 8/10, Batch 160/442, Training Loss: 0.6435\n",
      "Epoch 8/10, Batch 161/442, Training Loss: 0.7006\n",
      "Epoch 8/10, Batch 162/442, Training Loss: 0.5733\n",
      "Epoch 8/10, Batch 163/442, Training Loss: 0.3689\n",
      "Epoch 8/10, Batch 164/442, Training Loss: 0.6012\n",
      "Epoch 8/10, Batch 165/442, Training Loss: 0.2345\n",
      "Epoch 8/10, Batch 166/442, Training Loss: 0.8785\n",
      "Epoch 8/10, Batch 167/442, Training Loss: 0.3817\n",
      "Epoch 8/10, Batch 168/442, Training Loss: 0.5059\n",
      "Epoch 8/10, Batch 169/442, Training Loss: 0.4644\n",
      "Epoch 8/10, Batch 170/442, Training Loss: 0.5488\n",
      "Epoch 8/10, Batch 171/442, Training Loss: 0.5408\n",
      "Epoch 8/10, Batch 172/442, Training Loss: 0.5916\n",
      "Epoch 8/10, Batch 173/442, Training Loss: 0.6551\n",
      "Epoch 8/10, Batch 174/442, Training Loss: 0.4060\n",
      "Epoch 8/10, Batch 175/442, Training Loss: 0.3608\n",
      "Epoch 8/10, Batch 176/442, Training Loss: 0.7236\n",
      "Epoch 8/10, Batch 177/442, Training Loss: 0.5886\n",
      "Epoch 8/10, Batch 178/442, Training Loss: 0.7715\n",
      "Epoch 8/10, Batch 179/442, Training Loss: 0.4636\n",
      "Epoch 8/10, Batch 180/442, Training Loss: 0.2682\n",
      "Epoch 8/10, Batch 181/442, Training Loss: 0.5078\n",
      "Epoch 8/10, Batch 182/442, Training Loss: 0.4464\n",
      "Epoch 8/10, Batch 183/442, Training Loss: 0.4418\n",
      "Epoch 8/10, Batch 184/442, Training Loss: 0.5307\n",
      "Epoch 8/10, Batch 185/442, Training Loss: 0.3654\n",
      "Epoch 8/10, Batch 186/442, Training Loss: 0.4245\n",
      "Epoch 8/10, Batch 187/442, Training Loss: 0.3162\n",
      "Epoch 8/10, Batch 188/442, Training Loss: 0.4761\n",
      "Epoch 8/10, Batch 189/442, Training Loss: 0.3878\n",
      "Epoch 8/10, Batch 190/442, Training Loss: 0.3884\n",
      "Epoch 8/10, Batch 191/442, Training Loss: 0.4113\n",
      "Epoch 8/10, Batch 192/442, Training Loss: 0.5168\n",
      "Epoch 8/10, Batch 193/442, Training Loss: 0.3924\n",
      "Epoch 8/10, Batch 194/442, Training Loss: 0.5232\n",
      "Epoch 8/10, Batch 195/442, Training Loss: 0.3687\n",
      "Epoch 8/10, Batch 196/442, Training Loss: 0.6259\n",
      "Epoch 8/10, Batch 197/442, Training Loss: 0.4220\n",
      "Epoch 8/10, Batch 198/442, Training Loss: 0.4776\n",
      "Epoch 8/10, Batch 199/442, Training Loss: 0.5784\n",
      "Epoch 8/10, Batch 200/442, Training Loss: 0.7064\n",
      "Epoch 8/10, Batch 201/442, Training Loss: 0.3623\n",
      "Epoch 8/10, Batch 202/442, Training Loss: 0.4192\n",
      "Epoch 8/10, Batch 203/442, Training Loss: 0.4867\n",
      "Epoch 8/10, Batch 204/442, Training Loss: 1.0327\n",
      "Epoch 8/10, Batch 205/442, Training Loss: 0.4606\n",
      "Epoch 8/10, Batch 206/442, Training Loss: 0.3355\n",
      "Epoch 8/10, Batch 207/442, Training Loss: 0.3242\n",
      "Epoch 8/10, Batch 208/442, Training Loss: 0.5454\n",
      "Epoch 8/10, Batch 209/442, Training Loss: 0.2160\n",
      "Epoch 8/10, Batch 210/442, Training Loss: 0.5625\n",
      "Epoch 8/10, Batch 211/442, Training Loss: 0.3548\n",
      "Epoch 8/10, Batch 212/442, Training Loss: 0.4570\n",
      "Epoch 8/10, Batch 213/442, Training Loss: 0.4666\n",
      "Epoch 8/10, Batch 214/442, Training Loss: 0.4553\n",
      "Epoch 8/10, Batch 215/442, Training Loss: 0.6129\n",
      "Epoch 8/10, Batch 216/442, Training Loss: 0.3607\n",
      "Epoch 8/10, Batch 217/442, Training Loss: 0.4280\n",
      "Epoch 8/10, Batch 218/442, Training Loss: 0.6121\n",
      "Epoch 8/10, Batch 219/442, Training Loss: 0.3796\n",
      "Epoch 8/10, Batch 220/442, Training Loss: 0.6115\n",
      "Epoch 8/10, Batch 221/442, Training Loss: 0.9147\n",
      "Epoch 8/10, Batch 222/442, Training Loss: 0.5246\n",
      "Epoch 8/10, Batch 223/442, Training Loss: 0.6222\n",
      "Epoch 8/10, Batch 224/442, Training Loss: 0.3542\n",
      "Epoch 8/10, Batch 225/442, Training Loss: 0.5212\n",
      "Epoch 8/10, Batch 226/442, Training Loss: 0.5490\n",
      "Epoch 8/10, Batch 227/442, Training Loss: 0.4238\n",
      "Epoch 8/10, Batch 228/442, Training Loss: 0.4643\n",
      "Epoch 8/10, Batch 229/442, Training Loss: 0.6247\n",
      "Epoch 8/10, Batch 230/442, Training Loss: 0.6445\n",
      "Epoch 8/10, Batch 231/442, Training Loss: 0.4972\n",
      "Epoch 8/10, Batch 232/442, Training Loss: 0.6320\n",
      "Epoch 8/10, Batch 233/442, Training Loss: 0.6443\n",
      "Epoch 8/10, Batch 234/442, Training Loss: 0.4377\n",
      "Epoch 8/10, Batch 235/442, Training Loss: 0.4363\n",
      "Epoch 8/10, Batch 236/442, Training Loss: 0.5299\n",
      "Epoch 8/10, Batch 237/442, Training Loss: 0.6166\n",
      "Epoch 8/10, Batch 238/442, Training Loss: 0.4727\n",
      "Epoch 8/10, Batch 239/442, Training Loss: 0.4054\n",
      "Epoch 8/10, Batch 240/442, Training Loss: 0.5623\n",
      "Epoch 8/10, Batch 241/442, Training Loss: 0.7100\n",
      "Epoch 8/10, Batch 242/442, Training Loss: 0.3675\n",
      "Epoch 8/10, Batch 243/442, Training Loss: 0.4377\n",
      "Epoch 8/10, Batch 244/442, Training Loss: 0.3624\n",
      "Epoch 8/10, Batch 245/442, Training Loss: 0.4870\n",
      "Epoch 8/10, Batch 246/442, Training Loss: 0.6999\n",
      "Epoch 8/10, Batch 247/442, Training Loss: 0.3467\n",
      "Epoch 8/10, Batch 248/442, Training Loss: 0.4012\n",
      "Epoch 8/10, Batch 249/442, Training Loss: 0.6562\n",
      "Epoch 8/10, Batch 250/442, Training Loss: 0.4489\n",
      "Epoch 8/10, Batch 251/442, Training Loss: 0.6261\n",
      "Epoch 8/10, Batch 252/442, Training Loss: 0.4536\n",
      "Epoch 8/10, Batch 253/442, Training Loss: 0.3368\n",
      "Epoch 8/10, Batch 254/442, Training Loss: 0.8896\n",
      "Epoch 8/10, Batch 255/442, Training Loss: 0.6456\n",
      "Epoch 8/10, Batch 256/442, Training Loss: 0.5103\n",
      "Epoch 8/10, Batch 257/442, Training Loss: 0.3339\n",
      "Epoch 8/10, Batch 258/442, Training Loss: 0.6001\n",
      "Epoch 8/10, Batch 259/442, Training Loss: 0.3731\n",
      "Epoch 8/10, Batch 260/442, Training Loss: 0.2843\n",
      "Epoch 8/10, Batch 261/442, Training Loss: 0.7454\n",
      "Epoch 8/10, Batch 262/442, Training Loss: 0.2897\n",
      "Epoch 8/10, Batch 263/442, Training Loss: 0.6435\n",
      "Epoch 8/10, Batch 264/442, Training Loss: 0.5836\n",
      "Epoch 8/10, Batch 265/442, Training Loss: 0.2661\n",
      "Epoch 8/10, Batch 266/442, Training Loss: 0.6360\n",
      "Epoch 8/10, Batch 267/442, Training Loss: 0.4601\n",
      "Epoch 8/10, Batch 268/442, Training Loss: 0.3845\n",
      "Epoch 8/10, Batch 269/442, Training Loss: 0.4681\n",
      "Epoch 8/10, Batch 270/442, Training Loss: 0.5344\n",
      "Epoch 8/10, Batch 271/442, Training Loss: 0.3100\n",
      "Epoch 8/10, Batch 272/442, Training Loss: 0.6345\n",
      "Epoch 8/10, Batch 273/442, Training Loss: 0.5403\n",
      "Epoch 8/10, Batch 274/442, Training Loss: 0.4703\n",
      "Epoch 8/10, Batch 275/442, Training Loss: 0.3875\n",
      "Epoch 8/10, Batch 276/442, Training Loss: 0.6639\n",
      "Epoch 8/10, Batch 277/442, Training Loss: 0.3729\n",
      "Epoch 8/10, Batch 278/442, Training Loss: 0.4816\n",
      "Epoch 8/10, Batch 279/442, Training Loss: 0.4409\n",
      "Epoch 8/10, Batch 280/442, Training Loss: 0.3039\n",
      "Epoch 8/10, Batch 281/442, Training Loss: 0.6677\n",
      "Epoch 8/10, Batch 282/442, Training Loss: 0.6761\n",
      "Epoch 8/10, Batch 283/442, Training Loss: 0.4878\n",
      "Epoch 8/10, Batch 284/442, Training Loss: 0.3910\n",
      "Epoch 8/10, Batch 285/442, Training Loss: 0.7429\n",
      "Epoch 8/10, Batch 286/442, Training Loss: 0.7128\n",
      "Epoch 8/10, Batch 287/442, Training Loss: 0.7734\n",
      "Epoch 8/10, Batch 288/442, Training Loss: 0.5859\n",
      "Epoch 8/10, Batch 289/442, Training Loss: 0.3480\n",
      "Epoch 8/10, Batch 290/442, Training Loss: 0.6106\n",
      "Epoch 8/10, Batch 291/442, Training Loss: 0.3049\n",
      "Epoch 8/10, Batch 292/442, Training Loss: 0.4121\n",
      "Epoch 8/10, Batch 293/442, Training Loss: 0.5861\n",
      "Epoch 8/10, Batch 294/442, Training Loss: 0.3502\n",
      "Epoch 8/10, Batch 295/442, Training Loss: 0.4731\n",
      "Epoch 8/10, Batch 296/442, Training Loss: 0.3802\n",
      "Epoch 8/10, Batch 297/442, Training Loss: 0.5960\n",
      "Epoch 8/10, Batch 298/442, Training Loss: 0.5159\n",
      "Epoch 8/10, Batch 299/442, Training Loss: 0.4394\n",
      "Epoch 8/10, Batch 300/442, Training Loss: 0.3750\n",
      "Epoch 8/10, Batch 301/442, Training Loss: 0.4364\n",
      "Epoch 8/10, Batch 302/442, Training Loss: 0.4579\n",
      "Epoch 8/10, Batch 303/442, Training Loss: 0.5164\n",
      "Epoch 8/10, Batch 304/442, Training Loss: 0.4295\n",
      "Epoch 8/10, Batch 305/442, Training Loss: 0.5913\n",
      "Epoch 8/10, Batch 306/442, Training Loss: 0.5218\n",
      "Epoch 8/10, Batch 307/442, Training Loss: 0.3510\n",
      "Epoch 8/10, Batch 308/442, Training Loss: 0.2550\n",
      "Epoch 8/10, Batch 309/442, Training Loss: 0.2702\n",
      "Epoch 8/10, Batch 310/442, Training Loss: 0.4345\n",
      "Epoch 8/10, Batch 311/442, Training Loss: 0.3255\n",
      "Epoch 8/10, Batch 312/442, Training Loss: 0.2967\n",
      "Epoch 8/10, Batch 313/442, Training Loss: 0.4678\n",
      "Epoch 8/10, Batch 314/442, Training Loss: 0.3220\n",
      "Epoch 8/10, Batch 315/442, Training Loss: 0.4417\n",
      "Epoch 8/10, Batch 316/442, Training Loss: 0.6311\n",
      "Epoch 8/10, Batch 317/442, Training Loss: 0.2963\n",
      "Epoch 8/10, Batch 318/442, Training Loss: 0.6560\n",
      "Epoch 8/10, Batch 319/442, Training Loss: 0.5145\n",
      "Epoch 8/10, Batch 320/442, Training Loss: 0.5471\n",
      "Epoch 8/10, Batch 321/442, Training Loss: 0.4872\n",
      "Epoch 8/10, Batch 322/442, Training Loss: 0.4721\n",
      "Epoch 8/10, Batch 323/442, Training Loss: 0.5455\n",
      "Epoch 8/10, Batch 324/442, Training Loss: 0.4480\n",
      "Epoch 8/10, Batch 325/442, Training Loss: 0.5042\n",
      "Epoch 8/10, Batch 326/442, Training Loss: 0.6246\n",
      "Epoch 8/10, Batch 327/442, Training Loss: 0.3762\n",
      "Epoch 8/10, Batch 328/442, Training Loss: 0.4730\n",
      "Epoch 8/10, Batch 329/442, Training Loss: 0.3078\n",
      "Epoch 8/10, Batch 330/442, Training Loss: 0.4847\n",
      "Epoch 8/10, Batch 331/442, Training Loss: 0.7586\n",
      "Epoch 8/10, Batch 332/442, Training Loss: 0.5193\n",
      "Epoch 8/10, Batch 333/442, Training Loss: 0.4983\n",
      "Epoch 8/10, Batch 334/442, Training Loss: 0.3014\n",
      "Epoch 8/10, Batch 335/442, Training Loss: 0.3749\n",
      "Epoch 8/10, Batch 336/442, Training Loss: 0.5344\n",
      "Epoch 8/10, Batch 337/442, Training Loss: 0.4232\n",
      "Epoch 8/10, Batch 338/442, Training Loss: 0.4266\n",
      "Epoch 8/10, Batch 339/442, Training Loss: 0.6502\n",
      "Epoch 8/10, Batch 340/442, Training Loss: 0.4400\n",
      "Epoch 8/10, Batch 341/442, Training Loss: 0.5452\n",
      "Epoch 8/10, Batch 342/442, Training Loss: 0.5092\n",
      "Epoch 8/10, Batch 343/442, Training Loss: 0.7807\n",
      "Epoch 8/10, Batch 344/442, Training Loss: 0.2742\n",
      "Epoch 8/10, Batch 345/442, Training Loss: 0.6214\n",
      "Epoch 8/10, Batch 346/442, Training Loss: 0.5462\n",
      "Epoch 8/10, Batch 347/442, Training Loss: 0.6506\n",
      "Epoch 8/10, Batch 348/442, Training Loss: 0.5246\n",
      "Epoch 8/10, Batch 349/442, Training Loss: 0.9547\n",
      "Epoch 8/10, Batch 350/442, Training Loss: 0.5319\n",
      "Epoch 8/10, Batch 351/442, Training Loss: 0.7102\n",
      "Epoch 8/10, Batch 352/442, Training Loss: 0.4883\n",
      "Epoch 8/10, Batch 353/442, Training Loss: 0.6546\n",
      "Epoch 8/10, Batch 354/442, Training Loss: 0.4211\n",
      "Epoch 8/10, Batch 355/442, Training Loss: 0.5081\n",
      "Epoch 8/10, Batch 356/442, Training Loss: 0.3710\n",
      "Epoch 8/10, Batch 357/442, Training Loss: 0.3785\n",
      "Epoch 8/10, Batch 358/442, Training Loss: 0.6127\n",
      "Epoch 8/10, Batch 359/442, Training Loss: 0.3712\n",
      "Epoch 8/10, Batch 360/442, Training Loss: 0.5386\n",
      "Epoch 8/10, Batch 361/442, Training Loss: 0.4530\n",
      "Epoch 8/10, Batch 362/442, Training Loss: 0.3751\n",
      "Epoch 8/10, Batch 363/442, Training Loss: 0.4973\n",
      "Epoch 8/10, Batch 364/442, Training Loss: 0.6288\n",
      "Epoch 8/10, Batch 365/442, Training Loss: 0.3556\n",
      "Epoch 8/10, Batch 366/442, Training Loss: 0.5019\n",
      "Epoch 8/10, Batch 367/442, Training Loss: 0.2940\n",
      "Epoch 8/10, Batch 368/442, Training Loss: 0.4792\n",
      "Epoch 8/10, Batch 369/442, Training Loss: 0.4180\n",
      "Epoch 8/10, Batch 370/442, Training Loss: 0.4670\n",
      "Epoch 8/10, Batch 371/442, Training Loss: 0.4399\n",
      "Epoch 8/10, Batch 372/442, Training Loss: 0.3739\n",
      "Epoch 8/10, Batch 373/442, Training Loss: 0.3110\n",
      "Epoch 8/10, Batch 374/442, Training Loss: 0.3405\n",
      "Epoch 8/10, Batch 375/442, Training Loss: 0.3299\n",
      "Epoch 8/10, Batch 376/442, Training Loss: 0.2813\n",
      "Epoch 8/10, Batch 377/442, Training Loss: 0.5635\n",
      "Epoch 8/10, Batch 378/442, Training Loss: 0.3589\n",
      "Epoch 8/10, Batch 379/442, Training Loss: 0.5856\n",
      "Epoch 8/10, Batch 380/442, Training Loss: 0.4027\n",
      "Epoch 8/10, Batch 381/442, Training Loss: 0.7333\n",
      "Epoch 8/10, Batch 382/442, Training Loss: 0.3210\n",
      "Epoch 8/10, Batch 383/442, Training Loss: 0.4931\n",
      "Epoch 8/10, Batch 384/442, Training Loss: 0.4632\n",
      "Epoch 8/10, Batch 385/442, Training Loss: 0.4464\n",
      "Epoch 8/10, Batch 386/442, Training Loss: 0.5617\n",
      "Epoch 8/10, Batch 387/442, Training Loss: 0.3535\n",
      "Epoch 8/10, Batch 388/442, Training Loss: 0.4272\n",
      "Epoch 8/10, Batch 389/442, Training Loss: 0.4198\n",
      "Epoch 8/10, Batch 390/442, Training Loss: 0.4685\n",
      "Epoch 8/10, Batch 391/442, Training Loss: 0.5360\n",
      "Epoch 8/10, Batch 392/442, Training Loss: 0.2607\n",
      "Epoch 8/10, Batch 393/442, Training Loss: 0.8457\n",
      "Epoch 8/10, Batch 394/442, Training Loss: 0.5166\n",
      "Epoch 8/10, Batch 395/442, Training Loss: 0.4361\n",
      "Epoch 8/10, Batch 396/442, Training Loss: 0.5032\n",
      "Epoch 8/10, Batch 397/442, Training Loss: 0.6569\n",
      "Epoch 8/10, Batch 398/442, Training Loss: 0.5699\n",
      "Epoch 8/10, Batch 399/442, Training Loss: 0.4931\n",
      "Epoch 8/10, Batch 400/442, Training Loss: 0.4261\n",
      "Epoch 8/10, Batch 401/442, Training Loss: 0.4123\n",
      "Epoch 8/10, Batch 402/442, Training Loss: 0.5201\n",
      "Epoch 8/10, Batch 403/442, Training Loss: 0.5426\n",
      "Epoch 8/10, Batch 404/442, Training Loss: 0.5104\n",
      "Epoch 8/10, Batch 405/442, Training Loss: 0.5694\n",
      "Epoch 8/10, Batch 406/442, Training Loss: 0.4642\n",
      "Epoch 8/10, Batch 407/442, Training Loss: 0.3789\n",
      "Epoch 8/10, Batch 408/442, Training Loss: 0.5061\n",
      "Epoch 8/10, Batch 409/442, Training Loss: 0.3662\n",
      "Epoch 8/10, Batch 410/442, Training Loss: 0.3481\n",
      "Epoch 8/10, Batch 411/442, Training Loss: 0.5311\n",
      "Epoch 8/10, Batch 412/442, Training Loss: 0.4259\n",
      "Epoch 8/10, Batch 413/442, Training Loss: 0.6286\n",
      "Epoch 8/10, Batch 414/442, Training Loss: 0.4178\n",
      "Epoch 8/10, Batch 415/442, Training Loss: 0.4305\n",
      "Epoch 8/10, Batch 416/442, Training Loss: 0.4726\n",
      "Epoch 8/10, Batch 417/442, Training Loss: 0.3305\n",
      "Epoch 8/10, Batch 418/442, Training Loss: 0.5516\n",
      "Epoch 8/10, Batch 419/442, Training Loss: 0.4704\n",
      "Epoch 8/10, Batch 420/442, Training Loss: 0.3545\n",
      "Epoch 8/10, Batch 421/442, Training Loss: 0.4969\n",
      "Epoch 8/10, Batch 422/442, Training Loss: 0.3586\n",
      "Epoch 8/10, Batch 423/442, Training Loss: 0.4574\n",
      "Epoch 8/10, Batch 424/442, Training Loss: 0.5617\n",
      "Epoch 8/10, Batch 425/442, Training Loss: 0.3449\n",
      "Epoch 8/10, Batch 426/442, Training Loss: 0.4340\n",
      "Epoch 8/10, Batch 427/442, Training Loss: 0.4786\n",
      "Epoch 8/10, Batch 428/442, Training Loss: 0.6258\n",
      "Epoch 8/10, Batch 429/442, Training Loss: 0.3336\n",
      "Epoch 8/10, Batch 430/442, Training Loss: 0.3346\n",
      "Epoch 8/10, Batch 431/442, Training Loss: 0.6620\n",
      "Epoch 8/10, Batch 432/442, Training Loss: 0.5180\n",
      "Epoch 8/10, Batch 433/442, Training Loss: 0.5830\n",
      "Epoch 8/10, Batch 434/442, Training Loss: 0.5554\n",
      "Epoch 8/10, Batch 435/442, Training Loss: 0.5016\n",
      "Epoch 8/10, Batch 436/442, Training Loss: 0.3280\n",
      "Epoch 8/10, Batch 437/442, Training Loss: 0.2604\n",
      "Epoch 8/10, Batch 438/442, Training Loss: 0.4760\n",
      "Epoch 8/10, Batch 439/442, Training Loss: 0.7903\n",
      "Epoch 8/10, Batch 440/442, Training Loss: 0.5045\n",
      "Epoch 8/10, Batch 441/442, Training Loss: 0.3474\n",
      "Epoch 8/10, Batch 442/442, Training Loss: 0.4592\n",
      "Epoch 8/10, Training Loss: 0.4829, Validation Loss: 0.5885, Validation Accuracy: 0.7470\n",
      "Epoch 9/10, Batch 1/442, Training Loss: 0.3877\n",
      "Epoch 9/10, Batch 2/442, Training Loss: 0.3772\n",
      "Epoch 9/10, Batch 3/442, Training Loss: 0.5141\n",
      "Epoch 9/10, Batch 4/442, Training Loss: 0.6644\n",
      "Epoch 9/10, Batch 5/442, Training Loss: 0.4748\n",
      "Epoch 9/10, Batch 6/442, Training Loss: 0.2057\n",
      "Epoch 9/10, Batch 7/442, Training Loss: 0.2883\n",
      "Epoch 9/10, Batch 8/442, Training Loss: 0.5599\n",
      "Epoch 9/10, Batch 9/442, Training Loss: 0.3628\n",
      "Epoch 9/10, Batch 10/442, Training Loss: 0.3028\n",
      "Epoch 9/10, Batch 11/442, Training Loss: 0.4739\n",
      "Epoch 9/10, Batch 12/442, Training Loss: 0.5078\n",
      "Epoch 9/10, Batch 13/442, Training Loss: 0.5429\n",
      "Epoch 9/10, Batch 14/442, Training Loss: 0.6755\n",
      "Epoch 9/10, Batch 15/442, Training Loss: 0.3548\n",
      "Epoch 9/10, Batch 16/442, Training Loss: 0.4091\n",
      "Epoch 9/10, Batch 17/442, Training Loss: 0.4944\n",
      "Epoch 9/10, Batch 18/442, Training Loss: 0.4814\n",
      "Epoch 9/10, Batch 19/442, Training Loss: 0.4658\n",
      "Epoch 9/10, Batch 20/442, Training Loss: 0.3125\n",
      "Epoch 9/10, Batch 21/442, Training Loss: 0.5735\n",
      "Epoch 9/10, Batch 22/442, Training Loss: 0.4756\n",
      "Epoch 9/10, Batch 23/442, Training Loss: 0.4122\n",
      "Epoch 9/10, Batch 24/442, Training Loss: 0.5354\n",
      "Epoch 9/10, Batch 25/442, Training Loss: 0.5185\n",
      "Epoch 9/10, Batch 26/442, Training Loss: 0.4225\n",
      "Epoch 9/10, Batch 27/442, Training Loss: 0.4438\n",
      "Epoch 9/10, Batch 28/442, Training Loss: 0.5667\n",
      "Epoch 9/10, Batch 29/442, Training Loss: 0.6117\n",
      "Epoch 9/10, Batch 30/442, Training Loss: 0.4985\n",
      "Epoch 9/10, Batch 31/442, Training Loss: 0.3065\n",
      "Epoch 9/10, Batch 32/442, Training Loss: 0.5853\n",
      "Epoch 9/10, Batch 33/442, Training Loss: 0.4372\n",
      "Epoch 9/10, Batch 34/442, Training Loss: 0.3082\n",
      "Epoch 9/10, Batch 35/442, Training Loss: 0.4632\n",
      "Epoch 9/10, Batch 36/442, Training Loss: 0.4316\n",
      "Epoch 9/10, Batch 37/442, Training Loss: 0.4711\n",
      "Epoch 9/10, Batch 38/442, Training Loss: 0.3246\n",
      "Epoch 9/10, Batch 39/442, Training Loss: 0.3112\n",
      "Epoch 9/10, Batch 40/442, Training Loss: 0.4227\n",
      "Epoch 9/10, Batch 41/442, Training Loss: 0.3150\n",
      "Epoch 9/10, Batch 42/442, Training Loss: 0.5161\n",
      "Epoch 9/10, Batch 43/442, Training Loss: 0.4911\n",
      "Epoch 9/10, Batch 44/442, Training Loss: 0.2761\n",
      "Epoch 9/10, Batch 45/442, Training Loss: 0.3912\n",
      "Epoch 9/10, Batch 46/442, Training Loss: 0.4179\n",
      "Epoch 9/10, Batch 47/442, Training Loss: 0.2971\n",
      "Epoch 9/10, Batch 48/442, Training Loss: 0.4368\n",
      "Epoch 9/10, Batch 49/442, Training Loss: 0.4175\n",
      "Epoch 9/10, Batch 50/442, Training Loss: 0.3099\n",
      "Epoch 9/10, Batch 51/442, Training Loss: 0.3659\n",
      "Epoch 9/10, Batch 52/442, Training Loss: 0.3893\n",
      "Epoch 9/10, Batch 53/442, Training Loss: 0.5347\n",
      "Epoch 9/10, Batch 54/442, Training Loss: 0.4816\n",
      "Epoch 9/10, Batch 55/442, Training Loss: 0.3377\n",
      "Epoch 9/10, Batch 56/442, Training Loss: 0.4069\n",
      "Epoch 9/10, Batch 57/442, Training Loss: 0.5782\n",
      "Epoch 9/10, Batch 58/442, Training Loss: 0.2900\n",
      "Epoch 9/10, Batch 59/442, Training Loss: 0.3816\n",
      "Epoch 9/10, Batch 60/442, Training Loss: 0.3548\n",
      "Epoch 9/10, Batch 61/442, Training Loss: 0.4828\n",
      "Epoch 9/10, Batch 62/442, Training Loss: 0.6151\n",
      "Epoch 9/10, Batch 63/442, Training Loss: 0.5448\n",
      "Epoch 9/10, Batch 64/442, Training Loss: 0.5747\n",
      "Epoch 9/10, Batch 65/442, Training Loss: 0.6376\n",
      "Epoch 9/10, Batch 66/442, Training Loss: 0.3008\n",
      "Epoch 9/10, Batch 67/442, Training Loss: 0.5636\n",
      "Epoch 9/10, Batch 68/442, Training Loss: 0.5078\n",
      "Epoch 9/10, Batch 69/442, Training Loss: 0.5858\n",
      "Epoch 9/10, Batch 70/442, Training Loss: 0.4997\n",
      "Epoch 9/10, Batch 71/442, Training Loss: 0.6166\n",
      "Epoch 9/10, Batch 72/442, Training Loss: 0.4833\n",
      "Epoch 9/10, Batch 73/442, Training Loss: 0.4346\n",
      "Epoch 9/10, Batch 74/442, Training Loss: 0.3340\n",
      "Epoch 9/10, Batch 75/442, Training Loss: 0.4529\n",
      "Epoch 9/10, Batch 76/442, Training Loss: 0.4371\n",
      "Epoch 9/10, Batch 77/442, Training Loss: 0.7206\n",
      "Epoch 9/10, Batch 78/442, Training Loss: 0.7799\n",
      "Epoch 9/10, Batch 79/442, Training Loss: 0.4503\n",
      "Epoch 9/10, Batch 80/442, Training Loss: 0.4409\n",
      "Epoch 9/10, Batch 81/442, Training Loss: 0.3913\n",
      "Epoch 9/10, Batch 82/442, Training Loss: 0.5606\n",
      "Epoch 9/10, Batch 83/442, Training Loss: 0.3428\n",
      "Epoch 9/10, Batch 84/442, Training Loss: 0.4614\n",
      "Epoch 9/10, Batch 85/442, Training Loss: 0.5087\n",
      "Epoch 9/10, Batch 86/442, Training Loss: 0.5874\n",
      "Epoch 9/10, Batch 87/442, Training Loss: 0.3624\n",
      "Epoch 9/10, Batch 88/442, Training Loss: 0.5811\n",
      "Epoch 9/10, Batch 89/442, Training Loss: 0.4237\n",
      "Epoch 9/10, Batch 90/442, Training Loss: 0.3694\n",
      "Epoch 9/10, Batch 91/442, Training Loss: 0.3173\n",
      "Epoch 9/10, Batch 92/442, Training Loss: 0.4574\n",
      "Epoch 9/10, Batch 93/442, Training Loss: 0.5048\n",
      "Epoch 9/10, Batch 94/442, Training Loss: 0.6425\n",
      "Epoch 9/10, Batch 95/442, Training Loss: 0.4404\n",
      "Epoch 9/10, Batch 96/442, Training Loss: 0.5089\n",
      "Epoch 9/10, Batch 97/442, Training Loss: 0.4069\n",
      "Epoch 9/10, Batch 98/442, Training Loss: 0.6420\n",
      "Epoch 9/10, Batch 99/442, Training Loss: 0.5500\n",
      "Epoch 9/10, Batch 100/442, Training Loss: 0.3574\n",
      "Epoch 9/10, Batch 101/442, Training Loss: 0.5124\n",
      "Epoch 9/10, Batch 102/442, Training Loss: 0.5636\n",
      "Epoch 9/10, Batch 103/442, Training Loss: 0.6551\n",
      "Epoch 9/10, Batch 104/442, Training Loss: 0.4059\n",
      "Epoch 9/10, Batch 105/442, Training Loss: 0.4810\n",
      "Epoch 9/10, Batch 106/442, Training Loss: 0.2956\n",
      "Epoch 9/10, Batch 107/442, Training Loss: 0.4040\n",
      "Epoch 9/10, Batch 108/442, Training Loss: 0.3588\n",
      "Epoch 9/10, Batch 109/442, Training Loss: 0.3690\n",
      "Epoch 9/10, Batch 110/442, Training Loss: 0.6226\n",
      "Epoch 9/10, Batch 111/442, Training Loss: 0.4500\n",
      "Epoch 9/10, Batch 112/442, Training Loss: 0.5259\n",
      "Epoch 9/10, Batch 113/442, Training Loss: 0.3749\n",
      "Epoch 9/10, Batch 114/442, Training Loss: 0.4058\n",
      "Epoch 9/10, Batch 115/442, Training Loss: 0.3825\n",
      "Epoch 9/10, Batch 116/442, Training Loss: 0.5738\n",
      "Epoch 9/10, Batch 117/442, Training Loss: 0.2217\n",
      "Epoch 9/10, Batch 118/442, Training Loss: 0.4328\n",
      "Epoch 9/10, Batch 119/442, Training Loss: 0.4475\n",
      "Epoch 9/10, Batch 120/442, Training Loss: 0.3885\n",
      "Epoch 9/10, Batch 121/442, Training Loss: 0.4289\n",
      "Epoch 9/10, Batch 122/442, Training Loss: 0.4439\n",
      "Epoch 9/10, Batch 123/442, Training Loss: 0.4906\n",
      "Epoch 9/10, Batch 124/442, Training Loss: 0.3306\n",
      "Epoch 9/10, Batch 125/442, Training Loss: 0.5229\n",
      "Epoch 9/10, Batch 126/442, Training Loss: 0.5100\n",
      "Epoch 9/10, Batch 127/442, Training Loss: 0.2766\n",
      "Epoch 9/10, Batch 128/442, Training Loss: 0.4223\n",
      "Epoch 9/10, Batch 129/442, Training Loss: 0.4042\n",
      "Epoch 9/10, Batch 130/442, Training Loss: 0.4852\n",
      "Epoch 9/10, Batch 131/442, Training Loss: 0.3434\n",
      "Epoch 9/10, Batch 132/442, Training Loss: 0.5600\n",
      "Epoch 9/10, Batch 133/442, Training Loss: 0.3477\n",
      "Epoch 9/10, Batch 134/442, Training Loss: 0.4294\n",
      "Epoch 9/10, Batch 135/442, Training Loss: 0.4884\n",
      "Epoch 9/10, Batch 136/442, Training Loss: 0.3828\n",
      "Epoch 9/10, Batch 137/442, Training Loss: 0.6611\n",
      "Epoch 9/10, Batch 138/442, Training Loss: 0.4371\n",
      "Epoch 9/10, Batch 139/442, Training Loss: 0.4376\n",
      "Epoch 9/10, Batch 140/442, Training Loss: 0.5878\n",
      "Epoch 9/10, Batch 141/442, Training Loss: 0.6021\n",
      "Epoch 9/10, Batch 142/442, Training Loss: 0.3371\n",
      "Epoch 9/10, Batch 143/442, Training Loss: 0.3636\n",
      "Epoch 9/10, Batch 144/442, Training Loss: 0.6437\n",
      "Epoch 9/10, Batch 145/442, Training Loss: 0.4519\n",
      "Epoch 9/10, Batch 146/442, Training Loss: 0.6439\n",
      "Epoch 9/10, Batch 147/442, Training Loss: 0.4246\n",
      "Epoch 9/10, Batch 148/442, Training Loss: 0.4485\n",
      "Epoch 9/10, Batch 149/442, Training Loss: 0.2639\n",
      "Epoch 9/10, Batch 150/442, Training Loss: 0.6192\n",
      "Epoch 9/10, Batch 151/442, Training Loss: 0.4300\n",
      "Epoch 9/10, Batch 152/442, Training Loss: 0.6341\n",
      "Epoch 9/10, Batch 153/442, Training Loss: 0.4807\n",
      "Epoch 9/10, Batch 154/442, Training Loss: 0.4430\n",
      "Epoch 9/10, Batch 155/442, Training Loss: 0.4289\n",
      "Epoch 9/10, Batch 156/442, Training Loss: 0.6612\n",
      "Epoch 9/10, Batch 157/442, Training Loss: 0.8796\n",
      "Epoch 9/10, Batch 158/442, Training Loss: 0.4802\n",
      "Epoch 9/10, Batch 159/442, Training Loss: 0.4025\n",
      "Epoch 9/10, Batch 160/442, Training Loss: 0.5452\n",
      "Epoch 9/10, Batch 161/442, Training Loss: 0.6692\n",
      "Epoch 9/10, Batch 162/442, Training Loss: 0.4440\n",
      "Epoch 9/10, Batch 163/442, Training Loss: 0.4682\n",
      "Epoch 9/10, Batch 164/442, Training Loss: 0.4223\n",
      "Epoch 9/10, Batch 165/442, Training Loss: 0.4616\n",
      "Epoch 9/10, Batch 166/442, Training Loss: 0.3988\n",
      "Epoch 9/10, Batch 167/442, Training Loss: 0.4946\n",
      "Epoch 9/10, Batch 168/442, Training Loss: 0.4869\n",
      "Epoch 9/10, Batch 169/442, Training Loss: 0.4858\n",
      "Epoch 9/10, Batch 170/442, Training Loss: 0.4033\n",
      "Epoch 9/10, Batch 171/442, Training Loss: 0.4063\n",
      "Epoch 9/10, Batch 172/442, Training Loss: 0.4978\n",
      "Epoch 9/10, Batch 173/442, Training Loss: 0.3388\n",
      "Epoch 9/10, Batch 174/442, Training Loss: 0.5505\n",
      "Epoch 9/10, Batch 175/442, Training Loss: 0.3860\n",
      "Epoch 9/10, Batch 176/442, Training Loss: 0.5904\n",
      "Epoch 9/10, Batch 177/442, Training Loss: 0.2575\n",
      "Epoch 9/10, Batch 178/442, Training Loss: 0.6679\n",
      "Epoch 9/10, Batch 179/442, Training Loss: 0.4763\n",
      "Epoch 9/10, Batch 180/442, Training Loss: 0.4869\n",
      "Epoch 9/10, Batch 181/442, Training Loss: 0.6949\n",
      "Epoch 9/10, Batch 182/442, Training Loss: 0.3258\n",
      "Epoch 9/10, Batch 183/442, Training Loss: 0.5357\n",
      "Epoch 9/10, Batch 184/442, Training Loss: 0.4576\n",
      "Epoch 9/10, Batch 185/442, Training Loss: 0.5234\n",
      "Epoch 9/10, Batch 186/442, Training Loss: 0.4891\n",
      "Epoch 9/10, Batch 187/442, Training Loss: 0.6906\n",
      "Epoch 9/10, Batch 188/442, Training Loss: 0.4320\n",
      "Epoch 9/10, Batch 189/442, Training Loss: 0.5787\n",
      "Epoch 9/10, Batch 190/442, Training Loss: 0.4272\n",
      "Epoch 9/10, Batch 191/442, Training Loss: 0.5043\n",
      "Epoch 9/10, Batch 192/442, Training Loss: 0.5504\n",
      "Epoch 9/10, Batch 193/442, Training Loss: 0.4773\n",
      "Epoch 9/10, Batch 194/442, Training Loss: 0.3628\n",
      "Epoch 9/10, Batch 195/442, Training Loss: 0.3967\n",
      "Epoch 9/10, Batch 196/442, Training Loss: 0.4782\n",
      "Epoch 9/10, Batch 197/442, Training Loss: 0.3835\n",
      "Epoch 9/10, Batch 198/442, Training Loss: 0.3586\n",
      "Epoch 9/10, Batch 199/442, Training Loss: 0.5075\n",
      "Epoch 9/10, Batch 200/442, Training Loss: 0.5202\n",
      "Epoch 9/10, Batch 201/442, Training Loss: 0.6055\n",
      "Epoch 9/10, Batch 202/442, Training Loss: 0.6602\n",
      "Epoch 9/10, Batch 203/442, Training Loss: 0.5546\n",
      "Epoch 9/10, Batch 204/442, Training Loss: 0.2921\n",
      "Epoch 9/10, Batch 205/442, Training Loss: 0.4320\n",
      "Epoch 9/10, Batch 206/442, Training Loss: 0.4253\n",
      "Epoch 9/10, Batch 207/442, Training Loss: 0.4135\n",
      "Epoch 9/10, Batch 208/442, Training Loss: 0.2978\n",
      "Epoch 9/10, Batch 209/442, Training Loss: 0.4072\n",
      "Epoch 9/10, Batch 210/442, Training Loss: 0.5998\n",
      "Epoch 9/10, Batch 211/442, Training Loss: 0.4880\n",
      "Epoch 9/10, Batch 212/442, Training Loss: 0.2647\n",
      "Epoch 9/10, Batch 213/442, Training Loss: 0.5785\n",
      "Epoch 9/10, Batch 214/442, Training Loss: 0.2656\n",
      "Epoch 9/10, Batch 215/442, Training Loss: 0.5817\n",
      "Epoch 9/10, Batch 216/442, Training Loss: 0.3786\n",
      "Epoch 9/10, Batch 217/442, Training Loss: 0.3618\n",
      "Epoch 9/10, Batch 218/442, Training Loss: 0.3434\n",
      "Epoch 9/10, Batch 219/442, Training Loss: 0.3131\n",
      "Epoch 9/10, Batch 220/442, Training Loss: 0.2994\n",
      "Epoch 9/10, Batch 221/442, Training Loss: 0.4681\n",
      "Epoch 9/10, Batch 222/442, Training Loss: 0.3793\n",
      "Epoch 9/10, Batch 223/442, Training Loss: 0.5046\n",
      "Epoch 9/10, Batch 224/442, Training Loss: 0.3632\n",
      "Epoch 9/10, Batch 225/442, Training Loss: 0.3911\n",
      "Epoch 9/10, Batch 226/442, Training Loss: 0.4884\n",
      "Epoch 9/10, Batch 227/442, Training Loss: 0.5461\n",
      "Epoch 9/10, Batch 228/442, Training Loss: 0.5314\n",
      "Epoch 9/10, Batch 229/442, Training Loss: 0.4159\n",
      "Epoch 9/10, Batch 230/442, Training Loss: 0.3326\n",
      "Epoch 9/10, Batch 231/442, Training Loss: 0.3850\n",
      "Epoch 9/10, Batch 232/442, Training Loss: 0.2801\n",
      "Epoch 9/10, Batch 233/442, Training Loss: 0.3796\n",
      "Epoch 9/10, Batch 234/442, Training Loss: 0.4854\n",
      "Epoch 9/10, Batch 235/442, Training Loss: 0.7194\n",
      "Epoch 9/10, Batch 236/442, Training Loss: 0.2495\n",
      "Epoch 9/10, Batch 237/442, Training Loss: 0.4443\n",
      "Epoch 9/10, Batch 238/442, Training Loss: 0.4688\n",
      "Epoch 9/10, Batch 239/442, Training Loss: 0.3105\n",
      "Epoch 9/10, Batch 240/442, Training Loss: 0.4043\n",
      "Epoch 9/10, Batch 241/442, Training Loss: 0.5074\n",
      "Epoch 9/10, Batch 242/442, Training Loss: 0.3264\n",
      "Epoch 9/10, Batch 243/442, Training Loss: 0.3864\n",
      "Epoch 9/10, Batch 244/442, Training Loss: 0.5648\n",
      "Epoch 9/10, Batch 245/442, Training Loss: 0.5185\n",
      "Epoch 9/10, Batch 246/442, Training Loss: 0.5346\n",
      "Epoch 9/10, Batch 247/442, Training Loss: 0.3327\n",
      "Epoch 9/10, Batch 248/442, Training Loss: 0.4090\n",
      "Epoch 9/10, Batch 249/442, Training Loss: 0.4045\n",
      "Epoch 9/10, Batch 250/442, Training Loss: 0.8479\n",
      "Epoch 9/10, Batch 251/442, Training Loss: 0.2298\n",
      "Epoch 9/10, Batch 252/442, Training Loss: 0.6107\n",
      "Epoch 9/10, Batch 253/442, Training Loss: 0.2206\n",
      "Epoch 9/10, Batch 254/442, Training Loss: 0.4288\n",
      "Epoch 9/10, Batch 255/442, Training Loss: 0.4234\n",
      "Epoch 9/10, Batch 256/442, Training Loss: 0.4085\n",
      "Epoch 9/10, Batch 257/442, Training Loss: 0.2831\n",
      "Epoch 9/10, Batch 258/442, Training Loss: 0.4080\n",
      "Epoch 9/10, Batch 259/442, Training Loss: 0.3511\n",
      "Epoch 9/10, Batch 260/442, Training Loss: 0.6080\n",
      "Epoch 9/10, Batch 261/442, Training Loss: 0.3800\n",
      "Epoch 9/10, Batch 262/442, Training Loss: 0.3874\n",
      "Epoch 9/10, Batch 263/442, Training Loss: 0.3604\n",
      "Epoch 9/10, Batch 264/442, Training Loss: 0.2649\n",
      "Epoch 9/10, Batch 265/442, Training Loss: 0.3755\n",
      "Epoch 9/10, Batch 266/442, Training Loss: 0.3466\n",
      "Epoch 9/10, Batch 267/442, Training Loss: 0.4108\n",
      "Epoch 9/10, Batch 268/442, Training Loss: 0.5702\n",
      "Epoch 9/10, Batch 269/442, Training Loss: 0.5327\n",
      "Epoch 9/10, Batch 270/442, Training Loss: 0.4669\n",
      "Epoch 9/10, Batch 271/442, Training Loss: 0.5579\n",
      "Epoch 9/10, Batch 272/442, Training Loss: 0.4443\n",
      "Epoch 9/10, Batch 273/442, Training Loss: 0.4033\n",
      "Epoch 9/10, Batch 274/442, Training Loss: 0.3376\n",
      "Epoch 9/10, Batch 275/442, Training Loss: 0.4227\n",
      "Epoch 9/10, Batch 276/442, Training Loss: 0.4321\n",
      "Epoch 9/10, Batch 277/442, Training Loss: 0.3603\n",
      "Epoch 9/10, Batch 278/442, Training Loss: 0.3834\n",
      "Epoch 9/10, Batch 279/442, Training Loss: 0.3702\n",
      "Epoch 9/10, Batch 280/442, Training Loss: 0.5266\n",
      "Epoch 9/10, Batch 281/442, Training Loss: 0.4912\n",
      "Epoch 9/10, Batch 282/442, Training Loss: 0.6504\n",
      "Epoch 9/10, Batch 283/442, Training Loss: 0.6146\n",
      "Epoch 9/10, Batch 284/442, Training Loss: 0.5808\n",
      "Epoch 9/10, Batch 285/442, Training Loss: 0.3142\n",
      "Epoch 9/10, Batch 286/442, Training Loss: 0.2636\n",
      "Epoch 9/10, Batch 287/442, Training Loss: 0.4866\n",
      "Epoch 9/10, Batch 288/442, Training Loss: 0.4060\n",
      "Epoch 9/10, Batch 289/442, Training Loss: 0.3970\n",
      "Epoch 9/10, Batch 290/442, Training Loss: 0.5089\n",
      "Epoch 9/10, Batch 291/442, Training Loss: 0.6330\n",
      "Epoch 9/10, Batch 292/442, Training Loss: 0.6662\n",
      "Epoch 9/10, Batch 293/442, Training Loss: 0.4976\n",
      "Epoch 9/10, Batch 294/442, Training Loss: 0.4747\n",
      "Epoch 9/10, Batch 295/442, Training Loss: 0.4145\n",
      "Epoch 9/10, Batch 296/442, Training Loss: 0.2880\n",
      "Epoch 9/10, Batch 297/442, Training Loss: 0.3184\n",
      "Epoch 9/10, Batch 298/442, Training Loss: 0.3260\n",
      "Epoch 9/10, Batch 299/442, Training Loss: 0.5712\n",
      "Epoch 9/10, Batch 300/442, Training Loss: 0.6327\n",
      "Epoch 9/10, Batch 301/442, Training Loss: 0.4804\n",
      "Epoch 9/10, Batch 302/442, Training Loss: 0.4839\n",
      "Epoch 9/10, Batch 303/442, Training Loss: 0.5149\n",
      "Epoch 9/10, Batch 304/442, Training Loss: 0.5768\n",
      "Epoch 9/10, Batch 305/442, Training Loss: 0.2645\n",
      "Epoch 9/10, Batch 306/442, Training Loss: 0.4531\n",
      "Epoch 9/10, Batch 307/442, Training Loss: 0.4259\n",
      "Epoch 9/10, Batch 308/442, Training Loss: 0.5949\n",
      "Epoch 9/10, Batch 309/442, Training Loss: 0.4731\n",
      "Epoch 9/10, Batch 310/442, Training Loss: 0.5447\n",
      "Epoch 9/10, Batch 311/442, Training Loss: 0.2560\n",
      "Epoch 9/10, Batch 312/442, Training Loss: 0.4996\n",
      "Epoch 9/10, Batch 313/442, Training Loss: 0.3144\n",
      "Epoch 9/10, Batch 314/442, Training Loss: 0.4080\n",
      "Epoch 9/10, Batch 315/442, Training Loss: 0.4501\n",
      "Epoch 9/10, Batch 316/442, Training Loss: 0.4098\n",
      "Epoch 9/10, Batch 317/442, Training Loss: 0.3379\n",
      "Epoch 9/10, Batch 318/442, Training Loss: 0.4442\n",
      "Epoch 9/10, Batch 319/442, Training Loss: 0.5360\n",
      "Epoch 9/10, Batch 320/442, Training Loss: 0.4091\n",
      "Epoch 9/10, Batch 321/442, Training Loss: 0.7361\n",
      "Epoch 9/10, Batch 322/442, Training Loss: 0.5990\n",
      "Epoch 9/10, Batch 323/442, Training Loss: 0.6625\n",
      "Epoch 9/10, Batch 324/442, Training Loss: 0.3032\n",
      "Epoch 9/10, Batch 325/442, Training Loss: 0.3915\n",
      "Epoch 9/10, Batch 326/442, Training Loss: 0.4307\n",
      "Epoch 9/10, Batch 327/442, Training Loss: 0.5557\n",
      "Epoch 9/10, Batch 328/442, Training Loss: 0.2924\n",
      "Epoch 9/10, Batch 329/442, Training Loss: 0.5831\n",
      "Epoch 9/10, Batch 330/442, Training Loss: 0.6709\n",
      "Epoch 9/10, Batch 331/442, Training Loss: 0.4023\n",
      "Epoch 9/10, Batch 332/442, Training Loss: 0.5452\n",
      "Epoch 9/10, Batch 333/442, Training Loss: 0.4394\n",
      "Epoch 9/10, Batch 334/442, Training Loss: 0.2220\n",
      "Epoch 9/10, Batch 335/442, Training Loss: 0.4109\n",
      "Epoch 9/10, Batch 336/442, Training Loss: 0.6186\n",
      "Epoch 9/10, Batch 337/442, Training Loss: 0.4448\n",
      "Epoch 9/10, Batch 338/442, Training Loss: 0.4022\n",
      "Epoch 9/10, Batch 339/442, Training Loss: 0.2871\n",
      "Epoch 9/10, Batch 340/442, Training Loss: 0.3420\n",
      "Epoch 9/10, Batch 341/442, Training Loss: 0.3766\n",
      "Epoch 9/10, Batch 342/442, Training Loss: 0.3595\n",
      "Epoch 9/10, Batch 343/442, Training Loss: 0.3541\n",
      "Epoch 9/10, Batch 344/442, Training Loss: 0.5655\n",
      "Epoch 9/10, Batch 345/442, Training Loss: 0.5270\n",
      "Epoch 9/10, Batch 346/442, Training Loss: 0.4615\n",
      "Epoch 9/10, Batch 347/442, Training Loss: 0.5689\n",
      "Epoch 9/10, Batch 348/442, Training Loss: 0.6452\n",
      "Epoch 9/10, Batch 349/442, Training Loss: 0.3411\n",
      "Epoch 9/10, Batch 350/442, Training Loss: 0.3575\n",
      "Epoch 9/10, Batch 351/442, Training Loss: 0.4947\n",
      "Epoch 9/10, Batch 352/442, Training Loss: 0.6888\n",
      "Epoch 9/10, Batch 353/442, Training Loss: 0.4266\n",
      "Epoch 9/10, Batch 354/442, Training Loss: 0.8844\n",
      "Epoch 9/10, Batch 355/442, Training Loss: 0.4291\n",
      "Epoch 9/10, Batch 356/442, Training Loss: 0.1898\n",
      "Epoch 9/10, Batch 357/442, Training Loss: 0.4335\n",
      "Epoch 9/10, Batch 358/442, Training Loss: 0.4391\n",
      "Epoch 9/10, Batch 359/442, Training Loss: 0.3396\n",
      "Epoch 9/10, Batch 360/442, Training Loss: 0.2959\n",
      "Epoch 9/10, Batch 361/442, Training Loss: 0.3923\n",
      "Epoch 9/10, Batch 362/442, Training Loss: 0.3306\n",
      "Epoch 9/10, Batch 363/442, Training Loss: 0.4030\n",
      "Epoch 9/10, Batch 364/442, Training Loss: 0.3925\n",
      "Epoch 9/10, Batch 365/442, Training Loss: 0.6042\n",
      "Epoch 9/10, Batch 366/442, Training Loss: 0.5541\n",
      "Epoch 9/10, Batch 367/442, Training Loss: 0.3129\n",
      "Epoch 9/10, Batch 368/442, Training Loss: 0.4699\n",
      "Epoch 9/10, Batch 369/442, Training Loss: 0.3829\n",
      "Epoch 9/10, Batch 370/442, Training Loss: 0.5026\n",
      "Epoch 9/10, Batch 371/442, Training Loss: 0.5392\n",
      "Epoch 9/10, Batch 372/442, Training Loss: 0.4223\n",
      "Epoch 9/10, Batch 373/442, Training Loss: 0.7657\n",
      "Epoch 9/10, Batch 374/442, Training Loss: 0.2109\n",
      "Epoch 9/10, Batch 375/442, Training Loss: 0.5461\n",
      "Epoch 9/10, Batch 376/442, Training Loss: 0.5111\n",
      "Epoch 9/10, Batch 377/442, Training Loss: 0.3736\n",
      "Epoch 9/10, Batch 378/442, Training Loss: 0.4685\n",
      "Epoch 9/10, Batch 379/442, Training Loss: 0.2771\n",
      "Epoch 9/10, Batch 380/442, Training Loss: 0.5209\n",
      "Epoch 9/10, Batch 381/442, Training Loss: 0.3478\n",
      "Epoch 9/10, Batch 382/442, Training Loss: 0.5020\n",
      "Epoch 9/10, Batch 383/442, Training Loss: 0.5057\n",
      "Epoch 9/10, Batch 384/442, Training Loss: 0.5315\n",
      "Epoch 9/10, Batch 385/442, Training Loss: 0.4972\n",
      "Epoch 9/10, Batch 386/442, Training Loss: 0.6253\n",
      "Epoch 9/10, Batch 387/442, Training Loss: 0.6817\n",
      "Epoch 9/10, Batch 388/442, Training Loss: 0.3330\n",
      "Epoch 9/10, Batch 389/442, Training Loss: 0.4574\n",
      "Epoch 9/10, Batch 390/442, Training Loss: 0.3992\n",
      "Epoch 9/10, Batch 391/442, Training Loss: 0.5423\n",
      "Epoch 9/10, Batch 392/442, Training Loss: 0.4548\n",
      "Epoch 9/10, Batch 393/442, Training Loss: 0.4881\n",
      "Epoch 9/10, Batch 394/442, Training Loss: 0.3442\n",
      "Epoch 9/10, Batch 395/442, Training Loss: 0.3231\n",
      "Epoch 9/10, Batch 396/442, Training Loss: 0.4181\n",
      "Epoch 9/10, Batch 397/442, Training Loss: 0.3797\n",
      "Epoch 9/10, Batch 398/442, Training Loss: 0.3438\n",
      "Epoch 9/10, Batch 399/442, Training Loss: 0.5923\n",
      "Epoch 9/10, Batch 400/442, Training Loss: 0.5196\n",
      "Epoch 9/10, Batch 401/442, Training Loss: 0.3955\n",
      "Epoch 9/10, Batch 402/442, Training Loss: 0.2579\n",
      "Epoch 9/10, Batch 403/442, Training Loss: 0.3702\n",
      "Epoch 9/10, Batch 404/442, Training Loss: 0.4157\n",
      "Epoch 9/10, Batch 405/442, Training Loss: 0.4338\n",
      "Epoch 9/10, Batch 406/442, Training Loss: 0.6395\n",
      "Epoch 9/10, Batch 407/442, Training Loss: 0.4884\n",
      "Epoch 9/10, Batch 408/442, Training Loss: 0.5260\n",
      "Epoch 9/10, Batch 409/442, Training Loss: 0.3089\n",
      "Epoch 9/10, Batch 410/442, Training Loss: 0.3137\n",
      "Epoch 9/10, Batch 411/442, Training Loss: 0.6010\n",
      "Epoch 9/10, Batch 412/442, Training Loss: 0.4265\n",
      "Epoch 9/10, Batch 413/442, Training Loss: 0.4140\n",
      "Epoch 9/10, Batch 414/442, Training Loss: 0.3317\n",
      "Epoch 9/10, Batch 415/442, Training Loss: 0.4647\n",
      "Epoch 9/10, Batch 416/442, Training Loss: 0.3164\n",
      "Epoch 9/10, Batch 417/442, Training Loss: 0.3309\n",
      "Epoch 9/10, Batch 418/442, Training Loss: 0.6008\n",
      "Epoch 9/10, Batch 419/442, Training Loss: 0.6793\n",
      "Epoch 9/10, Batch 420/442, Training Loss: 0.4528\n",
      "Epoch 9/10, Batch 421/442, Training Loss: 0.6237\n",
      "Epoch 9/10, Batch 422/442, Training Loss: 0.6093\n",
      "Epoch 9/10, Batch 423/442, Training Loss: 0.6546\n",
      "Epoch 9/10, Batch 424/442, Training Loss: 0.4260\n",
      "Epoch 9/10, Batch 425/442, Training Loss: 0.5807\n",
      "Epoch 9/10, Batch 426/442, Training Loss: 0.4610\n",
      "Epoch 9/10, Batch 427/442, Training Loss: 0.5013\n",
      "Epoch 9/10, Batch 428/442, Training Loss: 0.3390\n",
      "Epoch 9/10, Batch 429/442, Training Loss: 0.6367\n",
      "Epoch 9/10, Batch 430/442, Training Loss: 0.7050\n",
      "Epoch 9/10, Batch 431/442, Training Loss: 0.3252\n",
      "Epoch 9/10, Batch 432/442, Training Loss: 0.4876\n",
      "Epoch 9/10, Batch 433/442, Training Loss: 0.5385\n",
      "Epoch 9/10, Batch 434/442, Training Loss: 0.3242\n",
      "Epoch 9/10, Batch 435/442, Training Loss: 0.4914\n",
      "Epoch 9/10, Batch 436/442, Training Loss: 0.3868\n",
      "Epoch 9/10, Batch 437/442, Training Loss: 0.4467\n",
      "Epoch 9/10, Batch 438/442, Training Loss: 0.2424\n",
      "Epoch 9/10, Batch 439/442, Training Loss: 0.3837\n",
      "Epoch 9/10, Batch 440/442, Training Loss: 0.4499\n",
      "Epoch 9/10, Batch 441/442, Training Loss: 0.7487\n",
      "Epoch 9/10, Batch 442/442, Training Loss: 0.7358\n",
      "Epoch 9/10, Training Loss: 0.4577, Validation Loss: 0.4465, Validation Accuracy: 0.8196\n",
      "Epoch 10/10, Batch 1/442, Training Loss: 0.4028\n",
      "Epoch 10/10, Batch 2/442, Training Loss: 0.3949\n",
      "Epoch 10/10, Batch 3/442, Training Loss: 0.3066\n",
      "Epoch 10/10, Batch 4/442, Training Loss: 0.4386\n",
      "Epoch 10/10, Batch 5/442, Training Loss: 0.5235\n",
      "Epoch 10/10, Batch 6/442, Training Loss: 0.2369\n",
      "Epoch 10/10, Batch 7/442, Training Loss: 0.4087\n",
      "Epoch 10/10, Batch 8/442, Training Loss: 0.4048\n",
      "Epoch 10/10, Batch 9/442, Training Loss: 0.3909\n",
      "Epoch 10/10, Batch 10/442, Training Loss: 0.2900\n",
      "Epoch 10/10, Batch 11/442, Training Loss: 0.4854\n",
      "Epoch 10/10, Batch 12/442, Training Loss: 0.2838\n",
      "Epoch 10/10, Batch 13/442, Training Loss: 0.3412\n",
      "Epoch 10/10, Batch 14/442, Training Loss: 0.3417\n",
      "Epoch 10/10, Batch 15/442, Training Loss: 0.4099\n",
      "Epoch 10/10, Batch 16/442, Training Loss: 0.3441\n",
      "Epoch 10/10, Batch 17/442, Training Loss: 0.2559\n",
      "Epoch 10/10, Batch 18/442, Training Loss: 0.1460\n",
      "Epoch 10/10, Batch 19/442, Training Loss: 0.5336\n",
      "Epoch 10/10, Batch 20/442, Training Loss: 0.4717\n",
      "Epoch 10/10, Batch 21/442, Training Loss: 0.6887\n",
      "Epoch 10/10, Batch 22/442, Training Loss: 0.4314\n",
      "Epoch 10/10, Batch 23/442, Training Loss: 0.6094\n",
      "Epoch 10/10, Batch 24/442, Training Loss: 0.3122\n",
      "Epoch 10/10, Batch 25/442, Training Loss: 0.4565\n",
      "Epoch 10/10, Batch 26/442, Training Loss: 0.2534\n",
      "Epoch 10/10, Batch 27/442, Training Loss: 0.4199\n",
      "Epoch 10/10, Batch 28/442, Training Loss: 0.3176\n",
      "Epoch 10/10, Batch 29/442, Training Loss: 0.3240\n",
      "Epoch 10/10, Batch 30/442, Training Loss: 0.2401\n",
      "Epoch 10/10, Batch 31/442, Training Loss: 0.7977\n",
      "Epoch 10/10, Batch 32/442, Training Loss: 0.3461\n",
      "Epoch 10/10, Batch 33/442, Training Loss: 0.3841\n",
      "Epoch 10/10, Batch 34/442, Training Loss: 0.3807\n",
      "Epoch 10/10, Batch 35/442, Training Loss: 0.4265\n",
      "Epoch 10/10, Batch 36/442, Training Loss: 0.4920\n",
      "Epoch 10/10, Batch 37/442, Training Loss: 0.3666\n",
      "Epoch 10/10, Batch 38/442, Training Loss: 0.4512\n",
      "Epoch 10/10, Batch 39/442, Training Loss: 0.3937\n",
      "Epoch 10/10, Batch 40/442, Training Loss: 0.2513\n",
      "Epoch 10/10, Batch 41/442, Training Loss: 0.3689\n",
      "Epoch 10/10, Batch 42/442, Training Loss: 0.3032\n",
      "Epoch 10/10, Batch 43/442, Training Loss: 0.3840\n",
      "Epoch 10/10, Batch 44/442, Training Loss: 0.5007\n",
      "Epoch 10/10, Batch 45/442, Training Loss: 0.3194\n",
      "Epoch 10/10, Batch 46/442, Training Loss: 0.2326\n",
      "Epoch 10/10, Batch 47/442, Training Loss: 0.4619\n",
      "Epoch 10/10, Batch 48/442, Training Loss: 0.3014\n",
      "Epoch 10/10, Batch 49/442, Training Loss: 0.7032\n",
      "Epoch 10/10, Batch 50/442, Training Loss: 0.4134\n",
      "Epoch 10/10, Batch 51/442, Training Loss: 0.5201\n",
      "Epoch 10/10, Batch 52/442, Training Loss: 0.4549\n",
      "Epoch 10/10, Batch 53/442, Training Loss: 0.2471\n",
      "Epoch 10/10, Batch 54/442, Training Loss: 0.3818\n",
      "Epoch 10/10, Batch 55/442, Training Loss: 0.6586\n",
      "Epoch 10/10, Batch 56/442, Training Loss: 0.3480\n",
      "Epoch 10/10, Batch 57/442, Training Loss: 0.5109\n",
      "Epoch 10/10, Batch 58/442, Training Loss: 0.3558\n",
      "Epoch 10/10, Batch 59/442, Training Loss: 0.2438\n",
      "Epoch 10/10, Batch 60/442, Training Loss: 0.3156\n",
      "Epoch 10/10, Batch 61/442, Training Loss: 0.3900\n",
      "Epoch 10/10, Batch 62/442, Training Loss: 0.4875\n",
      "Epoch 10/10, Batch 63/442, Training Loss: 0.4615\n",
      "Epoch 10/10, Batch 64/442, Training Loss: 0.4868\n",
      "Epoch 10/10, Batch 65/442, Training Loss: 0.4816\n",
      "Epoch 10/10, Batch 66/442, Training Loss: 0.6276\n",
      "Epoch 10/10, Batch 67/442, Training Loss: 0.7339\n",
      "Epoch 10/10, Batch 68/442, Training Loss: 0.3489\n",
      "Epoch 10/10, Batch 69/442, Training Loss: 0.3948\n",
      "Epoch 10/10, Batch 70/442, Training Loss: 0.2539\n",
      "Epoch 10/10, Batch 71/442, Training Loss: 0.3541\n",
      "Epoch 10/10, Batch 72/442, Training Loss: 0.4980\n",
      "Epoch 10/10, Batch 73/442, Training Loss: 0.2795\n",
      "Epoch 10/10, Batch 74/442, Training Loss: 0.4147\n",
      "Epoch 10/10, Batch 75/442, Training Loss: 0.4066\n",
      "Epoch 10/10, Batch 76/442, Training Loss: 0.3036\n",
      "Epoch 10/10, Batch 77/442, Training Loss: 0.2539\n",
      "Epoch 10/10, Batch 78/442, Training Loss: 0.4445\n",
      "Epoch 10/10, Batch 79/442, Training Loss: 0.2171\n",
      "Epoch 10/10, Batch 80/442, Training Loss: 0.4568\n",
      "Epoch 10/10, Batch 81/442, Training Loss: 0.2029\n",
      "Epoch 10/10, Batch 82/442, Training Loss: 0.3414\n",
      "Epoch 10/10, Batch 83/442, Training Loss: 0.3251\n",
      "Epoch 10/10, Batch 84/442, Training Loss: 0.4557\n",
      "Epoch 10/10, Batch 85/442, Training Loss: 0.4047\n",
      "Epoch 10/10, Batch 86/442, Training Loss: 0.5666\n",
      "Epoch 10/10, Batch 87/442, Training Loss: 0.4750\n",
      "Epoch 10/10, Batch 88/442, Training Loss: 0.1823\n",
      "Epoch 10/10, Batch 89/442, Training Loss: 0.7144\n",
      "Epoch 10/10, Batch 90/442, Training Loss: 0.5307\n",
      "Epoch 10/10, Batch 91/442, Training Loss: 0.6990\n",
      "Epoch 10/10, Batch 92/442, Training Loss: 0.3378\n",
      "Epoch 10/10, Batch 93/442, Training Loss: 0.4038\n",
      "Epoch 10/10, Batch 94/442, Training Loss: 0.2823\n",
      "Epoch 10/10, Batch 95/442, Training Loss: 0.6089\n",
      "Epoch 10/10, Batch 96/442, Training Loss: 0.4456\n",
      "Epoch 10/10, Batch 97/442, Training Loss: 0.3668\n",
      "Epoch 10/10, Batch 98/442, Training Loss: 0.5100\n",
      "Epoch 10/10, Batch 99/442, Training Loss: 0.6328\n",
      "Epoch 10/10, Batch 100/442, Training Loss: 0.4151\n",
      "Epoch 10/10, Batch 101/442, Training Loss: 0.3323\n",
      "Epoch 10/10, Batch 102/442, Training Loss: 0.3962\n",
      "Epoch 10/10, Batch 103/442, Training Loss: 0.5588\n",
      "Epoch 10/10, Batch 104/442, Training Loss: 0.3084\n",
      "Epoch 10/10, Batch 105/442, Training Loss: 0.5307\n",
      "Epoch 10/10, Batch 106/442, Training Loss: 0.5289\n",
      "Epoch 10/10, Batch 107/442, Training Loss: 0.2841\n",
      "Epoch 10/10, Batch 108/442, Training Loss: 0.3619\n",
      "Epoch 10/10, Batch 109/442, Training Loss: 0.3463\n",
      "Epoch 10/10, Batch 110/442, Training Loss: 0.3490\n",
      "Epoch 10/10, Batch 111/442, Training Loss: 0.3256\n",
      "Epoch 10/10, Batch 112/442, Training Loss: 0.4070\n",
      "Epoch 10/10, Batch 113/442, Training Loss: 0.4793\n",
      "Epoch 10/10, Batch 114/442, Training Loss: 0.5778\n",
      "Epoch 10/10, Batch 115/442, Training Loss: 0.6755\n",
      "Epoch 10/10, Batch 116/442, Training Loss: 0.4204\n",
      "Epoch 10/10, Batch 117/442, Training Loss: 0.4881\n",
      "Epoch 10/10, Batch 118/442, Training Loss: 0.5613\n",
      "Epoch 10/10, Batch 119/442, Training Loss: 0.5359\n",
      "Epoch 10/10, Batch 120/442, Training Loss: 0.2981\n",
      "Epoch 10/10, Batch 121/442, Training Loss: 0.4219\n",
      "Epoch 10/10, Batch 122/442, Training Loss: 0.4848\n",
      "Epoch 10/10, Batch 123/442, Training Loss: 0.5126\n",
      "Epoch 10/10, Batch 124/442, Training Loss: 0.4700\n",
      "Epoch 10/10, Batch 125/442, Training Loss: 0.4256\n",
      "Epoch 10/10, Batch 126/442, Training Loss: 0.4155\n",
      "Epoch 10/10, Batch 127/442, Training Loss: 0.5571\n",
      "Epoch 10/10, Batch 128/442, Training Loss: 0.4540\n",
      "Epoch 10/10, Batch 129/442, Training Loss: 0.4400\n",
      "Epoch 10/10, Batch 130/442, Training Loss: 0.4489\n",
      "Epoch 10/10, Batch 131/442, Training Loss: 0.5725\n",
      "Epoch 10/10, Batch 132/442, Training Loss: 0.4675\n",
      "Epoch 10/10, Batch 133/442, Training Loss: 0.5320\n",
      "Epoch 10/10, Batch 134/442, Training Loss: 0.3827\n",
      "Epoch 10/10, Batch 135/442, Training Loss: 0.4413\n",
      "Epoch 10/10, Batch 136/442, Training Loss: 0.4813\n",
      "Epoch 10/10, Batch 137/442, Training Loss: 0.3074\n",
      "Epoch 10/10, Batch 138/442, Training Loss: 0.3078\n",
      "Epoch 10/10, Batch 139/442, Training Loss: 0.1800\n",
      "Epoch 10/10, Batch 140/442, Training Loss: 0.4185\n",
      "Epoch 10/10, Batch 141/442, Training Loss: 0.4830\n",
      "Epoch 10/10, Batch 142/442, Training Loss: 0.3435\n",
      "Epoch 10/10, Batch 143/442, Training Loss: 0.4524\n",
      "Epoch 10/10, Batch 144/442, Training Loss: 0.3157\n",
      "Epoch 10/10, Batch 145/442, Training Loss: 0.4051\n",
      "Epoch 10/10, Batch 146/442, Training Loss: 0.3718\n",
      "Epoch 10/10, Batch 147/442, Training Loss: 0.4011\n",
      "Epoch 10/10, Batch 148/442, Training Loss: 0.3231\n",
      "Epoch 10/10, Batch 149/442, Training Loss: 0.2919\n",
      "Epoch 10/10, Batch 150/442, Training Loss: 0.3848\n",
      "Epoch 10/10, Batch 151/442, Training Loss: 0.3516\n",
      "Epoch 10/10, Batch 152/442, Training Loss: 0.6560\n",
      "Epoch 10/10, Batch 153/442, Training Loss: 0.4451\n",
      "Epoch 10/10, Batch 154/442, Training Loss: 0.2178\n",
      "Epoch 10/10, Batch 155/442, Training Loss: 0.2963\n",
      "Epoch 10/10, Batch 156/442, Training Loss: 0.3344\n",
      "Epoch 10/10, Batch 157/442, Training Loss: 0.4985\n",
      "Epoch 10/10, Batch 158/442, Training Loss: 0.4193\n",
      "Epoch 10/10, Batch 159/442, Training Loss: 0.5255\n",
      "Epoch 10/10, Batch 160/442, Training Loss: 0.4185\n",
      "Epoch 10/10, Batch 161/442, Training Loss: 0.2910\n",
      "Epoch 10/10, Batch 162/442, Training Loss: 0.3494\n",
      "Epoch 10/10, Batch 163/442, Training Loss: 0.3236\n",
      "Epoch 10/10, Batch 164/442, Training Loss: 0.2809\n",
      "Epoch 10/10, Batch 165/442, Training Loss: 0.5998\n",
      "Epoch 10/10, Batch 166/442, Training Loss: 0.4911\n",
      "Epoch 10/10, Batch 167/442, Training Loss: 0.7414\n",
      "Epoch 10/10, Batch 168/442, Training Loss: 0.3604\n",
      "Epoch 10/10, Batch 169/442, Training Loss: 0.1551\n",
      "Epoch 10/10, Batch 170/442, Training Loss: 0.5236\n",
      "Epoch 10/10, Batch 171/442, Training Loss: 0.3144\n",
      "Epoch 10/10, Batch 172/442, Training Loss: 0.5053\n",
      "Epoch 10/10, Batch 173/442, Training Loss: 0.2969\n",
      "Epoch 10/10, Batch 174/442, Training Loss: 0.3711\n",
      "Epoch 10/10, Batch 175/442, Training Loss: 0.2512\n",
      "Epoch 10/10, Batch 176/442, Training Loss: 0.3814\n",
      "Epoch 10/10, Batch 177/442, Training Loss: 0.3081\n",
      "Epoch 10/10, Batch 178/442, Training Loss: 0.5319\n",
      "Epoch 10/10, Batch 179/442, Training Loss: 0.4459\n",
      "Epoch 10/10, Batch 180/442, Training Loss: 0.7556\n",
      "Epoch 10/10, Batch 181/442, Training Loss: 0.3236\n",
      "Epoch 10/10, Batch 182/442, Training Loss: 0.3986\n",
      "Epoch 10/10, Batch 183/442, Training Loss: 0.2956\n",
      "Epoch 10/10, Batch 184/442, Training Loss: 0.4247\n",
      "Epoch 10/10, Batch 185/442, Training Loss: 0.3678\n",
      "Epoch 10/10, Batch 186/442, Training Loss: 0.3864\n",
      "Epoch 10/10, Batch 187/442, Training Loss: 0.3283\n",
      "Epoch 10/10, Batch 188/442, Training Loss: 0.5300\n",
      "Epoch 10/10, Batch 189/442, Training Loss: 0.2335\n",
      "Epoch 10/10, Batch 190/442, Training Loss: 0.3736\n",
      "Epoch 10/10, Batch 191/442, Training Loss: 0.3768\n",
      "Epoch 10/10, Batch 192/442, Training Loss: 0.4404\n",
      "Epoch 10/10, Batch 193/442, Training Loss: 0.5160\n",
      "Epoch 10/10, Batch 194/442, Training Loss: 0.2131\n",
      "Epoch 10/10, Batch 195/442, Training Loss: 0.3592\n",
      "Epoch 10/10, Batch 196/442, Training Loss: 0.4884\n",
      "Epoch 10/10, Batch 197/442, Training Loss: 0.4423\n",
      "Epoch 10/10, Batch 198/442, Training Loss: 0.2552\n",
      "Epoch 10/10, Batch 199/442, Training Loss: 0.3931\n",
      "Epoch 10/10, Batch 200/442, Training Loss: 0.2657\n",
      "Epoch 10/10, Batch 201/442, Training Loss: 0.2526\n",
      "Epoch 10/10, Batch 202/442, Training Loss: 0.4666\n",
      "Epoch 10/10, Batch 203/442, Training Loss: 0.4451\n",
      "Epoch 10/10, Batch 204/442, Training Loss: 0.1794\n",
      "Epoch 10/10, Batch 205/442, Training Loss: 0.3282\n",
      "Epoch 10/10, Batch 206/442, Training Loss: 0.4589\n",
      "Epoch 10/10, Batch 207/442, Training Loss: 0.5946\n",
      "Epoch 10/10, Batch 208/442, Training Loss: 0.5905\n",
      "Epoch 10/10, Batch 209/442, Training Loss: 0.5958\n",
      "Epoch 10/10, Batch 210/442, Training Loss: 0.3159\n",
      "Epoch 10/10, Batch 211/442, Training Loss: 0.3145\n",
      "Epoch 10/10, Batch 212/442, Training Loss: 0.4704\n",
      "Epoch 10/10, Batch 213/442, Training Loss: 0.6199\n",
      "Epoch 10/10, Batch 214/442, Training Loss: 0.4514\n",
      "Epoch 10/10, Batch 215/442, Training Loss: 0.3673\n",
      "Epoch 10/10, Batch 216/442, Training Loss: 0.5486\n",
      "Epoch 10/10, Batch 217/442, Training Loss: 0.6474\n",
      "Epoch 10/10, Batch 218/442, Training Loss: 0.4853\n",
      "Epoch 10/10, Batch 219/442, Training Loss: 0.5775\n",
      "Epoch 10/10, Batch 220/442, Training Loss: 0.6474\n",
      "Epoch 10/10, Batch 221/442, Training Loss: 0.4205\n",
      "Epoch 10/10, Batch 222/442, Training Loss: 0.3877\n",
      "Epoch 10/10, Batch 223/442, Training Loss: 0.5609\n",
      "Epoch 10/10, Batch 224/442, Training Loss: 0.3411\n",
      "Epoch 10/10, Batch 225/442, Training Loss: 0.6221\n",
      "Epoch 10/10, Batch 226/442, Training Loss: 0.4517\n",
      "Epoch 10/10, Batch 227/442, Training Loss: 0.3605\n",
      "Epoch 10/10, Batch 228/442, Training Loss: 0.5767\n",
      "Epoch 10/10, Batch 229/442, Training Loss: 0.5712\n",
      "Epoch 10/10, Batch 230/442, Training Loss: 0.3812\n",
      "Epoch 10/10, Batch 231/442, Training Loss: 0.2920\n",
      "Epoch 10/10, Batch 232/442, Training Loss: 0.4116\n",
      "Epoch 10/10, Batch 233/442, Training Loss: 0.5410\n",
      "Epoch 10/10, Batch 234/442, Training Loss: 0.3750\n",
      "Epoch 10/10, Batch 235/442, Training Loss: 0.6047\n",
      "Epoch 10/10, Batch 236/442, Training Loss: 0.3855\n",
      "Epoch 10/10, Batch 237/442, Training Loss: 0.3682\n",
      "Epoch 10/10, Batch 238/442, Training Loss: 0.5322\n",
      "Epoch 10/10, Batch 239/442, Training Loss: 0.6246\n",
      "Epoch 10/10, Batch 240/442, Training Loss: 0.3983\n",
      "Epoch 10/10, Batch 241/442, Training Loss: 0.3118\n",
      "Epoch 10/10, Batch 242/442, Training Loss: 0.4650\n",
      "Epoch 10/10, Batch 243/442, Training Loss: 0.3602\n",
      "Epoch 10/10, Batch 244/442, Training Loss: 0.5929\n",
      "Epoch 10/10, Batch 245/442, Training Loss: 0.5208\n",
      "Epoch 10/10, Batch 246/442, Training Loss: 0.2659\n",
      "Epoch 10/10, Batch 247/442, Training Loss: 0.4340\n",
      "Epoch 10/10, Batch 248/442, Training Loss: 0.3872\n",
      "Epoch 10/10, Batch 249/442, Training Loss: 0.3648\n",
      "Epoch 10/10, Batch 250/442, Training Loss: 0.2759\n",
      "Epoch 10/10, Batch 251/442, Training Loss: 0.2936\n",
      "Epoch 10/10, Batch 252/442, Training Loss: 0.2162\n",
      "Epoch 10/10, Batch 253/442, Training Loss: 0.3379\n",
      "Epoch 10/10, Batch 254/442, Training Loss: 0.5373\n",
      "Epoch 10/10, Batch 255/442, Training Loss: 0.4201\n",
      "Epoch 10/10, Batch 256/442, Training Loss: 0.2930\n",
      "Epoch 10/10, Batch 257/442, Training Loss: 0.3030\n",
      "Epoch 10/10, Batch 258/442, Training Loss: 0.4593\n",
      "Epoch 10/10, Batch 259/442, Training Loss: 0.2270\n",
      "Epoch 10/10, Batch 260/442, Training Loss: 0.2687\n",
      "Epoch 10/10, Batch 261/442, Training Loss: 0.6008\n",
      "Epoch 10/10, Batch 262/442, Training Loss: 0.4422\n",
      "Epoch 10/10, Batch 263/442, Training Loss: 0.4787\n",
      "Epoch 10/10, Batch 264/442, Training Loss: 0.3756\n",
      "Epoch 10/10, Batch 265/442, Training Loss: 0.2811\n",
      "Epoch 10/10, Batch 266/442, Training Loss: 0.4282\n",
      "Epoch 10/10, Batch 267/442, Training Loss: 0.4103\n",
      "Epoch 10/10, Batch 268/442, Training Loss: 0.2905\n",
      "Epoch 10/10, Batch 269/442, Training Loss: 0.4654\n",
      "Epoch 10/10, Batch 270/442, Training Loss: 0.3761\n",
      "Epoch 10/10, Batch 271/442, Training Loss: 0.3946\n",
      "Epoch 10/10, Batch 272/442, Training Loss: 0.4856\n",
      "Epoch 10/10, Batch 273/442, Training Loss: 0.3750\n",
      "Epoch 10/10, Batch 274/442, Training Loss: 0.2553\n",
      "Epoch 10/10, Batch 275/442, Training Loss: 0.4727\n",
      "Epoch 10/10, Batch 276/442, Training Loss: 0.3691\n",
      "Epoch 10/10, Batch 277/442, Training Loss: 0.6858\n",
      "Epoch 10/10, Batch 278/442, Training Loss: 0.4656\n",
      "Epoch 10/10, Batch 279/442, Training Loss: 0.6725\n",
      "Epoch 10/10, Batch 280/442, Training Loss: 0.4248\n",
      "Epoch 10/10, Batch 281/442, Training Loss: 0.2852\n",
      "Epoch 10/10, Batch 282/442, Training Loss: 0.5998\n",
      "Epoch 10/10, Batch 283/442, Training Loss: 0.4110\n",
      "Epoch 10/10, Batch 284/442, Training Loss: 0.4348\n",
      "Epoch 10/10, Batch 285/442, Training Loss: 0.2354\n",
      "Epoch 10/10, Batch 286/442, Training Loss: 0.2873\n",
      "Epoch 10/10, Batch 287/442, Training Loss: 0.4338\n",
      "Epoch 10/10, Batch 288/442, Training Loss: 0.4624\n",
      "Epoch 10/10, Batch 289/442, Training Loss: 0.2835\n",
      "Epoch 10/10, Batch 290/442, Training Loss: 0.4166\n",
      "Epoch 10/10, Batch 291/442, Training Loss: 0.3842\n",
      "Epoch 10/10, Batch 292/442, Training Loss: 0.3474\n",
      "Epoch 10/10, Batch 293/442, Training Loss: 0.2181\n",
      "Epoch 10/10, Batch 294/442, Training Loss: 0.6593\n",
      "Epoch 10/10, Batch 295/442, Training Loss: 0.4191\n",
      "Epoch 10/10, Batch 296/442, Training Loss: 0.3232\n",
      "Epoch 10/10, Batch 297/442, Training Loss: 0.5402\n",
      "Epoch 10/10, Batch 298/442, Training Loss: 0.3304\n",
      "Epoch 10/10, Batch 299/442, Training Loss: 0.4482\n",
      "Epoch 10/10, Batch 300/442, Training Loss: 0.3420\n",
      "Epoch 10/10, Batch 301/442, Training Loss: 0.4237\n",
      "Epoch 10/10, Batch 302/442, Training Loss: 0.2735\n",
      "Epoch 10/10, Batch 303/442, Training Loss: 0.4614\n",
      "Epoch 10/10, Batch 304/442, Training Loss: 0.5813\n",
      "Epoch 10/10, Batch 305/442, Training Loss: 0.4548\n",
      "Epoch 10/10, Batch 306/442, Training Loss: 0.4452\n",
      "Epoch 10/10, Batch 307/442, Training Loss: 0.4430\n",
      "Epoch 10/10, Batch 308/442, Training Loss: 0.4397\n",
      "Epoch 10/10, Batch 309/442, Training Loss: 0.6647\n",
      "Epoch 10/10, Batch 310/442, Training Loss: 0.8057\n",
      "Epoch 10/10, Batch 311/442, Training Loss: 0.2621\n",
      "Epoch 10/10, Batch 312/442, Training Loss: 0.4466\n",
      "Epoch 10/10, Batch 313/442, Training Loss: 0.4732\n",
      "Epoch 10/10, Batch 314/442, Training Loss: 0.5558\n",
      "Epoch 10/10, Batch 315/442, Training Loss: 0.6044\n",
      "Epoch 10/10, Batch 316/442, Training Loss: 0.3911\n",
      "Epoch 10/10, Batch 317/442, Training Loss: 0.4479\n",
      "Epoch 10/10, Batch 318/442, Training Loss: 0.4585\n",
      "Epoch 10/10, Batch 319/442, Training Loss: 0.5545\n",
      "Epoch 10/10, Batch 320/442, Training Loss: 0.3313\n",
      "Epoch 10/10, Batch 321/442, Training Loss: 0.3384\n",
      "Epoch 10/10, Batch 322/442, Training Loss: 0.3564\n",
      "Epoch 10/10, Batch 323/442, Training Loss: 0.3533\n",
      "Epoch 10/10, Batch 324/442, Training Loss: 0.3036\n",
      "Epoch 10/10, Batch 325/442, Training Loss: 0.4380\n",
      "Epoch 10/10, Batch 326/442, Training Loss: 0.4067\n",
      "Epoch 10/10, Batch 327/442, Training Loss: 0.7197\n",
      "Epoch 10/10, Batch 328/442, Training Loss: 0.6403\n",
      "Epoch 10/10, Batch 329/442, Training Loss: 0.5627\n",
      "Epoch 10/10, Batch 330/442, Training Loss: 0.5190\n",
      "Epoch 10/10, Batch 331/442, Training Loss: 0.5505\n",
      "Epoch 10/10, Batch 332/442, Training Loss: 0.4165\n",
      "Epoch 10/10, Batch 333/442, Training Loss: 0.5605\n",
      "Epoch 10/10, Batch 334/442, Training Loss: 0.3246\n",
      "Epoch 10/10, Batch 335/442, Training Loss: 0.6262\n",
      "Epoch 10/10, Batch 336/442, Training Loss: 0.4124\n",
      "Epoch 10/10, Batch 337/442, Training Loss: 0.2422\n",
      "Epoch 10/10, Batch 338/442, Training Loss: 0.4709\n",
      "Epoch 10/10, Batch 339/442, Training Loss: 0.3754\n",
      "Epoch 10/10, Batch 340/442, Training Loss: 0.2946\n",
      "Epoch 10/10, Batch 341/442, Training Loss: 0.3986\n",
      "Epoch 10/10, Batch 342/442, Training Loss: 0.3486\n",
      "Epoch 10/10, Batch 343/442, Training Loss: 0.5952\n",
      "Epoch 10/10, Batch 344/442, Training Loss: 0.3589\n",
      "Epoch 10/10, Batch 345/442, Training Loss: 0.2752\n",
      "Epoch 10/10, Batch 346/442, Training Loss: 0.4425\n",
      "Epoch 10/10, Batch 347/442, Training Loss: 0.3419\n",
      "Epoch 10/10, Batch 348/442, Training Loss: 0.4825\n",
      "Epoch 10/10, Batch 349/442, Training Loss: 0.3623\n",
      "Epoch 10/10, Batch 350/442, Training Loss: 0.3669\n",
      "Epoch 10/10, Batch 351/442, Training Loss: 0.4093\n",
      "Epoch 10/10, Batch 352/442, Training Loss: 0.4597\n",
      "Epoch 10/10, Batch 353/442, Training Loss: 0.3587\n",
      "Epoch 10/10, Batch 354/442, Training Loss: 0.5360\n",
      "Epoch 10/10, Batch 355/442, Training Loss: 0.5117\n",
      "Epoch 10/10, Batch 356/442, Training Loss: 0.3842\n",
      "Epoch 10/10, Batch 357/442, Training Loss: 0.4560\n",
      "Epoch 10/10, Batch 358/442, Training Loss: 0.2586\n",
      "Epoch 10/10, Batch 359/442, Training Loss: 0.3215\n",
      "Epoch 10/10, Batch 360/442, Training Loss: 0.5674\n",
      "Epoch 10/10, Batch 361/442, Training Loss: 0.5728\n",
      "Epoch 10/10, Batch 362/442, Training Loss: 0.5161\n",
      "Epoch 10/10, Batch 363/442, Training Loss: 0.4225\n",
      "Epoch 10/10, Batch 364/442, Training Loss: 0.4205\n",
      "Epoch 10/10, Batch 365/442, Training Loss: 0.3437\n",
      "Epoch 10/10, Batch 366/442, Training Loss: 0.4508\n",
      "Epoch 10/10, Batch 367/442, Training Loss: 0.4134\n",
      "Epoch 10/10, Batch 368/442, Training Loss: 0.3098\n",
      "Epoch 10/10, Batch 369/442, Training Loss: 0.3437\n",
      "Epoch 10/10, Batch 370/442, Training Loss: 0.2924\n",
      "Epoch 10/10, Batch 371/442, Training Loss: 0.6741\n",
      "Epoch 10/10, Batch 372/442, Training Loss: 0.3013\n",
      "Epoch 10/10, Batch 373/442, Training Loss: 0.2554\n",
      "Epoch 10/10, Batch 374/442, Training Loss: 0.3478\n",
      "Epoch 10/10, Batch 375/442, Training Loss: 0.3444\n",
      "Epoch 10/10, Batch 376/442, Training Loss: 0.4042\n",
      "Epoch 10/10, Batch 377/442, Training Loss: 0.5070\n",
      "Epoch 10/10, Batch 378/442, Training Loss: 0.7407\n",
      "Epoch 10/10, Batch 379/442, Training Loss: 0.5518\n",
      "Epoch 10/10, Batch 380/442, Training Loss: 0.4188\n",
      "Epoch 10/10, Batch 381/442, Training Loss: 0.5852\n",
      "Epoch 10/10, Batch 382/442, Training Loss: 0.4125\n",
      "Epoch 10/10, Batch 383/442, Training Loss: 0.3310\n",
      "Epoch 10/10, Batch 384/442, Training Loss: 0.3729\n",
      "Epoch 10/10, Batch 385/442, Training Loss: 0.5204\n",
      "Epoch 10/10, Batch 386/442, Training Loss: 0.4948\n",
      "Epoch 10/10, Batch 387/442, Training Loss: 0.4671\n",
      "Epoch 10/10, Batch 388/442, Training Loss: 0.3660\n",
      "Epoch 10/10, Batch 389/442, Training Loss: 0.4295\n",
      "Epoch 10/10, Batch 390/442, Training Loss: 0.3082\n",
      "Epoch 10/10, Batch 391/442, Training Loss: 0.3881\n",
      "Epoch 10/10, Batch 392/442, Training Loss: 0.6211\n",
      "Epoch 10/10, Batch 393/442, Training Loss: 0.4534\n",
      "Epoch 10/10, Batch 394/442, Training Loss: 0.3163\n",
      "Epoch 10/10, Batch 395/442, Training Loss: 0.2596\n",
      "Epoch 10/10, Batch 396/442, Training Loss: 0.4429\n",
      "Epoch 10/10, Batch 397/442, Training Loss: 0.5086\n",
      "Epoch 10/10, Batch 398/442, Training Loss: 0.3597\n",
      "Epoch 10/10, Batch 399/442, Training Loss: 0.4897\n",
      "Epoch 10/10, Batch 400/442, Training Loss: 0.5767\n",
      "Epoch 10/10, Batch 401/442, Training Loss: 0.3028\n",
      "Epoch 10/10, Batch 402/442, Training Loss: 0.2908\n",
      "Epoch 10/10, Batch 403/442, Training Loss: 0.4917\n",
      "Epoch 10/10, Batch 404/442, Training Loss: 0.4441\n",
      "Epoch 10/10, Batch 405/442, Training Loss: 0.5146\n",
      "Epoch 10/10, Batch 406/442, Training Loss: 0.6903\n",
      "Epoch 10/10, Batch 407/442, Training Loss: 0.3967\n",
      "Epoch 10/10, Batch 408/442, Training Loss: 0.5101\n",
      "Epoch 10/10, Batch 409/442, Training Loss: 0.2798\n",
      "Epoch 10/10, Batch 410/442, Training Loss: 0.1762\n",
      "Epoch 10/10, Batch 411/442, Training Loss: 0.4717\n",
      "Epoch 10/10, Batch 412/442, Training Loss: 0.3116\n",
      "Epoch 10/10, Batch 413/442, Training Loss: 0.4271\n",
      "Epoch 10/10, Batch 414/442, Training Loss: 0.3412\n",
      "Epoch 10/10, Batch 415/442, Training Loss: 0.4140\n",
      "Epoch 10/10, Batch 416/442, Training Loss: 0.4534\n",
      "Epoch 10/10, Batch 417/442, Training Loss: 0.4820\n",
      "Epoch 10/10, Batch 418/442, Training Loss: 0.5552\n",
      "Epoch 10/10, Batch 419/442, Training Loss: 0.3865\n",
      "Epoch 10/10, Batch 420/442, Training Loss: 0.3473\n",
      "Epoch 10/10, Batch 421/442, Training Loss: 0.3211\n",
      "Epoch 10/10, Batch 422/442, Training Loss: 0.5189\n",
      "Epoch 10/10, Batch 423/442, Training Loss: 0.5042\n",
      "Epoch 10/10, Batch 424/442, Training Loss: 0.1945\n",
      "Epoch 10/10, Batch 425/442, Training Loss: 0.3313\n",
      "Epoch 10/10, Batch 426/442, Training Loss: 0.3528\n",
      "Epoch 10/10, Batch 427/442, Training Loss: 0.2418\n",
      "Epoch 10/10, Batch 428/442, Training Loss: 0.1807\n",
      "Epoch 10/10, Batch 429/442, Training Loss: 0.3813\n",
      "Epoch 10/10, Batch 430/442, Training Loss: 0.3841\n",
      "Epoch 10/10, Batch 431/442, Training Loss: 0.5222\n",
      "Epoch 10/10, Batch 432/442, Training Loss: 0.5201\n",
      "Epoch 10/10, Batch 433/442, Training Loss: 0.2757\n",
      "Epoch 10/10, Batch 434/442, Training Loss: 0.3815\n",
      "Epoch 10/10, Batch 435/442, Training Loss: 0.3542\n",
      "Epoch 10/10, Batch 436/442, Training Loss: 0.4288\n",
      "Epoch 10/10, Batch 437/442, Training Loss: 0.4822\n",
      "Epoch 10/10, Batch 438/442, Training Loss: 0.3293\n",
      "Epoch 10/10, Batch 439/442, Training Loss: 0.5705\n",
      "Epoch 10/10, Batch 440/442, Training Loss: 0.3585\n",
      "Epoch 10/10, Batch 441/442, Training Loss: 0.8481\n",
      "Epoch 10/10, Batch 442/442, Training Loss: 0.5798\n",
      "Epoch 10/10, Training Loss: 0.4207, Validation Loss: 0.4887, Validation Accuracy: 0.8070\n",
      "Test Loss: 0.4738, Test Accuracy: 0.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-27 20:23:59,560] Trial 2 finished with value: 0.446455329368563 and parameters: {'batch_size': 32, 'learning_rate': 0.0036333389999519293, 'weight_decay': 1.2022950123015283e-06}. Best is trial 0 with value: 0.2196991708036512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/883, Training Loss: 1.3854\n",
      "Epoch 1/10, Batch 2/883, Training Loss: 3.8887\n",
      "Epoch 1/10, Batch 3/883, Training Loss: 2.9946\n",
      "Epoch 1/10, Batch 4/883, Training Loss: 2.3365\n",
      "Epoch 1/10, Batch 5/883, Training Loss: 1.9075\n",
      "Epoch 1/10, Batch 6/883, Training Loss: 2.8220\n",
      "Epoch 1/10, Batch 7/883, Training Loss: 3.1399\n",
      "Epoch 1/10, Batch 8/883, Training Loss: 2.9855\n",
      "Epoch 1/10, Batch 9/883, Training Loss: 1.9982\n",
      "Epoch 1/10, Batch 10/883, Training Loss: 2.0543\n",
      "Epoch 1/10, Batch 11/883, Training Loss: 1.2037\n",
      "Epoch 1/10, Batch 12/883, Training Loss: 1.0874\n",
      "Epoch 1/10, Batch 13/883, Training Loss: 2.0908\n",
      "Epoch 1/10, Batch 14/883, Training Loss: 1.5815\n",
      "Epoch 1/10, Batch 15/883, Training Loss: 0.6989\n",
      "Epoch 1/10, Batch 16/883, Training Loss: 2.0420\n",
      "Epoch 1/10, Batch 17/883, Training Loss: 2.1085\n",
      "Epoch 1/10, Batch 18/883, Training Loss: 1.3202\n",
      "Epoch 1/10, Batch 19/883, Training Loss: 1.1209\n",
      "Epoch 1/10, Batch 20/883, Training Loss: 1.3163\n",
      "Epoch 1/10, Batch 21/883, Training Loss: 1.4990\n",
      "Epoch 1/10, Batch 22/883, Training Loss: 0.9683\n",
      "Epoch 1/10, Batch 23/883, Training Loss: 1.2375\n",
      "Epoch 1/10, Batch 24/883, Training Loss: 1.1721\n",
      "Epoch 1/10, Batch 25/883, Training Loss: 1.2217\n",
      "Epoch 1/10, Batch 26/883, Training Loss: 1.0801\n",
      "Epoch 1/10, Batch 27/883, Training Loss: 2.1956\n",
      "Epoch 1/10, Batch 28/883, Training Loss: 1.5211\n",
      "Epoch 1/10, Batch 29/883, Training Loss: 1.4138\n",
      "Epoch 1/10, Batch 30/883, Training Loss: 1.6589\n",
      "Epoch 1/10, Batch 31/883, Training Loss: 1.2373\n",
      "Epoch 1/10, Batch 32/883, Training Loss: 0.9551\n",
      "Epoch 1/10, Batch 33/883, Training Loss: 1.0162\n",
      "Epoch 1/10, Batch 34/883, Training Loss: 0.8445\n",
      "Epoch 1/10, Batch 35/883, Training Loss: 1.1227\n",
      "Epoch 1/10, Batch 36/883, Training Loss: 0.9767\n",
      "Epoch 1/10, Batch 37/883, Training Loss: 0.8519\n",
      "Epoch 1/10, Batch 38/883, Training Loss: 1.3821\n",
      "Epoch 1/10, Batch 39/883, Training Loss: 1.0912\n",
      "Epoch 1/10, Batch 40/883, Training Loss: 1.3806\n",
      "Epoch 1/10, Batch 41/883, Training Loss: 1.1074\n",
      "Epoch 1/10, Batch 42/883, Training Loss: 1.6646\n",
      "Epoch 1/10, Batch 43/883, Training Loss: 1.0806\n",
      "Epoch 1/10, Batch 44/883, Training Loss: 0.9647\n",
      "Epoch 1/10, Batch 45/883, Training Loss: 1.1154\n",
      "Epoch 1/10, Batch 46/883, Training Loss: 1.2812\n",
      "Epoch 1/10, Batch 47/883, Training Loss: 0.9647\n",
      "Epoch 1/10, Batch 48/883, Training Loss: 0.9189\n",
      "Epoch 1/10, Batch 49/883, Training Loss: 1.2579\n",
      "Epoch 1/10, Batch 50/883, Training Loss: 0.9443\n",
      "Epoch 1/10, Batch 51/883, Training Loss: 1.1672\n",
      "Epoch 1/10, Batch 52/883, Training Loss: 1.1883\n",
      "Epoch 1/10, Batch 53/883, Training Loss: 1.2061\n",
      "Epoch 1/10, Batch 54/883, Training Loss: 0.9487\n",
      "Epoch 1/10, Batch 55/883, Training Loss: 1.0810\n",
      "Epoch 1/10, Batch 56/883, Training Loss: 1.2428\n",
      "Epoch 1/10, Batch 57/883, Training Loss: 1.4192\n",
      "Epoch 1/10, Batch 58/883, Training Loss: 1.2216\n",
      "Epoch 1/10, Batch 59/883, Training Loss: 1.2378\n",
      "Epoch 1/10, Batch 60/883, Training Loss: 1.0956\n",
      "Epoch 1/10, Batch 61/883, Training Loss: 0.9453\n",
      "Epoch 1/10, Batch 62/883, Training Loss: 1.2370\n",
      "Epoch 1/10, Batch 63/883, Training Loss: 1.6047\n",
      "Epoch 1/10, Batch 64/883, Training Loss: 1.1715\n",
      "Epoch 1/10, Batch 65/883, Training Loss: 1.4222\n",
      "Epoch 1/10, Batch 66/883, Training Loss: 0.8882\n",
      "Epoch 1/10, Batch 67/883, Training Loss: 1.2498\n",
      "Epoch 1/10, Batch 68/883, Training Loss: 1.1752\n",
      "Epoch 1/10, Batch 69/883, Training Loss: 1.1441\n",
      "Epoch 1/10, Batch 70/883, Training Loss: 1.1775\n",
      "Epoch 1/10, Batch 71/883, Training Loss: 1.3512\n",
      "Epoch 1/10, Batch 72/883, Training Loss: 1.3796\n",
      "Epoch 1/10, Batch 73/883, Training Loss: 1.5224\n",
      "Epoch 1/10, Batch 74/883, Training Loss: 1.4801\n",
      "Epoch 1/10, Batch 75/883, Training Loss: 1.0589\n",
      "Epoch 1/10, Batch 76/883, Training Loss: 1.1999\n",
      "Epoch 1/10, Batch 77/883, Training Loss: 1.0319\n",
      "Epoch 1/10, Batch 78/883, Training Loss: 1.0607\n",
      "Epoch 1/10, Batch 79/883, Training Loss: 0.8726\n",
      "Epoch 1/10, Batch 80/883, Training Loss: 0.9557\n",
      "Epoch 1/10, Batch 81/883, Training Loss: 1.3339\n",
      "Epoch 1/10, Batch 82/883, Training Loss: 0.8506\n",
      "Epoch 1/10, Batch 83/883, Training Loss: 1.0370\n",
      "Epoch 1/10, Batch 84/883, Training Loss: 0.9614\n",
      "Epoch 1/10, Batch 85/883, Training Loss: 0.8999\n",
      "Epoch 1/10, Batch 86/883, Training Loss: 1.4111\n",
      "Epoch 1/10, Batch 87/883, Training Loss: 1.2371\n",
      "Epoch 1/10, Batch 88/883, Training Loss: 0.9149\n",
      "Epoch 1/10, Batch 89/883, Training Loss: 1.3310\n",
      "Epoch 1/10, Batch 90/883, Training Loss: 1.1100\n",
      "Epoch 1/10, Batch 91/883, Training Loss: 0.7918\n",
      "Epoch 1/10, Batch 92/883, Training Loss: 1.0686\n",
      "Epoch 1/10, Batch 93/883, Training Loss: 0.8757\n",
      "Epoch 1/10, Batch 94/883, Training Loss: 1.2796\n",
      "Epoch 1/10, Batch 95/883, Training Loss: 1.3406\n",
      "Epoch 1/10, Batch 96/883, Training Loss: 1.1409\n",
      "Epoch 1/10, Batch 97/883, Training Loss: 0.9392\n",
      "Epoch 1/10, Batch 98/883, Training Loss: 1.4560\n",
      "Epoch 1/10, Batch 99/883, Training Loss: 0.8313\n",
      "Epoch 1/10, Batch 100/883, Training Loss: 1.5333\n",
      "Epoch 1/10, Batch 101/883, Training Loss: 0.6890\n",
      "Epoch 1/10, Batch 102/883, Training Loss: 0.6753\n",
      "Epoch 1/10, Batch 103/883, Training Loss: 1.0252\n",
      "Epoch 1/10, Batch 104/883, Training Loss: 1.0913\n",
      "Epoch 1/10, Batch 105/883, Training Loss: 1.1885\n",
      "Epoch 1/10, Batch 106/883, Training Loss: 0.7750\n",
      "Epoch 1/10, Batch 107/883, Training Loss: 1.4102\n",
      "Epoch 1/10, Batch 108/883, Training Loss: 0.9434\n",
      "Epoch 1/10, Batch 109/883, Training Loss: 1.1479\n",
      "Epoch 1/10, Batch 110/883, Training Loss: 1.4942\n",
      "Epoch 1/10, Batch 111/883, Training Loss: 0.7844\n",
      "Epoch 1/10, Batch 112/883, Training Loss: 0.9690\n",
      "Epoch 1/10, Batch 113/883, Training Loss: 0.9619\n",
      "Epoch 1/10, Batch 114/883, Training Loss: 0.9904\n",
      "Epoch 1/10, Batch 115/883, Training Loss: 1.1761\n",
      "Epoch 1/10, Batch 116/883, Training Loss: 1.0265\n",
      "Epoch 1/10, Batch 117/883, Training Loss: 1.1174\n",
      "Epoch 1/10, Batch 118/883, Training Loss: 0.9814\n",
      "Epoch 1/10, Batch 119/883, Training Loss: 0.9641\n",
      "Epoch 1/10, Batch 120/883, Training Loss: 0.9813\n",
      "Epoch 1/10, Batch 121/883, Training Loss: 0.9319\n",
      "Epoch 1/10, Batch 122/883, Training Loss: 1.3254\n",
      "Epoch 1/10, Batch 123/883, Training Loss: 1.4217\n",
      "Epoch 1/10, Batch 124/883, Training Loss: 1.2426\n",
      "Epoch 1/10, Batch 125/883, Training Loss: 1.0835\n",
      "Epoch 1/10, Batch 126/883, Training Loss: 1.3772\n",
      "Epoch 1/10, Batch 127/883, Training Loss: 1.1764\n",
      "Epoch 1/10, Batch 128/883, Training Loss: 0.8667\n",
      "Epoch 1/10, Batch 129/883, Training Loss: 0.9126\n",
      "Epoch 1/10, Batch 130/883, Training Loss: 1.0458\n",
      "Epoch 1/10, Batch 131/883, Training Loss: 0.8594\n",
      "Epoch 1/10, Batch 132/883, Training Loss: 1.1547\n",
      "Epoch 1/10, Batch 133/883, Training Loss: 1.5096\n",
      "Epoch 1/10, Batch 134/883, Training Loss: 1.1535\n",
      "Epoch 1/10, Batch 135/883, Training Loss: 0.8201\n",
      "Epoch 1/10, Batch 136/883, Training Loss: 1.1609\n",
      "Epoch 1/10, Batch 137/883, Training Loss: 1.1602\n",
      "Epoch 1/10, Batch 138/883, Training Loss: 1.8307\n",
      "Epoch 1/10, Batch 139/883, Training Loss: 1.5346\n",
      "Epoch 1/10, Batch 140/883, Training Loss: 0.7438\n",
      "Epoch 1/10, Batch 141/883, Training Loss: 1.0651\n",
      "Epoch 1/10, Batch 142/883, Training Loss: 0.9898\n",
      "Epoch 1/10, Batch 143/883, Training Loss: 0.9855\n",
      "Epoch 1/10, Batch 144/883, Training Loss: 0.9207\n",
      "Epoch 1/10, Batch 145/883, Training Loss: 0.9424\n",
      "Epoch 1/10, Batch 146/883, Training Loss: 1.1878\n",
      "Epoch 1/10, Batch 147/883, Training Loss: 1.1050\n",
      "Epoch 1/10, Batch 148/883, Training Loss: 1.0099\n",
      "Epoch 1/10, Batch 149/883, Training Loss: 1.1428\n",
      "Epoch 1/10, Batch 150/883, Training Loss: 0.9756\n",
      "Epoch 1/10, Batch 151/883, Training Loss: 1.0733\n",
      "Epoch 1/10, Batch 152/883, Training Loss: 0.7910\n",
      "Epoch 1/10, Batch 153/883, Training Loss: 1.1379\n",
      "Epoch 1/10, Batch 154/883, Training Loss: 1.0401\n",
      "Epoch 1/10, Batch 155/883, Training Loss: 0.8734\n",
      "Epoch 1/10, Batch 156/883, Training Loss: 0.9739\n",
      "Epoch 1/10, Batch 157/883, Training Loss: 1.1668\n",
      "Epoch 1/10, Batch 158/883, Training Loss: 0.6453\n",
      "Epoch 1/10, Batch 159/883, Training Loss: 1.6293\n",
      "Epoch 1/10, Batch 160/883, Training Loss: 0.7983\n",
      "Epoch 1/10, Batch 161/883, Training Loss: 1.4385\n",
      "Epoch 1/10, Batch 162/883, Training Loss: 1.0285\n",
      "Epoch 1/10, Batch 163/883, Training Loss: 0.9018\n",
      "Epoch 1/10, Batch 164/883, Training Loss: 1.0945\n",
      "Epoch 1/10, Batch 165/883, Training Loss: 0.8261\n",
      "Epoch 1/10, Batch 166/883, Training Loss: 0.8280\n",
      "Epoch 1/10, Batch 167/883, Training Loss: 1.2565\n",
      "Epoch 1/10, Batch 168/883, Training Loss: 0.9600\n",
      "Epoch 1/10, Batch 169/883, Training Loss: 1.1717\n",
      "Epoch 1/10, Batch 170/883, Training Loss: 0.9255\n",
      "Epoch 1/10, Batch 171/883, Training Loss: 0.9892\n",
      "Epoch 1/10, Batch 172/883, Training Loss: 0.9174\n",
      "Epoch 1/10, Batch 173/883, Training Loss: 0.9741\n",
      "Epoch 1/10, Batch 174/883, Training Loss: 0.8498\n",
      "Epoch 1/10, Batch 175/883, Training Loss: 0.9036\n",
      "Epoch 1/10, Batch 176/883, Training Loss: 0.8311\n",
      "Epoch 1/10, Batch 177/883, Training Loss: 1.0118\n",
      "Epoch 1/10, Batch 178/883, Training Loss: 0.6553\n",
      "Epoch 1/10, Batch 179/883, Training Loss: 1.3241\n",
      "Epoch 1/10, Batch 180/883, Training Loss: 0.9817\n",
      "Epoch 1/10, Batch 181/883, Training Loss: 1.1547\n",
      "Epoch 1/10, Batch 182/883, Training Loss: 0.8848\n",
      "Epoch 1/10, Batch 183/883, Training Loss: 0.9348\n",
      "Epoch 1/10, Batch 184/883, Training Loss: 0.8831\n",
      "Epoch 1/10, Batch 185/883, Training Loss: 1.2209\n",
      "Epoch 1/10, Batch 186/883, Training Loss: 1.3632\n",
      "Epoch 1/10, Batch 187/883, Training Loss: 1.0538\n",
      "Epoch 1/10, Batch 188/883, Training Loss: 1.0249\n",
      "Epoch 1/10, Batch 189/883, Training Loss: 1.1366\n",
      "Epoch 1/10, Batch 190/883, Training Loss: 0.7694\n",
      "Epoch 1/10, Batch 191/883, Training Loss: 0.8232\n",
      "Epoch 1/10, Batch 192/883, Training Loss: 1.1725\n",
      "Epoch 1/10, Batch 193/883, Training Loss: 0.9156\n",
      "Epoch 1/10, Batch 194/883, Training Loss: 0.9285\n",
      "Epoch 1/10, Batch 195/883, Training Loss: 1.0460\n",
      "Epoch 1/10, Batch 196/883, Training Loss: 0.8200\n",
      "Epoch 1/10, Batch 197/883, Training Loss: 1.0538\n",
      "Epoch 1/10, Batch 198/883, Training Loss: 0.9969\n",
      "Epoch 1/10, Batch 199/883, Training Loss: 1.1513\n",
      "Epoch 1/10, Batch 200/883, Training Loss: 0.9477\n",
      "Epoch 1/10, Batch 201/883, Training Loss: 0.9989\n",
      "Epoch 1/10, Batch 202/883, Training Loss: 1.1574\n",
      "Epoch 1/10, Batch 203/883, Training Loss: 1.3261\n",
      "Epoch 1/10, Batch 204/883, Training Loss: 0.9350\n",
      "Epoch 1/10, Batch 205/883, Training Loss: 1.0733\n",
      "Epoch 1/10, Batch 206/883, Training Loss: 0.8004\n",
      "Epoch 1/10, Batch 207/883, Training Loss: 1.0413\n",
      "Epoch 1/10, Batch 208/883, Training Loss: 0.8606\n",
      "Epoch 1/10, Batch 209/883, Training Loss: 1.0656\n",
      "Epoch 1/10, Batch 210/883, Training Loss: 0.9109\n",
      "Epoch 1/10, Batch 211/883, Training Loss: 1.1598\n",
      "Epoch 1/10, Batch 212/883, Training Loss: 0.9658\n",
      "Epoch 1/10, Batch 213/883, Training Loss: 0.9847\n",
      "Epoch 1/10, Batch 214/883, Training Loss: 0.8882\n",
      "Epoch 1/10, Batch 215/883, Training Loss: 0.8945\n",
      "Epoch 1/10, Batch 216/883, Training Loss: 0.8752\n",
      "Epoch 1/10, Batch 217/883, Training Loss: 1.2205\n",
      "Epoch 1/10, Batch 218/883, Training Loss: 0.6670\n",
      "Epoch 1/10, Batch 219/883, Training Loss: 1.1303\n",
      "Epoch 1/10, Batch 220/883, Training Loss: 1.4481\n",
      "Epoch 1/10, Batch 221/883, Training Loss: 1.1525\n",
      "Epoch 1/10, Batch 222/883, Training Loss: 1.0507\n",
      "Epoch 1/10, Batch 223/883, Training Loss: 1.0432\n",
      "Epoch 1/10, Batch 224/883, Training Loss: 1.0546\n",
      "Epoch 1/10, Batch 225/883, Training Loss: 0.9321\n",
      "Epoch 1/10, Batch 226/883, Training Loss: 1.0777\n",
      "Epoch 1/10, Batch 227/883, Training Loss: 0.9955\n",
      "Epoch 1/10, Batch 228/883, Training Loss: 0.9813\n",
      "Epoch 1/10, Batch 229/883, Training Loss: 0.9524\n",
      "Epoch 1/10, Batch 230/883, Training Loss: 0.9596\n",
      "Epoch 1/10, Batch 231/883, Training Loss: 0.8462\n",
      "Epoch 1/10, Batch 232/883, Training Loss: 0.8638\n",
      "Epoch 1/10, Batch 233/883, Training Loss: 0.9767\n",
      "Epoch 1/10, Batch 234/883, Training Loss: 1.0016\n",
      "Epoch 1/10, Batch 235/883, Training Loss: 0.8720\n",
      "Epoch 1/10, Batch 236/883, Training Loss: 0.8847\n",
      "Epoch 1/10, Batch 237/883, Training Loss: 1.1082\n",
      "Epoch 1/10, Batch 238/883, Training Loss: 1.0958\n",
      "Epoch 1/10, Batch 239/883, Training Loss: 1.0971\n",
      "Epoch 1/10, Batch 240/883, Training Loss: 0.9457\n",
      "Epoch 1/10, Batch 241/883, Training Loss: 1.0398\n",
      "Epoch 1/10, Batch 242/883, Training Loss: 0.8939\n",
      "Epoch 1/10, Batch 243/883, Training Loss: 0.9933\n",
      "Epoch 1/10, Batch 244/883, Training Loss: 1.3344\n",
      "Epoch 1/10, Batch 245/883, Training Loss: 0.8548\n",
      "Epoch 1/10, Batch 246/883, Training Loss: 0.9214\n",
      "Epoch 1/10, Batch 247/883, Training Loss: 0.9170\n",
      "Epoch 1/10, Batch 248/883, Training Loss: 0.6655\n",
      "Epoch 1/10, Batch 249/883, Training Loss: 1.0662\n",
      "Epoch 1/10, Batch 250/883, Training Loss: 0.8816\n",
      "Epoch 1/10, Batch 251/883, Training Loss: 0.8598\n",
      "Epoch 1/10, Batch 252/883, Training Loss: 1.0550\n",
      "Epoch 1/10, Batch 253/883, Training Loss: 1.0921\n",
      "Epoch 1/10, Batch 254/883, Training Loss: 1.2046\n",
      "Epoch 1/10, Batch 255/883, Training Loss: 1.0076\n",
      "Epoch 1/10, Batch 256/883, Training Loss: 0.8762\n",
      "Epoch 1/10, Batch 257/883, Training Loss: 0.8564\n",
      "Epoch 1/10, Batch 258/883, Training Loss: 0.7941\n",
      "Epoch 1/10, Batch 259/883, Training Loss: 0.8451\n",
      "Epoch 1/10, Batch 260/883, Training Loss: 1.0367\n",
      "Epoch 1/10, Batch 261/883, Training Loss: 0.8894\n",
      "Epoch 1/10, Batch 262/883, Training Loss: 1.0119\n",
      "Epoch 1/10, Batch 263/883, Training Loss: 0.9235\n",
      "Epoch 1/10, Batch 264/883, Training Loss: 1.5405\n",
      "Epoch 1/10, Batch 265/883, Training Loss: 0.8670\n",
      "Epoch 1/10, Batch 266/883, Training Loss: 1.0620\n",
      "Epoch 1/10, Batch 267/883, Training Loss: 0.7206\n",
      "Epoch 1/10, Batch 268/883, Training Loss: 1.0281\n",
      "Epoch 1/10, Batch 269/883, Training Loss: 1.0127\n",
      "Epoch 1/10, Batch 270/883, Training Loss: 0.8589\n",
      "Epoch 1/10, Batch 271/883, Training Loss: 0.7702\n",
      "Epoch 1/10, Batch 272/883, Training Loss: 1.0124\n",
      "Epoch 1/10, Batch 273/883, Training Loss: 1.0121\n",
      "Epoch 1/10, Batch 274/883, Training Loss: 0.9488\n",
      "Epoch 1/10, Batch 275/883, Training Loss: 0.9334\n",
      "Epoch 1/10, Batch 276/883, Training Loss: 0.9217\n",
      "Epoch 1/10, Batch 277/883, Training Loss: 0.8372\n",
      "Epoch 1/10, Batch 278/883, Training Loss: 0.7591\n",
      "Epoch 1/10, Batch 279/883, Training Loss: 0.6920\n",
      "Epoch 1/10, Batch 280/883, Training Loss: 0.8025\n",
      "Epoch 1/10, Batch 281/883, Training Loss: 0.8205\n",
      "Epoch 1/10, Batch 282/883, Training Loss: 0.7270\n",
      "Epoch 1/10, Batch 283/883, Training Loss: 1.0182\n",
      "Epoch 1/10, Batch 284/883, Training Loss: 0.8327\n",
      "Epoch 1/10, Batch 285/883, Training Loss: 0.8230\n",
      "Epoch 1/10, Batch 286/883, Training Loss: 0.5922\n",
      "Epoch 1/10, Batch 287/883, Training Loss: 0.6235\n",
      "Epoch 1/10, Batch 288/883, Training Loss: 1.2140\n",
      "Epoch 1/10, Batch 289/883, Training Loss: 1.3490\n",
      "Epoch 1/10, Batch 290/883, Training Loss: 0.7924\n",
      "Epoch 1/10, Batch 291/883, Training Loss: 1.0443\n",
      "Epoch 1/10, Batch 292/883, Training Loss: 0.8903\n",
      "Epoch 1/10, Batch 293/883, Training Loss: 0.9960\n",
      "Epoch 1/10, Batch 294/883, Training Loss: 1.2878\n",
      "Epoch 1/10, Batch 295/883, Training Loss: 0.7159\n",
      "Epoch 1/10, Batch 296/883, Training Loss: 0.7874\n",
      "Epoch 1/10, Batch 297/883, Training Loss: 0.9549\n",
      "Epoch 1/10, Batch 298/883, Training Loss: 0.9495\n",
      "Epoch 1/10, Batch 299/883, Training Loss: 1.1088\n",
      "Epoch 1/10, Batch 300/883, Training Loss: 0.7807\n",
      "Epoch 1/10, Batch 301/883, Training Loss: 1.0246\n",
      "Epoch 1/10, Batch 302/883, Training Loss: 0.8909\n",
      "Epoch 1/10, Batch 303/883, Training Loss: 0.9153\n",
      "Epoch 1/10, Batch 304/883, Training Loss: 1.0977\n",
      "Epoch 1/10, Batch 305/883, Training Loss: 0.7639\n",
      "Epoch 1/10, Batch 306/883, Training Loss: 1.0207\n",
      "Epoch 1/10, Batch 307/883, Training Loss: 1.0100\n",
      "Epoch 1/10, Batch 308/883, Training Loss: 1.1271\n",
      "Epoch 1/10, Batch 309/883, Training Loss: 0.8963\n",
      "Epoch 1/10, Batch 310/883, Training Loss: 0.6111\n",
      "Epoch 1/10, Batch 311/883, Training Loss: 0.8785\n",
      "Epoch 1/10, Batch 312/883, Training Loss: 0.6157\n",
      "Epoch 1/10, Batch 313/883, Training Loss: 1.2559\n",
      "Epoch 1/10, Batch 314/883, Training Loss: 0.7892\n",
      "Epoch 1/10, Batch 315/883, Training Loss: 1.0010\n",
      "Epoch 1/10, Batch 316/883, Training Loss: 0.8856\n",
      "Epoch 1/10, Batch 317/883, Training Loss: 0.9650\n",
      "Epoch 1/10, Batch 318/883, Training Loss: 0.8763\n",
      "Epoch 1/10, Batch 319/883, Training Loss: 0.8184\n",
      "Epoch 1/10, Batch 320/883, Training Loss: 0.7716\n",
      "Epoch 1/10, Batch 321/883, Training Loss: 0.8792\n",
      "Epoch 1/10, Batch 322/883, Training Loss: 0.8710\n",
      "Epoch 1/10, Batch 323/883, Training Loss: 0.8101\n",
      "Epoch 1/10, Batch 324/883, Training Loss: 0.8166\n",
      "Epoch 1/10, Batch 325/883, Training Loss: 0.7893\n",
      "Epoch 1/10, Batch 326/883, Training Loss: 0.8988\n",
      "Epoch 1/10, Batch 327/883, Training Loss: 1.5385\n",
      "Epoch 1/10, Batch 328/883, Training Loss: 0.9558\n",
      "Epoch 1/10, Batch 329/883, Training Loss: 1.2438\n",
      "Epoch 1/10, Batch 330/883, Training Loss: 0.8192\n",
      "Epoch 1/10, Batch 331/883, Training Loss: 0.9149\n",
      "Epoch 1/10, Batch 332/883, Training Loss: 0.9673\n",
      "Epoch 1/10, Batch 333/883, Training Loss: 0.7958\n",
      "Epoch 1/10, Batch 334/883, Training Loss: 0.8518\n",
      "Epoch 1/10, Batch 335/883, Training Loss: 0.8397\n",
      "Epoch 1/10, Batch 336/883, Training Loss: 0.8920\n",
      "Epoch 1/10, Batch 337/883, Training Loss: 0.8577\n",
      "Epoch 1/10, Batch 338/883, Training Loss: 1.0853\n",
      "Epoch 1/10, Batch 339/883, Training Loss: 0.7103\n",
      "Epoch 1/10, Batch 340/883, Training Loss: 0.9487\n",
      "Epoch 1/10, Batch 341/883, Training Loss: 1.0225\n",
      "Epoch 1/10, Batch 342/883, Training Loss: 0.7889\n",
      "Epoch 1/10, Batch 343/883, Training Loss: 1.0211\n",
      "Epoch 1/10, Batch 344/883, Training Loss: 0.9495\n",
      "Epoch 1/10, Batch 345/883, Training Loss: 0.9647\n",
      "Epoch 1/10, Batch 346/883, Training Loss: 1.0765\n",
      "Epoch 1/10, Batch 347/883, Training Loss: 1.0372\n",
      "Epoch 1/10, Batch 348/883, Training Loss: 0.9854\n",
      "Epoch 1/10, Batch 349/883, Training Loss: 0.7866\n",
      "Epoch 1/10, Batch 350/883, Training Loss: 0.8907\n",
      "Epoch 1/10, Batch 351/883, Training Loss: 1.1544\n",
      "Epoch 1/10, Batch 352/883, Training Loss: 0.8154\n",
      "Epoch 1/10, Batch 353/883, Training Loss: 0.8958\n",
      "Epoch 1/10, Batch 354/883, Training Loss: 0.9909\n",
      "Epoch 1/10, Batch 355/883, Training Loss: 0.9147\n",
      "Epoch 1/10, Batch 356/883, Training Loss: 0.9888\n",
      "Epoch 1/10, Batch 357/883, Training Loss: 0.8659\n",
      "Epoch 1/10, Batch 358/883, Training Loss: 1.0445\n",
      "Epoch 1/10, Batch 359/883, Training Loss: 0.9408\n",
      "Epoch 1/10, Batch 360/883, Training Loss: 1.0009\n",
      "Epoch 1/10, Batch 361/883, Training Loss: 0.9582\n",
      "Epoch 1/10, Batch 362/883, Training Loss: 0.9242\n",
      "Epoch 1/10, Batch 363/883, Training Loss: 1.1916\n",
      "Epoch 1/10, Batch 364/883, Training Loss: 0.9644\n",
      "Epoch 1/10, Batch 365/883, Training Loss: 0.8495\n",
      "Epoch 1/10, Batch 366/883, Training Loss: 0.9940\n",
      "Epoch 1/10, Batch 367/883, Training Loss: 0.7013\n",
      "Epoch 1/10, Batch 368/883, Training Loss: 0.8295\n",
      "Epoch 1/10, Batch 369/883, Training Loss: 1.1053\n",
      "Epoch 1/10, Batch 370/883, Training Loss: 0.8172\n",
      "Epoch 1/10, Batch 371/883, Training Loss: 0.9667\n",
      "Epoch 1/10, Batch 372/883, Training Loss: 1.0725\n",
      "Epoch 1/10, Batch 373/883, Training Loss: 0.7997\n",
      "Epoch 1/10, Batch 374/883, Training Loss: 1.0875\n",
      "Epoch 1/10, Batch 375/883, Training Loss: 0.8939\n",
      "Epoch 1/10, Batch 376/883, Training Loss: 0.7454\n",
      "Epoch 1/10, Batch 377/883, Training Loss: 0.8090\n",
      "Epoch 1/10, Batch 378/883, Training Loss: 1.1087\n",
      "Epoch 1/10, Batch 379/883, Training Loss: 0.7963\n",
      "Epoch 1/10, Batch 380/883, Training Loss: 0.8418\n",
      "Epoch 1/10, Batch 381/883, Training Loss: 1.1088\n",
      "Epoch 1/10, Batch 382/883, Training Loss: 1.0253\n",
      "Epoch 1/10, Batch 383/883, Training Loss: 0.9080\n",
      "Epoch 1/10, Batch 384/883, Training Loss: 0.8791\n",
      "Epoch 1/10, Batch 385/883, Training Loss: 0.8785\n",
      "Epoch 1/10, Batch 386/883, Training Loss: 1.0382\n",
      "Epoch 1/10, Batch 387/883, Training Loss: 0.8788\n",
      "Epoch 1/10, Batch 388/883, Training Loss: 0.7880\n",
      "Epoch 1/10, Batch 389/883, Training Loss: 0.8670\n",
      "Epoch 1/10, Batch 390/883, Training Loss: 1.8754\n",
      "Epoch 1/10, Batch 391/883, Training Loss: 0.7846\n",
      "Epoch 1/10, Batch 392/883, Training Loss: 0.8369\n",
      "Epoch 1/10, Batch 393/883, Training Loss: 0.9027\n",
      "Epoch 1/10, Batch 394/883, Training Loss: 0.9329\n",
      "Epoch 1/10, Batch 395/883, Training Loss: 0.9790\n",
      "Epoch 1/10, Batch 396/883, Training Loss: 0.8371\n",
      "Epoch 1/10, Batch 397/883, Training Loss: 0.7702\n",
      "Epoch 1/10, Batch 398/883, Training Loss: 0.8735\n",
      "Epoch 1/10, Batch 399/883, Training Loss: 1.0768\n",
      "Epoch 1/10, Batch 400/883, Training Loss: 0.8456\n",
      "Epoch 1/10, Batch 401/883, Training Loss: 0.9499\n",
      "Epoch 1/10, Batch 402/883, Training Loss: 1.0495\n",
      "Epoch 1/10, Batch 403/883, Training Loss: 0.9380\n",
      "Epoch 1/10, Batch 404/883, Training Loss: 0.8382\n",
      "Epoch 1/10, Batch 405/883, Training Loss: 1.0231\n",
      "Epoch 1/10, Batch 406/883, Training Loss: 1.1980\n",
      "Epoch 1/10, Batch 407/883, Training Loss: 0.9312\n",
      "Epoch 1/10, Batch 408/883, Training Loss: 0.7868\n",
      "Epoch 1/10, Batch 409/883, Training Loss: 1.0349\n",
      "Epoch 1/10, Batch 410/883, Training Loss: 0.7616\n",
      "Epoch 1/10, Batch 411/883, Training Loss: 0.9429\n",
      "Epoch 1/10, Batch 412/883, Training Loss: 0.8713\n",
      "Epoch 1/10, Batch 413/883, Training Loss: 0.8186\n",
      "Epoch 1/10, Batch 414/883, Training Loss: 1.0004\n",
      "Epoch 1/10, Batch 415/883, Training Loss: 0.9630\n",
      "Epoch 1/10, Batch 416/883, Training Loss: 0.8859\n",
      "Epoch 1/10, Batch 417/883, Training Loss: 0.8325\n",
      "Epoch 1/10, Batch 418/883, Training Loss: 1.0299\n",
      "Epoch 1/10, Batch 419/883, Training Loss: 1.0346\n",
      "Epoch 1/10, Batch 420/883, Training Loss: 1.0382\n",
      "Epoch 1/10, Batch 421/883, Training Loss: 0.7877\n",
      "Epoch 1/10, Batch 422/883, Training Loss: 0.8712\n",
      "Epoch 1/10, Batch 423/883, Training Loss: 0.9925\n",
      "Epoch 1/10, Batch 424/883, Training Loss: 1.1443\n",
      "Epoch 1/10, Batch 425/883, Training Loss: 0.9553\n",
      "Epoch 1/10, Batch 426/883, Training Loss: 0.8840\n",
      "Epoch 1/10, Batch 427/883, Training Loss: 1.2376\n",
      "Epoch 1/10, Batch 428/883, Training Loss: 0.8394\n",
      "Epoch 1/10, Batch 429/883, Training Loss: 0.9086\n",
      "Epoch 1/10, Batch 430/883, Training Loss: 1.0365\n",
      "Epoch 1/10, Batch 431/883, Training Loss: 1.0608\n",
      "Epoch 1/10, Batch 432/883, Training Loss: 0.9922\n",
      "Epoch 1/10, Batch 433/883, Training Loss: 1.1763\n",
      "Epoch 1/10, Batch 434/883, Training Loss: 0.8528\n",
      "Epoch 1/10, Batch 435/883, Training Loss: 1.0156\n",
      "Epoch 1/10, Batch 436/883, Training Loss: 0.8337\n",
      "Epoch 1/10, Batch 437/883, Training Loss: 0.9283\n",
      "Epoch 1/10, Batch 438/883, Training Loss: 0.9040\n",
      "Epoch 1/10, Batch 439/883, Training Loss: 0.9741\n",
      "Epoch 1/10, Batch 440/883, Training Loss: 0.9340\n",
      "Epoch 1/10, Batch 441/883, Training Loss: 0.9128\n",
      "Epoch 1/10, Batch 442/883, Training Loss: 0.9319\n",
      "Epoch 1/10, Batch 443/883, Training Loss: 0.9745\n",
      "Epoch 1/10, Batch 444/883, Training Loss: 0.7910\n",
      "Epoch 1/10, Batch 445/883, Training Loss: 0.8007\n",
      "Epoch 1/10, Batch 446/883, Training Loss: 0.9409\n",
      "Epoch 1/10, Batch 447/883, Training Loss: 0.8585\n",
      "Epoch 1/10, Batch 448/883, Training Loss: 0.9285\n",
      "Epoch 1/10, Batch 449/883, Training Loss: 0.8812\n",
      "Epoch 1/10, Batch 450/883, Training Loss: 0.9233\n",
      "Epoch 1/10, Batch 451/883, Training Loss: 0.8207\n",
      "Epoch 1/10, Batch 452/883, Training Loss: 1.4504\n",
      "Epoch 1/10, Batch 453/883, Training Loss: 0.6762\n",
      "Epoch 1/10, Batch 454/883, Training Loss: 0.7668\n",
      "Epoch 1/10, Batch 455/883, Training Loss: 0.9404\n",
      "Epoch 1/10, Batch 456/883, Training Loss: 0.8606\n",
      "Epoch 1/10, Batch 457/883, Training Loss: 0.8897\n",
      "Epoch 1/10, Batch 458/883, Training Loss: 1.1741\n",
      "Epoch 1/10, Batch 459/883, Training Loss: 0.7478\n",
      "Epoch 1/10, Batch 460/883, Training Loss: 0.8957\n",
      "Epoch 1/10, Batch 461/883, Training Loss: 1.0144\n",
      "Epoch 1/10, Batch 462/883, Training Loss: 0.9100\n",
      "Epoch 1/10, Batch 463/883, Training Loss: 0.8266\n",
      "Epoch 1/10, Batch 464/883, Training Loss: 1.0049\n",
      "Epoch 1/10, Batch 465/883, Training Loss: 0.8850\n",
      "Epoch 1/10, Batch 466/883, Training Loss: 0.7202\n",
      "Epoch 1/10, Batch 467/883, Training Loss: 0.6521\n",
      "Epoch 1/10, Batch 468/883, Training Loss: 0.8985\n",
      "Epoch 1/10, Batch 469/883, Training Loss: 0.8644\n",
      "Epoch 1/10, Batch 470/883, Training Loss: 0.8715\n",
      "Epoch 1/10, Batch 471/883, Training Loss: 0.9186\n",
      "Epoch 1/10, Batch 472/883, Training Loss: 0.8762\n",
      "Epoch 1/10, Batch 473/883, Training Loss: 1.2472\n",
      "Epoch 1/10, Batch 474/883, Training Loss: 0.8471\n",
      "Epoch 1/10, Batch 475/883, Training Loss: 0.5826\n",
      "Epoch 1/10, Batch 476/883, Training Loss: 0.7647\n",
      "Epoch 1/10, Batch 477/883, Training Loss: 0.8708\n",
      "Epoch 1/10, Batch 478/883, Training Loss: 0.6620\n",
      "Epoch 1/10, Batch 479/883, Training Loss: 0.8780\n",
      "Epoch 1/10, Batch 480/883, Training Loss: 0.7524\n",
      "Epoch 1/10, Batch 481/883, Training Loss: 0.9238\n",
      "Epoch 1/10, Batch 482/883, Training Loss: 0.9663\n",
      "Epoch 1/10, Batch 483/883, Training Loss: 0.7829\n",
      "Epoch 1/10, Batch 484/883, Training Loss: 0.9668\n",
      "Epoch 1/10, Batch 485/883, Training Loss: 0.7875\n",
      "Epoch 1/10, Batch 486/883, Training Loss: 0.6390\n",
      "Epoch 1/10, Batch 487/883, Training Loss: 0.8260\n",
      "Epoch 1/10, Batch 488/883, Training Loss: 0.9274\n",
      "Epoch 1/10, Batch 489/883, Training Loss: 0.9412\n",
      "Epoch 1/10, Batch 490/883, Training Loss: 1.1748\n",
      "Epoch 1/10, Batch 491/883, Training Loss: 1.1135\n",
      "Epoch 1/10, Batch 492/883, Training Loss: 0.9996\n",
      "Epoch 1/10, Batch 493/883, Training Loss: 0.7275\n",
      "Epoch 1/10, Batch 494/883, Training Loss: 1.0675\n",
      "Epoch 1/10, Batch 495/883, Training Loss: 0.6799\n",
      "Epoch 1/10, Batch 496/883, Training Loss: 0.9130\n",
      "Epoch 1/10, Batch 497/883, Training Loss: 0.7040\n",
      "Epoch 1/10, Batch 498/883, Training Loss: 0.8860\n",
      "Epoch 1/10, Batch 499/883, Training Loss: 0.7913\n",
      "Epoch 1/10, Batch 500/883, Training Loss: 1.4089\n",
      "Epoch 1/10, Batch 501/883, Training Loss: 1.0058\n",
      "Epoch 1/10, Batch 502/883, Training Loss: 0.8130\n",
      "Epoch 1/10, Batch 503/883, Training Loss: 0.7053\n",
      "Epoch 1/10, Batch 504/883, Training Loss: 0.8063\n",
      "Epoch 1/10, Batch 505/883, Training Loss: 1.0178\n",
      "Epoch 1/10, Batch 506/883, Training Loss: 1.0165\n",
      "Epoch 1/10, Batch 507/883, Training Loss: 0.9614\n",
      "Epoch 1/10, Batch 508/883, Training Loss: 0.9232\n",
      "Epoch 1/10, Batch 509/883, Training Loss: 0.7554\n",
      "Epoch 1/10, Batch 510/883, Training Loss: 0.7816\n",
      "Epoch 1/10, Batch 511/883, Training Loss: 0.8263\n",
      "Epoch 1/10, Batch 512/883, Training Loss: 0.8880\n",
      "Epoch 1/10, Batch 513/883, Training Loss: 1.1057\n",
      "Epoch 1/10, Batch 514/883, Training Loss: 0.9834\n",
      "Epoch 1/10, Batch 515/883, Training Loss: 0.9608\n",
      "Epoch 1/10, Batch 516/883, Training Loss: 1.2623\n",
      "Epoch 1/10, Batch 517/883, Training Loss: 0.8639\n",
      "Epoch 1/10, Batch 518/883, Training Loss: 1.0133\n",
      "Epoch 1/10, Batch 519/883, Training Loss: 0.9637\n",
      "Epoch 1/10, Batch 520/883, Training Loss: 0.9020\n",
      "Epoch 1/10, Batch 521/883, Training Loss: 0.7559\n",
      "Epoch 1/10, Batch 522/883, Training Loss: 1.1921\n",
      "Epoch 1/10, Batch 523/883, Training Loss: 1.0680\n",
      "Epoch 1/10, Batch 524/883, Training Loss: 0.9939\n",
      "Epoch 1/10, Batch 525/883, Training Loss: 0.7834\n",
      "Epoch 1/10, Batch 526/883, Training Loss: 0.8807\n",
      "Epoch 1/10, Batch 527/883, Training Loss: 1.2533\n",
      "Epoch 1/10, Batch 528/883, Training Loss: 0.8975\n",
      "Epoch 1/10, Batch 529/883, Training Loss: 0.8793\n",
      "Epoch 1/10, Batch 530/883, Training Loss: 0.9790\n",
      "Epoch 1/10, Batch 531/883, Training Loss: 0.7745\n",
      "Epoch 1/10, Batch 532/883, Training Loss: 0.8794\n",
      "Epoch 1/10, Batch 533/883, Training Loss: 0.8494\n",
      "Epoch 1/10, Batch 534/883, Training Loss: 0.8834\n",
      "Epoch 1/10, Batch 535/883, Training Loss: 0.9343\n",
      "Epoch 1/10, Batch 536/883, Training Loss: 0.7440\n",
      "Epoch 1/10, Batch 537/883, Training Loss: 1.0434\n",
      "Epoch 1/10, Batch 538/883, Training Loss: 0.9066\n",
      "Epoch 1/10, Batch 539/883, Training Loss: 0.8090\n",
      "Epoch 1/10, Batch 540/883, Training Loss: 0.9219\n",
      "Epoch 1/10, Batch 541/883, Training Loss: 0.9339\n",
      "Epoch 1/10, Batch 542/883, Training Loss: 0.9712\n",
      "Epoch 1/10, Batch 543/883, Training Loss: 0.8405\n",
      "Epoch 1/10, Batch 544/883, Training Loss: 0.7086\n",
      "Epoch 1/10, Batch 545/883, Training Loss: 0.9008\n",
      "Epoch 1/10, Batch 546/883, Training Loss: 1.1838\n",
      "Epoch 1/10, Batch 547/883, Training Loss: 0.8372\n",
      "Epoch 1/10, Batch 548/883, Training Loss: 1.1467\n",
      "Epoch 1/10, Batch 549/883, Training Loss: 1.4027\n",
      "Epoch 1/10, Batch 550/883, Training Loss: 0.8021\n",
      "Epoch 1/10, Batch 551/883, Training Loss: 1.0177\n",
      "Epoch 1/10, Batch 552/883, Training Loss: 1.0959\n",
      "Epoch 1/10, Batch 553/883, Training Loss: 0.9514\n",
      "Epoch 1/10, Batch 554/883, Training Loss: 0.8752\n",
      "Epoch 1/10, Batch 555/883, Training Loss: 0.7592\n",
      "Epoch 1/10, Batch 556/883, Training Loss: 1.5800\n",
      "Epoch 1/10, Batch 557/883, Training Loss: 0.8161\n",
      "Epoch 1/10, Batch 558/883, Training Loss: 0.9557\n",
      "Epoch 1/10, Batch 559/883, Training Loss: 0.8735\n",
      "Epoch 1/10, Batch 560/883, Training Loss: 0.9746\n",
      "Epoch 1/10, Batch 561/883, Training Loss: 0.9310\n",
      "Epoch 1/10, Batch 562/883, Training Loss: 0.9064\n",
      "Epoch 1/10, Batch 563/883, Training Loss: 0.7775\n",
      "Epoch 1/10, Batch 564/883, Training Loss: 0.8224\n",
      "Epoch 1/10, Batch 565/883, Training Loss: 0.8437\n",
      "Epoch 1/10, Batch 566/883, Training Loss: 0.8895\n",
      "Epoch 1/10, Batch 567/883, Training Loss: 1.0142\n",
      "Epoch 1/10, Batch 568/883, Training Loss: 0.8048\n",
      "Epoch 1/10, Batch 569/883, Training Loss: 0.8995\n",
      "Epoch 1/10, Batch 570/883, Training Loss: 0.9009\n",
      "Epoch 1/10, Batch 571/883, Training Loss: 0.7169\n",
      "Epoch 1/10, Batch 572/883, Training Loss: 0.8434\n",
      "Epoch 1/10, Batch 573/883, Training Loss: 1.2574\n",
      "Epoch 1/10, Batch 574/883, Training Loss: 1.0119\n",
      "Epoch 1/10, Batch 575/883, Training Loss: 1.0566\n",
      "Epoch 1/10, Batch 576/883, Training Loss: 0.5956\n",
      "Epoch 1/10, Batch 577/883, Training Loss: 0.8389\n",
      "Epoch 1/10, Batch 578/883, Training Loss: 0.9411\n",
      "Epoch 1/10, Batch 579/883, Training Loss: 0.7734\n",
      "Epoch 1/10, Batch 580/883, Training Loss: 0.8434\n",
      "Epoch 1/10, Batch 581/883, Training Loss: 0.7896\n",
      "Epoch 1/10, Batch 582/883, Training Loss: 0.8927\n",
      "Epoch 1/10, Batch 583/883, Training Loss: 1.0253\n",
      "Epoch 1/10, Batch 584/883, Training Loss: 0.6992\n",
      "Epoch 1/10, Batch 585/883, Training Loss: 0.9057\n",
      "Epoch 1/10, Batch 586/883, Training Loss: 1.0855\n",
      "Epoch 1/10, Batch 587/883, Training Loss: 0.8759\n",
      "Epoch 1/10, Batch 588/883, Training Loss: 0.8726\n",
      "Epoch 1/10, Batch 589/883, Training Loss: 0.9001\n",
      "Epoch 1/10, Batch 590/883, Training Loss: 0.9698\n",
      "Epoch 1/10, Batch 591/883, Training Loss: 0.7047\n",
      "Epoch 1/10, Batch 592/883, Training Loss: 0.6420\n",
      "Epoch 1/10, Batch 593/883, Training Loss: 0.7334\n",
      "Epoch 1/10, Batch 594/883, Training Loss: 0.8554\n",
      "Epoch 1/10, Batch 595/883, Training Loss: 0.8035\n",
      "Epoch 1/10, Batch 596/883, Training Loss: 1.1854\n",
      "Epoch 1/10, Batch 597/883, Training Loss: 0.9312\n",
      "Epoch 1/10, Batch 598/883, Training Loss: 0.8099\n",
      "Epoch 1/10, Batch 599/883, Training Loss: 0.9273\n",
      "Epoch 1/10, Batch 600/883, Training Loss: 0.9503\n",
      "Epoch 1/10, Batch 601/883, Training Loss: 0.7896\n",
      "Epoch 1/10, Batch 602/883, Training Loss: 0.8399\n",
      "Epoch 1/10, Batch 603/883, Training Loss: 0.7811\n",
      "Epoch 1/10, Batch 604/883, Training Loss: 0.7957\n",
      "Epoch 1/10, Batch 605/883, Training Loss: 0.8507\n",
      "Epoch 1/10, Batch 606/883, Training Loss: 0.7623\n",
      "Epoch 1/10, Batch 607/883, Training Loss: 0.8341\n",
      "Epoch 1/10, Batch 608/883, Training Loss: 0.7938\n",
      "Epoch 1/10, Batch 609/883, Training Loss: 0.7555\n",
      "Epoch 1/10, Batch 610/883, Training Loss: 0.8073\n",
      "Epoch 1/10, Batch 611/883, Training Loss: 0.8435\n",
      "Epoch 1/10, Batch 612/883, Training Loss: 0.9354\n",
      "Epoch 1/10, Batch 613/883, Training Loss: 1.0401\n",
      "Epoch 1/10, Batch 614/883, Training Loss: 0.9344\n",
      "Epoch 1/10, Batch 615/883, Training Loss: 0.7004\n",
      "Epoch 1/10, Batch 616/883, Training Loss: 0.9412\n",
      "Epoch 1/10, Batch 617/883, Training Loss: 0.9466\n",
      "Epoch 1/10, Batch 618/883, Training Loss: 1.1218\n",
      "Epoch 1/10, Batch 619/883, Training Loss: 0.7370\n",
      "Epoch 1/10, Batch 620/883, Training Loss: 0.9567\n",
      "Epoch 1/10, Batch 621/883, Training Loss: 0.7017\n",
      "Epoch 1/10, Batch 622/883, Training Loss: 0.8142\n",
      "Epoch 1/10, Batch 623/883, Training Loss: 0.7763\n",
      "Epoch 1/10, Batch 624/883, Training Loss: 1.1142\n",
      "Epoch 1/10, Batch 625/883, Training Loss: 0.8498\n",
      "Epoch 1/10, Batch 626/883, Training Loss: 0.8328\n",
      "Epoch 1/10, Batch 627/883, Training Loss: 1.0101\n",
      "Epoch 1/10, Batch 628/883, Training Loss: 0.9350\n",
      "Epoch 1/10, Batch 629/883, Training Loss: 0.8058\n",
      "Epoch 1/10, Batch 630/883, Training Loss: 0.8156\n",
      "Epoch 1/10, Batch 631/883, Training Loss: 0.8333\n",
      "Epoch 1/10, Batch 632/883, Training Loss: 0.7863\n",
      "Epoch 1/10, Batch 633/883, Training Loss: 1.0450\n",
      "Epoch 1/10, Batch 634/883, Training Loss: 0.7612\n",
      "Epoch 1/10, Batch 635/883, Training Loss: 0.7403\n",
      "Epoch 1/10, Batch 636/883, Training Loss: 0.9991\n",
      "Epoch 1/10, Batch 637/883, Training Loss: 0.9540\n",
      "Epoch 1/10, Batch 638/883, Training Loss: 0.9166\n",
      "Epoch 1/10, Batch 639/883, Training Loss: 1.0804\n",
      "Epoch 1/10, Batch 640/883, Training Loss: 0.9986\n",
      "Epoch 1/10, Batch 641/883, Training Loss: 0.9302\n",
      "Epoch 1/10, Batch 642/883, Training Loss: 0.9893\n",
      "Epoch 1/10, Batch 643/883, Training Loss: 0.6529\n",
      "Epoch 1/10, Batch 644/883, Training Loss: 0.9426\n",
      "Epoch 1/10, Batch 645/883, Training Loss: 0.6642\n",
      "Epoch 1/10, Batch 646/883, Training Loss: 0.9951\n",
      "Epoch 1/10, Batch 647/883, Training Loss: 1.0416\n",
      "Epoch 1/10, Batch 648/883, Training Loss: 0.9403\n",
      "Epoch 1/10, Batch 649/883, Training Loss: 0.6924\n",
      "Epoch 1/10, Batch 650/883, Training Loss: 0.8067\n",
      "Epoch 1/10, Batch 651/883, Training Loss: 0.8995\n",
      "Epoch 1/10, Batch 652/883, Training Loss: 1.0635\n",
      "Epoch 1/10, Batch 653/883, Training Loss: 0.8491\n",
      "Epoch 1/10, Batch 654/883, Training Loss: 0.8925\n",
      "Epoch 1/10, Batch 655/883, Training Loss: 0.7723\n",
      "Epoch 1/10, Batch 656/883, Training Loss: 0.7877\n",
      "Epoch 1/10, Batch 657/883, Training Loss: 0.9423\n",
      "Epoch 1/10, Batch 658/883, Training Loss: 0.8704\n",
      "Epoch 1/10, Batch 659/883, Training Loss: 0.8407\n",
      "Epoch 1/10, Batch 660/883, Training Loss: 0.8701\n",
      "Epoch 1/10, Batch 661/883, Training Loss: 1.0322\n",
      "Epoch 1/10, Batch 662/883, Training Loss: 0.8430\n",
      "Epoch 1/10, Batch 663/883, Training Loss: 0.8220\n",
      "Epoch 1/10, Batch 664/883, Training Loss: 1.0401\n",
      "Epoch 1/10, Batch 665/883, Training Loss: 0.8923\n",
      "Epoch 1/10, Batch 666/883, Training Loss: 1.1216\n",
      "Epoch 1/10, Batch 667/883, Training Loss: 0.9640\n",
      "Epoch 1/10, Batch 668/883, Training Loss: 0.9391\n",
      "Epoch 1/10, Batch 669/883, Training Loss: 0.8906\n",
      "Epoch 1/10, Batch 670/883, Training Loss: 0.9526\n",
      "Epoch 1/10, Batch 671/883, Training Loss: 1.0273\n",
      "Epoch 1/10, Batch 672/883, Training Loss: 0.8400\n",
      "Epoch 1/10, Batch 673/883, Training Loss: 1.0521\n",
      "Epoch 1/10, Batch 674/883, Training Loss: 0.8921\n",
      "Epoch 1/10, Batch 675/883, Training Loss: 0.8508\n",
      "Epoch 1/10, Batch 676/883, Training Loss: 0.8673\n",
      "Epoch 1/10, Batch 677/883, Training Loss: 0.8507\n",
      "Epoch 1/10, Batch 678/883, Training Loss: 0.8287\n",
      "Epoch 1/10, Batch 679/883, Training Loss: 0.9655\n",
      "Epoch 1/10, Batch 680/883, Training Loss: 1.2059\n",
      "Epoch 1/10, Batch 681/883, Training Loss: 0.7506\n",
      "Epoch 1/10, Batch 682/883, Training Loss: 1.0154\n",
      "Epoch 1/10, Batch 683/883, Training Loss: 0.8521\n",
      "Epoch 1/10, Batch 684/883, Training Loss: 1.0779\n",
      "Epoch 1/10, Batch 685/883, Training Loss: 0.7394\n",
      "Epoch 1/10, Batch 686/883, Training Loss: 0.7641\n",
      "Epoch 1/10, Batch 687/883, Training Loss: 0.9514\n",
      "Epoch 1/10, Batch 688/883, Training Loss: 0.9055\n",
      "Epoch 1/10, Batch 689/883, Training Loss: 0.8092\n",
      "Epoch 1/10, Batch 690/883, Training Loss: 0.7694\n",
      "Epoch 1/10, Batch 691/883, Training Loss: 0.9805\n",
      "Epoch 1/10, Batch 692/883, Training Loss: 0.6858\n",
      "Epoch 1/10, Batch 693/883, Training Loss: 0.8442\n",
      "Epoch 1/10, Batch 694/883, Training Loss: 0.9256\n",
      "Epoch 1/10, Batch 695/883, Training Loss: 0.8455\n",
      "Epoch 1/10, Batch 696/883, Training Loss: 0.7607\n",
      "Epoch 1/10, Batch 697/883, Training Loss: 0.9027\n",
      "Epoch 1/10, Batch 698/883, Training Loss: 0.8702\n",
      "Epoch 1/10, Batch 699/883, Training Loss: 0.7507\n",
      "Epoch 1/10, Batch 700/883, Training Loss: 0.7198\n",
      "Epoch 1/10, Batch 701/883, Training Loss: 0.7629\n",
      "Epoch 1/10, Batch 702/883, Training Loss: 0.8856\n",
      "Epoch 1/10, Batch 703/883, Training Loss: 1.7300\n",
      "Epoch 1/10, Batch 704/883, Training Loss: 0.9332\n",
      "Epoch 1/10, Batch 705/883, Training Loss: 0.9210\n",
      "Epoch 1/10, Batch 706/883, Training Loss: 0.7617\n",
      "Epoch 1/10, Batch 707/883, Training Loss: 0.7688\n",
      "Epoch 1/10, Batch 708/883, Training Loss: 0.6903\n",
      "Epoch 1/10, Batch 709/883, Training Loss: 0.8556\n",
      "Epoch 1/10, Batch 710/883, Training Loss: 0.7804\n",
      "Epoch 1/10, Batch 711/883, Training Loss: 0.8462\n",
      "Epoch 1/10, Batch 712/883, Training Loss: 0.6385\n",
      "Epoch 1/10, Batch 713/883, Training Loss: 0.6850\n",
      "Epoch 1/10, Batch 714/883, Training Loss: 0.9614\n",
      "Epoch 1/10, Batch 715/883, Training Loss: 0.9882\n",
      "Epoch 1/10, Batch 716/883, Training Loss: 0.8701\n",
      "Epoch 1/10, Batch 717/883, Training Loss: 0.7682\n",
      "Epoch 1/10, Batch 718/883, Training Loss: 0.7635\n",
      "Epoch 1/10, Batch 719/883, Training Loss: 0.8963\n",
      "Epoch 1/10, Batch 720/883, Training Loss: 0.7051\n",
      "Epoch 1/10, Batch 721/883, Training Loss: 0.7634\n",
      "Epoch 1/10, Batch 722/883, Training Loss: 0.8650\n",
      "Epoch 1/10, Batch 723/883, Training Loss: 0.6531\n",
      "Epoch 1/10, Batch 724/883, Training Loss: 1.2431\n",
      "Epoch 1/10, Batch 725/883, Training Loss: 0.7150\n",
      "Epoch 1/10, Batch 726/883, Training Loss: 0.6119\n",
      "Epoch 1/10, Batch 727/883, Training Loss: 0.8987\n",
      "Epoch 1/10, Batch 728/883, Training Loss: 0.8280\n",
      "Epoch 1/10, Batch 729/883, Training Loss: 1.3152\n",
      "Epoch 1/10, Batch 730/883, Training Loss: 0.8343\n",
      "Epoch 1/10, Batch 731/883, Training Loss: 0.8255\n",
      "Epoch 1/10, Batch 732/883, Training Loss: 0.8231\n",
      "Epoch 1/10, Batch 733/883, Training Loss: 1.2749\n",
      "Epoch 1/10, Batch 734/883, Training Loss: 0.9194\n",
      "Epoch 1/10, Batch 735/883, Training Loss: 0.7042\n",
      "Epoch 1/10, Batch 736/883, Training Loss: 0.8902\n",
      "Epoch 1/10, Batch 737/883, Training Loss: 0.9020\n",
      "Epoch 1/10, Batch 738/883, Training Loss: 0.8633\n",
      "Epoch 1/10, Batch 739/883, Training Loss: 1.1252\n",
      "Epoch 1/10, Batch 740/883, Training Loss: 0.7862\n",
      "Epoch 1/10, Batch 741/883, Training Loss: 0.9228\n",
      "Epoch 1/10, Batch 742/883, Training Loss: 0.7640\n",
      "Epoch 1/10, Batch 743/883, Training Loss: 0.8041\n",
      "Epoch 1/10, Batch 744/883, Training Loss: 1.0380\n",
      "Epoch 1/10, Batch 745/883, Training Loss: 0.7111\n",
      "Epoch 1/10, Batch 746/883, Training Loss: 0.8535\n",
      "Epoch 1/10, Batch 747/883, Training Loss: 0.6522\n",
      "Epoch 1/10, Batch 748/883, Training Loss: 0.9216\n",
      "Epoch 1/10, Batch 749/883, Training Loss: 1.0209\n",
      "Epoch 1/10, Batch 750/883, Training Loss: 1.0466\n",
      "Epoch 1/10, Batch 751/883, Training Loss: 0.9902\n",
      "Epoch 1/10, Batch 752/883, Training Loss: 1.1377\n",
      "Epoch 1/10, Batch 753/883, Training Loss: 1.0047\n",
      "Epoch 1/10, Batch 754/883, Training Loss: 0.8028\n",
      "Epoch 1/10, Batch 755/883, Training Loss: 0.8202\n",
      "Epoch 1/10, Batch 756/883, Training Loss: 0.7891\n",
      "Epoch 1/10, Batch 757/883, Training Loss: 0.7571\n",
      "Epoch 1/10, Batch 758/883, Training Loss: 0.9099\n",
      "Epoch 1/10, Batch 759/883, Training Loss: 1.0850\n",
      "Epoch 1/10, Batch 760/883, Training Loss: 1.0332\n",
      "Epoch 1/10, Batch 761/883, Training Loss: 0.6823\n",
      "Epoch 1/10, Batch 762/883, Training Loss: 0.8193\n",
      "Epoch 1/10, Batch 763/883, Training Loss: 0.8979\n",
      "Epoch 1/10, Batch 764/883, Training Loss: 0.6749\n",
      "Epoch 1/10, Batch 765/883, Training Loss: 0.8392\n",
      "Epoch 1/10, Batch 766/883, Training Loss: 0.5810\n",
      "Epoch 1/10, Batch 767/883, Training Loss: 0.7541\n",
      "Epoch 1/10, Batch 768/883, Training Loss: 0.9466\n",
      "Epoch 1/10, Batch 769/883, Training Loss: 1.0211\n",
      "Epoch 1/10, Batch 770/883, Training Loss: 0.8268\n",
      "Epoch 1/10, Batch 771/883, Training Loss: 0.7592\n",
      "Epoch 1/10, Batch 772/883, Training Loss: 0.8451\n",
      "Epoch 1/10, Batch 773/883, Training Loss: 0.6976\n",
      "Epoch 1/10, Batch 774/883, Training Loss: 0.6811\n",
      "Epoch 1/10, Batch 775/883, Training Loss: 0.6450\n",
      "Epoch 1/10, Batch 776/883, Training Loss: 1.3958\n",
      "Epoch 1/10, Batch 777/883, Training Loss: 1.2579\n",
      "Epoch 1/10, Batch 778/883, Training Loss: 1.1883\n",
      "Epoch 1/10, Batch 779/883, Training Loss: 1.2630\n",
      "Epoch 1/10, Batch 780/883, Training Loss: 0.7151\n",
      "Epoch 1/10, Batch 781/883, Training Loss: 1.0902\n",
      "Epoch 1/10, Batch 782/883, Training Loss: 0.8818\n",
      "Epoch 1/10, Batch 783/883, Training Loss: 0.7793\n",
      "Epoch 1/10, Batch 784/883, Training Loss: 0.7262\n",
      "Epoch 1/10, Batch 785/883, Training Loss: 0.7659\n",
      "Epoch 1/10, Batch 786/883, Training Loss: 0.8518\n",
      "Epoch 1/10, Batch 787/883, Training Loss: 1.0452\n",
      "Epoch 1/10, Batch 788/883, Training Loss: 0.9226\n",
      "Epoch 1/10, Batch 789/883, Training Loss: 0.7838\n",
      "Epoch 1/10, Batch 790/883, Training Loss: 1.0022\n",
      "Epoch 1/10, Batch 791/883, Training Loss: 0.8134\n",
      "Epoch 1/10, Batch 792/883, Training Loss: 0.8722\n",
      "Epoch 1/10, Batch 793/883, Training Loss: 0.8304\n",
      "Epoch 1/10, Batch 794/883, Training Loss: 0.7926\n",
      "Epoch 1/10, Batch 795/883, Training Loss: 0.8954\n",
      "Epoch 1/10, Batch 796/883, Training Loss: 0.9419\n",
      "Epoch 1/10, Batch 797/883, Training Loss: 0.6529\n",
      "Epoch 1/10, Batch 798/883, Training Loss: 0.8676\n",
      "Epoch 1/10, Batch 799/883, Training Loss: 1.1065\n",
      "Epoch 1/10, Batch 800/883, Training Loss: 0.7787\n",
      "Epoch 1/10, Batch 801/883, Training Loss: 0.7971\n",
      "Epoch 1/10, Batch 802/883, Training Loss: 0.8197\n",
      "Epoch 1/10, Batch 803/883, Training Loss: 0.7964\n",
      "Epoch 1/10, Batch 804/883, Training Loss: 1.1482\n",
      "Epoch 1/10, Batch 805/883, Training Loss: 0.7140\n",
      "Epoch 1/10, Batch 806/883, Training Loss: 1.0671\n",
      "Epoch 1/10, Batch 807/883, Training Loss: 0.7490\n",
      "Epoch 1/10, Batch 808/883, Training Loss: 0.7938\n",
      "Epoch 1/10, Batch 809/883, Training Loss: 0.9279\n",
      "Epoch 1/10, Batch 810/883, Training Loss: 0.8947\n",
      "Epoch 1/10, Batch 811/883, Training Loss: 0.8834\n",
      "Epoch 1/10, Batch 812/883, Training Loss: 0.6673\n",
      "Epoch 1/10, Batch 813/883, Training Loss: 0.7473\n",
      "Epoch 1/10, Batch 814/883, Training Loss: 0.8786\n",
      "Epoch 1/10, Batch 815/883, Training Loss: 0.9537\n",
      "Epoch 1/10, Batch 816/883, Training Loss: 0.9471\n",
      "Epoch 1/10, Batch 817/883, Training Loss: 0.6865\n",
      "Epoch 1/10, Batch 818/883, Training Loss: 0.7686\n",
      "Epoch 1/10, Batch 819/883, Training Loss: 1.1260\n",
      "Epoch 1/10, Batch 820/883, Training Loss: 0.9167\n",
      "Epoch 1/10, Batch 821/883, Training Loss: 0.8716\n",
      "Epoch 1/10, Batch 822/883, Training Loss: 0.7421\n",
      "Epoch 1/10, Batch 823/883, Training Loss: 0.9761\n",
      "Epoch 1/10, Batch 824/883, Training Loss: 0.7559\n",
      "Epoch 1/10, Batch 825/883, Training Loss: 0.7294\n",
      "Epoch 1/10, Batch 826/883, Training Loss: 1.0411\n",
      "Epoch 1/10, Batch 827/883, Training Loss: 0.9856\n",
      "Epoch 1/10, Batch 828/883, Training Loss: 0.7116\n",
      "Epoch 1/10, Batch 829/883, Training Loss: 0.9530\n",
      "Epoch 1/10, Batch 830/883, Training Loss: 0.8480\n",
      "Epoch 1/10, Batch 831/883, Training Loss: 0.8289\n",
      "Epoch 1/10, Batch 832/883, Training Loss: 0.9208\n",
      "Epoch 1/10, Batch 833/883, Training Loss: 0.7118\n",
      "Epoch 1/10, Batch 834/883, Training Loss: 0.7213\n",
      "Epoch 1/10, Batch 835/883, Training Loss: 0.8267\n",
      "Epoch 1/10, Batch 836/883, Training Loss: 0.9020\n",
      "Epoch 1/10, Batch 837/883, Training Loss: 0.9345\n",
      "Epoch 1/10, Batch 838/883, Training Loss: 1.2673\n",
      "Epoch 1/10, Batch 839/883, Training Loss: 0.7825\n",
      "Epoch 1/10, Batch 840/883, Training Loss: 0.9010\n",
      "Epoch 1/10, Batch 841/883, Training Loss: 0.7438\n",
      "Epoch 1/10, Batch 842/883, Training Loss: 0.8206\n",
      "Epoch 1/10, Batch 843/883, Training Loss: 0.8202\n",
      "Epoch 1/10, Batch 844/883, Training Loss: 0.8021\n",
      "Epoch 1/10, Batch 845/883, Training Loss: 0.8382\n",
      "Epoch 1/10, Batch 846/883, Training Loss: 0.6955\n",
      "Epoch 1/10, Batch 847/883, Training Loss: 0.8691\n",
      "Epoch 1/10, Batch 848/883, Training Loss: 0.7971\n",
      "Epoch 1/10, Batch 849/883, Training Loss: 0.7808\n",
      "Epoch 1/10, Batch 850/883, Training Loss: 0.7167\n",
      "Epoch 1/10, Batch 851/883, Training Loss: 0.7317\n",
      "Epoch 1/10, Batch 852/883, Training Loss: 0.7317\n",
      "Epoch 1/10, Batch 853/883, Training Loss: 0.7756\n",
      "Epoch 1/10, Batch 854/883, Training Loss: 0.7320\n",
      "Epoch 1/10, Batch 855/883, Training Loss: 0.9907\n",
      "Epoch 1/10, Batch 856/883, Training Loss: 0.6893\n",
      "Epoch 1/10, Batch 857/883, Training Loss: 0.8981\n",
      "Epoch 1/10, Batch 858/883, Training Loss: 0.9428\n",
      "Epoch 1/10, Batch 859/883, Training Loss: 0.8673\n",
      "Epoch 1/10, Batch 860/883, Training Loss: 0.8451\n",
      "Epoch 1/10, Batch 861/883, Training Loss: 1.3429\n",
      "Epoch 1/10, Batch 862/883, Training Loss: 0.6304\n",
      "Epoch 1/10, Batch 863/883, Training Loss: 1.0526\n",
      "Epoch 1/10, Batch 864/883, Training Loss: 0.7813\n",
      "Epoch 1/10, Batch 865/883, Training Loss: 0.8357\n",
      "Epoch 1/10, Batch 866/883, Training Loss: 0.7073\n",
      "Epoch 1/10, Batch 867/883, Training Loss: 1.0164\n",
      "Epoch 1/10, Batch 868/883, Training Loss: 0.6564\n",
      "Epoch 1/10, Batch 869/883, Training Loss: 0.9314\n",
      "Epoch 1/10, Batch 870/883, Training Loss: 0.6999\n",
      "Epoch 1/10, Batch 871/883, Training Loss: 0.7635\n",
      "Epoch 1/10, Batch 872/883, Training Loss: 1.1837\n",
      "Epoch 1/10, Batch 873/883, Training Loss: 0.7928\n",
      "Epoch 1/10, Batch 874/883, Training Loss: 0.8446\n",
      "Epoch 1/10, Batch 875/883, Training Loss: 0.9064\n",
      "Epoch 1/10, Batch 876/883, Training Loss: 0.7715\n",
      "Epoch 1/10, Batch 877/883, Training Loss: 1.0134\n",
      "Epoch 1/10, Batch 878/883, Training Loss: 0.7840\n",
      "Epoch 1/10, Batch 879/883, Training Loss: 0.9299\n",
      "Epoch 1/10, Batch 880/883, Training Loss: 0.8696\n",
      "Epoch 1/10, Batch 881/883, Training Loss: 0.7240\n",
      "Epoch 1/10, Batch 882/883, Training Loss: 1.0254\n",
      "Epoch 1/10, Batch 883/883, Training Loss: 0.8564\n",
      "Epoch 1/10, Training Loss: 0.9728, Validation Loss: 0.9185, Validation Accuracy: 0.5760\n",
      "Epoch 2/10, Batch 1/883, Training Loss: 0.8679\n",
      "Epoch 2/10, Batch 2/883, Training Loss: 0.7416\n",
      "Epoch 2/10, Batch 3/883, Training Loss: 0.8182\n",
      "Epoch 2/10, Batch 4/883, Training Loss: 0.7515\n",
      "Epoch 2/10, Batch 5/883, Training Loss: 0.8266\n",
      "Epoch 2/10, Batch 6/883, Training Loss: 0.8285\n",
      "Epoch 2/10, Batch 7/883, Training Loss: 0.6654\n",
      "Epoch 2/10, Batch 8/883, Training Loss: 0.6596\n",
      "Epoch 2/10, Batch 9/883, Training Loss: 0.7483\n",
      "Epoch 2/10, Batch 10/883, Training Loss: 0.9642\n",
      "Epoch 2/10, Batch 11/883, Training Loss: 1.0261\n",
      "Epoch 2/10, Batch 12/883, Training Loss: 0.8448\n",
      "Epoch 2/10, Batch 13/883, Training Loss: 0.9641\n",
      "Epoch 2/10, Batch 14/883, Training Loss: 0.9180\n",
      "Epoch 2/10, Batch 15/883, Training Loss: 0.8888\n",
      "Epoch 2/10, Batch 16/883, Training Loss: 0.7004\n",
      "Epoch 2/10, Batch 17/883, Training Loss: 0.8135\n",
      "Epoch 2/10, Batch 18/883, Training Loss: 1.0068\n",
      "Epoch 2/10, Batch 19/883, Training Loss: 0.9297\n",
      "Epoch 2/10, Batch 20/883, Training Loss: 0.9134\n",
      "Epoch 2/10, Batch 21/883, Training Loss: 0.7616\n",
      "Epoch 2/10, Batch 22/883, Training Loss: 1.0268\n",
      "Epoch 2/10, Batch 23/883, Training Loss: 0.8582\n",
      "Epoch 2/10, Batch 24/883, Training Loss: 0.8683\n",
      "Epoch 2/10, Batch 25/883, Training Loss: 0.7695\n",
      "Epoch 2/10, Batch 26/883, Training Loss: 1.1861\n",
      "Epoch 2/10, Batch 27/883, Training Loss: 1.1090\n",
      "Epoch 2/10, Batch 28/883, Training Loss: 0.8883\n",
      "Epoch 2/10, Batch 29/883, Training Loss: 0.7628\n",
      "Epoch 2/10, Batch 30/883, Training Loss: 0.8299\n",
      "Epoch 2/10, Batch 31/883, Training Loss: 0.9885\n",
      "Epoch 2/10, Batch 32/883, Training Loss: 0.8798\n",
      "Epoch 2/10, Batch 33/883, Training Loss: 0.7654\n",
      "Epoch 2/10, Batch 34/883, Training Loss: 0.7754\n",
      "Epoch 2/10, Batch 35/883, Training Loss: 1.0008\n",
      "Epoch 2/10, Batch 36/883, Training Loss: 0.8433\n",
      "Epoch 2/10, Batch 37/883, Training Loss: 0.6492\n",
      "Epoch 2/10, Batch 38/883, Training Loss: 0.8076\n",
      "Epoch 2/10, Batch 39/883, Training Loss: 1.0434\n",
      "Epoch 2/10, Batch 40/883, Training Loss: 0.8620\n",
      "Epoch 2/10, Batch 41/883, Training Loss: 1.2609\n",
      "Epoch 2/10, Batch 42/883, Training Loss: 1.2138\n",
      "Epoch 2/10, Batch 43/883, Training Loss: 0.9016\n",
      "Epoch 2/10, Batch 44/883, Training Loss: 1.1610\n",
      "Epoch 2/10, Batch 45/883, Training Loss: 0.9070\n",
      "Epoch 2/10, Batch 46/883, Training Loss: 0.7454\n",
      "Epoch 2/10, Batch 47/883, Training Loss: 0.7938\n",
      "Epoch 2/10, Batch 48/883, Training Loss: 0.8902\n",
      "Epoch 2/10, Batch 49/883, Training Loss: 0.7963\n",
      "Epoch 2/10, Batch 50/883, Training Loss: 0.8256\n",
      "Epoch 2/10, Batch 51/883, Training Loss: 0.9234\n",
      "Epoch 2/10, Batch 52/883, Training Loss: 0.9811\n",
      "Epoch 2/10, Batch 53/883, Training Loss: 1.0331\n",
      "Epoch 2/10, Batch 54/883, Training Loss: 0.8565\n",
      "Epoch 2/10, Batch 55/883, Training Loss: 1.0535\n",
      "Epoch 2/10, Batch 56/883, Training Loss: 0.8876\n",
      "Epoch 2/10, Batch 57/883, Training Loss: 0.8968\n",
      "Epoch 2/10, Batch 58/883, Training Loss: 0.9823\n",
      "Epoch 2/10, Batch 59/883, Training Loss: 1.0146\n",
      "Epoch 2/10, Batch 60/883, Training Loss: 0.8181\n",
      "Epoch 2/10, Batch 61/883, Training Loss: 0.8701\n",
      "Epoch 2/10, Batch 62/883, Training Loss: 0.8119\n",
      "Epoch 2/10, Batch 63/883, Training Loss: 0.8619\n",
      "Epoch 2/10, Batch 64/883, Training Loss: 0.7700\n",
      "Epoch 2/10, Batch 65/883, Training Loss: 0.7822\n",
      "Epoch 2/10, Batch 66/883, Training Loss: 0.9736\n",
      "Epoch 2/10, Batch 67/883, Training Loss: 0.7944\n",
      "Epoch 2/10, Batch 68/883, Training Loss: 0.9525\n",
      "Epoch 2/10, Batch 69/883, Training Loss: 0.9322\n",
      "Epoch 2/10, Batch 70/883, Training Loss: 0.7215\n",
      "Epoch 2/10, Batch 71/883, Training Loss: 1.1037\n",
      "Epoch 2/10, Batch 72/883, Training Loss: 0.9183\n",
      "Epoch 2/10, Batch 73/883, Training Loss: 0.6410\n",
      "Epoch 2/10, Batch 74/883, Training Loss: 0.8722\n",
      "Epoch 2/10, Batch 75/883, Training Loss: 0.7741\n",
      "Epoch 2/10, Batch 76/883, Training Loss: 0.6578\n",
      "Epoch 2/10, Batch 77/883, Training Loss: 1.0159\n",
      "Epoch 2/10, Batch 78/883, Training Loss: 0.8114\n",
      "Epoch 2/10, Batch 79/883, Training Loss: 0.9564\n",
      "Epoch 2/10, Batch 80/883, Training Loss: 0.9618\n",
      "Epoch 2/10, Batch 81/883, Training Loss: 0.8347\n",
      "Epoch 2/10, Batch 82/883, Training Loss: 0.9379\n",
      "Epoch 2/10, Batch 83/883, Training Loss: 0.8194\n",
      "Epoch 2/10, Batch 84/883, Training Loss: 0.9239\n",
      "Epoch 2/10, Batch 85/883, Training Loss: 0.7926\n",
      "Epoch 2/10, Batch 86/883, Training Loss: 0.8805\n",
      "Epoch 2/10, Batch 87/883, Training Loss: 0.7017\n",
      "Epoch 2/10, Batch 88/883, Training Loss: 0.9150\n",
      "Epoch 2/10, Batch 89/883, Training Loss: 1.1527\n",
      "Epoch 2/10, Batch 90/883, Training Loss: 0.7988\n",
      "Epoch 2/10, Batch 91/883, Training Loss: 0.9584\n",
      "Epoch 2/10, Batch 92/883, Training Loss: 0.7648\n",
      "Epoch 2/10, Batch 93/883, Training Loss: 0.7916\n",
      "Epoch 2/10, Batch 94/883, Training Loss: 0.7981\n",
      "Epoch 2/10, Batch 95/883, Training Loss: 1.1083\n",
      "Epoch 2/10, Batch 96/883, Training Loss: 0.8796\n",
      "Epoch 2/10, Batch 97/883, Training Loss: 1.4503\n",
      "Epoch 2/10, Batch 98/883, Training Loss: 0.9584\n",
      "Epoch 2/10, Batch 99/883, Training Loss: 0.6166\n",
      "Epoch 2/10, Batch 100/883, Training Loss: 0.8490\n",
      "Epoch 2/10, Batch 101/883, Training Loss: 1.1398\n",
      "Epoch 2/10, Batch 102/883, Training Loss: 0.8072\n",
      "Epoch 2/10, Batch 103/883, Training Loss: 1.0473\n",
      "Epoch 2/10, Batch 104/883, Training Loss: 0.8259\n",
      "Epoch 2/10, Batch 105/883, Training Loss: 1.0295\n",
      "Epoch 2/10, Batch 106/883, Training Loss: 1.0513\n",
      "Epoch 2/10, Batch 107/883, Training Loss: 0.8953\n",
      "Epoch 2/10, Batch 108/883, Training Loss: 0.9080\n",
      "Epoch 2/10, Batch 109/883, Training Loss: 1.1816\n",
      "Epoch 2/10, Batch 110/883, Training Loss: 0.9529\n",
      "Epoch 2/10, Batch 111/883, Training Loss: 0.9243\n",
      "Epoch 2/10, Batch 112/883, Training Loss: 1.1492\n",
      "Epoch 2/10, Batch 113/883, Training Loss: 0.9599\n",
      "Epoch 2/10, Batch 114/883, Training Loss: 0.9304\n",
      "Epoch 2/10, Batch 115/883, Training Loss: 0.8335\n",
      "Epoch 2/10, Batch 116/883, Training Loss: 0.9809\n",
      "Epoch 2/10, Batch 117/883, Training Loss: 0.9257\n",
      "Epoch 2/10, Batch 118/883, Training Loss: 0.9367\n",
      "Epoch 2/10, Batch 119/883, Training Loss: 0.8327\n",
      "Epoch 2/10, Batch 120/883, Training Loss: 1.1323\n",
      "Epoch 2/10, Batch 121/883, Training Loss: 0.9680\n",
      "Epoch 2/10, Batch 122/883, Training Loss: 1.0250\n",
      "Epoch 2/10, Batch 123/883, Training Loss: 0.9814\n",
      "Epoch 2/10, Batch 124/883, Training Loss: 1.2202\n",
      "Epoch 2/10, Batch 125/883, Training Loss: 0.9046\n",
      "Epoch 2/10, Batch 126/883, Training Loss: 0.8626\n",
      "Epoch 2/10, Batch 127/883, Training Loss: 1.0110\n",
      "Epoch 2/10, Batch 128/883, Training Loss: 0.9124\n",
      "Epoch 2/10, Batch 129/883, Training Loss: 0.9087\n",
      "Epoch 2/10, Batch 130/883, Training Loss: 0.8568\n",
      "Epoch 2/10, Batch 131/883, Training Loss: 0.8166\n",
      "Epoch 2/10, Batch 132/883, Training Loss: 1.0885\n",
      "Epoch 2/10, Batch 133/883, Training Loss: 0.8206\n",
      "Epoch 2/10, Batch 134/883, Training Loss: 1.2173\n",
      "Epoch 2/10, Batch 135/883, Training Loss: 0.8749\n",
      "Epoch 2/10, Batch 136/883, Training Loss: 0.9582\n",
      "Epoch 2/10, Batch 137/883, Training Loss: 0.9901\n",
      "Epoch 2/10, Batch 138/883, Training Loss: 0.7612\n",
      "Epoch 2/10, Batch 139/883, Training Loss: 1.1362\n",
      "Epoch 2/10, Batch 140/883, Training Loss: 1.2875\n",
      "Epoch 2/10, Batch 141/883, Training Loss: 0.6695\n",
      "Epoch 2/10, Batch 142/883, Training Loss: 1.0825\n",
      "Epoch 2/10, Batch 143/883, Training Loss: 0.8264\n",
      "Epoch 2/10, Batch 144/883, Training Loss: 0.7413\n",
      "Epoch 2/10, Batch 145/883, Training Loss: 0.7810\n",
      "Epoch 2/10, Batch 146/883, Training Loss: 0.9030\n",
      "Epoch 2/10, Batch 147/883, Training Loss: 1.0292\n",
      "Epoch 2/10, Batch 148/883, Training Loss: 0.7382\n",
      "Epoch 2/10, Batch 149/883, Training Loss: 0.6760\n",
      "Epoch 2/10, Batch 150/883, Training Loss: 0.8394\n",
      "Epoch 2/10, Batch 151/883, Training Loss: 0.7442\n",
      "Epoch 2/10, Batch 152/883, Training Loss: 0.8867\n",
      "Epoch 2/10, Batch 153/883, Training Loss: 1.1127\n",
      "Epoch 2/10, Batch 154/883, Training Loss: 0.6243\n",
      "Epoch 2/10, Batch 155/883, Training Loss: 0.8020\n",
      "Epoch 2/10, Batch 156/883, Training Loss: 1.0377\n",
      "Epoch 2/10, Batch 157/883, Training Loss: 1.0241\n",
      "Epoch 2/10, Batch 158/883, Training Loss: 0.7627\n",
      "Epoch 2/10, Batch 159/883, Training Loss: 0.7827\n",
      "Epoch 2/10, Batch 160/883, Training Loss: 0.7849\n",
      "Epoch 2/10, Batch 161/883, Training Loss: 1.1379\n",
      "Epoch 2/10, Batch 162/883, Training Loss: 0.7289\n",
      "Epoch 2/10, Batch 163/883, Training Loss: 0.8094\n",
      "Epoch 2/10, Batch 164/883, Training Loss: 0.6612\n",
      "Epoch 2/10, Batch 165/883, Training Loss: 0.8767\n",
      "Epoch 2/10, Batch 166/883, Training Loss: 0.9277\n",
      "Epoch 2/10, Batch 167/883, Training Loss: 0.7388\n",
      "Epoch 2/10, Batch 168/883, Training Loss: 0.9896\n",
      "Epoch 2/10, Batch 169/883, Training Loss: 0.9822\n",
      "Epoch 2/10, Batch 170/883, Training Loss: 1.0889\n",
      "Epoch 2/10, Batch 171/883, Training Loss: 0.8536\n",
      "Epoch 2/10, Batch 172/883, Training Loss: 0.8249\n",
      "Epoch 2/10, Batch 173/883, Training Loss: 0.7537\n",
      "Epoch 2/10, Batch 174/883, Training Loss: 0.6363\n",
      "Epoch 2/10, Batch 175/883, Training Loss: 0.7867\n",
      "Epoch 2/10, Batch 176/883, Training Loss: 0.8253\n",
      "Epoch 2/10, Batch 177/883, Training Loss: 0.6498\n",
      "Epoch 2/10, Batch 178/883, Training Loss: 0.9379\n",
      "Epoch 2/10, Batch 179/883, Training Loss: 0.8459\n",
      "Epoch 2/10, Batch 180/883, Training Loss: 0.9379\n",
      "Epoch 2/10, Batch 181/883, Training Loss: 0.7917\n",
      "Epoch 2/10, Batch 182/883, Training Loss: 0.7695\n",
      "Epoch 2/10, Batch 183/883, Training Loss: 0.9892\n",
      "Epoch 2/10, Batch 184/883, Training Loss: 0.8435\n",
      "Epoch 2/10, Batch 185/883, Training Loss: 0.7060\n",
      "Epoch 2/10, Batch 186/883, Training Loss: 0.5956\n",
      "Epoch 2/10, Batch 187/883, Training Loss: 0.8795\n",
      "Epoch 2/10, Batch 188/883, Training Loss: 0.6425\n",
      "Epoch 2/10, Batch 189/883, Training Loss: 1.0845\n",
      "Epoch 2/10, Batch 190/883, Training Loss: 1.0305\n",
      "Epoch 2/10, Batch 191/883, Training Loss: 1.1590\n",
      "Epoch 2/10, Batch 192/883, Training Loss: 0.9016\n",
      "Epoch 2/10, Batch 193/883, Training Loss: 0.7818\n",
      "Epoch 2/10, Batch 194/883, Training Loss: 0.8554\n",
      "Epoch 2/10, Batch 195/883, Training Loss: 0.9690\n",
      "Epoch 2/10, Batch 196/883, Training Loss: 0.7799\n",
      "Epoch 2/10, Batch 197/883, Training Loss: 0.6651\n",
      "Epoch 2/10, Batch 198/883, Training Loss: 0.6198\n",
      "Epoch 2/10, Batch 199/883, Training Loss: 0.7134\n",
      "Epoch 2/10, Batch 200/883, Training Loss: 1.3542\n",
      "Epoch 2/10, Batch 201/883, Training Loss: 1.0620\n",
      "Epoch 2/10, Batch 202/883, Training Loss: 0.7904\n",
      "Epoch 2/10, Batch 203/883, Training Loss: 1.1642\n",
      "Epoch 2/10, Batch 204/883, Training Loss: 0.7430\n",
      "Epoch 2/10, Batch 205/883, Training Loss: 0.9570\n",
      "Epoch 2/10, Batch 206/883, Training Loss: 0.8870\n",
      "Epoch 2/10, Batch 207/883, Training Loss: 0.8678\n",
      "Epoch 2/10, Batch 208/883, Training Loss: 0.9317\n",
      "Epoch 2/10, Batch 209/883, Training Loss: 0.9237\n",
      "Epoch 2/10, Batch 210/883, Training Loss: 0.9479\n",
      "Epoch 2/10, Batch 211/883, Training Loss: 0.7161\n",
      "Epoch 2/10, Batch 212/883, Training Loss: 0.8780\n",
      "Epoch 2/10, Batch 213/883, Training Loss: 0.7934\n",
      "Epoch 2/10, Batch 214/883, Training Loss: 0.8977\n",
      "Epoch 2/10, Batch 215/883, Training Loss: 0.8469\n",
      "Epoch 2/10, Batch 216/883, Training Loss: 0.8586\n",
      "Epoch 2/10, Batch 217/883, Training Loss: 1.0023\n",
      "Epoch 2/10, Batch 218/883, Training Loss: 0.8356\n",
      "Epoch 2/10, Batch 219/883, Training Loss: 0.9358\n",
      "Epoch 2/10, Batch 220/883, Training Loss: 0.7742\n",
      "Epoch 2/10, Batch 221/883, Training Loss: 0.7918\n",
      "Epoch 2/10, Batch 222/883, Training Loss: 0.7302\n",
      "Epoch 2/10, Batch 223/883, Training Loss: 0.7625\n",
      "Epoch 2/10, Batch 224/883, Training Loss: 0.7268\n",
      "Epoch 2/10, Batch 225/883, Training Loss: 0.6587\n",
      "Epoch 2/10, Batch 226/883, Training Loss: 0.7464\n",
      "Epoch 2/10, Batch 227/883, Training Loss: 0.7227\n",
      "Epoch 2/10, Batch 228/883, Training Loss: 1.3311\n",
      "Epoch 2/10, Batch 229/883, Training Loss: 1.0198\n",
      "Epoch 2/10, Batch 230/883, Training Loss: 0.6611\n",
      "Epoch 2/10, Batch 231/883, Training Loss: 0.7931\n",
      "Epoch 2/10, Batch 232/883, Training Loss: 0.9914\n",
      "Epoch 2/10, Batch 233/883, Training Loss: 0.8280\n",
      "Epoch 2/10, Batch 234/883, Training Loss: 0.8100\n",
      "Epoch 2/10, Batch 235/883, Training Loss: 1.1278\n",
      "Epoch 2/10, Batch 236/883, Training Loss: 1.0066\n",
      "Epoch 2/10, Batch 237/883, Training Loss: 1.0155\n",
      "Epoch 2/10, Batch 238/883, Training Loss: 0.8204\n",
      "Epoch 2/10, Batch 239/883, Training Loss: 0.8223\n",
      "Epoch 2/10, Batch 240/883, Training Loss: 1.0296\n",
      "Epoch 2/10, Batch 241/883, Training Loss: 0.8751\n",
      "Epoch 2/10, Batch 242/883, Training Loss: 0.9060\n",
      "Epoch 2/10, Batch 243/883, Training Loss: 0.9529\n",
      "Epoch 2/10, Batch 244/883, Training Loss: 0.9259\n",
      "Epoch 2/10, Batch 245/883, Training Loss: 1.0064\n",
      "Epoch 2/10, Batch 246/883, Training Loss: 0.9116\n",
      "Epoch 2/10, Batch 247/883, Training Loss: 0.7461\n",
      "Epoch 2/10, Batch 248/883, Training Loss: 0.7131\n",
      "Epoch 2/10, Batch 249/883, Training Loss: 0.7264\n",
      "Epoch 2/10, Batch 250/883, Training Loss: 0.8819\n",
      "Epoch 2/10, Batch 251/883, Training Loss: 0.6296\n",
      "Epoch 2/10, Batch 252/883, Training Loss: 0.9077\n",
      "Epoch 2/10, Batch 253/883, Training Loss: 0.9451\n",
      "Epoch 2/10, Batch 254/883, Training Loss: 0.7487\n",
      "Epoch 2/10, Batch 255/883, Training Loss: 1.1267\n",
      "Epoch 2/10, Batch 256/883, Training Loss: 0.7152\n",
      "Epoch 2/10, Batch 257/883, Training Loss: 0.6687\n",
      "Epoch 2/10, Batch 258/883, Training Loss: 0.8877\n",
      "Epoch 2/10, Batch 259/883, Training Loss: 0.7023\n",
      "Epoch 2/10, Batch 260/883, Training Loss: 0.5848\n",
      "Epoch 2/10, Batch 261/883, Training Loss: 0.6305\n",
      "Epoch 2/10, Batch 262/883, Training Loss: 1.0131\n",
      "Epoch 2/10, Batch 263/883, Training Loss: 0.8571\n",
      "Epoch 2/10, Batch 264/883, Training Loss: 0.9787\n",
      "Epoch 2/10, Batch 265/883, Training Loss: 1.0109\n",
      "Epoch 2/10, Batch 266/883, Training Loss: 0.9315\n",
      "Epoch 2/10, Batch 267/883, Training Loss: 0.8116\n",
      "Epoch 2/10, Batch 268/883, Training Loss: 0.8967\n",
      "Epoch 2/10, Batch 269/883, Training Loss: 0.9788\n",
      "Epoch 2/10, Batch 270/883, Training Loss: 0.9565\n",
      "Epoch 2/10, Batch 271/883, Training Loss: 0.7590\n",
      "Epoch 2/10, Batch 272/883, Training Loss: 1.0099\n",
      "Epoch 2/10, Batch 273/883, Training Loss: 0.8334\n",
      "Epoch 2/10, Batch 274/883, Training Loss: 0.9069\n",
      "Epoch 2/10, Batch 275/883, Training Loss: 0.6903\n",
      "Epoch 2/10, Batch 276/883, Training Loss: 1.1981\n",
      "Epoch 2/10, Batch 277/883, Training Loss: 0.8770\n",
      "Epoch 2/10, Batch 278/883, Training Loss: 0.8467\n",
      "Epoch 2/10, Batch 279/883, Training Loss: 1.2610\n",
      "Epoch 2/10, Batch 280/883, Training Loss: 0.6381\n",
      "Epoch 2/10, Batch 281/883, Training Loss: 0.7132\n",
      "Epoch 2/10, Batch 282/883, Training Loss: 0.8128\n",
      "Epoch 2/10, Batch 283/883, Training Loss: 0.8431\n",
      "Epoch 2/10, Batch 284/883, Training Loss: 0.7695\n",
      "Epoch 2/10, Batch 285/883, Training Loss: 0.8627\n",
      "Epoch 2/10, Batch 286/883, Training Loss: 1.0228\n",
      "Epoch 2/10, Batch 287/883, Training Loss: 0.7237\n",
      "Epoch 2/10, Batch 288/883, Training Loss: 0.6599\n",
      "Epoch 2/10, Batch 289/883, Training Loss: 1.0213\n",
      "Epoch 2/10, Batch 290/883, Training Loss: 0.7529\n",
      "Epoch 2/10, Batch 291/883, Training Loss: 1.0713\n",
      "Epoch 2/10, Batch 292/883, Training Loss: 1.0382\n",
      "Epoch 2/10, Batch 293/883, Training Loss: 1.2610\n",
      "Epoch 2/10, Batch 294/883, Training Loss: 0.9539\n",
      "Epoch 2/10, Batch 295/883, Training Loss: 0.8663\n",
      "Epoch 2/10, Batch 296/883, Training Loss: 0.8091\n",
      "Epoch 2/10, Batch 297/883, Training Loss: 0.8857\n",
      "Epoch 2/10, Batch 298/883, Training Loss: 1.0359\n",
      "Epoch 2/10, Batch 299/883, Training Loss: 0.6811\n",
      "Epoch 2/10, Batch 300/883, Training Loss: 1.1871\n",
      "Epoch 2/10, Batch 301/883, Training Loss: 0.9666\n",
      "Epoch 2/10, Batch 302/883, Training Loss: 0.7936\n",
      "Epoch 2/10, Batch 303/883, Training Loss: 0.9019\n",
      "Epoch 2/10, Batch 304/883, Training Loss: 0.9144\n",
      "Epoch 2/10, Batch 305/883, Training Loss: 0.8512\n",
      "Epoch 2/10, Batch 306/883, Training Loss: 0.9073\n",
      "Epoch 2/10, Batch 307/883, Training Loss: 0.8386\n",
      "Epoch 2/10, Batch 308/883, Training Loss: 0.9881\n",
      "Epoch 2/10, Batch 309/883, Training Loss: 0.8464\n",
      "Epoch 2/10, Batch 310/883, Training Loss: 1.0321\n",
      "Epoch 2/10, Batch 311/883, Training Loss: 0.6932\n",
      "Epoch 2/10, Batch 312/883, Training Loss: 0.8525\n",
      "Epoch 2/10, Batch 313/883, Training Loss: 0.8386\n",
      "Epoch 2/10, Batch 314/883, Training Loss: 0.8049\n",
      "Epoch 2/10, Batch 315/883, Training Loss: 0.9702\n",
      "Epoch 2/10, Batch 316/883, Training Loss: 0.6286\n",
      "Epoch 2/10, Batch 317/883, Training Loss: 0.8555\n",
      "Epoch 2/10, Batch 318/883, Training Loss: 0.9921\n",
      "Epoch 2/10, Batch 319/883, Training Loss: 0.9616\n",
      "Epoch 2/10, Batch 320/883, Training Loss: 0.8733\n",
      "Epoch 2/10, Batch 321/883, Training Loss: 0.8519\n",
      "Epoch 2/10, Batch 322/883, Training Loss: 0.9034\n",
      "Epoch 2/10, Batch 323/883, Training Loss: 1.2635\n",
      "Epoch 2/10, Batch 324/883, Training Loss: 0.7363\n",
      "Epoch 2/10, Batch 325/883, Training Loss: 1.1071\n",
      "Epoch 2/10, Batch 326/883, Training Loss: 0.8062\n",
      "Epoch 2/10, Batch 327/883, Training Loss: 0.9084\n",
      "Epoch 2/10, Batch 328/883, Training Loss: 0.7622\n",
      "Epoch 2/10, Batch 329/883, Training Loss: 0.9725\n",
      "Epoch 2/10, Batch 330/883, Training Loss: 0.8364\n",
      "Epoch 2/10, Batch 331/883, Training Loss: 0.8315\n",
      "Epoch 2/10, Batch 332/883, Training Loss: 1.0037\n",
      "Epoch 2/10, Batch 333/883, Training Loss: 0.8486\n",
      "Epoch 2/10, Batch 334/883, Training Loss: 0.7075\n",
      "Epoch 2/10, Batch 335/883, Training Loss: 0.8209\n",
      "Epoch 2/10, Batch 336/883, Training Loss: 0.7538\n",
      "Epoch 2/10, Batch 337/883, Training Loss: 0.7147\n",
      "Epoch 2/10, Batch 338/883, Training Loss: 0.7249\n",
      "Epoch 2/10, Batch 339/883, Training Loss: 0.6797\n",
      "Epoch 2/10, Batch 340/883, Training Loss: 0.7790\n",
      "Epoch 2/10, Batch 341/883, Training Loss: 1.0159\n",
      "Epoch 2/10, Batch 342/883, Training Loss: 0.6936\n",
      "Epoch 2/10, Batch 343/883, Training Loss: 0.7639\n",
      "Epoch 2/10, Batch 344/883, Training Loss: 1.3734\n",
      "Epoch 2/10, Batch 345/883, Training Loss: 1.2067\n",
      "Epoch 2/10, Batch 346/883, Training Loss: 1.0890\n",
      "Epoch 2/10, Batch 347/883, Training Loss: 0.8449\n",
      "Epoch 2/10, Batch 348/883, Training Loss: 0.6993\n",
      "Epoch 2/10, Batch 349/883, Training Loss: 0.7048\n",
      "Epoch 2/10, Batch 350/883, Training Loss: 0.9030\n",
      "Epoch 2/10, Batch 351/883, Training Loss: 1.0467\n",
      "Epoch 2/10, Batch 352/883, Training Loss: 0.9646\n",
      "Epoch 2/10, Batch 353/883, Training Loss: 0.7146\n",
      "Epoch 2/10, Batch 354/883, Training Loss: 1.2966\n",
      "Epoch 2/10, Batch 355/883, Training Loss: 0.7777\n",
      "Epoch 2/10, Batch 356/883, Training Loss: 1.0645\n",
      "Epoch 2/10, Batch 357/883, Training Loss: 0.7492\n",
      "Epoch 2/10, Batch 358/883, Training Loss: 0.9147\n",
      "Epoch 2/10, Batch 359/883, Training Loss: 0.7333\n",
      "Epoch 2/10, Batch 360/883, Training Loss: 0.8856\n",
      "Epoch 2/10, Batch 361/883, Training Loss: 0.6938\n",
      "Epoch 2/10, Batch 362/883, Training Loss: 0.8227\n",
      "Epoch 2/10, Batch 363/883, Training Loss: 0.7642\n",
      "Epoch 2/10, Batch 364/883, Training Loss: 1.0107\n",
      "Epoch 2/10, Batch 365/883, Training Loss: 0.7563\n",
      "Epoch 2/10, Batch 366/883, Training Loss: 1.0717\n",
      "Epoch 2/10, Batch 367/883, Training Loss: 0.7938\n",
      "Epoch 2/10, Batch 368/883, Training Loss: 0.8413\n",
      "Epoch 2/10, Batch 369/883, Training Loss: 0.9433\n",
      "Epoch 2/10, Batch 370/883, Training Loss: 0.9323\n",
      "Epoch 2/10, Batch 371/883, Training Loss: 0.7593\n",
      "Epoch 2/10, Batch 372/883, Training Loss: 0.8997\n",
      "Epoch 2/10, Batch 373/883, Training Loss: 0.8432\n",
      "Epoch 2/10, Batch 374/883, Training Loss: 0.9647\n",
      "Epoch 2/10, Batch 375/883, Training Loss: 0.7140\n",
      "Epoch 2/10, Batch 376/883, Training Loss: 0.8596\n",
      "Epoch 2/10, Batch 377/883, Training Loss: 0.9114\n",
      "Epoch 2/10, Batch 378/883, Training Loss: 0.7290\n",
      "Epoch 2/10, Batch 379/883, Training Loss: 0.7733\n",
      "Epoch 2/10, Batch 380/883, Training Loss: 0.8284\n",
      "Epoch 2/10, Batch 381/883, Training Loss: 0.7862\n",
      "Epoch 2/10, Batch 382/883, Training Loss: 0.8419\n",
      "Epoch 2/10, Batch 383/883, Training Loss: 0.9647\n",
      "Epoch 2/10, Batch 384/883, Training Loss: 0.7972\n",
      "Epoch 2/10, Batch 385/883, Training Loss: 0.9034\n",
      "Epoch 2/10, Batch 386/883, Training Loss: 1.3221\n",
      "Epoch 2/10, Batch 387/883, Training Loss: 0.9420\n",
      "Epoch 2/10, Batch 388/883, Training Loss: 1.0181\n",
      "Epoch 2/10, Batch 389/883, Training Loss: 0.7763\n",
      "Epoch 2/10, Batch 390/883, Training Loss: 1.0122\n",
      "Epoch 2/10, Batch 391/883, Training Loss: 0.8517\n",
      "Epoch 2/10, Batch 392/883, Training Loss: 0.9585\n",
      "Epoch 2/10, Batch 393/883, Training Loss: 0.7957\n",
      "Epoch 2/10, Batch 394/883, Training Loss: 0.7278\n",
      "Epoch 2/10, Batch 395/883, Training Loss: 0.8195\n",
      "Epoch 2/10, Batch 396/883, Training Loss: 0.7947\n",
      "Epoch 2/10, Batch 397/883, Training Loss: 0.7317\n",
      "Epoch 2/10, Batch 398/883, Training Loss: 0.9232\n",
      "Epoch 2/10, Batch 399/883, Training Loss: 0.7879\n",
      "Epoch 2/10, Batch 400/883, Training Loss: 0.9424\n",
      "Epoch 2/10, Batch 401/883, Training Loss: 0.7736\n",
      "Epoch 2/10, Batch 402/883, Training Loss: 0.8987\n",
      "Epoch 2/10, Batch 403/883, Training Loss: 0.8568\n",
      "Epoch 2/10, Batch 404/883, Training Loss: 0.9602\n",
      "Epoch 2/10, Batch 405/883, Training Loss: 0.6389\n",
      "Epoch 2/10, Batch 406/883, Training Loss: 0.8553\n",
      "Epoch 2/10, Batch 407/883, Training Loss: 0.9138\n",
      "Epoch 2/10, Batch 408/883, Training Loss: 0.6015\n",
      "Epoch 2/10, Batch 409/883, Training Loss: 0.8927\n",
      "Epoch 2/10, Batch 410/883, Training Loss: 0.8507\n",
      "Epoch 2/10, Batch 411/883, Training Loss: 0.7309\n",
      "Epoch 2/10, Batch 412/883, Training Loss: 0.8017\n",
      "Epoch 2/10, Batch 413/883, Training Loss: 0.7349\n",
      "Epoch 2/10, Batch 414/883, Training Loss: 0.5989\n",
      "Epoch 2/10, Batch 415/883, Training Loss: 0.9535\n",
      "Epoch 2/10, Batch 416/883, Training Loss: 0.7912\n",
      "Epoch 2/10, Batch 417/883, Training Loss: 0.8529\n",
      "Epoch 2/10, Batch 418/883, Training Loss: 1.0863\n",
      "Epoch 2/10, Batch 419/883, Training Loss: 0.8098\n",
      "Epoch 2/10, Batch 420/883, Training Loss: 0.9943\n",
      "Epoch 2/10, Batch 421/883, Training Loss: 1.1002\n",
      "Epoch 2/10, Batch 422/883, Training Loss: 1.1538\n",
      "Epoch 2/10, Batch 423/883, Training Loss: 0.6924\n",
      "Epoch 2/10, Batch 424/883, Training Loss: 0.8514\n",
      "Epoch 2/10, Batch 425/883, Training Loss: 1.2791\n",
      "Epoch 2/10, Batch 426/883, Training Loss: 0.7511\n",
      "Epoch 2/10, Batch 427/883, Training Loss: 0.7230\n",
      "Epoch 2/10, Batch 428/883, Training Loss: 0.8234\n",
      "Epoch 2/10, Batch 429/883, Training Loss: 0.9422\n",
      "Epoch 2/10, Batch 430/883, Training Loss: 0.7012\n",
      "Epoch 2/10, Batch 431/883, Training Loss: 0.7856\n",
      "Epoch 2/10, Batch 432/883, Training Loss: 0.7949\n",
      "Epoch 2/10, Batch 433/883, Training Loss: 0.7503\n",
      "Epoch 2/10, Batch 434/883, Training Loss: 0.8708\n",
      "Epoch 2/10, Batch 435/883, Training Loss: 0.7529\n",
      "Epoch 2/10, Batch 436/883, Training Loss: 0.8495\n",
      "Epoch 2/10, Batch 437/883, Training Loss: 0.9506\n",
      "Epoch 2/10, Batch 438/883, Training Loss: 0.8053\n",
      "Epoch 2/10, Batch 439/883, Training Loss: 0.8557\n",
      "Epoch 2/10, Batch 440/883, Training Loss: 0.9066\n",
      "Epoch 2/10, Batch 441/883, Training Loss: 0.6349\n",
      "Epoch 2/10, Batch 442/883, Training Loss: 0.8371\n",
      "Epoch 2/10, Batch 443/883, Training Loss: 0.7652\n",
      "Epoch 2/10, Batch 444/883, Training Loss: 0.8687\n",
      "Epoch 2/10, Batch 445/883, Training Loss: 0.7803\n",
      "Epoch 2/10, Batch 446/883, Training Loss: 0.7379\n",
      "Epoch 2/10, Batch 447/883, Training Loss: 0.8351\n",
      "Epoch 2/10, Batch 448/883, Training Loss: 0.6446\n",
      "Epoch 2/10, Batch 449/883, Training Loss: 0.6919\n",
      "Epoch 2/10, Batch 450/883, Training Loss: 0.8788\n",
      "Epoch 2/10, Batch 451/883, Training Loss: 0.9256\n",
      "Epoch 2/10, Batch 452/883, Training Loss: 0.8943\n",
      "Epoch 2/10, Batch 453/883, Training Loss: 1.0848\n",
      "Epoch 2/10, Batch 454/883, Training Loss: 0.7500\n",
      "Epoch 2/10, Batch 455/883, Training Loss: 0.8107\n",
      "Epoch 2/10, Batch 456/883, Training Loss: 0.8410\n",
      "Epoch 2/10, Batch 457/883, Training Loss: 0.9380\n",
      "Epoch 2/10, Batch 458/883, Training Loss: 0.8792\n",
      "Epoch 2/10, Batch 459/883, Training Loss: 0.6601\n",
      "Epoch 2/10, Batch 460/883, Training Loss: 1.2114\n",
      "Epoch 2/10, Batch 461/883, Training Loss: 1.3189\n",
      "Epoch 2/10, Batch 462/883, Training Loss: 0.7953\n",
      "Epoch 2/10, Batch 463/883, Training Loss: 0.9627\n",
      "Epoch 2/10, Batch 464/883, Training Loss: 0.8794\n",
      "Epoch 2/10, Batch 465/883, Training Loss: 0.7163\n",
      "Epoch 2/10, Batch 466/883, Training Loss: 0.6743\n",
      "Epoch 2/10, Batch 467/883, Training Loss: 0.7645\n",
      "Epoch 2/10, Batch 468/883, Training Loss: 1.1117\n",
      "Epoch 2/10, Batch 469/883, Training Loss: 0.9786\n",
      "Epoch 2/10, Batch 470/883, Training Loss: 0.7813\n",
      "Epoch 2/10, Batch 471/883, Training Loss: 1.0763\n",
      "Epoch 2/10, Batch 472/883, Training Loss: 0.9252\n",
      "Epoch 2/10, Batch 473/883, Training Loss: 0.8841\n",
      "Epoch 2/10, Batch 474/883, Training Loss: 0.7951\n",
      "Epoch 2/10, Batch 475/883, Training Loss: 0.9552\n",
      "Epoch 2/10, Batch 476/883, Training Loss: 1.0016\n",
      "Epoch 2/10, Batch 477/883, Training Loss: 0.7941\n",
      "Epoch 2/10, Batch 478/883, Training Loss: 0.6672\n",
      "Epoch 2/10, Batch 479/883, Training Loss: 0.9393\n",
      "Epoch 2/10, Batch 480/883, Training Loss: 0.9835\n",
      "Epoch 2/10, Batch 481/883, Training Loss: 0.8219\n",
      "Epoch 2/10, Batch 482/883, Training Loss: 0.7118\n",
      "Epoch 2/10, Batch 483/883, Training Loss: 0.6751\n",
      "Epoch 2/10, Batch 484/883, Training Loss: 0.7247\n",
      "Epoch 2/10, Batch 485/883, Training Loss: 1.0409\n",
      "Epoch 2/10, Batch 486/883, Training Loss: 0.8593\n",
      "Epoch 2/10, Batch 487/883, Training Loss: 0.8034\n",
      "Epoch 2/10, Batch 488/883, Training Loss: 0.6559\n",
      "Epoch 2/10, Batch 489/883, Training Loss: 0.9641\n",
      "Epoch 2/10, Batch 490/883, Training Loss: 0.8426\n",
      "Epoch 2/10, Batch 491/883, Training Loss: 0.8254\n",
      "Epoch 2/10, Batch 492/883, Training Loss: 0.7290\n",
      "Epoch 2/10, Batch 493/883, Training Loss: 0.9951\n",
      "Epoch 2/10, Batch 494/883, Training Loss: 0.7565\n",
      "Epoch 2/10, Batch 495/883, Training Loss: 0.8107\n",
      "Epoch 2/10, Batch 496/883, Training Loss: 0.6588\n",
      "Epoch 2/10, Batch 497/883, Training Loss: 0.8447\n",
      "Epoch 2/10, Batch 498/883, Training Loss: 1.1310\n",
      "Epoch 2/10, Batch 499/883, Training Loss: 0.9537\n",
      "Epoch 2/10, Batch 500/883, Training Loss: 0.8662\n",
      "Epoch 2/10, Batch 501/883, Training Loss: 0.8978\n",
      "Epoch 2/10, Batch 502/883, Training Loss: 0.9233\n",
      "Epoch 2/10, Batch 503/883, Training Loss: 0.6939\n",
      "Epoch 2/10, Batch 504/883, Training Loss: 0.9764\n",
      "Epoch 2/10, Batch 505/883, Training Loss: 0.7374\n",
      "Epoch 2/10, Batch 506/883, Training Loss: 0.9687\n",
      "Epoch 2/10, Batch 507/883, Training Loss: 0.6516\n",
      "Epoch 2/10, Batch 508/883, Training Loss: 0.6805\n",
      "Epoch 2/10, Batch 509/883, Training Loss: 0.8733\n",
      "Epoch 2/10, Batch 510/883, Training Loss: 0.8648\n",
      "Epoch 2/10, Batch 511/883, Training Loss: 0.8487\n",
      "Epoch 2/10, Batch 512/883, Training Loss: 0.8845\n",
      "Epoch 2/10, Batch 513/883, Training Loss: 0.7617\n",
      "Epoch 2/10, Batch 514/883, Training Loss: 0.6495\n",
      "Epoch 2/10, Batch 515/883, Training Loss: 0.6800\n",
      "Epoch 2/10, Batch 516/883, Training Loss: 0.6783\n",
      "Epoch 2/10, Batch 517/883, Training Loss: 0.7922\n",
      "Epoch 2/10, Batch 518/883, Training Loss: 1.0779\n",
      "Epoch 2/10, Batch 519/883, Training Loss: 0.8547\n",
      "Epoch 2/10, Batch 520/883, Training Loss: 0.7947\n",
      "Epoch 2/10, Batch 521/883, Training Loss: 0.9232\n",
      "Epoch 2/10, Batch 522/883, Training Loss: 0.7009\n",
      "Epoch 2/10, Batch 523/883, Training Loss: 0.6564\n",
      "Epoch 2/10, Batch 524/883, Training Loss: 0.7870\n",
      "Epoch 2/10, Batch 525/883, Training Loss: 1.1016\n",
      "Epoch 2/10, Batch 526/883, Training Loss: 0.8131\n",
      "Epoch 2/10, Batch 527/883, Training Loss: 0.6885\n",
      "Epoch 2/10, Batch 528/883, Training Loss: 1.0373\n",
      "Epoch 2/10, Batch 529/883, Training Loss: 0.8815\n",
      "Epoch 2/10, Batch 530/883, Training Loss: 0.8048\n",
      "Epoch 2/10, Batch 531/883, Training Loss: 0.7007\n",
      "Epoch 2/10, Batch 532/883, Training Loss: 0.7772\n",
      "Epoch 2/10, Batch 533/883, Training Loss: 0.8604\n",
      "Epoch 2/10, Batch 534/883, Training Loss: 0.6327\n",
      "Epoch 2/10, Batch 535/883, Training Loss: 0.7854\n",
      "Epoch 2/10, Batch 536/883, Training Loss: 0.8499\n",
      "Epoch 2/10, Batch 537/883, Training Loss: 1.0792\n",
      "Epoch 2/10, Batch 538/883, Training Loss: 0.7719\n",
      "Epoch 2/10, Batch 539/883, Training Loss: 1.0180\n",
      "Epoch 2/10, Batch 540/883, Training Loss: 0.8364\n",
      "Epoch 2/10, Batch 541/883, Training Loss: 1.2427\n",
      "Epoch 2/10, Batch 542/883, Training Loss: 0.8262\n",
      "Epoch 2/10, Batch 543/883, Training Loss: 0.7345\n",
      "Epoch 2/10, Batch 544/883, Training Loss: 0.7748\n",
      "Epoch 2/10, Batch 545/883, Training Loss: 1.0380\n",
      "Epoch 2/10, Batch 546/883, Training Loss: 0.8973\n",
      "Epoch 2/10, Batch 547/883, Training Loss: 0.7770\n",
      "Epoch 2/10, Batch 548/883, Training Loss: 0.8305\n",
      "Epoch 2/10, Batch 549/883, Training Loss: 0.9161\n",
      "Epoch 2/10, Batch 550/883, Training Loss: 0.9540\n",
      "Epoch 2/10, Batch 551/883, Training Loss: 1.0357\n",
      "Epoch 2/10, Batch 552/883, Training Loss: 1.0740\n",
      "Epoch 2/10, Batch 553/883, Training Loss: 0.8701\n",
      "Epoch 2/10, Batch 554/883, Training Loss: 0.9496\n",
      "Epoch 2/10, Batch 555/883, Training Loss: 0.8881\n",
      "Epoch 2/10, Batch 556/883, Training Loss: 0.7709\n",
      "Epoch 2/10, Batch 557/883, Training Loss: 0.8659\n",
      "Epoch 2/10, Batch 558/883, Training Loss: 0.9329\n",
      "Epoch 2/10, Batch 559/883, Training Loss: 0.7517\n",
      "Epoch 2/10, Batch 560/883, Training Loss: 0.7875\n",
      "Epoch 2/10, Batch 561/883, Training Loss: 0.9946\n",
      "Epoch 2/10, Batch 562/883, Training Loss: 0.8195\n",
      "Epoch 2/10, Batch 563/883, Training Loss: 0.8950\n",
      "Epoch 2/10, Batch 564/883, Training Loss: 0.8537\n",
      "Epoch 2/10, Batch 565/883, Training Loss: 0.6647\n",
      "Epoch 2/10, Batch 566/883, Training Loss: 0.7800\n",
      "Epoch 2/10, Batch 567/883, Training Loss: 0.5920\n",
      "Epoch 2/10, Batch 568/883, Training Loss: 0.9631\n",
      "Epoch 2/10, Batch 569/883, Training Loss: 0.6821\n",
      "Epoch 2/10, Batch 570/883, Training Loss: 0.6658\n",
      "Epoch 2/10, Batch 571/883, Training Loss: 0.8986\n",
      "Epoch 2/10, Batch 572/883, Training Loss: 0.7403\n",
      "Epoch 2/10, Batch 573/883, Training Loss: 0.8798\n",
      "Epoch 2/10, Batch 574/883, Training Loss: 0.7415\n",
      "Epoch 2/10, Batch 575/883, Training Loss: 0.7139\n",
      "Epoch 2/10, Batch 576/883, Training Loss: 1.0662\n",
      "Epoch 2/10, Batch 577/883, Training Loss: 0.6353\n",
      "Epoch 2/10, Batch 578/883, Training Loss: 0.7983\n",
      "Epoch 2/10, Batch 579/883, Training Loss: 1.1332\n",
      "Epoch 2/10, Batch 580/883, Training Loss: 1.1282\n",
      "Epoch 2/10, Batch 581/883, Training Loss: 0.9820\n",
      "Epoch 2/10, Batch 582/883, Training Loss: 0.8857\n",
      "Epoch 2/10, Batch 583/883, Training Loss: 0.8412\n",
      "Epoch 2/10, Batch 584/883, Training Loss: 0.8615\n",
      "Epoch 2/10, Batch 585/883, Training Loss: 0.8001\n",
      "Epoch 2/10, Batch 586/883, Training Loss: 0.9519\n",
      "Epoch 2/10, Batch 587/883, Training Loss: 0.8731\n",
      "Epoch 2/10, Batch 588/883, Training Loss: 0.6995\n",
      "Epoch 2/10, Batch 589/883, Training Loss: 0.8156\n",
      "Epoch 2/10, Batch 590/883, Training Loss: 0.8000\n",
      "Epoch 2/10, Batch 591/883, Training Loss: 0.6916\n",
      "Epoch 2/10, Batch 592/883, Training Loss: 0.8093\n",
      "Epoch 2/10, Batch 593/883, Training Loss: 0.6374\n",
      "Epoch 2/10, Batch 594/883, Training Loss: 0.7665\n",
      "Epoch 2/10, Batch 595/883, Training Loss: 0.6888\n",
      "Epoch 2/10, Batch 596/883, Training Loss: 0.7976\n",
      "Epoch 2/10, Batch 597/883, Training Loss: 0.9984\n",
      "Epoch 2/10, Batch 598/883, Training Loss: 0.7197\n",
      "Epoch 2/10, Batch 599/883, Training Loss: 0.9301\n",
      "Epoch 2/10, Batch 600/883, Training Loss: 1.0626\n",
      "Epoch 2/10, Batch 601/883, Training Loss: 0.7011\n",
      "Epoch 2/10, Batch 602/883, Training Loss: 0.7098\n",
      "Epoch 2/10, Batch 603/883, Training Loss: 0.8864\n",
      "Epoch 2/10, Batch 604/883, Training Loss: 0.8236\n",
      "Epoch 2/10, Batch 605/883, Training Loss: 0.8337\n",
      "Epoch 2/10, Batch 606/883, Training Loss: 0.7877\n",
      "Epoch 2/10, Batch 607/883, Training Loss: 0.8615\n",
      "Epoch 2/10, Batch 608/883, Training Loss: 0.7828\n",
      "Epoch 2/10, Batch 609/883, Training Loss: 0.8807\n",
      "Epoch 2/10, Batch 610/883, Training Loss: 1.1349\n",
      "Epoch 2/10, Batch 611/883, Training Loss: 0.7623\n",
      "Epoch 2/10, Batch 612/883, Training Loss: 0.9147\n",
      "Epoch 2/10, Batch 613/883, Training Loss: 0.7756\n",
      "Epoch 2/10, Batch 614/883, Training Loss: 0.8232\n",
      "Epoch 2/10, Batch 615/883, Training Loss: 0.8293\n",
      "Epoch 2/10, Batch 616/883, Training Loss: 0.6578\n",
      "Epoch 2/10, Batch 617/883, Training Loss: 0.9103\n",
      "Epoch 2/10, Batch 618/883, Training Loss: 0.7380\n",
      "Epoch 2/10, Batch 619/883, Training Loss: 1.1231\n",
      "Epoch 2/10, Batch 620/883, Training Loss: 0.8043\n",
      "Epoch 2/10, Batch 621/883, Training Loss: 1.0545\n",
      "Epoch 2/10, Batch 622/883, Training Loss: 1.3111\n",
      "Epoch 2/10, Batch 623/883, Training Loss: 0.7573\n",
      "Epoch 2/10, Batch 624/883, Training Loss: 0.8613\n",
      "Epoch 2/10, Batch 625/883, Training Loss: 1.0449\n",
      "Epoch 2/10, Batch 626/883, Training Loss: 1.0000\n",
      "Epoch 2/10, Batch 627/883, Training Loss: 0.8035\n",
      "Epoch 2/10, Batch 628/883, Training Loss: 0.6206\n",
      "Epoch 2/10, Batch 629/883, Training Loss: 0.7677\n",
      "Epoch 2/10, Batch 630/883, Training Loss: 1.0592\n",
      "Epoch 2/10, Batch 631/883, Training Loss: 0.7573\n",
      "Epoch 2/10, Batch 632/883, Training Loss: 0.9513\n",
      "Epoch 2/10, Batch 633/883, Training Loss: 0.8624\n",
      "Epoch 2/10, Batch 634/883, Training Loss: 0.8147\n",
      "Epoch 2/10, Batch 635/883, Training Loss: 0.7916\n",
      "Epoch 2/10, Batch 636/883, Training Loss: 0.8934\n",
      "Epoch 2/10, Batch 637/883, Training Loss: 1.0486\n",
      "Epoch 2/10, Batch 638/883, Training Loss: 0.7637\n",
      "Epoch 2/10, Batch 639/883, Training Loss: 1.1726\n",
      "Epoch 2/10, Batch 640/883, Training Loss: 0.7371\n",
      "Epoch 2/10, Batch 641/883, Training Loss: 0.7908\n",
      "Epoch 2/10, Batch 642/883, Training Loss: 1.0613\n",
      "Epoch 2/10, Batch 643/883, Training Loss: 0.7480\n",
      "Epoch 2/10, Batch 644/883, Training Loss: 0.7852\n",
      "Epoch 2/10, Batch 645/883, Training Loss: 0.7196\n",
      "Epoch 2/10, Batch 646/883, Training Loss: 1.0594\n",
      "Epoch 2/10, Batch 647/883, Training Loss: 1.1678\n",
      "Epoch 2/10, Batch 648/883, Training Loss: 0.9010\n",
      "Epoch 2/10, Batch 649/883, Training Loss: 0.7995\n",
      "Epoch 2/10, Batch 650/883, Training Loss: 0.6915\n",
      "Epoch 2/10, Batch 651/883, Training Loss: 0.7737\n",
      "Epoch 2/10, Batch 652/883, Training Loss: 0.7396\n",
      "Epoch 2/10, Batch 653/883, Training Loss: 0.8278\n",
      "Epoch 2/10, Batch 654/883, Training Loss: 0.9502\n",
      "Epoch 2/10, Batch 655/883, Training Loss: 0.6803\n",
      "Epoch 2/10, Batch 656/883, Training Loss: 1.0286\n",
      "Epoch 2/10, Batch 657/883, Training Loss: 0.6543\n",
      "Epoch 2/10, Batch 658/883, Training Loss: 0.6452\n",
      "Epoch 2/10, Batch 659/883, Training Loss: 0.8071\n",
      "Epoch 2/10, Batch 660/883, Training Loss: 0.9734\n",
      "Epoch 2/10, Batch 661/883, Training Loss: 0.8448\n",
      "Epoch 2/10, Batch 662/883, Training Loss: 0.6191\n",
      "Epoch 2/10, Batch 663/883, Training Loss: 0.7185\n",
      "Epoch 2/10, Batch 664/883, Training Loss: 0.7016\n",
      "Epoch 2/10, Batch 665/883, Training Loss: 0.9564\n",
      "Epoch 2/10, Batch 666/883, Training Loss: 1.4930\n",
      "Epoch 2/10, Batch 667/883, Training Loss: 0.6904\n",
      "Epoch 2/10, Batch 668/883, Training Loss: 0.9195\n",
      "Epoch 2/10, Batch 669/883, Training Loss: 0.9648\n",
      "Epoch 2/10, Batch 670/883, Training Loss: 0.6463\n",
      "Epoch 2/10, Batch 671/883, Training Loss: 1.0342\n",
      "Epoch 2/10, Batch 672/883, Training Loss: 0.8777\n",
      "Epoch 2/10, Batch 673/883, Training Loss: 0.7117\n",
      "Epoch 2/10, Batch 674/883, Training Loss: 0.6488\n",
      "Epoch 2/10, Batch 675/883, Training Loss: 0.7457\n",
      "Epoch 2/10, Batch 676/883, Training Loss: 0.9986\n",
      "Epoch 2/10, Batch 677/883, Training Loss: 0.7321\n",
      "Epoch 2/10, Batch 678/883, Training Loss: 0.9315\n",
      "Epoch 2/10, Batch 679/883, Training Loss: 0.8080\n",
      "Epoch 2/10, Batch 680/883, Training Loss: 0.6634\n",
      "Epoch 2/10, Batch 681/883, Training Loss: 0.7682\n",
      "Epoch 2/10, Batch 682/883, Training Loss: 0.8380\n",
      "Epoch 2/10, Batch 683/883, Training Loss: 0.8021\n",
      "Epoch 2/10, Batch 684/883, Training Loss: 0.8287\n",
      "Epoch 2/10, Batch 685/883, Training Loss: 1.1450\n",
      "Epoch 2/10, Batch 686/883, Training Loss: 0.9056\n",
      "Epoch 2/10, Batch 687/883, Training Loss: 0.8100\n",
      "Epoch 2/10, Batch 688/883, Training Loss: 1.0555\n",
      "Epoch 2/10, Batch 689/883, Training Loss: 0.9708\n",
      "Epoch 2/10, Batch 690/883, Training Loss: 0.7262\n",
      "Epoch 2/10, Batch 691/883, Training Loss: 0.8766\n",
      "Epoch 2/10, Batch 692/883, Training Loss: 1.0382\n",
      "Epoch 2/10, Batch 693/883, Training Loss: 0.8686\n",
      "Epoch 2/10, Batch 694/883, Training Loss: 0.9256\n",
      "Epoch 2/10, Batch 695/883, Training Loss: 0.8592\n",
      "Epoch 2/10, Batch 696/883, Training Loss: 0.8929\n",
      "Epoch 2/10, Batch 697/883, Training Loss: 0.8205\n",
      "Epoch 2/10, Batch 698/883, Training Loss: 0.7182\n",
      "Epoch 2/10, Batch 699/883, Training Loss: 0.8244\n",
      "Epoch 2/10, Batch 700/883, Training Loss: 0.8775\n",
      "Epoch 2/10, Batch 701/883, Training Loss: 0.6893\n",
      "Epoch 2/10, Batch 702/883, Training Loss: 0.6476\n",
      "Epoch 2/10, Batch 703/883, Training Loss: 0.7471\n",
      "Epoch 2/10, Batch 704/883, Training Loss: 0.7522\n",
      "Epoch 2/10, Batch 705/883, Training Loss: 0.7610\n",
      "Epoch 2/10, Batch 706/883, Training Loss: 0.9818\n",
      "Epoch 2/10, Batch 707/883, Training Loss: 0.8582\n",
      "Epoch 2/10, Batch 708/883, Training Loss: 1.1611\n",
      "Epoch 2/10, Batch 709/883, Training Loss: 0.7461\n",
      "Epoch 2/10, Batch 710/883, Training Loss: 0.7676\n",
      "Epoch 2/10, Batch 711/883, Training Loss: 0.8838\n",
      "Epoch 2/10, Batch 712/883, Training Loss: 1.0232\n",
      "Epoch 2/10, Batch 713/883, Training Loss: 0.6769\n",
      "Epoch 2/10, Batch 714/883, Training Loss: 0.7680\n",
      "Epoch 2/10, Batch 715/883, Training Loss: 1.0419\n",
      "Epoch 2/10, Batch 716/883, Training Loss: 0.7674\n",
      "Epoch 2/10, Batch 717/883, Training Loss: 1.0576\n",
      "Epoch 2/10, Batch 718/883, Training Loss: 0.7448\n",
      "Epoch 2/10, Batch 719/883, Training Loss: 0.6968\n",
      "Epoch 2/10, Batch 720/883, Training Loss: 0.8292\n",
      "Epoch 2/10, Batch 721/883, Training Loss: 1.0064\n",
      "Epoch 2/10, Batch 722/883, Training Loss: 0.7549\n",
      "Epoch 2/10, Batch 723/883, Training Loss: 0.8582\n",
      "Epoch 2/10, Batch 724/883, Training Loss: 0.7338\n",
      "Epoch 2/10, Batch 725/883, Training Loss: 0.8587\n",
      "Epoch 2/10, Batch 726/883, Training Loss: 0.8158\n",
      "Epoch 2/10, Batch 727/883, Training Loss: 0.8838\n",
      "Epoch 2/10, Batch 728/883, Training Loss: 0.6618\n",
      "Epoch 2/10, Batch 729/883, Training Loss: 0.8192\n",
      "Epoch 2/10, Batch 730/883, Training Loss: 0.8093\n",
      "Epoch 2/10, Batch 731/883, Training Loss: 0.8032\n",
      "Epoch 2/10, Batch 732/883, Training Loss: 0.7434\n",
      "Epoch 2/10, Batch 733/883, Training Loss: 1.0149\n",
      "Epoch 2/10, Batch 734/883, Training Loss: 0.8680\n",
      "Epoch 2/10, Batch 735/883, Training Loss: 0.8233\n",
      "Epoch 2/10, Batch 736/883, Training Loss: 0.6687\n",
      "Epoch 2/10, Batch 737/883, Training Loss: 0.8515\n",
      "Epoch 2/10, Batch 738/883, Training Loss: 1.2289\n",
      "Epoch 2/10, Batch 739/883, Training Loss: 0.6845\n",
      "Epoch 2/10, Batch 740/883, Training Loss: 0.9276\n",
      "Epoch 2/10, Batch 741/883, Training Loss: 0.9525\n",
      "Epoch 2/10, Batch 742/883, Training Loss: 0.8685\n",
      "Epoch 2/10, Batch 743/883, Training Loss: 0.8503\n",
      "Epoch 2/10, Batch 744/883, Training Loss: 0.7892\n",
      "Epoch 2/10, Batch 745/883, Training Loss: 0.8086\n",
      "Epoch 2/10, Batch 746/883, Training Loss: 1.0436\n",
      "Epoch 2/10, Batch 747/883, Training Loss: 0.9717\n",
      "Epoch 2/10, Batch 748/883, Training Loss: 0.8737\n",
      "Epoch 2/10, Batch 749/883, Training Loss: 0.9797\n",
      "Epoch 2/10, Batch 750/883, Training Loss: 0.7272\n",
      "Epoch 2/10, Batch 751/883, Training Loss: 0.7746\n",
      "Epoch 2/10, Batch 752/883, Training Loss: 0.8449\n",
      "Epoch 2/10, Batch 753/883, Training Loss: 0.6971\n",
      "Epoch 2/10, Batch 754/883, Training Loss: 0.7792\n",
      "Epoch 2/10, Batch 755/883, Training Loss: 0.7739\n",
      "Epoch 2/10, Batch 756/883, Training Loss: 0.7477\n",
      "Epoch 2/10, Batch 757/883, Training Loss: 0.5858\n",
      "Epoch 2/10, Batch 758/883, Training Loss: 0.6954\n",
      "Epoch 2/10, Batch 759/883, Training Loss: 0.7544\n",
      "Epoch 2/10, Batch 760/883, Training Loss: 0.7478\n",
      "Epoch 2/10, Batch 761/883, Training Loss: 0.9186\n",
      "Epoch 2/10, Batch 762/883, Training Loss: 0.8991\n",
      "Epoch 2/10, Batch 763/883, Training Loss: 1.4078\n",
      "Epoch 2/10, Batch 764/883, Training Loss: 0.5543\n",
      "Epoch 2/10, Batch 765/883, Training Loss: 0.9360\n",
      "Epoch 2/10, Batch 766/883, Training Loss: 1.0489\n",
      "Epoch 2/10, Batch 767/883, Training Loss: 0.6406\n",
      "Epoch 2/10, Batch 768/883, Training Loss: 0.8051\n",
      "Epoch 2/10, Batch 769/883, Training Loss: 0.9718\n",
      "Epoch 2/10, Batch 770/883, Training Loss: 1.0106\n",
      "Epoch 2/10, Batch 771/883, Training Loss: 0.8040\n",
      "Epoch 2/10, Batch 772/883, Training Loss: 0.8311\n",
      "Epoch 2/10, Batch 773/883, Training Loss: 1.0060\n",
      "Epoch 2/10, Batch 774/883, Training Loss: 1.0952\n",
      "Epoch 2/10, Batch 775/883, Training Loss: 0.6695\n",
      "Epoch 2/10, Batch 776/883, Training Loss: 0.7534\n",
      "Epoch 2/10, Batch 777/883, Training Loss: 0.8547\n",
      "Epoch 2/10, Batch 778/883, Training Loss: 0.9853\n",
      "Epoch 2/10, Batch 779/883, Training Loss: 0.8464\n",
      "Epoch 2/10, Batch 780/883, Training Loss: 0.7809\n",
      "Epoch 2/10, Batch 781/883, Training Loss: 0.7489\n",
      "Epoch 2/10, Batch 782/883, Training Loss: 0.7601\n",
      "Epoch 2/10, Batch 783/883, Training Loss: 0.8288\n",
      "Epoch 2/10, Batch 784/883, Training Loss: 0.8085\n",
      "Epoch 2/10, Batch 785/883, Training Loss: 0.9601\n",
      "Epoch 2/10, Batch 786/883, Training Loss: 0.8445\n",
      "Epoch 2/10, Batch 787/883, Training Loss: 0.6878\n",
      "Epoch 2/10, Batch 788/883, Training Loss: 0.7994\n",
      "Epoch 2/10, Batch 789/883, Training Loss: 1.0271\n",
      "Epoch 2/10, Batch 790/883, Training Loss: 0.7371\n",
      "Epoch 2/10, Batch 791/883, Training Loss: 0.8392\n",
      "Epoch 2/10, Batch 792/883, Training Loss: 0.7468\n",
      "Epoch 2/10, Batch 793/883, Training Loss: 0.7814\n",
      "Epoch 2/10, Batch 794/883, Training Loss: 0.8263\n",
      "Epoch 2/10, Batch 795/883, Training Loss: 0.7762\n",
      "Epoch 2/10, Batch 796/883, Training Loss: 0.6230\n",
      "Epoch 2/10, Batch 797/883, Training Loss: 0.6134\n",
      "Epoch 2/10, Batch 798/883, Training Loss: 0.7715\n",
      "Epoch 2/10, Batch 799/883, Training Loss: 0.5251\n",
      "Epoch 2/10, Batch 800/883, Training Loss: 0.7205\n",
      "Epoch 2/10, Batch 801/883, Training Loss: 1.0849\n",
      "Epoch 2/10, Batch 802/883, Training Loss: 0.7478\n",
      "Epoch 2/10, Batch 803/883, Training Loss: 0.8673\n",
      "Epoch 2/10, Batch 804/883, Training Loss: 0.9434\n",
      "Epoch 2/10, Batch 805/883, Training Loss: 0.7227\n",
      "Epoch 2/10, Batch 806/883, Training Loss: 0.8704\n",
      "Epoch 2/10, Batch 807/883, Training Loss: 0.7054\n",
      "Epoch 2/10, Batch 808/883, Training Loss: 0.7888\n",
      "Epoch 2/10, Batch 809/883, Training Loss: 0.8069\n",
      "Epoch 2/10, Batch 810/883, Training Loss: 0.9176\n",
      "Epoch 2/10, Batch 811/883, Training Loss: 1.1171\n",
      "Epoch 2/10, Batch 812/883, Training Loss: 0.8928\n",
      "Epoch 2/10, Batch 813/883, Training Loss: 0.7331\n",
      "Epoch 2/10, Batch 814/883, Training Loss: 0.7031\n",
      "Epoch 2/10, Batch 815/883, Training Loss: 0.8744\n",
      "Epoch 2/10, Batch 816/883, Training Loss: 0.6870\n",
      "Epoch 2/10, Batch 817/883, Training Loss: 0.7668\n",
      "Epoch 2/10, Batch 818/883, Training Loss: 0.7895\n",
      "Epoch 2/10, Batch 819/883, Training Loss: 0.6929\n",
      "Epoch 2/10, Batch 820/883, Training Loss: 0.6073\n",
      "Epoch 2/10, Batch 821/883, Training Loss: 1.1154\n",
      "Epoch 2/10, Batch 822/883, Training Loss: 0.8718\n",
      "Epoch 2/10, Batch 823/883, Training Loss: 0.8260\n",
      "Epoch 2/10, Batch 824/883, Training Loss: 0.8677\n",
      "Epoch 2/10, Batch 825/883, Training Loss: 0.9657\n",
      "Epoch 2/10, Batch 826/883, Training Loss: 0.7977\n",
      "Epoch 2/10, Batch 827/883, Training Loss: 0.9511\n",
      "Epoch 2/10, Batch 828/883, Training Loss: 0.5848\n",
      "Epoch 2/10, Batch 829/883, Training Loss: 1.1140\n",
      "Epoch 2/10, Batch 830/883, Training Loss: 1.2374\n",
      "Epoch 2/10, Batch 831/883, Training Loss: 0.7422\n",
      "Epoch 2/10, Batch 832/883, Training Loss: 1.1556\n",
      "Epoch 2/10, Batch 833/883, Training Loss: 0.7726\n",
      "Epoch 2/10, Batch 834/883, Training Loss: 1.0270\n",
      "Epoch 2/10, Batch 835/883, Training Loss: 0.7042\n",
      "Epoch 2/10, Batch 836/883, Training Loss: 0.7378\n",
      "Epoch 2/10, Batch 837/883, Training Loss: 0.7863\n",
      "Epoch 2/10, Batch 838/883, Training Loss: 0.8157\n",
      "Epoch 2/10, Batch 839/883, Training Loss: 1.0649\n",
      "Epoch 2/10, Batch 840/883, Training Loss: 0.6281\n",
      "Epoch 2/10, Batch 841/883, Training Loss: 0.7324\n",
      "Epoch 2/10, Batch 842/883, Training Loss: 0.6215\n",
      "Epoch 2/10, Batch 843/883, Training Loss: 0.7754\n",
      "Epoch 2/10, Batch 844/883, Training Loss: 0.7435\n",
      "Epoch 2/10, Batch 845/883, Training Loss: 0.7744\n",
      "Epoch 2/10, Batch 846/883, Training Loss: 0.6673\n",
      "Epoch 2/10, Batch 847/883, Training Loss: 0.7967\n",
      "Epoch 2/10, Batch 848/883, Training Loss: 0.5955\n",
      "Epoch 2/10, Batch 849/883, Training Loss: 1.0007\n",
      "Epoch 2/10, Batch 850/883, Training Loss: 0.6751\n",
      "Epoch 2/10, Batch 851/883, Training Loss: 0.8178\n",
      "Epoch 2/10, Batch 852/883, Training Loss: 0.5949\n",
      "Epoch 2/10, Batch 853/883, Training Loss: 0.8571\n",
      "Epoch 2/10, Batch 854/883, Training Loss: 0.6947\n",
      "Epoch 2/10, Batch 855/883, Training Loss: 0.8158\n",
      "Epoch 2/10, Batch 856/883, Training Loss: 1.0051\n",
      "Epoch 2/10, Batch 857/883, Training Loss: 0.8608\n",
      "Epoch 2/10, Batch 858/883, Training Loss: 0.7254\n",
      "Epoch 2/10, Batch 859/883, Training Loss: 0.8334\n",
      "Epoch 2/10, Batch 860/883, Training Loss: 0.7612\n",
      "Epoch 2/10, Batch 861/883, Training Loss: 0.6302\n",
      "Epoch 2/10, Batch 862/883, Training Loss: 0.9294\n",
      "Epoch 2/10, Batch 863/883, Training Loss: 0.6966\n",
      "Epoch 2/10, Batch 864/883, Training Loss: 0.9742\n",
      "Epoch 2/10, Batch 865/883, Training Loss: 0.9341\n",
      "Epoch 2/10, Batch 866/883, Training Loss: 0.8378\n",
      "Epoch 2/10, Batch 867/883, Training Loss: 0.8073\n",
      "Epoch 2/10, Batch 868/883, Training Loss: 0.8318\n",
      "Epoch 2/10, Batch 869/883, Training Loss: 0.7473\n",
      "Epoch 2/10, Batch 870/883, Training Loss: 0.9558\n",
      "Epoch 2/10, Batch 871/883, Training Loss: 0.7406\n",
      "Epoch 2/10, Batch 872/883, Training Loss: 0.7950\n",
      "Epoch 2/10, Batch 873/883, Training Loss: 0.7695\n",
      "Epoch 2/10, Batch 874/883, Training Loss: 0.8239\n",
      "Epoch 2/10, Batch 875/883, Training Loss: 1.1560\n",
      "Epoch 2/10, Batch 876/883, Training Loss: 0.8336\n",
      "Epoch 2/10, Batch 877/883, Training Loss: 1.0038\n",
      "Epoch 2/10, Batch 878/883, Training Loss: 0.7301\n",
      "Epoch 2/10, Batch 879/883, Training Loss: 0.7683\n",
      "Epoch 2/10, Batch 880/883, Training Loss: 0.7849\n",
      "Epoch 2/10, Batch 881/883, Training Loss: 0.7202\n",
      "Epoch 2/10, Batch 882/883, Training Loss: 0.6681\n",
      "Epoch 2/10, Batch 883/883, Training Loss: 0.9204\n",
      "Epoch 2/10, Training Loss: 0.8616, Validation Loss: 0.9708, Validation Accuracy: 0.5544\n",
      "Epoch 3/10, Batch 1/883, Training Loss: 0.8459\n",
      "Epoch 3/10, Batch 2/883, Training Loss: 0.6997\n",
      "Epoch 3/10, Batch 3/883, Training Loss: 0.9277\n",
      "Epoch 3/10, Batch 4/883, Training Loss: 0.6711\n",
      "Epoch 3/10, Batch 5/883, Training Loss: 1.0212\n",
      "Epoch 3/10, Batch 6/883, Training Loss: 0.6609\n",
      "Epoch 3/10, Batch 7/883, Training Loss: 1.0052\n",
      "Epoch 3/10, Batch 8/883, Training Loss: 0.6781\n",
      "Epoch 3/10, Batch 9/883, Training Loss: 0.7524\n",
      "Epoch 3/10, Batch 10/883, Training Loss: 0.5543\n",
      "Epoch 3/10, Batch 11/883, Training Loss: 0.8292\n",
      "Epoch 3/10, Batch 12/883, Training Loss: 0.8291\n",
      "Epoch 3/10, Batch 13/883, Training Loss: 0.7266\n",
      "Epoch 3/10, Batch 14/883, Training Loss: 0.8436\n",
      "Epoch 3/10, Batch 15/883, Training Loss: 0.8834\n",
      "Epoch 3/10, Batch 16/883, Training Loss: 0.7017\n",
      "Epoch 3/10, Batch 17/883, Training Loss: 0.7260\n",
      "Epoch 3/10, Batch 18/883, Training Loss: 0.8700\n",
      "Epoch 3/10, Batch 19/883, Training Loss: 0.8207\n",
      "Epoch 3/10, Batch 20/883, Training Loss: 0.8307\n",
      "Epoch 3/10, Batch 21/883, Training Loss: 0.8289\n",
      "Epoch 3/10, Batch 22/883, Training Loss: 0.9032\n",
      "Epoch 3/10, Batch 23/883, Training Loss: 0.7997\n",
      "Epoch 3/10, Batch 24/883, Training Loss: 0.8824\n",
      "Epoch 3/10, Batch 25/883, Training Loss: 1.0967\n",
      "Epoch 3/10, Batch 26/883, Training Loss: 0.8693\n",
      "Epoch 3/10, Batch 27/883, Training Loss: 1.0501\n",
      "Epoch 3/10, Batch 28/883, Training Loss: 0.6882\n",
      "Epoch 3/10, Batch 29/883, Training Loss: 1.1168\n",
      "Epoch 3/10, Batch 30/883, Training Loss: 0.6953\n",
      "Epoch 3/10, Batch 31/883, Training Loss: 0.9421\n",
      "Epoch 3/10, Batch 32/883, Training Loss: 0.6350\n",
      "Epoch 3/10, Batch 33/883, Training Loss: 0.7714\n",
      "Epoch 3/10, Batch 34/883, Training Loss: 0.7066\n",
      "Epoch 3/10, Batch 35/883, Training Loss: 0.8087\n",
      "Epoch 3/10, Batch 36/883, Training Loss: 0.9475\n",
      "Epoch 3/10, Batch 37/883, Training Loss: 0.8471\n",
      "Epoch 3/10, Batch 38/883, Training Loss: 0.8350\n",
      "Epoch 3/10, Batch 39/883, Training Loss: 0.6146\n",
      "Epoch 3/10, Batch 40/883, Training Loss: 0.8921\n",
      "Epoch 3/10, Batch 41/883, Training Loss: 0.9403\n",
      "Epoch 3/10, Batch 42/883, Training Loss: 0.8187\n",
      "Epoch 3/10, Batch 43/883, Training Loss: 0.9191\n",
      "Epoch 3/10, Batch 44/883, Training Loss: 1.1414\n",
      "Epoch 3/10, Batch 45/883, Training Loss: 0.5687\n",
      "Epoch 3/10, Batch 46/883, Training Loss: 0.7763\n",
      "Epoch 3/10, Batch 47/883, Training Loss: 0.6281\n",
      "Epoch 3/10, Batch 48/883, Training Loss: 0.8254\n",
      "Epoch 3/10, Batch 49/883, Training Loss: 0.9551\n",
      "Epoch 3/10, Batch 50/883, Training Loss: 0.8893\n",
      "Epoch 3/10, Batch 51/883, Training Loss: 1.0536\n",
      "Epoch 3/10, Batch 52/883, Training Loss: 0.6645\n",
      "Epoch 3/10, Batch 53/883, Training Loss: 1.0697\n",
      "Epoch 3/10, Batch 54/883, Training Loss: 0.9211\n",
      "Epoch 3/10, Batch 55/883, Training Loss: 0.7292\n",
      "Epoch 3/10, Batch 56/883, Training Loss: 1.1010\n",
      "Epoch 3/10, Batch 57/883, Training Loss: 0.6328\n",
      "Epoch 3/10, Batch 58/883, Training Loss: 0.6482\n",
      "Epoch 3/10, Batch 59/883, Training Loss: 0.7841\n",
      "Epoch 3/10, Batch 60/883, Training Loss: 0.6044\n",
      "Epoch 3/10, Batch 61/883, Training Loss: 0.8001\n",
      "Epoch 3/10, Batch 62/883, Training Loss: 0.8386\n",
      "Epoch 3/10, Batch 63/883, Training Loss: 0.6683\n",
      "Epoch 3/10, Batch 64/883, Training Loss: 0.6635\n",
      "Epoch 3/10, Batch 65/883, Training Loss: 0.5597\n",
      "Epoch 3/10, Batch 66/883, Training Loss: 0.7354\n",
      "Epoch 3/10, Batch 67/883, Training Loss: 0.6194\n",
      "Epoch 3/10, Batch 68/883, Training Loss: 0.4682\n",
      "Epoch 3/10, Batch 69/883, Training Loss: 0.7178\n",
      "Epoch 3/10, Batch 70/883, Training Loss: 0.7349\n",
      "Epoch 3/10, Batch 71/883, Training Loss: 0.8879\n",
      "Epoch 3/10, Batch 72/883, Training Loss: 0.6577\n",
      "Epoch 3/10, Batch 73/883, Training Loss: 0.7627\n",
      "Epoch 3/10, Batch 74/883, Training Loss: 1.2326\n",
      "Epoch 3/10, Batch 75/883, Training Loss: 1.0050\n",
      "Epoch 3/10, Batch 76/883, Training Loss: 0.9094\n",
      "Epoch 3/10, Batch 77/883, Training Loss: 1.3170\n",
      "Epoch 3/10, Batch 78/883, Training Loss: 0.8231\n",
      "Epoch 3/10, Batch 79/883, Training Loss: 1.2968\n",
      "Epoch 3/10, Batch 80/883, Training Loss: 0.7782\n",
      "Epoch 3/10, Batch 81/883, Training Loss: 1.0120\n",
      "Epoch 3/10, Batch 82/883, Training Loss: 0.6489\n",
      "Epoch 3/10, Batch 83/883, Training Loss: 0.6866\n",
      "Epoch 3/10, Batch 84/883, Training Loss: 0.8480\n",
      "Epoch 3/10, Batch 85/883, Training Loss: 0.8342\n",
      "Epoch 3/10, Batch 86/883, Training Loss: 0.7381\n",
      "Epoch 3/10, Batch 87/883, Training Loss: 0.7716\n",
      "Epoch 3/10, Batch 88/883, Training Loss: 0.7467\n",
      "Epoch 3/10, Batch 89/883, Training Loss: 0.7678\n",
      "Epoch 3/10, Batch 90/883, Training Loss: 0.8172\n",
      "Epoch 3/10, Batch 91/883, Training Loss: 0.7157\n",
      "Epoch 3/10, Batch 92/883, Training Loss: 0.9961\n",
      "Epoch 3/10, Batch 93/883, Training Loss: 0.7758\n",
      "Epoch 3/10, Batch 94/883, Training Loss: 0.8128\n",
      "Epoch 3/10, Batch 95/883, Training Loss: 0.8599\n",
      "Epoch 3/10, Batch 96/883, Training Loss: 0.7695\n",
      "Epoch 3/10, Batch 97/883, Training Loss: 0.8943\n",
      "Epoch 3/10, Batch 98/883, Training Loss: 0.7484\n",
      "Epoch 3/10, Batch 99/883, Training Loss: 0.7026\n",
      "Epoch 3/10, Batch 100/883, Training Loss: 0.6526\n",
      "Epoch 3/10, Batch 101/883, Training Loss: 0.8726\n",
      "Epoch 3/10, Batch 102/883, Training Loss: 1.1043\n",
      "Epoch 3/10, Batch 103/883, Training Loss: 0.7134\n",
      "Epoch 3/10, Batch 104/883, Training Loss: 0.6531\n",
      "Epoch 3/10, Batch 105/883, Training Loss: 0.8337\n",
      "Epoch 3/10, Batch 106/883, Training Loss: 0.8106\n",
      "Epoch 3/10, Batch 107/883, Training Loss: 0.6043\n",
      "Epoch 3/10, Batch 108/883, Training Loss: 0.6815\n",
      "Epoch 3/10, Batch 109/883, Training Loss: 1.0665\n",
      "Epoch 3/10, Batch 110/883, Training Loss: 0.8967\n",
      "Epoch 3/10, Batch 111/883, Training Loss: 0.6587\n",
      "Epoch 3/10, Batch 112/883, Training Loss: 0.6184\n",
      "Epoch 3/10, Batch 113/883, Training Loss: 0.5967\n",
      "Epoch 3/10, Batch 114/883, Training Loss: 0.8388\n",
      "Epoch 3/10, Batch 115/883, Training Loss: 0.8951\n",
      "Epoch 3/10, Batch 116/883, Training Loss: 0.7842\n",
      "Epoch 3/10, Batch 117/883, Training Loss: 0.7569\n",
      "Epoch 3/10, Batch 118/883, Training Loss: 1.0204\n",
      "Epoch 3/10, Batch 119/883, Training Loss: 0.7458\n",
      "Epoch 3/10, Batch 120/883, Training Loss: 0.6291\n",
      "Epoch 3/10, Batch 121/883, Training Loss: 0.8243\n",
      "Epoch 3/10, Batch 122/883, Training Loss: 0.7071\n",
      "Epoch 3/10, Batch 123/883, Training Loss: 0.6688\n",
      "Epoch 3/10, Batch 124/883, Training Loss: 1.0666\n",
      "Epoch 3/10, Batch 125/883, Training Loss: 0.7092\n",
      "Epoch 3/10, Batch 126/883, Training Loss: 0.7720\n",
      "Epoch 3/10, Batch 127/883, Training Loss: 0.5894\n",
      "Epoch 3/10, Batch 128/883, Training Loss: 0.8272\n",
      "Epoch 3/10, Batch 129/883, Training Loss: 0.7383\n",
      "Epoch 3/10, Batch 130/883, Training Loss: 0.6910\n",
      "Epoch 3/10, Batch 131/883, Training Loss: 0.5981\n",
      "Epoch 3/10, Batch 132/883, Training Loss: 0.6078\n",
      "Epoch 3/10, Batch 133/883, Training Loss: 0.9942\n",
      "Epoch 3/10, Batch 134/883, Training Loss: 0.8476\n",
      "Epoch 3/10, Batch 135/883, Training Loss: 0.7686\n",
      "Epoch 3/10, Batch 136/883, Training Loss: 0.6713\n",
      "Epoch 3/10, Batch 137/883, Training Loss: 0.6772\n",
      "Epoch 3/10, Batch 138/883, Training Loss: 0.8411\n",
      "Epoch 3/10, Batch 139/883, Training Loss: 1.0006\n",
      "Epoch 3/10, Batch 140/883, Training Loss: 0.6033\n",
      "Epoch 3/10, Batch 141/883, Training Loss: 0.9087\n",
      "Epoch 3/10, Batch 142/883, Training Loss: 0.8966\n",
      "Epoch 3/10, Batch 143/883, Training Loss: 0.6789\n",
      "Epoch 3/10, Batch 144/883, Training Loss: 0.7834\n",
      "Epoch 3/10, Batch 145/883, Training Loss: 0.7692\n",
      "Epoch 3/10, Batch 146/883, Training Loss: 0.7207\n",
      "Epoch 3/10, Batch 147/883, Training Loss: 0.7593\n",
      "Epoch 3/10, Batch 148/883, Training Loss: 0.7860\n",
      "Epoch 3/10, Batch 149/883, Training Loss: 0.7297\n",
      "Epoch 3/10, Batch 150/883, Training Loss: 0.7595\n",
      "Epoch 3/10, Batch 151/883, Training Loss: 0.8361\n",
      "Epoch 3/10, Batch 152/883, Training Loss: 0.7435\n",
      "Epoch 3/10, Batch 153/883, Training Loss: 0.6729\n",
      "Epoch 3/10, Batch 154/883, Training Loss: 1.0886\n",
      "Epoch 3/10, Batch 155/883, Training Loss: 0.6734\n",
      "Epoch 3/10, Batch 156/883, Training Loss: 0.8751\n",
      "Epoch 3/10, Batch 157/883, Training Loss: 0.9158\n",
      "Epoch 3/10, Batch 158/883, Training Loss: 1.0982\n",
      "Epoch 3/10, Batch 159/883, Training Loss: 0.6550\n",
      "Epoch 3/10, Batch 160/883, Training Loss: 0.8892\n",
      "Epoch 3/10, Batch 161/883, Training Loss: 0.7147\n",
      "Epoch 3/10, Batch 162/883, Training Loss: 0.9948\n",
      "Epoch 3/10, Batch 163/883, Training Loss: 0.8033\n",
      "Epoch 3/10, Batch 164/883, Training Loss: 1.0241\n",
      "Epoch 3/10, Batch 165/883, Training Loss: 0.6097\n",
      "Epoch 3/10, Batch 166/883, Training Loss: 1.2300\n",
      "Epoch 3/10, Batch 167/883, Training Loss: 0.7183\n",
      "Epoch 3/10, Batch 168/883, Training Loss: 1.1023\n",
      "Epoch 3/10, Batch 169/883, Training Loss: 0.7067\n",
      "Epoch 3/10, Batch 170/883, Training Loss: 0.6272\n",
      "Epoch 3/10, Batch 171/883, Training Loss: 0.8528\n",
      "Epoch 3/10, Batch 172/883, Training Loss: 0.7511\n",
      "Epoch 3/10, Batch 173/883, Training Loss: 0.9567\n",
      "Epoch 3/10, Batch 174/883, Training Loss: 1.0303\n",
      "Epoch 3/10, Batch 175/883, Training Loss: 0.9520\n",
      "Epoch 3/10, Batch 176/883, Training Loss: 0.7630\n",
      "Epoch 3/10, Batch 177/883, Training Loss: 0.8976\n",
      "Epoch 3/10, Batch 178/883, Training Loss: 1.0844\n",
      "Epoch 3/10, Batch 179/883, Training Loss: 0.6356\n",
      "Epoch 3/10, Batch 180/883, Training Loss: 0.8727\n",
      "Epoch 3/10, Batch 181/883, Training Loss: 0.4982\n",
      "Epoch 3/10, Batch 182/883, Training Loss: 0.9234\n",
      "Epoch 3/10, Batch 183/883, Training Loss: 0.6413\n",
      "Epoch 3/10, Batch 184/883, Training Loss: 1.0896\n",
      "Epoch 3/10, Batch 185/883, Training Loss: 0.5392\n",
      "Epoch 3/10, Batch 186/883, Training Loss: 0.9640\n",
      "Epoch 3/10, Batch 187/883, Training Loss: 0.9071\n",
      "Epoch 3/10, Batch 188/883, Training Loss: 0.6828\n",
      "Epoch 3/10, Batch 189/883, Training Loss: 0.6915\n",
      "Epoch 3/10, Batch 190/883, Training Loss: 0.8340\n",
      "Epoch 3/10, Batch 191/883, Training Loss: 0.6997\n",
      "Epoch 3/10, Batch 192/883, Training Loss: 0.7599\n",
      "Epoch 3/10, Batch 193/883, Training Loss: 0.7832\n",
      "Epoch 3/10, Batch 194/883, Training Loss: 0.7601\n",
      "Epoch 3/10, Batch 195/883, Training Loss: 0.8576\n",
      "Epoch 3/10, Batch 196/883, Training Loss: 0.6323\n",
      "Epoch 3/10, Batch 197/883, Training Loss: 0.8745\n",
      "Epoch 3/10, Batch 198/883, Training Loss: 0.9202\n",
      "Epoch 3/10, Batch 199/883, Training Loss: 0.9223\n",
      "Epoch 3/10, Batch 200/883, Training Loss: 0.6629\n",
      "Epoch 3/10, Batch 201/883, Training Loss: 0.6789\n",
      "Epoch 3/10, Batch 202/883, Training Loss: 0.6704\n",
      "Epoch 3/10, Batch 203/883, Training Loss: 0.7057\n",
      "Epoch 3/10, Batch 204/883, Training Loss: 0.9176\n",
      "Epoch 3/10, Batch 205/883, Training Loss: 0.9487\n",
      "Epoch 3/10, Batch 206/883, Training Loss: 0.7690\n",
      "Epoch 3/10, Batch 207/883, Training Loss: 0.6834\n",
      "Epoch 3/10, Batch 208/883, Training Loss: 0.7726\n",
      "Epoch 3/10, Batch 209/883, Training Loss: 0.6471\n",
      "Epoch 3/10, Batch 210/883, Training Loss: 0.8423\n",
      "Epoch 3/10, Batch 211/883, Training Loss: 0.7834\n",
      "Epoch 3/10, Batch 212/883, Training Loss: 0.7479\n",
      "Epoch 3/10, Batch 213/883, Training Loss: 1.0804\n",
      "Epoch 3/10, Batch 214/883, Training Loss: 0.7701\n",
      "Epoch 3/10, Batch 215/883, Training Loss: 0.8007\n",
      "Epoch 3/10, Batch 216/883, Training Loss: 0.8089\n",
      "Epoch 3/10, Batch 217/883, Training Loss: 0.9442\n",
      "Epoch 3/10, Batch 218/883, Training Loss: 0.8017\n",
      "Epoch 3/10, Batch 219/883, Training Loss: 0.6891\n",
      "Epoch 3/10, Batch 220/883, Training Loss: 0.9478\n",
      "Epoch 3/10, Batch 221/883, Training Loss: 0.5867\n",
      "Epoch 3/10, Batch 222/883, Training Loss: 0.5556\n",
      "Epoch 3/10, Batch 223/883, Training Loss: 0.7079\n",
      "Epoch 3/10, Batch 224/883, Training Loss: 0.7251\n",
      "Epoch 3/10, Batch 225/883, Training Loss: 0.8224\n",
      "Epoch 3/10, Batch 226/883, Training Loss: 0.6602\n",
      "Epoch 3/10, Batch 227/883, Training Loss: 0.9344\n",
      "Epoch 3/10, Batch 228/883, Training Loss: 1.1968\n",
      "Epoch 3/10, Batch 229/883, Training Loss: 0.9611\n",
      "Epoch 3/10, Batch 230/883, Training Loss: 0.8205\n",
      "Epoch 3/10, Batch 231/883, Training Loss: 0.6898\n",
      "Epoch 3/10, Batch 232/883, Training Loss: 0.9889\n",
      "Epoch 3/10, Batch 233/883, Training Loss: 0.5325\n",
      "Epoch 3/10, Batch 234/883, Training Loss: 0.6173\n",
      "Epoch 3/10, Batch 235/883, Training Loss: 0.6581\n",
      "Epoch 3/10, Batch 236/883, Training Loss: 0.9281\n",
      "Epoch 3/10, Batch 237/883, Training Loss: 0.8963\n",
      "Epoch 3/10, Batch 238/883, Training Loss: 0.7033\n",
      "Epoch 3/10, Batch 239/883, Training Loss: 0.5619\n",
      "Epoch 3/10, Batch 240/883, Training Loss: 0.8593\n",
      "Epoch 3/10, Batch 241/883, Training Loss: 0.8996\n",
      "Epoch 3/10, Batch 242/883, Training Loss: 0.7932\n",
      "Epoch 3/10, Batch 243/883, Training Loss: 0.7578\n",
      "Epoch 3/10, Batch 244/883, Training Loss: 0.7086\n",
      "Epoch 3/10, Batch 245/883, Training Loss: 0.7290\n",
      "Epoch 3/10, Batch 246/883, Training Loss: 0.9722\n",
      "Epoch 3/10, Batch 247/883, Training Loss: 1.0913\n",
      "Epoch 3/10, Batch 248/883, Training Loss: 0.8555\n",
      "Epoch 3/10, Batch 249/883, Training Loss: 0.9531\n",
      "Epoch 3/10, Batch 250/883, Training Loss: 0.9158\n",
      "Epoch 3/10, Batch 251/883, Training Loss: 0.6652\n",
      "Epoch 3/10, Batch 252/883, Training Loss: 1.1381\n",
      "Epoch 3/10, Batch 253/883, Training Loss: 0.6981\n",
      "Epoch 3/10, Batch 254/883, Training Loss: 1.0016\n",
      "Epoch 3/10, Batch 255/883, Training Loss: 0.9401\n",
      "Epoch 3/10, Batch 256/883, Training Loss: 0.6401\n",
      "Epoch 3/10, Batch 257/883, Training Loss: 0.7681\n",
      "Epoch 3/10, Batch 258/883, Training Loss: 0.9409\n",
      "Epoch 3/10, Batch 259/883, Training Loss: 0.6237\n",
      "Epoch 3/10, Batch 260/883, Training Loss: 0.7702\n",
      "Epoch 3/10, Batch 261/883, Training Loss: 0.5940\n",
      "Epoch 3/10, Batch 262/883, Training Loss: 0.7667\n",
      "Epoch 3/10, Batch 263/883, Training Loss: 0.9487\n",
      "Epoch 3/10, Batch 264/883, Training Loss: 0.6367\n",
      "Epoch 3/10, Batch 265/883, Training Loss: 0.9319\n",
      "Epoch 3/10, Batch 266/883, Training Loss: 0.8193\n",
      "Epoch 3/10, Batch 267/883, Training Loss: 0.8867\n",
      "Epoch 3/10, Batch 268/883, Training Loss: 0.7555\n",
      "Epoch 3/10, Batch 269/883, Training Loss: 0.8245\n",
      "Epoch 3/10, Batch 270/883, Training Loss: 0.6492\n",
      "Epoch 3/10, Batch 271/883, Training Loss: 0.8283\n",
      "Epoch 3/10, Batch 272/883, Training Loss: 0.7218\n",
      "Epoch 3/10, Batch 273/883, Training Loss: 0.5758\n",
      "Epoch 3/10, Batch 274/883, Training Loss: 0.7652\n",
      "Epoch 3/10, Batch 275/883, Training Loss: 0.5713\n",
      "Epoch 3/10, Batch 276/883, Training Loss: 1.0883\n",
      "Epoch 3/10, Batch 277/883, Training Loss: 0.9254\n",
      "Epoch 3/10, Batch 278/883, Training Loss: 0.7889\n",
      "Epoch 3/10, Batch 279/883, Training Loss: 0.8418\n",
      "Epoch 3/10, Batch 280/883, Training Loss: 1.0275\n",
      "Epoch 3/10, Batch 281/883, Training Loss: 1.0334\n",
      "Epoch 3/10, Batch 282/883, Training Loss: 0.9494\n",
      "Epoch 3/10, Batch 283/883, Training Loss: 0.5862\n",
      "Epoch 3/10, Batch 284/883, Training Loss: 0.8164\n",
      "Epoch 3/10, Batch 285/883, Training Loss: 0.9594\n",
      "Epoch 3/10, Batch 286/883, Training Loss: 1.0046\n",
      "Epoch 3/10, Batch 287/883, Training Loss: 0.5855\n",
      "Epoch 3/10, Batch 288/883, Training Loss: 0.7177\n",
      "Epoch 3/10, Batch 289/883, Training Loss: 0.8753\n",
      "Epoch 3/10, Batch 290/883, Training Loss: 1.0534\n",
      "Epoch 3/10, Batch 291/883, Training Loss: 0.9100\n",
      "Epoch 3/10, Batch 292/883, Training Loss: 0.8185\n",
      "Epoch 3/10, Batch 293/883, Training Loss: 0.6595\n",
      "Epoch 3/10, Batch 294/883, Training Loss: 0.7737\n",
      "Epoch 3/10, Batch 295/883, Training Loss: 0.5764\n",
      "Epoch 3/10, Batch 296/883, Training Loss: 1.0895\n",
      "Epoch 3/10, Batch 297/883, Training Loss: 0.9700\n",
      "Epoch 3/10, Batch 298/883, Training Loss: 0.8185\n",
      "Epoch 3/10, Batch 299/883, Training Loss: 0.7581\n",
      "Epoch 3/10, Batch 300/883, Training Loss: 0.8020\n",
      "Epoch 3/10, Batch 301/883, Training Loss: 0.6170\n",
      "Epoch 3/10, Batch 302/883, Training Loss: 0.7286\n",
      "Epoch 3/10, Batch 303/883, Training Loss: 0.6966\n",
      "Epoch 3/10, Batch 304/883, Training Loss: 0.7386\n",
      "Epoch 3/10, Batch 305/883, Training Loss: 1.0271\n",
      "Epoch 3/10, Batch 306/883, Training Loss: 0.8820\n",
      "Epoch 3/10, Batch 307/883, Training Loss: 0.6173\n",
      "Epoch 3/10, Batch 308/883, Training Loss: 0.5892\n",
      "Epoch 3/10, Batch 309/883, Training Loss: 0.9744\n",
      "Epoch 3/10, Batch 310/883, Training Loss: 1.1613\n",
      "Epoch 3/10, Batch 311/883, Training Loss: 0.6272\n",
      "Epoch 3/10, Batch 312/883, Training Loss: 0.7059\n",
      "Epoch 3/10, Batch 313/883, Training Loss: 0.7506\n",
      "Epoch 3/10, Batch 314/883, Training Loss: 0.6712\n",
      "Epoch 3/10, Batch 315/883, Training Loss: 0.7653\n",
      "Epoch 3/10, Batch 316/883, Training Loss: 1.1469\n",
      "Epoch 3/10, Batch 317/883, Training Loss: 0.5956\n",
      "Epoch 3/10, Batch 318/883, Training Loss: 0.6221\n",
      "Epoch 3/10, Batch 319/883, Training Loss: 0.7654\n",
      "Epoch 3/10, Batch 320/883, Training Loss: 0.9657\n",
      "Epoch 3/10, Batch 321/883, Training Loss: 0.8280\n",
      "Epoch 3/10, Batch 322/883, Training Loss: 0.6539\n",
      "Epoch 3/10, Batch 323/883, Training Loss: 0.7898\n",
      "Epoch 3/10, Batch 324/883, Training Loss: 0.6130\n",
      "Epoch 3/10, Batch 325/883, Training Loss: 0.6902\n",
      "Epoch 3/10, Batch 326/883, Training Loss: 1.0690\n",
      "Epoch 3/10, Batch 327/883, Training Loss: 0.6284\n",
      "Epoch 3/10, Batch 328/883, Training Loss: 0.7505\n",
      "Epoch 3/10, Batch 329/883, Training Loss: 0.6345\n",
      "Epoch 3/10, Batch 330/883, Training Loss: 0.6086\n",
      "Epoch 3/10, Batch 331/883, Training Loss: 0.9408\n",
      "Epoch 3/10, Batch 332/883, Training Loss: 1.0321\n",
      "Epoch 3/10, Batch 333/883, Training Loss: 0.5783\n",
      "Epoch 3/10, Batch 334/883, Training Loss: 0.7733\n",
      "Epoch 3/10, Batch 335/883, Training Loss: 0.6936\n",
      "Epoch 3/10, Batch 336/883, Training Loss: 0.8646\n",
      "Epoch 3/10, Batch 337/883, Training Loss: 0.6586\n",
      "Epoch 3/10, Batch 338/883, Training Loss: 1.1985\n",
      "Epoch 3/10, Batch 339/883, Training Loss: 0.6762\n",
      "Epoch 3/10, Batch 340/883, Training Loss: 0.9751\n",
      "Epoch 3/10, Batch 341/883, Training Loss: 0.9739\n",
      "Epoch 3/10, Batch 342/883, Training Loss: 0.7433\n",
      "Epoch 3/10, Batch 343/883, Training Loss: 0.7369\n",
      "Epoch 3/10, Batch 344/883, Training Loss: 0.6465\n",
      "Epoch 3/10, Batch 345/883, Training Loss: 0.7517\n",
      "Epoch 3/10, Batch 346/883, Training Loss: 0.8046\n",
      "Epoch 3/10, Batch 347/883, Training Loss: 0.9302\n",
      "Epoch 3/10, Batch 348/883, Training Loss: 0.6506\n",
      "Epoch 3/10, Batch 349/883, Training Loss: 0.9797\n",
      "Epoch 3/10, Batch 350/883, Training Loss: 0.6439\n",
      "Epoch 3/10, Batch 351/883, Training Loss: 0.8416\n",
      "Epoch 3/10, Batch 352/883, Training Loss: 0.7995\n",
      "Epoch 3/10, Batch 353/883, Training Loss: 0.7278\n",
      "Epoch 3/10, Batch 354/883, Training Loss: 0.7547\n",
      "Epoch 3/10, Batch 355/883, Training Loss: 0.7415\n",
      "Epoch 3/10, Batch 356/883, Training Loss: 0.7020\n",
      "Epoch 3/10, Batch 357/883, Training Loss: 0.6951\n",
      "Epoch 3/10, Batch 358/883, Training Loss: 0.8874\n",
      "Epoch 3/10, Batch 359/883, Training Loss: 0.9305\n",
      "Epoch 3/10, Batch 360/883, Training Loss: 0.9766\n",
      "Epoch 3/10, Batch 361/883, Training Loss: 0.9262\n",
      "Epoch 3/10, Batch 362/883, Training Loss: 0.7577\n",
      "Epoch 3/10, Batch 363/883, Training Loss: 0.9226\n",
      "Epoch 3/10, Batch 364/883, Training Loss: 0.8100\n",
      "Epoch 3/10, Batch 365/883, Training Loss: 0.9324\n",
      "Epoch 3/10, Batch 366/883, Training Loss: 0.8920\n",
      "Epoch 3/10, Batch 367/883, Training Loss: 0.7982\n",
      "Epoch 3/10, Batch 368/883, Training Loss: 0.7292\n",
      "Epoch 3/10, Batch 369/883, Training Loss: 0.9207\n",
      "Epoch 3/10, Batch 370/883, Training Loss: 0.7181\n",
      "Epoch 3/10, Batch 371/883, Training Loss: 0.8505\n",
      "Epoch 3/10, Batch 372/883, Training Loss: 0.7877\n",
      "Epoch 3/10, Batch 373/883, Training Loss: 0.6071\n",
      "Epoch 3/10, Batch 374/883, Training Loss: 0.5477\n",
      "Epoch 3/10, Batch 375/883, Training Loss: 0.9164\n",
      "Epoch 3/10, Batch 376/883, Training Loss: 0.6536\n",
      "Epoch 3/10, Batch 377/883, Training Loss: 0.7117\n",
      "Epoch 3/10, Batch 378/883, Training Loss: 0.6748\n",
      "Epoch 3/10, Batch 379/883, Training Loss: 0.5602\n",
      "Epoch 3/10, Batch 380/883, Training Loss: 0.7907\n",
      "Epoch 3/10, Batch 381/883, Training Loss: 0.7581\n",
      "Epoch 3/10, Batch 382/883, Training Loss: 1.0443\n",
      "Epoch 3/10, Batch 383/883, Training Loss: 1.1401\n",
      "Epoch 3/10, Batch 384/883, Training Loss: 0.3952\n",
      "Epoch 3/10, Batch 385/883, Training Loss: 0.6875\n",
      "Epoch 3/10, Batch 386/883, Training Loss: 0.9415\n",
      "Epoch 3/10, Batch 387/883, Training Loss: 0.5209\n",
      "Epoch 3/10, Batch 388/883, Training Loss: 0.6788\n",
      "Epoch 3/10, Batch 389/883, Training Loss: 0.9132\n",
      "Epoch 3/10, Batch 390/883, Training Loss: 0.8089\n",
      "Epoch 3/10, Batch 391/883, Training Loss: 1.0852\n",
      "Epoch 3/10, Batch 392/883, Training Loss: 0.9168\n",
      "Epoch 3/10, Batch 393/883, Training Loss: 0.8817\n",
      "Epoch 3/10, Batch 394/883, Training Loss: 0.8496\n",
      "Epoch 3/10, Batch 395/883, Training Loss: 0.6949\n",
      "Epoch 3/10, Batch 396/883, Training Loss: 0.8969\n",
      "Epoch 3/10, Batch 397/883, Training Loss: 0.9206\n",
      "Epoch 3/10, Batch 398/883, Training Loss: 0.8208\n",
      "Epoch 3/10, Batch 399/883, Training Loss: 0.9688\n",
      "Epoch 3/10, Batch 400/883, Training Loss: 0.8941\n",
      "Epoch 3/10, Batch 401/883, Training Loss: 0.5893\n",
      "Epoch 3/10, Batch 402/883, Training Loss: 1.0040\n",
      "Epoch 3/10, Batch 403/883, Training Loss: 0.8807\n",
      "Epoch 3/10, Batch 404/883, Training Loss: 0.6897\n",
      "Epoch 3/10, Batch 405/883, Training Loss: 0.6811\n",
      "Epoch 3/10, Batch 406/883, Training Loss: 0.7746\n",
      "Epoch 3/10, Batch 407/883, Training Loss: 0.8473\n",
      "Epoch 3/10, Batch 408/883, Training Loss: 0.6632\n",
      "Epoch 3/10, Batch 409/883, Training Loss: 0.8059\n",
      "Epoch 3/10, Batch 410/883, Training Loss: 0.6597\n",
      "Epoch 3/10, Batch 411/883, Training Loss: 0.9622\n",
      "Epoch 3/10, Batch 412/883, Training Loss: 0.9270\n",
      "Epoch 3/10, Batch 413/883, Training Loss: 0.9053\n",
      "Epoch 3/10, Batch 414/883, Training Loss: 0.7763\n",
      "Epoch 3/10, Batch 415/883, Training Loss: 0.8774\n",
      "Epoch 3/10, Batch 416/883, Training Loss: 0.8738\n",
      "Epoch 3/10, Batch 417/883, Training Loss: 0.5501\n",
      "Epoch 3/10, Batch 418/883, Training Loss: 0.6044\n",
      "Epoch 3/10, Batch 419/883, Training Loss: 0.8499\n",
      "Epoch 3/10, Batch 420/883, Training Loss: 1.4836\n",
      "Epoch 3/10, Batch 421/883, Training Loss: 0.6743\n",
      "Epoch 3/10, Batch 422/883, Training Loss: 0.8384\n",
      "Epoch 3/10, Batch 423/883, Training Loss: 0.6165\n",
      "Epoch 3/10, Batch 424/883, Training Loss: 0.6782\n",
      "Epoch 3/10, Batch 425/883, Training Loss: 0.8299\n",
      "Epoch 3/10, Batch 426/883, Training Loss: 0.7531\n",
      "Epoch 3/10, Batch 427/883, Training Loss: 0.7400\n",
      "Epoch 3/10, Batch 428/883, Training Loss: 0.8201\n",
      "Epoch 3/10, Batch 429/883, Training Loss: 0.6676\n",
      "Epoch 3/10, Batch 430/883, Training Loss: 1.0488\n",
      "Epoch 3/10, Batch 431/883, Training Loss: 0.7305\n",
      "Epoch 3/10, Batch 432/883, Training Loss: 0.7924\n",
      "Epoch 3/10, Batch 433/883, Training Loss: 1.0972\n",
      "Epoch 3/10, Batch 434/883, Training Loss: 0.6584\n",
      "Epoch 3/10, Batch 435/883, Training Loss: 0.7779\n",
      "Epoch 3/10, Batch 436/883, Training Loss: 0.8727\n",
      "Epoch 3/10, Batch 437/883, Training Loss: 0.8085\n",
      "Epoch 3/10, Batch 438/883, Training Loss: 0.8123\n",
      "Epoch 3/10, Batch 439/883, Training Loss: 0.9815\n",
      "Epoch 3/10, Batch 440/883, Training Loss: 0.7228\n",
      "Epoch 3/10, Batch 441/883, Training Loss: 1.0177\n",
      "Epoch 3/10, Batch 442/883, Training Loss: 0.5769\n",
      "Epoch 3/10, Batch 443/883, Training Loss: 0.8420\n",
      "Epoch 3/10, Batch 444/883, Training Loss: 0.7633\n",
      "Epoch 3/10, Batch 445/883, Training Loss: 0.8968\n",
      "Epoch 3/10, Batch 446/883, Training Loss: 0.7700\n",
      "Epoch 3/10, Batch 447/883, Training Loss: 0.8827\n",
      "Epoch 3/10, Batch 448/883, Training Loss: 0.7744\n",
      "Epoch 3/10, Batch 449/883, Training Loss: 1.0458\n",
      "Epoch 3/10, Batch 450/883, Training Loss: 0.9531\n",
      "Epoch 3/10, Batch 451/883, Training Loss: 0.9297\n",
      "Epoch 3/10, Batch 452/883, Training Loss: 1.0326\n",
      "Epoch 3/10, Batch 453/883, Training Loss: 0.8781\n",
      "Epoch 3/10, Batch 454/883, Training Loss: 0.8055\n",
      "Epoch 3/10, Batch 455/883, Training Loss: 0.8132\n",
      "Epoch 3/10, Batch 456/883, Training Loss: 0.7825\n",
      "Epoch 3/10, Batch 457/883, Training Loss: 0.8203\n",
      "Epoch 3/10, Batch 458/883, Training Loss: 0.8246\n",
      "Epoch 3/10, Batch 459/883, Training Loss: 0.6236\n",
      "Epoch 3/10, Batch 460/883, Training Loss: 0.8863\n",
      "Epoch 3/10, Batch 461/883, Training Loss: 0.7553\n",
      "Epoch 3/10, Batch 462/883, Training Loss: 0.8381\n",
      "Epoch 3/10, Batch 463/883, Training Loss: 0.6115\n",
      "Epoch 3/10, Batch 464/883, Training Loss: 0.6825\n",
      "Epoch 3/10, Batch 465/883, Training Loss: 1.1937\n",
      "Epoch 3/10, Batch 466/883, Training Loss: 0.8716\n",
      "Epoch 3/10, Batch 467/883, Training Loss: 0.8978\n",
      "Epoch 3/10, Batch 468/883, Training Loss: 0.7175\n",
      "Epoch 3/10, Batch 469/883, Training Loss: 0.7566\n",
      "Epoch 3/10, Batch 470/883, Training Loss: 0.8213\n",
      "Epoch 3/10, Batch 471/883, Training Loss: 0.6144\n",
      "Epoch 3/10, Batch 472/883, Training Loss: 0.9952\n",
      "Epoch 3/10, Batch 473/883, Training Loss: 0.7523\n",
      "Epoch 3/10, Batch 474/883, Training Loss: 0.5385\n",
      "Epoch 3/10, Batch 475/883, Training Loss: 0.6542\n",
      "Epoch 3/10, Batch 476/883, Training Loss: 0.8143\n",
      "Epoch 3/10, Batch 477/883, Training Loss: 0.6855\n",
      "Epoch 3/10, Batch 478/883, Training Loss: 0.6129\n",
      "Epoch 3/10, Batch 479/883, Training Loss: 0.6527\n",
      "Epoch 3/10, Batch 480/883, Training Loss: 0.5200\n",
      "Epoch 3/10, Batch 481/883, Training Loss: 0.8720\n",
      "Epoch 3/10, Batch 482/883, Training Loss: 1.1726\n",
      "Epoch 3/10, Batch 483/883, Training Loss: 0.6575\n",
      "Epoch 3/10, Batch 484/883, Training Loss: 1.1166\n",
      "Epoch 3/10, Batch 485/883, Training Loss: 1.0645\n",
      "Epoch 3/10, Batch 486/883, Training Loss: 0.9094\n",
      "Epoch 3/10, Batch 487/883, Training Loss: 0.7136\n",
      "Epoch 3/10, Batch 488/883, Training Loss: 0.8674\n",
      "Epoch 3/10, Batch 489/883, Training Loss: 0.9372\n",
      "Epoch 3/10, Batch 490/883, Training Loss: 0.8587\n",
      "Epoch 3/10, Batch 491/883, Training Loss: 0.8535\n",
      "Epoch 3/10, Batch 492/883, Training Loss: 0.8922\n",
      "Epoch 3/10, Batch 493/883, Training Loss: 0.9407\n",
      "Epoch 3/10, Batch 494/883, Training Loss: 0.6135\n",
      "Epoch 3/10, Batch 495/883, Training Loss: 0.6192\n",
      "Epoch 3/10, Batch 496/883, Training Loss: 0.6727\n",
      "Epoch 3/10, Batch 497/883, Training Loss: 0.7934\n",
      "Epoch 3/10, Batch 498/883, Training Loss: 0.6666\n",
      "Epoch 3/10, Batch 499/883, Training Loss: 0.8144\n",
      "Epoch 3/10, Batch 500/883, Training Loss: 0.7552\n",
      "Epoch 3/10, Batch 501/883, Training Loss: 0.7582\n",
      "Epoch 3/10, Batch 502/883, Training Loss: 0.9727\n",
      "Epoch 3/10, Batch 503/883, Training Loss: 0.9115\n",
      "Epoch 3/10, Batch 504/883, Training Loss: 0.7621\n",
      "Epoch 3/10, Batch 505/883, Training Loss: 0.6610\n",
      "Epoch 3/10, Batch 506/883, Training Loss: 0.7423\n",
      "Epoch 3/10, Batch 507/883, Training Loss: 0.8559\n",
      "Epoch 3/10, Batch 508/883, Training Loss: 0.6819\n",
      "Epoch 3/10, Batch 509/883, Training Loss: 0.9180\n",
      "Epoch 3/10, Batch 510/883, Training Loss: 0.6830\n",
      "Epoch 3/10, Batch 511/883, Training Loss: 0.9407\n",
      "Epoch 3/10, Batch 512/883, Training Loss: 0.9961\n",
      "Epoch 3/10, Batch 513/883, Training Loss: 0.6555\n",
      "Epoch 3/10, Batch 514/883, Training Loss: 0.7654\n",
      "Epoch 3/10, Batch 515/883, Training Loss: 0.9566\n",
      "Epoch 3/10, Batch 516/883, Training Loss: 0.7073\n",
      "Epoch 3/10, Batch 517/883, Training Loss: 1.0961\n",
      "Epoch 3/10, Batch 518/883, Training Loss: 0.8826\n",
      "Epoch 3/10, Batch 519/883, Training Loss: 1.1151\n",
      "Epoch 3/10, Batch 520/883, Training Loss: 0.7908\n",
      "Epoch 3/10, Batch 521/883, Training Loss: 0.6283\n",
      "Epoch 3/10, Batch 522/883, Training Loss: 0.6983\n",
      "Epoch 3/10, Batch 523/883, Training Loss: 0.5447\n",
      "Epoch 3/10, Batch 524/883, Training Loss: 0.9590\n",
      "Epoch 3/10, Batch 525/883, Training Loss: 0.7463\n",
      "Epoch 3/10, Batch 526/883, Training Loss: 0.6347\n",
      "Epoch 3/10, Batch 527/883, Training Loss: 0.7750\n",
      "Epoch 3/10, Batch 528/883, Training Loss: 0.8056\n",
      "Epoch 3/10, Batch 529/883, Training Loss: 0.7325\n",
      "Epoch 3/10, Batch 530/883, Training Loss: 0.8042\n",
      "Epoch 3/10, Batch 531/883, Training Loss: 0.6700\n",
      "Epoch 3/10, Batch 532/883, Training Loss: 1.1043\n",
      "Epoch 3/10, Batch 533/883, Training Loss: 0.6183\n",
      "Epoch 3/10, Batch 534/883, Training Loss: 0.9156\n",
      "Epoch 3/10, Batch 535/883, Training Loss: 0.6997\n",
      "Epoch 3/10, Batch 536/883, Training Loss: 0.8130\n",
      "Epoch 3/10, Batch 537/883, Training Loss: 0.7233\n",
      "Epoch 3/10, Batch 538/883, Training Loss: 0.8216\n",
      "Epoch 3/10, Batch 539/883, Training Loss: 0.7406\n",
      "Epoch 3/10, Batch 540/883, Training Loss: 0.7494\n",
      "Epoch 3/10, Batch 541/883, Training Loss: 0.6807\n",
      "Epoch 3/10, Batch 542/883, Training Loss: 0.8059\n",
      "Epoch 3/10, Batch 543/883, Training Loss: 1.0390\n",
      "Epoch 3/10, Batch 544/883, Training Loss: 0.9026\n",
      "Epoch 3/10, Batch 545/883, Training Loss: 0.6122\n",
      "Epoch 3/10, Batch 546/883, Training Loss: 0.8400\n",
      "Epoch 3/10, Batch 547/883, Training Loss: 0.9867\n",
      "Epoch 3/10, Batch 548/883, Training Loss: 0.7209\n",
      "Epoch 3/10, Batch 549/883, Training Loss: 0.8641\n",
      "Epoch 3/10, Batch 550/883, Training Loss: 0.9340\n",
      "Epoch 3/10, Batch 551/883, Training Loss: 0.7348\n",
      "Epoch 3/10, Batch 552/883, Training Loss: 1.2581\n",
      "Epoch 3/10, Batch 553/883, Training Loss: 0.7983\n",
      "Epoch 3/10, Batch 554/883, Training Loss: 0.6342\n",
      "Epoch 3/10, Batch 555/883, Training Loss: 1.0051\n",
      "Epoch 3/10, Batch 556/883, Training Loss: 0.7088\n",
      "Epoch 3/10, Batch 557/883, Training Loss: 0.7065\n",
      "Epoch 3/10, Batch 558/883, Training Loss: 0.9016\n",
      "Epoch 3/10, Batch 559/883, Training Loss: 0.7768\n",
      "Epoch 3/10, Batch 560/883, Training Loss: 0.8897\n",
      "Epoch 3/10, Batch 561/883, Training Loss: 0.8926\n",
      "Epoch 3/10, Batch 562/883, Training Loss: 0.8685\n",
      "Epoch 3/10, Batch 563/883, Training Loss: 0.9530\n",
      "Epoch 3/10, Batch 564/883, Training Loss: 0.5462\n",
      "Epoch 3/10, Batch 565/883, Training Loss: 0.7611\n",
      "Epoch 3/10, Batch 566/883, Training Loss: 0.7180\n",
      "Epoch 3/10, Batch 567/883, Training Loss: 0.6293\n",
      "Epoch 3/10, Batch 568/883, Training Loss: 0.9987\n",
      "Epoch 3/10, Batch 569/883, Training Loss: 0.9531\n",
      "Epoch 3/10, Batch 570/883, Training Loss: 0.6929\n",
      "Epoch 3/10, Batch 571/883, Training Loss: 0.9039\n",
      "Epoch 3/10, Batch 572/883, Training Loss: 0.8277\n",
      "Epoch 3/10, Batch 573/883, Training Loss: 0.7497\n",
      "Epoch 3/10, Batch 574/883, Training Loss: 0.7962\n",
      "Epoch 3/10, Batch 575/883, Training Loss: 0.6162\n",
      "Epoch 3/10, Batch 576/883, Training Loss: 0.8839\n",
      "Epoch 3/10, Batch 577/883, Training Loss: 0.6790\n",
      "Epoch 3/10, Batch 578/883, Training Loss: 0.8757\n",
      "Epoch 3/10, Batch 579/883, Training Loss: 0.9354\n",
      "Epoch 3/10, Batch 580/883, Training Loss: 0.5857\n",
      "Epoch 3/10, Batch 581/883, Training Loss: 1.1574\n",
      "Epoch 3/10, Batch 582/883, Training Loss: 1.1448\n",
      "Epoch 3/10, Batch 583/883, Training Loss: 0.6304\n",
      "Epoch 3/10, Batch 584/883, Training Loss: 0.6357\n",
      "Epoch 3/10, Batch 585/883, Training Loss: 0.7580\n",
      "Epoch 3/10, Batch 586/883, Training Loss: 0.6207\n",
      "Epoch 3/10, Batch 587/883, Training Loss: 0.8082\n",
      "Epoch 3/10, Batch 588/883, Training Loss: 0.5270\n",
      "Epoch 3/10, Batch 589/883, Training Loss: 0.7562\n",
      "Epoch 3/10, Batch 590/883, Training Loss: 0.6440\n",
      "Epoch 3/10, Batch 591/883, Training Loss: 0.7183\n",
      "Epoch 3/10, Batch 592/883, Training Loss: 0.8307\n",
      "Epoch 3/10, Batch 593/883, Training Loss: 0.7278\n",
      "Epoch 3/10, Batch 594/883, Training Loss: 0.7392\n",
      "Epoch 3/10, Batch 595/883, Training Loss: 0.8241\n",
      "Epoch 3/10, Batch 596/883, Training Loss: 0.9098\n",
      "Epoch 3/10, Batch 597/883, Training Loss: 1.0327\n",
      "Epoch 3/10, Batch 598/883, Training Loss: 0.8268\n",
      "Epoch 3/10, Batch 599/883, Training Loss: 0.9386\n",
      "Epoch 3/10, Batch 600/883, Training Loss: 0.9360\n",
      "Epoch 3/10, Batch 601/883, Training Loss: 0.7570\n",
      "Epoch 3/10, Batch 602/883, Training Loss: 0.5745\n",
      "Epoch 3/10, Batch 603/883, Training Loss: 0.7762\n",
      "Epoch 3/10, Batch 604/883, Training Loss: 0.7411\n",
      "Epoch 3/10, Batch 605/883, Training Loss: 0.9466\n",
      "Epoch 3/10, Batch 606/883, Training Loss: 0.6589\n",
      "Epoch 3/10, Batch 607/883, Training Loss: 0.6313\n",
      "Epoch 3/10, Batch 608/883, Training Loss: 0.7648\n",
      "Epoch 3/10, Batch 609/883, Training Loss: 0.8017\n",
      "Epoch 3/10, Batch 610/883, Training Loss: 0.6302\n",
      "Epoch 3/10, Batch 611/883, Training Loss: 0.6529\n",
      "Epoch 3/10, Batch 612/883, Training Loss: 0.6462\n",
      "Epoch 3/10, Batch 613/883, Training Loss: 1.0131\n",
      "Epoch 3/10, Batch 614/883, Training Loss: 0.5486\n",
      "Epoch 3/10, Batch 615/883, Training Loss: 1.2058\n",
      "Epoch 3/10, Batch 616/883, Training Loss: 0.7736\n",
      "Epoch 3/10, Batch 617/883, Training Loss: 0.6853\n",
      "Epoch 3/10, Batch 618/883, Training Loss: 0.5568\n",
      "Epoch 3/10, Batch 619/883, Training Loss: 0.9474\n",
      "Epoch 3/10, Batch 620/883, Training Loss: 0.9552\n",
      "Epoch 3/10, Batch 621/883, Training Loss: 0.9058\n",
      "Epoch 3/10, Batch 622/883, Training Loss: 0.6674\n",
      "Epoch 3/10, Batch 623/883, Training Loss: 0.8355\n",
      "Epoch 3/10, Batch 624/883, Training Loss: 0.7147\n",
      "Epoch 3/10, Batch 625/883, Training Loss: 0.6796\n",
      "Epoch 3/10, Batch 626/883, Training Loss: 0.6373\n",
      "Epoch 3/10, Batch 627/883, Training Loss: 0.9256\n",
      "Epoch 3/10, Batch 628/883, Training Loss: 0.7495\n",
      "Epoch 3/10, Batch 629/883, Training Loss: 0.7989\n",
      "Epoch 3/10, Batch 630/883, Training Loss: 0.7657\n",
      "Epoch 3/10, Batch 631/883, Training Loss: 0.5830\n",
      "Epoch 3/10, Batch 632/883, Training Loss: 0.9927\n",
      "Epoch 3/10, Batch 633/883, Training Loss: 1.1534\n",
      "Epoch 3/10, Batch 634/883, Training Loss: 0.6888\n",
      "Epoch 3/10, Batch 635/883, Training Loss: 1.0598\n",
      "Epoch 3/10, Batch 636/883, Training Loss: 0.6888\n",
      "Epoch 3/10, Batch 637/883, Training Loss: 0.9598\n",
      "Epoch 3/10, Batch 638/883, Training Loss: 0.7819\n",
      "Epoch 3/10, Batch 639/883, Training Loss: 0.7594\n",
      "Epoch 3/10, Batch 640/883, Training Loss: 0.5626\n",
      "Epoch 3/10, Batch 641/883, Training Loss: 0.8209\n",
      "Epoch 3/10, Batch 642/883, Training Loss: 0.8317\n",
      "Epoch 3/10, Batch 643/883, Training Loss: 0.7479\n",
      "Epoch 3/10, Batch 644/883, Training Loss: 0.9551\n",
      "Epoch 3/10, Batch 645/883, Training Loss: 0.7283\n",
      "Epoch 3/10, Batch 646/883, Training Loss: 0.8408\n",
      "Epoch 3/10, Batch 647/883, Training Loss: 0.6774\n",
      "Epoch 3/10, Batch 648/883, Training Loss: 0.6545\n",
      "Epoch 3/10, Batch 649/883, Training Loss: 0.8732\n",
      "Epoch 3/10, Batch 650/883, Training Loss: 0.8142\n",
      "Epoch 3/10, Batch 651/883, Training Loss: 0.7307\n",
      "Epoch 3/10, Batch 652/883, Training Loss: 0.7701\n",
      "Epoch 3/10, Batch 653/883, Training Loss: 0.6769\n",
      "Epoch 3/10, Batch 654/883, Training Loss: 0.8551\n",
      "Epoch 3/10, Batch 655/883, Training Loss: 0.6340\n",
      "Epoch 3/10, Batch 656/883, Training Loss: 0.8533\n",
      "Epoch 3/10, Batch 657/883, Training Loss: 0.7572\n",
      "Epoch 3/10, Batch 658/883, Training Loss: 1.3595\n",
      "Epoch 3/10, Batch 659/883, Training Loss: 0.7160\n",
      "Epoch 3/10, Batch 660/883, Training Loss: 0.7431\n",
      "Epoch 3/10, Batch 661/883, Training Loss: 0.8929\n",
      "Epoch 3/10, Batch 662/883, Training Loss: 0.7478\n",
      "Epoch 3/10, Batch 663/883, Training Loss: 0.9262\n",
      "Epoch 3/10, Batch 664/883, Training Loss: 0.9867\n",
      "Epoch 3/10, Batch 665/883, Training Loss: 0.7967\n",
      "Epoch 3/10, Batch 666/883, Training Loss: 0.8549\n",
      "Epoch 3/10, Batch 667/883, Training Loss: 0.9905\n",
      "Epoch 3/10, Batch 668/883, Training Loss: 0.7338\n",
      "Epoch 3/10, Batch 669/883, Training Loss: 1.0074\n",
      "Epoch 3/10, Batch 670/883, Training Loss: 0.7661\n",
      "Epoch 3/10, Batch 671/883, Training Loss: 0.8936\n",
      "Epoch 3/10, Batch 672/883, Training Loss: 0.8750\n",
      "Epoch 3/10, Batch 673/883, Training Loss: 0.9419\n",
      "Epoch 3/10, Batch 674/883, Training Loss: 0.6778\n",
      "Epoch 3/10, Batch 675/883, Training Loss: 1.0397\n",
      "Epoch 3/10, Batch 676/883, Training Loss: 0.8991\n",
      "Epoch 3/10, Batch 677/883, Training Loss: 0.8071\n",
      "Epoch 3/10, Batch 678/883, Training Loss: 0.9850\n",
      "Epoch 3/10, Batch 679/883, Training Loss: 0.8133\n",
      "Epoch 3/10, Batch 680/883, Training Loss: 0.6423\n",
      "Epoch 3/10, Batch 681/883, Training Loss: 0.8150\n",
      "Epoch 3/10, Batch 682/883, Training Loss: 0.8569\n",
      "Epoch 3/10, Batch 683/883, Training Loss: 0.8816\n",
      "Epoch 3/10, Batch 684/883, Training Loss: 1.1742\n",
      "Epoch 3/10, Batch 685/883, Training Loss: 1.1191\n",
      "Epoch 3/10, Batch 686/883, Training Loss: 0.8730\n",
      "Epoch 3/10, Batch 687/883, Training Loss: 0.7275\n",
      "Epoch 3/10, Batch 688/883, Training Loss: 0.9519\n",
      "Epoch 3/10, Batch 689/883, Training Loss: 0.8505\n",
      "Epoch 3/10, Batch 690/883, Training Loss: 0.7987\n",
      "Epoch 3/10, Batch 691/883, Training Loss: 0.8585\n",
      "Epoch 3/10, Batch 692/883, Training Loss: 0.8177\n",
      "Epoch 3/10, Batch 693/883, Training Loss: 0.6780\n",
      "Epoch 3/10, Batch 694/883, Training Loss: 0.9533\n",
      "Epoch 3/10, Batch 695/883, Training Loss: 0.9061\n",
      "Epoch 3/10, Batch 696/883, Training Loss: 0.7828\n",
      "Epoch 3/10, Batch 697/883, Training Loss: 1.0655\n",
      "Epoch 3/10, Batch 698/883, Training Loss: 0.5596\n",
      "Epoch 3/10, Batch 699/883, Training Loss: 0.9000\n",
      "Epoch 3/10, Batch 700/883, Training Loss: 0.6546\n",
      "Epoch 3/10, Batch 701/883, Training Loss: 0.7590\n",
      "Epoch 3/10, Batch 702/883, Training Loss: 0.7076\n",
      "Epoch 3/10, Batch 703/883, Training Loss: 1.1094\n",
      "Epoch 3/10, Batch 704/883, Training Loss: 0.7381\n",
      "Epoch 3/10, Batch 705/883, Training Loss: 0.7521\n",
      "Epoch 3/10, Batch 706/883, Training Loss: 0.6886\n",
      "Epoch 3/10, Batch 707/883, Training Loss: 0.6872\n",
      "Epoch 3/10, Batch 708/883, Training Loss: 0.6362\n",
      "Epoch 3/10, Batch 709/883, Training Loss: 0.7441\n",
      "Epoch 3/10, Batch 710/883, Training Loss: 0.9357\n",
      "Epoch 3/10, Batch 711/883, Training Loss: 0.7264\n",
      "Epoch 3/10, Batch 712/883, Training Loss: 0.9322\n",
      "Epoch 3/10, Batch 713/883, Training Loss: 0.8736\n",
      "Epoch 3/10, Batch 714/883, Training Loss: 0.9859\n",
      "Epoch 3/10, Batch 715/883, Training Loss: 0.6435\n",
      "Epoch 3/10, Batch 716/883, Training Loss: 0.6635\n",
      "Epoch 3/10, Batch 717/883, Training Loss: 0.8109\n",
      "Epoch 3/10, Batch 718/883, Training Loss: 0.6634\n",
      "Epoch 3/10, Batch 719/883, Training Loss: 0.8330\n",
      "Epoch 3/10, Batch 720/883, Training Loss: 0.7969\n",
      "Epoch 3/10, Batch 721/883, Training Loss: 0.6131\n",
      "Epoch 3/10, Batch 722/883, Training Loss: 0.9225\n",
      "Epoch 3/10, Batch 723/883, Training Loss: 0.6859\n",
      "Epoch 3/10, Batch 724/883, Training Loss: 0.7428\n",
      "Epoch 3/10, Batch 725/883, Training Loss: 0.9004\n",
      "Epoch 3/10, Batch 726/883, Training Loss: 0.7217\n",
      "Epoch 3/10, Batch 727/883, Training Loss: 0.8895\n",
      "Epoch 3/10, Batch 728/883, Training Loss: 0.7863\n",
      "Epoch 3/10, Batch 729/883, Training Loss: 0.8088\n",
      "Epoch 3/10, Batch 730/883, Training Loss: 0.5911\n",
      "Epoch 3/10, Batch 731/883, Training Loss: 0.5840\n",
      "Epoch 3/10, Batch 732/883, Training Loss: 0.7881\n",
      "Epoch 3/10, Batch 733/883, Training Loss: 0.8494\n",
      "Epoch 3/10, Batch 734/883, Training Loss: 0.6349\n",
      "Epoch 3/10, Batch 735/883, Training Loss: 0.7067\n",
      "Epoch 3/10, Batch 736/883, Training Loss: 0.6728\n",
      "Epoch 3/10, Batch 737/883, Training Loss: 0.6404\n",
      "Epoch 3/10, Batch 738/883, Training Loss: 0.5405\n",
      "Epoch 3/10, Batch 739/883, Training Loss: 0.6111\n",
      "Epoch 3/10, Batch 740/883, Training Loss: 0.7380\n",
      "Epoch 3/10, Batch 741/883, Training Loss: 0.6670\n",
      "Epoch 3/10, Batch 742/883, Training Loss: 0.8065\n",
      "Epoch 3/10, Batch 743/883, Training Loss: 0.3511\n",
      "Epoch 3/10, Batch 744/883, Training Loss: 1.0049\n",
      "Epoch 3/10, Batch 745/883, Training Loss: 1.1118\n",
      "Epoch 3/10, Batch 746/883, Training Loss: 0.6776\n",
      "Epoch 3/10, Batch 747/883, Training Loss: 0.9034\n",
      "Epoch 3/10, Batch 748/883, Training Loss: 0.7791\n",
      "Epoch 3/10, Batch 749/883, Training Loss: 0.5086\n",
      "Epoch 3/10, Batch 750/883, Training Loss: 0.5384\n",
      "Epoch 3/10, Batch 751/883, Training Loss: 0.7223\n",
      "Epoch 3/10, Batch 752/883, Training Loss: 0.7626\n",
      "Epoch 3/10, Batch 753/883, Training Loss: 0.7113\n",
      "Epoch 3/10, Batch 754/883, Training Loss: 1.2117\n",
      "Epoch 3/10, Batch 755/883, Training Loss: 0.7108\n",
      "Epoch 3/10, Batch 756/883, Training Loss: 0.6795\n",
      "Epoch 3/10, Batch 757/883, Training Loss: 0.7302\n",
      "Epoch 3/10, Batch 758/883, Training Loss: 0.7523\n",
      "Epoch 3/10, Batch 759/883, Training Loss: 0.5344\n",
      "Epoch 3/10, Batch 760/883, Training Loss: 1.0403\n",
      "Epoch 3/10, Batch 761/883, Training Loss: 0.6288\n",
      "Epoch 3/10, Batch 762/883, Training Loss: 0.7609\n",
      "Epoch 3/10, Batch 763/883, Training Loss: 0.7845\n",
      "Epoch 3/10, Batch 764/883, Training Loss: 0.7951\n",
      "Epoch 3/10, Batch 765/883, Training Loss: 0.9131\n",
      "Epoch 3/10, Batch 766/883, Training Loss: 0.7584\n",
      "Epoch 3/10, Batch 767/883, Training Loss: 0.7413\n",
      "Epoch 3/10, Batch 768/883, Training Loss: 0.8275\n",
      "Epoch 3/10, Batch 769/883, Training Loss: 0.8979\n",
      "Epoch 3/10, Batch 770/883, Training Loss: 0.6676\n",
      "Epoch 3/10, Batch 771/883, Training Loss: 0.8043\n",
      "Epoch 3/10, Batch 772/883, Training Loss: 1.0986\n",
      "Epoch 3/10, Batch 773/883, Training Loss: 1.2860\n",
      "Epoch 3/10, Batch 774/883, Training Loss: 0.7255\n",
      "Epoch 3/10, Batch 775/883, Training Loss: 0.7175\n",
      "Epoch 3/10, Batch 776/883, Training Loss: 0.6743\n",
      "Epoch 3/10, Batch 777/883, Training Loss: 0.6825\n",
      "Epoch 3/10, Batch 778/883, Training Loss: 1.1644\n",
      "Epoch 3/10, Batch 779/883, Training Loss: 0.6697\n",
      "Epoch 3/10, Batch 780/883, Training Loss: 0.7631\n",
      "Epoch 3/10, Batch 781/883, Training Loss: 0.7182\n",
      "Epoch 3/10, Batch 782/883, Training Loss: 0.7002\n",
      "Epoch 3/10, Batch 783/883, Training Loss: 0.6692\n",
      "Epoch 3/10, Batch 784/883, Training Loss: 0.7020\n",
      "Epoch 3/10, Batch 785/883, Training Loss: 0.7603\n",
      "Epoch 3/10, Batch 786/883, Training Loss: 0.7310\n",
      "Epoch 3/10, Batch 787/883, Training Loss: 0.8126\n",
      "Epoch 3/10, Batch 788/883, Training Loss: 0.7797\n",
      "Epoch 3/10, Batch 789/883, Training Loss: 0.6606\n",
      "Epoch 3/10, Batch 790/883, Training Loss: 0.8256\n",
      "Epoch 3/10, Batch 791/883, Training Loss: 0.6606\n",
      "Epoch 3/10, Batch 792/883, Training Loss: 0.7972\n",
      "Epoch 3/10, Batch 793/883, Training Loss: 1.0118\n",
      "Epoch 3/10, Batch 794/883, Training Loss: 0.9108\n",
      "Epoch 3/10, Batch 795/883, Training Loss: 0.9652\n",
      "Epoch 3/10, Batch 796/883, Training Loss: 0.8073\n",
      "Epoch 3/10, Batch 797/883, Training Loss: 0.6759\n",
      "Epoch 3/10, Batch 798/883, Training Loss: 0.5424\n",
      "Epoch 3/10, Batch 799/883, Training Loss: 0.5922\n",
      "Epoch 3/10, Batch 800/883, Training Loss: 0.7886\n",
      "Epoch 3/10, Batch 801/883, Training Loss: 0.6391\n",
      "Epoch 3/10, Batch 802/883, Training Loss: 0.7227\n",
      "Epoch 3/10, Batch 803/883, Training Loss: 0.9150\n",
      "Epoch 3/10, Batch 804/883, Training Loss: 0.7687\n",
      "Epoch 3/10, Batch 805/883, Training Loss: 0.9465\n",
      "Epoch 3/10, Batch 806/883, Training Loss: 0.8650\n",
      "Epoch 3/10, Batch 807/883, Training Loss: 0.8044\n",
      "Epoch 3/10, Batch 808/883, Training Loss: 0.7724\n",
      "Epoch 3/10, Batch 809/883, Training Loss: 0.8435\n",
      "Epoch 3/10, Batch 810/883, Training Loss: 0.6279\n",
      "Epoch 3/10, Batch 811/883, Training Loss: 0.7976\n",
      "Epoch 3/10, Batch 812/883, Training Loss: 0.7020\n",
      "Epoch 3/10, Batch 813/883, Training Loss: 0.6991\n",
      "Epoch 3/10, Batch 814/883, Training Loss: 1.1333\n",
      "Epoch 3/10, Batch 815/883, Training Loss: 0.8248\n",
      "Epoch 3/10, Batch 816/883, Training Loss: 0.7259\n",
      "Epoch 3/10, Batch 817/883, Training Loss: 0.7001\n",
      "Epoch 3/10, Batch 818/883, Training Loss: 0.8274\n",
      "Epoch 3/10, Batch 819/883, Training Loss: 0.6949\n",
      "Epoch 3/10, Batch 820/883, Training Loss: 0.8468\n",
      "Epoch 3/10, Batch 821/883, Training Loss: 0.9104\n",
      "Epoch 3/10, Batch 822/883, Training Loss: 0.9014\n",
      "Epoch 3/10, Batch 823/883, Training Loss: 1.0960\n",
      "Epoch 3/10, Batch 824/883, Training Loss: 0.7075\n",
      "Epoch 3/10, Batch 825/883, Training Loss: 0.8606\n",
      "Epoch 3/10, Batch 826/883, Training Loss: 0.6793\n",
      "Epoch 3/10, Batch 827/883, Training Loss: 0.8097\n",
      "Epoch 3/10, Batch 828/883, Training Loss: 0.7624\n",
      "Epoch 3/10, Batch 829/883, Training Loss: 0.7887\n",
      "Epoch 3/10, Batch 830/883, Training Loss: 0.9608\n",
      "Epoch 3/10, Batch 831/883, Training Loss: 1.0428\n",
      "Epoch 3/10, Batch 832/883, Training Loss: 0.6340\n",
      "Epoch 3/10, Batch 833/883, Training Loss: 0.9586\n",
      "Epoch 3/10, Batch 834/883, Training Loss: 1.0591\n",
      "Epoch 3/10, Batch 835/883, Training Loss: 0.9586\n",
      "Epoch 3/10, Batch 836/883, Training Loss: 0.6292\n",
      "Epoch 3/10, Batch 837/883, Training Loss: 0.9258\n",
      "Epoch 3/10, Batch 838/883, Training Loss: 0.9293\n",
      "Epoch 3/10, Batch 839/883, Training Loss: 0.5475\n",
      "Epoch 3/10, Batch 840/883, Training Loss: 0.6315\n",
      "Epoch 3/10, Batch 841/883, Training Loss: 0.7727\n",
      "Epoch 3/10, Batch 842/883, Training Loss: 0.7087\n",
      "Epoch 3/10, Batch 843/883, Training Loss: 0.9828\n",
      "Epoch 3/10, Batch 844/883, Training Loss: 0.8417\n",
      "Epoch 3/10, Batch 845/883, Training Loss: 0.7285\n",
      "Epoch 3/10, Batch 846/883, Training Loss: 0.8234\n",
      "Epoch 3/10, Batch 847/883, Training Loss: 0.8296\n",
      "Epoch 3/10, Batch 848/883, Training Loss: 0.8203\n",
      "Epoch 3/10, Batch 849/883, Training Loss: 1.1684\n",
      "Epoch 3/10, Batch 850/883, Training Loss: 0.6818\n",
      "Epoch 3/10, Batch 851/883, Training Loss: 0.7843\n",
      "Epoch 3/10, Batch 852/883, Training Loss: 0.9160\n",
      "Epoch 3/10, Batch 853/883, Training Loss: 0.9024\n",
      "Epoch 3/10, Batch 854/883, Training Loss: 0.7649\n",
      "Epoch 3/10, Batch 855/883, Training Loss: 0.7938\n",
      "Epoch 3/10, Batch 856/883, Training Loss: 0.8255\n",
      "Epoch 3/10, Batch 857/883, Training Loss: 0.9843\n",
      "Epoch 3/10, Batch 858/883, Training Loss: 0.7459\n",
      "Epoch 3/10, Batch 859/883, Training Loss: 0.7018\n",
      "Epoch 3/10, Batch 860/883, Training Loss: 0.9404\n",
      "Epoch 3/10, Batch 861/883, Training Loss: 0.9332\n",
      "Epoch 3/10, Batch 862/883, Training Loss: 0.7488\n",
      "Epoch 3/10, Batch 863/883, Training Loss: 0.6938\n",
      "Epoch 3/10, Batch 864/883, Training Loss: 0.8911\n",
      "Epoch 3/10, Batch 865/883, Training Loss: 0.6313\n",
      "Epoch 3/10, Batch 866/883, Training Loss: 0.7217\n",
      "Epoch 3/10, Batch 867/883, Training Loss: 0.6772\n",
      "Epoch 3/10, Batch 868/883, Training Loss: 0.8904\n",
      "Epoch 3/10, Batch 869/883, Training Loss: 0.9374\n",
      "Epoch 3/10, Batch 870/883, Training Loss: 0.4787\n",
      "Epoch 3/10, Batch 871/883, Training Loss: 0.8288\n",
      "Epoch 3/10, Batch 872/883, Training Loss: 0.8670\n",
      "Epoch 3/10, Batch 873/883, Training Loss: 0.7289\n",
      "Epoch 3/10, Batch 874/883, Training Loss: 0.7366\n",
      "Epoch 3/10, Batch 875/883, Training Loss: 0.5703\n",
      "Epoch 3/10, Batch 876/883, Training Loss: 0.7790\n",
      "Epoch 3/10, Batch 877/883, Training Loss: 0.7204\n",
      "Epoch 3/10, Batch 878/883, Training Loss: 0.5264\n",
      "Epoch 3/10, Batch 879/883, Training Loss: 0.9333\n",
      "Epoch 3/10, Batch 880/883, Training Loss: 0.7743\n",
      "Epoch 3/10, Batch 881/883, Training Loss: 1.1182\n",
      "Epoch 3/10, Batch 882/883, Training Loss: 0.6490\n",
      "Epoch 3/10, Batch 883/883, Training Loss: 0.7550\n",
      "Epoch 3/10, Training Loss: 0.8039, Validation Loss: 0.7748, Validation Accuracy: 0.6412\n",
      "Epoch 4/10, Batch 1/883, Training Loss: 0.7349\n",
      "Epoch 4/10, Batch 2/883, Training Loss: 0.9266\n",
      "Epoch 4/10, Batch 3/883, Training Loss: 1.0039\n",
      "Epoch 4/10, Batch 4/883, Training Loss: 0.8242\n",
      "Epoch 4/10, Batch 5/883, Training Loss: 0.8595\n",
      "Epoch 4/10, Batch 6/883, Training Loss: 0.8099\n",
      "Epoch 4/10, Batch 7/883, Training Loss: 0.8138\n",
      "Epoch 4/10, Batch 8/883, Training Loss: 0.7865\n",
      "Epoch 4/10, Batch 9/883, Training Loss: 0.6925\n",
      "Epoch 4/10, Batch 10/883, Training Loss: 0.7152\n",
      "Epoch 4/10, Batch 11/883, Training Loss: 1.1151\n",
      "Epoch 4/10, Batch 12/883, Training Loss: 0.5523\n",
      "Epoch 4/10, Batch 13/883, Training Loss: 0.8165\n",
      "Epoch 4/10, Batch 14/883, Training Loss: 0.7048\n",
      "Epoch 4/10, Batch 15/883, Training Loss: 0.9590\n",
      "Epoch 4/10, Batch 16/883, Training Loss: 0.5770\n",
      "Epoch 4/10, Batch 17/883, Training Loss: 0.7917\n",
      "Epoch 4/10, Batch 18/883, Training Loss: 0.4856\n",
      "Epoch 4/10, Batch 19/883, Training Loss: 0.8108\n",
      "Epoch 4/10, Batch 20/883, Training Loss: 1.1519\n",
      "Epoch 4/10, Batch 21/883, Training Loss: 0.8655\n",
      "Epoch 4/10, Batch 22/883, Training Loss: 0.7793\n",
      "Epoch 4/10, Batch 23/883, Training Loss: 0.9287\n",
      "Epoch 4/10, Batch 24/883, Training Loss: 0.9568\n",
      "Epoch 4/10, Batch 25/883, Training Loss: 0.5529\n",
      "Epoch 4/10, Batch 26/883, Training Loss: 0.8915\n",
      "Epoch 4/10, Batch 27/883, Training Loss: 0.5732\n",
      "Epoch 4/10, Batch 28/883, Training Loss: 0.6060\n",
      "Epoch 4/10, Batch 29/883, Training Loss: 0.5842\n",
      "Epoch 4/10, Batch 30/883, Training Loss: 1.0505\n",
      "Epoch 4/10, Batch 31/883, Training Loss: 0.9575\n",
      "Epoch 4/10, Batch 32/883, Training Loss: 0.6454\n",
      "Epoch 4/10, Batch 33/883, Training Loss: 0.7261\n",
      "Epoch 4/10, Batch 34/883, Training Loss: 0.7778\n",
      "Epoch 4/10, Batch 35/883, Training Loss: 0.7782\n",
      "Epoch 4/10, Batch 36/883, Training Loss: 0.7542\n",
      "Epoch 4/10, Batch 37/883, Training Loss: 0.7229\n",
      "Epoch 4/10, Batch 38/883, Training Loss: 0.7567\n",
      "Epoch 4/10, Batch 39/883, Training Loss: 0.9272\n",
      "Epoch 4/10, Batch 40/883, Training Loss: 0.9070\n",
      "Epoch 4/10, Batch 41/883, Training Loss: 0.7312\n",
      "Epoch 4/10, Batch 42/883, Training Loss: 0.9147\n",
      "Epoch 4/10, Batch 43/883, Training Loss: 0.5960\n",
      "Epoch 4/10, Batch 44/883, Training Loss: 0.7280\n",
      "Epoch 4/10, Batch 45/883, Training Loss: 0.9034\n",
      "Epoch 4/10, Batch 46/883, Training Loss: 0.7911\n",
      "Epoch 4/10, Batch 47/883, Training Loss: 0.8049\n",
      "Epoch 4/10, Batch 48/883, Training Loss: 1.1707\n",
      "Epoch 4/10, Batch 49/883, Training Loss: 0.9855\n",
      "Epoch 4/10, Batch 50/883, Training Loss: 0.9354\n",
      "Epoch 4/10, Batch 51/883, Training Loss: 1.0498\n",
      "Epoch 4/10, Batch 52/883, Training Loss: 0.9958\n",
      "Epoch 4/10, Batch 53/883, Training Loss: 0.9969\n",
      "Epoch 4/10, Batch 54/883, Training Loss: 0.8482\n",
      "Epoch 4/10, Batch 55/883, Training Loss: 0.9755\n",
      "Epoch 4/10, Batch 56/883, Training Loss: 0.6944\n",
      "Epoch 4/10, Batch 57/883, Training Loss: 0.6986\n",
      "Epoch 4/10, Batch 58/883, Training Loss: 0.6200\n",
      "Epoch 4/10, Batch 59/883, Training Loss: 0.6763\n",
      "Epoch 4/10, Batch 60/883, Training Loss: 0.8153\n",
      "Epoch 4/10, Batch 61/883, Training Loss: 0.6887\n",
      "Epoch 4/10, Batch 62/883, Training Loss: 0.9251\n",
      "Epoch 4/10, Batch 63/883, Training Loss: 0.8992\n",
      "Epoch 4/10, Batch 64/883, Training Loss: 0.6731\n",
      "Epoch 4/10, Batch 65/883, Training Loss: 0.5264\n",
      "Epoch 4/10, Batch 66/883, Training Loss: 1.4227\n",
      "Epoch 4/10, Batch 67/883, Training Loss: 0.6518\n",
      "Epoch 4/10, Batch 68/883, Training Loss: 0.9503\n",
      "Epoch 4/10, Batch 69/883, Training Loss: 0.6551\n",
      "Epoch 4/10, Batch 70/883, Training Loss: 0.6178\n",
      "Epoch 4/10, Batch 71/883, Training Loss: 0.6874\n",
      "Epoch 4/10, Batch 72/883, Training Loss: 1.0236\n",
      "Epoch 4/10, Batch 73/883, Training Loss: 0.7234\n",
      "Epoch 4/10, Batch 74/883, Training Loss: 1.0047\n",
      "Epoch 4/10, Batch 75/883, Training Loss: 0.5837\n",
      "Epoch 4/10, Batch 76/883, Training Loss: 0.9002\n",
      "Epoch 4/10, Batch 77/883, Training Loss: 0.7762\n",
      "Epoch 4/10, Batch 78/883, Training Loss: 0.9062\n",
      "Epoch 4/10, Batch 79/883, Training Loss: 0.7902\n",
      "Epoch 4/10, Batch 80/883, Training Loss: 1.1878\n",
      "Epoch 4/10, Batch 81/883, Training Loss: 0.8553\n",
      "Epoch 4/10, Batch 82/883, Training Loss: 0.8056\n",
      "Epoch 4/10, Batch 83/883, Training Loss: 0.7030\n",
      "Epoch 4/10, Batch 84/883, Training Loss: 0.7485\n",
      "Epoch 4/10, Batch 85/883, Training Loss: 0.5713\n",
      "Epoch 4/10, Batch 86/883, Training Loss: 0.7280\n",
      "Epoch 4/10, Batch 87/883, Training Loss: 0.9046\n",
      "Epoch 4/10, Batch 88/883, Training Loss: 0.6791\n",
      "Epoch 4/10, Batch 89/883, Training Loss: 0.6547\n",
      "Epoch 4/10, Batch 90/883, Training Loss: 0.8234\n",
      "Epoch 4/10, Batch 91/883, Training Loss: 0.8752\n",
      "Epoch 4/10, Batch 92/883, Training Loss: 0.9669\n",
      "Epoch 4/10, Batch 93/883, Training Loss: 0.7604\n",
      "Epoch 4/10, Batch 94/883, Training Loss: 0.6722\n",
      "Epoch 4/10, Batch 95/883, Training Loss: 0.7201\n",
      "Epoch 4/10, Batch 96/883, Training Loss: 0.7698\n",
      "Epoch 4/10, Batch 97/883, Training Loss: 0.6235\n",
      "Epoch 4/10, Batch 98/883, Training Loss: 0.5957\n",
      "Epoch 4/10, Batch 99/883, Training Loss: 0.6322\n",
      "Epoch 4/10, Batch 100/883, Training Loss: 1.0521\n",
      "Epoch 4/10, Batch 101/883, Training Loss: 0.8433\n",
      "Epoch 4/10, Batch 102/883, Training Loss: 0.8345\n",
      "Epoch 4/10, Batch 103/883, Training Loss: 1.1539\n",
      "Epoch 4/10, Batch 104/883, Training Loss: 0.6748\n",
      "Epoch 4/10, Batch 105/883, Training Loss: 0.6915\n",
      "Epoch 4/10, Batch 106/883, Training Loss: 0.7712\n",
      "Epoch 4/10, Batch 107/883, Training Loss: 0.6189\n",
      "Epoch 4/10, Batch 108/883, Training Loss: 0.6286\n",
      "Epoch 4/10, Batch 109/883, Training Loss: 0.6454\n",
      "Epoch 4/10, Batch 110/883, Training Loss: 0.9932\n",
      "Epoch 4/10, Batch 111/883, Training Loss: 0.5607\n",
      "Epoch 4/10, Batch 112/883, Training Loss: 0.6378\n",
      "Epoch 4/10, Batch 113/883, Training Loss: 0.7888\n",
      "Epoch 4/10, Batch 114/883, Training Loss: 1.3190\n",
      "Epoch 4/10, Batch 115/883, Training Loss: 0.8906\n",
      "Epoch 4/10, Batch 116/883, Training Loss: 0.6404\n",
      "Epoch 4/10, Batch 117/883, Training Loss: 0.7970\n",
      "Epoch 4/10, Batch 118/883, Training Loss: 0.9720\n",
      "Epoch 4/10, Batch 119/883, Training Loss: 0.8615\n",
      "Epoch 4/10, Batch 120/883, Training Loss: 0.6908\n",
      "Epoch 4/10, Batch 121/883, Training Loss: 0.9605\n",
      "Epoch 4/10, Batch 122/883, Training Loss: 0.7042\n",
      "Epoch 4/10, Batch 123/883, Training Loss: 0.6953\n",
      "Epoch 4/10, Batch 124/883, Training Loss: 0.9756\n",
      "Epoch 4/10, Batch 125/883, Training Loss: 0.9634\n",
      "Epoch 4/10, Batch 126/883, Training Loss: 1.1013\n",
      "Epoch 4/10, Batch 127/883, Training Loss: 0.7039\n",
      "Epoch 4/10, Batch 128/883, Training Loss: 0.7172\n",
      "Epoch 4/10, Batch 129/883, Training Loss: 0.9091\n",
      "Epoch 4/10, Batch 130/883, Training Loss: 0.7126\n",
      "Epoch 4/10, Batch 131/883, Training Loss: 1.0894\n",
      "Epoch 4/10, Batch 132/883, Training Loss: 0.8866\n",
      "Epoch 4/10, Batch 133/883, Training Loss: 0.8169\n",
      "Epoch 4/10, Batch 134/883, Training Loss: 0.5752\n",
      "Epoch 4/10, Batch 135/883, Training Loss: 0.7001\n",
      "Epoch 4/10, Batch 136/883, Training Loss: 0.5695\n",
      "Epoch 4/10, Batch 137/883, Training Loss: 0.6029\n",
      "Epoch 4/10, Batch 138/883, Training Loss: 1.0001\n",
      "Epoch 4/10, Batch 139/883, Training Loss: 0.6600\n",
      "Epoch 4/10, Batch 140/883, Training Loss: 0.8385\n",
      "Epoch 4/10, Batch 141/883, Training Loss: 0.6929\n",
      "Epoch 4/10, Batch 142/883, Training Loss: 0.6434\n",
      "Epoch 4/10, Batch 143/883, Training Loss: 0.6416\n",
      "Epoch 4/10, Batch 144/883, Training Loss: 0.7424\n",
      "Epoch 4/10, Batch 145/883, Training Loss: 0.7077\n",
      "Epoch 4/10, Batch 146/883, Training Loss: 0.6406\n",
      "Epoch 4/10, Batch 147/883, Training Loss: 0.7501\n",
      "Epoch 4/10, Batch 148/883, Training Loss: 0.9393\n",
      "Epoch 4/10, Batch 149/883, Training Loss: 0.9458\n",
      "Epoch 4/10, Batch 150/883, Training Loss: 0.5529\n",
      "Epoch 4/10, Batch 151/883, Training Loss: 0.9878\n",
      "Epoch 4/10, Batch 152/883, Training Loss: 0.5079\n",
      "Epoch 4/10, Batch 153/883, Training Loss: 0.6970\n",
      "Epoch 4/10, Batch 154/883, Training Loss: 0.6857\n",
      "Epoch 4/10, Batch 155/883, Training Loss: 1.3302\n",
      "Epoch 4/10, Batch 156/883, Training Loss: 0.6100\n",
      "Epoch 4/10, Batch 157/883, Training Loss: 0.8018\n",
      "Epoch 4/10, Batch 158/883, Training Loss: 0.7986\n",
      "Epoch 4/10, Batch 159/883, Training Loss: 0.9072\n",
      "Epoch 4/10, Batch 160/883, Training Loss: 0.5875\n",
      "Epoch 4/10, Batch 161/883, Training Loss: 0.7034\n",
      "Epoch 4/10, Batch 162/883, Training Loss: 0.6296\n",
      "Epoch 4/10, Batch 163/883, Training Loss: 0.9228\n",
      "Epoch 4/10, Batch 164/883, Training Loss: 0.9262\n",
      "Epoch 4/10, Batch 165/883, Training Loss: 1.0263\n",
      "Epoch 4/10, Batch 166/883, Training Loss: 0.8323\n",
      "Epoch 4/10, Batch 167/883, Training Loss: 0.6414\n",
      "Epoch 4/10, Batch 168/883, Training Loss: 0.9558\n",
      "Epoch 4/10, Batch 169/883, Training Loss: 0.9570\n",
      "Epoch 4/10, Batch 170/883, Training Loss: 0.9507\n",
      "Epoch 4/10, Batch 171/883, Training Loss: 0.7141\n",
      "Epoch 4/10, Batch 172/883, Training Loss: 0.6945\n",
      "Epoch 4/10, Batch 173/883, Training Loss: 0.7619\n",
      "Epoch 4/10, Batch 174/883, Training Loss: 0.7604\n",
      "Epoch 4/10, Batch 175/883, Training Loss: 0.7201\n",
      "Epoch 4/10, Batch 176/883, Training Loss: 0.7199\n",
      "Epoch 4/10, Batch 177/883, Training Loss: 0.6899\n",
      "Epoch 4/10, Batch 178/883, Training Loss: 1.1316\n",
      "Epoch 4/10, Batch 179/883, Training Loss: 0.7670\n",
      "Epoch 4/10, Batch 180/883, Training Loss: 0.5850\n",
      "Epoch 4/10, Batch 181/883, Training Loss: 1.3745\n",
      "Epoch 4/10, Batch 182/883, Training Loss: 0.9002\n",
      "Epoch 4/10, Batch 183/883, Training Loss: 0.9731\n",
      "Epoch 4/10, Batch 184/883, Training Loss: 0.8290\n",
      "Epoch 4/10, Batch 185/883, Training Loss: 0.9697\n",
      "Epoch 4/10, Batch 186/883, Training Loss: 0.5631\n",
      "Epoch 4/10, Batch 187/883, Training Loss: 0.8222\n",
      "Epoch 4/10, Batch 188/883, Training Loss: 0.7198\n",
      "Epoch 4/10, Batch 189/883, Training Loss: 0.8216\n",
      "Epoch 4/10, Batch 190/883, Training Loss: 0.6058\n",
      "Epoch 4/10, Batch 191/883, Training Loss: 0.7492\n",
      "Epoch 4/10, Batch 192/883, Training Loss: 0.7621\n",
      "Epoch 4/10, Batch 193/883, Training Loss: 0.8986\n",
      "Epoch 4/10, Batch 194/883, Training Loss: 0.8356\n",
      "Epoch 4/10, Batch 195/883, Training Loss: 0.6827\n",
      "Epoch 4/10, Batch 196/883, Training Loss: 0.9417\n",
      "Epoch 4/10, Batch 197/883, Training Loss: 0.8547\n",
      "Epoch 4/10, Batch 198/883, Training Loss: 0.7509\n",
      "Epoch 4/10, Batch 199/883, Training Loss: 0.8487\n",
      "Epoch 4/10, Batch 200/883, Training Loss: 0.7692\n",
      "Epoch 4/10, Batch 201/883, Training Loss: 0.7298\n",
      "Epoch 4/10, Batch 202/883, Training Loss: 0.4707\n",
      "Epoch 4/10, Batch 203/883, Training Loss: 1.2498\n",
      "Epoch 4/10, Batch 204/883, Training Loss: 0.9469\n",
      "Epoch 4/10, Batch 205/883, Training Loss: 0.8372\n",
      "Epoch 4/10, Batch 206/883, Training Loss: 1.1297\n",
      "Epoch 4/10, Batch 207/883, Training Loss: 0.6259\n",
      "Epoch 4/10, Batch 208/883, Training Loss: 0.8379\n",
      "Epoch 4/10, Batch 209/883, Training Loss: 0.8759\n",
      "Epoch 4/10, Batch 210/883, Training Loss: 0.7218\n",
      "Epoch 4/10, Batch 211/883, Training Loss: 0.7376\n",
      "Epoch 4/10, Batch 212/883, Training Loss: 0.7897\n",
      "Epoch 4/10, Batch 213/883, Training Loss: 0.8854\n",
      "Epoch 4/10, Batch 214/883, Training Loss: 0.9990\n",
      "Epoch 4/10, Batch 215/883, Training Loss: 1.0369\n",
      "Epoch 4/10, Batch 216/883, Training Loss: 0.9410\n",
      "Epoch 4/10, Batch 217/883, Training Loss: 0.7591\n",
      "Epoch 4/10, Batch 218/883, Training Loss: 0.7420\n",
      "Epoch 4/10, Batch 219/883, Training Loss: 0.9342\n",
      "Epoch 4/10, Batch 220/883, Training Loss: 0.6981\n",
      "Epoch 4/10, Batch 221/883, Training Loss: 0.6861\n",
      "Epoch 4/10, Batch 222/883, Training Loss: 0.5693\n",
      "Epoch 4/10, Batch 223/883, Training Loss: 0.5552\n",
      "Epoch 4/10, Batch 224/883, Training Loss: 1.1487\n",
      "Epoch 4/10, Batch 225/883, Training Loss: 1.0753\n",
      "Epoch 4/10, Batch 226/883, Training Loss: 0.7298\n",
      "Epoch 4/10, Batch 227/883, Training Loss: 0.8286\n",
      "Epoch 4/10, Batch 228/883, Training Loss: 0.6881\n",
      "Epoch 4/10, Batch 229/883, Training Loss: 0.7458\n",
      "Epoch 4/10, Batch 230/883, Training Loss: 0.7862\n",
      "Epoch 4/10, Batch 231/883, Training Loss: 0.6108\n",
      "Epoch 4/10, Batch 232/883, Training Loss: 1.1497\n",
      "Epoch 4/10, Batch 233/883, Training Loss: 0.9449\n",
      "Epoch 4/10, Batch 234/883, Training Loss: 0.9740\n",
      "Epoch 4/10, Batch 235/883, Training Loss: 0.6512\n",
      "Epoch 4/10, Batch 236/883, Training Loss: 0.6099\n",
      "Epoch 4/10, Batch 237/883, Training Loss: 0.8460\n",
      "Epoch 4/10, Batch 238/883, Training Loss: 0.7097\n",
      "Epoch 4/10, Batch 239/883, Training Loss: 0.6812\n",
      "Epoch 4/10, Batch 240/883, Training Loss: 1.0162\n",
      "Epoch 4/10, Batch 241/883, Training Loss: 0.7354\n",
      "Epoch 4/10, Batch 242/883, Training Loss: 0.6831\n",
      "Epoch 4/10, Batch 243/883, Training Loss: 0.9519\n",
      "Epoch 4/10, Batch 244/883, Training Loss: 0.7669\n",
      "Epoch 4/10, Batch 245/883, Training Loss: 0.7826\n",
      "Epoch 4/10, Batch 246/883, Training Loss: 1.1882\n",
      "Epoch 4/10, Batch 247/883, Training Loss: 0.6686\n",
      "Epoch 4/10, Batch 248/883, Training Loss: 0.7430\n",
      "Epoch 4/10, Batch 249/883, Training Loss: 0.8524\n",
      "Epoch 4/10, Batch 250/883, Training Loss: 0.7704\n",
      "Epoch 4/10, Batch 251/883, Training Loss: 0.6591\n",
      "Epoch 4/10, Batch 252/883, Training Loss: 0.8526\n",
      "Epoch 4/10, Batch 253/883, Training Loss: 0.7400\n",
      "Epoch 4/10, Batch 254/883, Training Loss: 0.6005\n",
      "Epoch 4/10, Batch 255/883, Training Loss: 0.8069\n",
      "Epoch 4/10, Batch 256/883, Training Loss: 0.6407\n",
      "Epoch 4/10, Batch 257/883, Training Loss: 0.7434\n",
      "Epoch 4/10, Batch 258/883, Training Loss: 0.6475\n",
      "Epoch 4/10, Batch 259/883, Training Loss: 1.0281\n",
      "Epoch 4/10, Batch 260/883, Training Loss: 0.7464\n",
      "Epoch 4/10, Batch 261/883, Training Loss: 0.9242\n",
      "Epoch 4/10, Batch 262/883, Training Loss: 0.8215\n",
      "Epoch 4/10, Batch 263/883, Training Loss: 0.4988\n",
      "Epoch 4/10, Batch 264/883, Training Loss: 0.4960\n",
      "Epoch 4/10, Batch 265/883, Training Loss: 0.8526\n",
      "Epoch 4/10, Batch 266/883, Training Loss: 0.5487\n",
      "Epoch 4/10, Batch 267/883, Training Loss: 0.7220\n",
      "Epoch 4/10, Batch 268/883, Training Loss: 0.5575\n",
      "Epoch 4/10, Batch 269/883, Training Loss: 0.6812\n",
      "Epoch 4/10, Batch 270/883, Training Loss: 0.7515\n",
      "Epoch 4/10, Batch 271/883, Training Loss: 0.7339\n",
      "Epoch 4/10, Batch 272/883, Training Loss: 0.7883\n",
      "Epoch 4/10, Batch 273/883, Training Loss: 0.7296\n",
      "Epoch 4/10, Batch 274/883, Training Loss: 0.6446\n",
      "Epoch 4/10, Batch 275/883, Training Loss: 1.0511\n",
      "Epoch 4/10, Batch 276/883, Training Loss: 0.5830\n",
      "Epoch 4/10, Batch 277/883, Training Loss: 1.1111\n",
      "Epoch 4/10, Batch 278/883, Training Loss: 0.6181\n",
      "Epoch 4/10, Batch 279/883, Training Loss: 0.8434\n",
      "Epoch 4/10, Batch 280/883, Training Loss: 0.7996\n",
      "Epoch 4/10, Batch 281/883, Training Loss: 0.7490\n",
      "Epoch 4/10, Batch 282/883, Training Loss: 0.5696\n",
      "Epoch 4/10, Batch 283/883, Training Loss: 0.6813\n",
      "Epoch 4/10, Batch 284/883, Training Loss: 1.0015\n",
      "Epoch 4/10, Batch 285/883, Training Loss: 0.7916\n",
      "Epoch 4/10, Batch 286/883, Training Loss: 0.5865\n",
      "Epoch 4/10, Batch 287/883, Training Loss: 0.7245\n",
      "Epoch 4/10, Batch 288/883, Training Loss: 0.6887\n",
      "Epoch 4/10, Batch 289/883, Training Loss: 0.7330\n",
      "Epoch 4/10, Batch 290/883, Training Loss: 0.6610\n",
      "Epoch 4/10, Batch 291/883, Training Loss: 0.8516\n",
      "Epoch 4/10, Batch 292/883, Training Loss: 0.6843\n",
      "Epoch 4/10, Batch 293/883, Training Loss: 0.8565\n",
      "Epoch 4/10, Batch 294/883, Training Loss: 0.6201\n",
      "Epoch 4/10, Batch 295/883, Training Loss: 0.8912\n",
      "Epoch 4/10, Batch 296/883, Training Loss: 0.7113\n",
      "Epoch 4/10, Batch 297/883, Training Loss: 0.9516\n",
      "Epoch 4/10, Batch 298/883, Training Loss: 0.9515\n",
      "Epoch 4/10, Batch 299/883, Training Loss: 1.0945\n",
      "Epoch 4/10, Batch 300/883, Training Loss: 0.7045\n",
      "Epoch 4/10, Batch 301/883, Training Loss: 0.6227\n",
      "Epoch 4/10, Batch 302/883, Training Loss: 0.9071\n",
      "Epoch 4/10, Batch 303/883, Training Loss: 0.8925\n",
      "Epoch 4/10, Batch 304/883, Training Loss: 0.7065\n",
      "Epoch 4/10, Batch 305/883, Training Loss: 0.7576\n",
      "Epoch 4/10, Batch 306/883, Training Loss: 0.6704\n",
      "Epoch 4/10, Batch 307/883, Training Loss: 0.6166\n",
      "Epoch 4/10, Batch 308/883, Training Loss: 0.7088\n",
      "Epoch 4/10, Batch 309/883, Training Loss: 0.6212\n",
      "Epoch 4/10, Batch 310/883, Training Loss: 1.2149\n",
      "Epoch 4/10, Batch 311/883, Training Loss: 0.7906\n",
      "Epoch 4/10, Batch 312/883, Training Loss: 0.7460\n",
      "Epoch 4/10, Batch 313/883, Training Loss: 0.7295\n",
      "Epoch 4/10, Batch 314/883, Training Loss: 0.7834\n",
      "Epoch 4/10, Batch 315/883, Training Loss: 0.6844\n",
      "Epoch 4/10, Batch 316/883, Training Loss: 0.7795\n",
      "Epoch 4/10, Batch 317/883, Training Loss: 0.6296\n",
      "Epoch 4/10, Batch 318/883, Training Loss: 0.7448\n",
      "Epoch 4/10, Batch 319/883, Training Loss: 0.7907\n",
      "Epoch 4/10, Batch 320/883, Training Loss: 1.4736\n",
      "Epoch 4/10, Batch 321/883, Training Loss: 0.6824\n",
      "Epoch 4/10, Batch 322/883, Training Loss: 0.6883\n",
      "Epoch 4/10, Batch 323/883, Training Loss: 1.0201\n",
      "Epoch 4/10, Batch 324/883, Training Loss: 0.7633\n",
      "Epoch 4/10, Batch 325/883, Training Loss: 0.8676\n",
      "Epoch 4/10, Batch 326/883, Training Loss: 0.9527\n",
      "Epoch 4/10, Batch 327/883, Training Loss: 0.8044\n",
      "Epoch 4/10, Batch 328/883, Training Loss: 0.9811\n",
      "Epoch 4/10, Batch 329/883, Training Loss: 0.5902\n",
      "Epoch 4/10, Batch 330/883, Training Loss: 0.7167\n",
      "Epoch 4/10, Batch 331/883, Training Loss: 0.7887\n",
      "Epoch 4/10, Batch 332/883, Training Loss: 0.7013\n",
      "Epoch 4/10, Batch 333/883, Training Loss: 0.6284\n",
      "Epoch 4/10, Batch 334/883, Training Loss: 0.6611\n",
      "Epoch 4/10, Batch 335/883, Training Loss: 0.6361\n",
      "Epoch 4/10, Batch 336/883, Training Loss: 0.9545\n",
      "Epoch 4/10, Batch 337/883, Training Loss: 0.7733\n",
      "Epoch 4/10, Batch 338/883, Training Loss: 0.5667\n",
      "Epoch 4/10, Batch 339/883, Training Loss: 0.8930\n",
      "Epoch 4/10, Batch 340/883, Training Loss: 0.5643\n",
      "Epoch 4/10, Batch 341/883, Training Loss: 0.5690\n",
      "Epoch 4/10, Batch 342/883, Training Loss: 0.7835\n",
      "Epoch 4/10, Batch 343/883, Training Loss: 0.7069\n",
      "Epoch 4/10, Batch 344/883, Training Loss: 0.6364\n",
      "Epoch 4/10, Batch 345/883, Training Loss: 0.7084\n",
      "Epoch 4/10, Batch 346/883, Training Loss: 1.0905\n",
      "Epoch 4/10, Batch 347/883, Training Loss: 0.5676\n",
      "Epoch 4/10, Batch 348/883, Training Loss: 0.9445\n",
      "Epoch 4/10, Batch 349/883, Training Loss: 0.5034\n",
      "Epoch 4/10, Batch 350/883, Training Loss: 0.9527\n",
      "Epoch 4/10, Batch 351/883, Training Loss: 1.0615\n",
      "Epoch 4/10, Batch 352/883, Training Loss: 0.9023\n",
      "Epoch 4/10, Batch 353/883, Training Loss: 0.6380\n",
      "Epoch 4/10, Batch 354/883, Training Loss: 0.7624\n",
      "Epoch 4/10, Batch 355/883, Training Loss: 0.8405\n",
      "Epoch 4/10, Batch 356/883, Training Loss: 0.7449\n",
      "Epoch 4/10, Batch 357/883, Training Loss: 1.1182\n",
      "Epoch 4/10, Batch 358/883, Training Loss: 0.9520\n",
      "Epoch 4/10, Batch 359/883, Training Loss: 0.8618\n",
      "Epoch 4/10, Batch 360/883, Training Loss: 0.7684\n",
      "Epoch 4/10, Batch 361/883, Training Loss: 0.7894\n",
      "Epoch 4/10, Batch 362/883, Training Loss: 0.8493\n",
      "Epoch 4/10, Batch 363/883, Training Loss: 0.6105\n",
      "Epoch 4/10, Batch 364/883, Training Loss: 0.8385\n",
      "Epoch 4/10, Batch 365/883, Training Loss: 0.8018\n",
      "Epoch 4/10, Batch 366/883, Training Loss: 0.5247\n",
      "Epoch 4/10, Batch 367/883, Training Loss: 0.6309\n",
      "Epoch 4/10, Batch 368/883, Training Loss: 0.6296\n",
      "Epoch 4/10, Batch 369/883, Training Loss: 0.7257\n",
      "Epoch 4/10, Batch 370/883, Training Loss: 0.5287\n",
      "Epoch 4/10, Batch 371/883, Training Loss: 1.0733\n",
      "Epoch 4/10, Batch 372/883, Training Loss: 0.6182\n",
      "Epoch 4/10, Batch 373/883, Training Loss: 0.8876\n",
      "Epoch 4/10, Batch 374/883, Training Loss: 0.9739\n",
      "Epoch 4/10, Batch 375/883, Training Loss: 0.7270\n",
      "Epoch 4/10, Batch 376/883, Training Loss: 0.8275\n",
      "Epoch 4/10, Batch 377/883, Training Loss: 0.5872\n",
      "Epoch 4/10, Batch 378/883, Training Loss: 1.2000\n",
      "Epoch 4/10, Batch 379/883, Training Loss: 0.6454\n",
      "Epoch 4/10, Batch 380/883, Training Loss: 0.7888\n",
      "Epoch 4/10, Batch 381/883, Training Loss: 0.6808\n",
      "Epoch 4/10, Batch 382/883, Training Loss: 1.0282\n",
      "Epoch 4/10, Batch 383/883, Training Loss: 0.6116\n",
      "Epoch 4/10, Batch 384/883, Training Loss: 0.9215\n",
      "Epoch 4/10, Batch 385/883, Training Loss: 0.8250\n",
      "Epoch 4/10, Batch 386/883, Training Loss: 0.7832\n",
      "Epoch 4/10, Batch 387/883, Training Loss: 0.6940\n",
      "Epoch 4/10, Batch 388/883, Training Loss: 0.6313\n",
      "Epoch 4/10, Batch 389/883, Training Loss: 0.5705\n",
      "Epoch 4/10, Batch 390/883, Training Loss: 0.7967\n",
      "Epoch 4/10, Batch 391/883, Training Loss: 0.7932\n",
      "Epoch 4/10, Batch 392/883, Training Loss: 1.1985\n",
      "Epoch 4/10, Batch 393/883, Training Loss: 0.5820\n",
      "Epoch 4/10, Batch 394/883, Training Loss: 0.5739\n",
      "Epoch 4/10, Batch 395/883, Training Loss: 0.6215\n",
      "Epoch 4/10, Batch 396/883, Training Loss: 0.9938\n",
      "Epoch 4/10, Batch 397/883, Training Loss: 0.9290\n",
      "Epoch 4/10, Batch 398/883, Training Loss: 0.6335\n",
      "Epoch 4/10, Batch 399/883, Training Loss: 1.0665\n",
      "Epoch 4/10, Batch 400/883, Training Loss: 0.6826\n",
      "Epoch 4/10, Batch 401/883, Training Loss: 0.6501\n",
      "Epoch 4/10, Batch 402/883, Training Loss: 0.5301\n",
      "Epoch 4/10, Batch 403/883, Training Loss: 0.8235\n",
      "Epoch 4/10, Batch 404/883, Training Loss: 0.7158\n",
      "Epoch 4/10, Batch 405/883, Training Loss: 0.6371\n",
      "Epoch 4/10, Batch 406/883, Training Loss: 0.5197\n",
      "Epoch 4/10, Batch 407/883, Training Loss: 0.8489\n",
      "Epoch 4/10, Batch 408/883, Training Loss: 0.7941\n",
      "Epoch 4/10, Batch 409/883, Training Loss: 0.7065\n",
      "Epoch 4/10, Batch 410/883, Training Loss: 0.7763\n",
      "Epoch 4/10, Batch 411/883, Training Loss: 0.7323\n",
      "Epoch 4/10, Batch 412/883, Training Loss: 0.8270\n",
      "Epoch 4/10, Batch 413/883, Training Loss: 0.7171\n",
      "Epoch 4/10, Batch 414/883, Training Loss: 1.0927\n",
      "Epoch 4/10, Batch 415/883, Training Loss: 1.0219\n",
      "Epoch 4/10, Batch 416/883, Training Loss: 0.6640\n",
      "Epoch 4/10, Batch 417/883, Training Loss: 0.8486\n",
      "Epoch 4/10, Batch 418/883, Training Loss: 0.6544\n",
      "Epoch 4/10, Batch 419/883, Training Loss: 0.7966\n",
      "Epoch 4/10, Batch 420/883, Training Loss: 0.6834\n",
      "Epoch 4/10, Batch 421/883, Training Loss: 1.2178\n",
      "Epoch 4/10, Batch 422/883, Training Loss: 0.7798\n",
      "Epoch 4/10, Batch 423/883, Training Loss: 0.7319\n",
      "Epoch 4/10, Batch 424/883, Training Loss: 0.6668\n",
      "Epoch 4/10, Batch 425/883, Training Loss: 0.9393\n",
      "Epoch 4/10, Batch 426/883, Training Loss: 0.8745\n",
      "Epoch 4/10, Batch 427/883, Training Loss: 0.6732\n",
      "Epoch 4/10, Batch 428/883, Training Loss: 0.6125\n",
      "Epoch 4/10, Batch 429/883, Training Loss: 0.7484\n",
      "Epoch 4/10, Batch 430/883, Training Loss: 1.0531\n",
      "Epoch 4/10, Batch 431/883, Training Loss: 0.8111\n",
      "Epoch 4/10, Batch 432/883, Training Loss: 0.5361\n",
      "Epoch 4/10, Batch 433/883, Training Loss: 0.9803\n",
      "Epoch 4/10, Batch 434/883, Training Loss: 0.6890\n",
      "Epoch 4/10, Batch 435/883, Training Loss: 0.6493\n",
      "Epoch 4/10, Batch 436/883, Training Loss: 0.8474\n",
      "Epoch 4/10, Batch 437/883, Training Loss: 0.6891\n",
      "Epoch 4/10, Batch 438/883, Training Loss: 0.9640\n",
      "Epoch 4/10, Batch 439/883, Training Loss: 0.7979\n",
      "Epoch 4/10, Batch 440/883, Training Loss: 0.7323\n",
      "Epoch 4/10, Batch 441/883, Training Loss: 0.5693\n",
      "Epoch 4/10, Batch 442/883, Training Loss: 0.6104\n",
      "Epoch 4/10, Batch 443/883, Training Loss: 1.0513\n",
      "Epoch 4/10, Batch 444/883, Training Loss: 0.6896\n",
      "Epoch 4/10, Batch 445/883, Training Loss: 0.8756\n",
      "Epoch 4/10, Batch 446/883, Training Loss: 1.2775\n",
      "Epoch 4/10, Batch 447/883, Training Loss: 0.9574\n",
      "Epoch 4/10, Batch 448/883, Training Loss: 0.5883\n",
      "Epoch 4/10, Batch 449/883, Training Loss: 0.9914\n",
      "Epoch 4/10, Batch 450/883, Training Loss: 0.7197\n",
      "Epoch 4/10, Batch 451/883, Training Loss: 0.8232\n",
      "Epoch 4/10, Batch 452/883, Training Loss: 0.7044\n",
      "Epoch 4/10, Batch 453/883, Training Loss: 0.8064\n",
      "Epoch 4/10, Batch 454/883, Training Loss: 0.7527\n",
      "Epoch 4/10, Batch 455/883, Training Loss: 0.5776\n",
      "Epoch 4/10, Batch 456/883, Training Loss: 0.7430\n",
      "Epoch 4/10, Batch 457/883, Training Loss: 0.8754\n",
      "Epoch 4/10, Batch 458/883, Training Loss: 0.6729\n",
      "Epoch 4/10, Batch 459/883, Training Loss: 0.8194\n",
      "Epoch 4/10, Batch 460/883, Training Loss: 0.7209\n",
      "Epoch 4/10, Batch 461/883, Training Loss: 0.7073\n",
      "Epoch 4/10, Batch 462/883, Training Loss: 0.8758\n",
      "Epoch 4/10, Batch 463/883, Training Loss: 0.7658\n",
      "Epoch 4/10, Batch 464/883, Training Loss: 0.9311\n",
      "Epoch 4/10, Batch 465/883, Training Loss: 0.7393\n",
      "Epoch 4/10, Batch 466/883, Training Loss: 0.9685\n",
      "Epoch 4/10, Batch 467/883, Training Loss: 0.8703\n",
      "Epoch 4/10, Batch 468/883, Training Loss: 0.8519\n",
      "Epoch 4/10, Batch 469/883, Training Loss: 0.5621\n",
      "Epoch 4/10, Batch 470/883, Training Loss: 1.1586\n",
      "Epoch 4/10, Batch 471/883, Training Loss: 0.7709\n",
      "Epoch 4/10, Batch 472/883, Training Loss: 0.5425\n",
      "Epoch 4/10, Batch 473/883, Training Loss: 0.8998\n",
      "Epoch 4/10, Batch 474/883, Training Loss: 0.8309\n",
      "Epoch 4/10, Batch 475/883, Training Loss: 0.9097\n",
      "Epoch 4/10, Batch 476/883, Training Loss: 0.7960\n",
      "Epoch 4/10, Batch 477/883, Training Loss: 0.8112\n",
      "Epoch 4/10, Batch 478/883, Training Loss: 0.8496\n",
      "Epoch 4/10, Batch 479/883, Training Loss: 0.8660\n",
      "Epoch 4/10, Batch 480/883, Training Loss: 0.6911\n",
      "Epoch 4/10, Batch 481/883, Training Loss: 0.9232\n",
      "Epoch 4/10, Batch 482/883, Training Loss: 0.7720\n",
      "Epoch 4/10, Batch 483/883, Training Loss: 1.1599\n",
      "Epoch 4/10, Batch 484/883, Training Loss: 0.8440\n",
      "Epoch 4/10, Batch 485/883, Training Loss: 0.6632\n",
      "Epoch 4/10, Batch 486/883, Training Loss: 0.7111\n",
      "Epoch 4/10, Batch 487/883, Training Loss: 0.7710\n",
      "Epoch 4/10, Batch 488/883, Training Loss: 0.8305\n",
      "Epoch 4/10, Batch 489/883, Training Loss: 0.8371\n",
      "Epoch 4/10, Batch 490/883, Training Loss: 0.6093\n",
      "Epoch 4/10, Batch 491/883, Training Loss: 1.0856\n",
      "Epoch 4/10, Batch 492/883, Training Loss: 0.8063\n",
      "Epoch 4/10, Batch 493/883, Training Loss: 1.0367\n",
      "Epoch 4/10, Batch 494/883, Training Loss: 0.7574\n",
      "Epoch 4/10, Batch 495/883, Training Loss: 0.7100\n",
      "Epoch 4/10, Batch 496/883, Training Loss: 0.9248\n",
      "Epoch 4/10, Batch 497/883, Training Loss: 0.6538\n",
      "Epoch 4/10, Batch 498/883, Training Loss: 0.9390\n",
      "Epoch 4/10, Batch 499/883, Training Loss: 0.7671\n",
      "Epoch 4/10, Batch 500/883, Training Loss: 0.6666\n",
      "Epoch 4/10, Batch 501/883, Training Loss: 0.5625\n",
      "Epoch 4/10, Batch 502/883, Training Loss: 0.7515\n",
      "Epoch 4/10, Batch 503/883, Training Loss: 0.9358\n",
      "Epoch 4/10, Batch 504/883, Training Loss: 0.7774\n",
      "Epoch 4/10, Batch 505/883, Training Loss: 0.6102\n",
      "Epoch 4/10, Batch 506/883, Training Loss: 0.8785\n",
      "Epoch 4/10, Batch 507/883, Training Loss: 0.7995\n",
      "Epoch 4/10, Batch 508/883, Training Loss: 0.6138\n",
      "Epoch 4/10, Batch 509/883, Training Loss: 0.8395\n",
      "Epoch 4/10, Batch 510/883, Training Loss: 0.8618\n",
      "Epoch 4/10, Batch 511/883, Training Loss: 0.9293\n",
      "Epoch 4/10, Batch 512/883, Training Loss: 0.6330\n",
      "Epoch 4/10, Batch 513/883, Training Loss: 0.7930\n",
      "Epoch 4/10, Batch 514/883, Training Loss: 0.6335\n",
      "Epoch 4/10, Batch 515/883, Training Loss: 0.7908\n",
      "Epoch 4/10, Batch 516/883, Training Loss: 0.7281\n",
      "Epoch 4/10, Batch 517/883, Training Loss: 0.6334\n",
      "Epoch 4/10, Batch 518/883, Training Loss: 0.6629\n",
      "Epoch 4/10, Batch 519/883, Training Loss: 0.7964\n",
      "Epoch 4/10, Batch 520/883, Training Loss: 0.7292\n",
      "Epoch 4/10, Batch 521/883, Training Loss: 1.0415\n",
      "Epoch 4/10, Batch 522/883, Training Loss: 0.7436\n",
      "Epoch 4/10, Batch 523/883, Training Loss: 0.7705\n",
      "Epoch 4/10, Batch 524/883, Training Loss: 0.8184\n",
      "Epoch 4/10, Batch 525/883, Training Loss: 0.8189\n",
      "Epoch 4/10, Batch 526/883, Training Loss: 0.7020\n",
      "Epoch 4/10, Batch 527/883, Training Loss: 0.7014\n",
      "Epoch 4/10, Batch 528/883, Training Loss: 0.6867\n",
      "Epoch 4/10, Batch 529/883, Training Loss: 0.8206\n",
      "Epoch 4/10, Batch 530/883, Training Loss: 0.9652\n",
      "Epoch 4/10, Batch 531/883, Training Loss: 0.7090\n",
      "Epoch 4/10, Batch 532/883, Training Loss: 0.7603\n",
      "Epoch 4/10, Batch 533/883, Training Loss: 0.7928\n",
      "Epoch 4/10, Batch 534/883, Training Loss: 0.7288\n",
      "Epoch 4/10, Batch 535/883, Training Loss: 0.6957\n",
      "Epoch 4/10, Batch 536/883, Training Loss: 0.6291\n",
      "Epoch 4/10, Batch 537/883, Training Loss: 0.9022\n",
      "Epoch 4/10, Batch 538/883, Training Loss: 0.7291\n",
      "Epoch 4/10, Batch 539/883, Training Loss: 0.6987\n",
      "Epoch 4/10, Batch 540/883, Training Loss: 0.8631\n",
      "Epoch 4/10, Batch 541/883, Training Loss: 0.8909\n",
      "Epoch 4/10, Batch 542/883, Training Loss: 0.6022\n",
      "Epoch 4/10, Batch 543/883, Training Loss: 0.7035\n",
      "Epoch 4/10, Batch 544/883, Training Loss: 0.6761\n",
      "Epoch 4/10, Batch 545/883, Training Loss: 0.8252\n",
      "Epoch 4/10, Batch 546/883, Training Loss: 0.9199\n",
      "Epoch 4/10, Batch 547/883, Training Loss: 0.8309\n",
      "Epoch 4/10, Batch 548/883, Training Loss: 0.6879\n",
      "Epoch 4/10, Batch 549/883, Training Loss: 0.6062\n",
      "Epoch 4/10, Batch 550/883, Training Loss: 0.5379\n",
      "Epoch 4/10, Batch 551/883, Training Loss: 0.9462\n",
      "Epoch 4/10, Batch 552/883, Training Loss: 0.8604\n",
      "Epoch 4/10, Batch 553/883, Training Loss: 0.6961\n",
      "Epoch 4/10, Batch 554/883, Training Loss: 0.6594\n",
      "Epoch 4/10, Batch 555/883, Training Loss: 0.6278\n",
      "Epoch 4/10, Batch 556/883, Training Loss: 0.7706\n",
      "Epoch 4/10, Batch 557/883, Training Loss: 0.7018\n",
      "Epoch 4/10, Batch 558/883, Training Loss: 0.5213\n",
      "Epoch 4/10, Batch 559/883, Training Loss: 0.5694\n",
      "Epoch 4/10, Batch 560/883, Training Loss: 0.6439\n",
      "Epoch 4/10, Batch 561/883, Training Loss: 0.7158\n",
      "Epoch 4/10, Batch 562/883, Training Loss: 0.5447\n",
      "Epoch 4/10, Batch 563/883, Training Loss: 0.5283\n",
      "Epoch 4/10, Batch 564/883, Training Loss: 0.8120\n",
      "Epoch 4/10, Batch 565/883, Training Loss: 0.8644\n",
      "Epoch 4/10, Batch 566/883, Training Loss: 0.5340\n",
      "Epoch 4/10, Batch 567/883, Training Loss: 0.6021\n",
      "Epoch 4/10, Batch 568/883, Training Loss: 0.7982\n",
      "Epoch 4/10, Batch 569/883, Training Loss: 0.8876\n",
      "Epoch 4/10, Batch 570/883, Training Loss: 0.9086\n",
      "Epoch 4/10, Batch 571/883, Training Loss: 0.8387\n",
      "Epoch 4/10, Batch 572/883, Training Loss: 0.6927\n",
      "Epoch 4/10, Batch 573/883, Training Loss: 0.8251\n",
      "Epoch 4/10, Batch 574/883, Training Loss: 0.7050\n",
      "Epoch 4/10, Batch 575/883, Training Loss: 0.6262\n",
      "Epoch 4/10, Batch 576/883, Training Loss: 0.8558\n",
      "Epoch 4/10, Batch 577/883, Training Loss: 0.6726\n",
      "Epoch 4/10, Batch 578/883, Training Loss: 0.6282\n",
      "Epoch 4/10, Batch 579/883, Training Loss: 0.8309\n",
      "Epoch 4/10, Batch 580/883, Training Loss: 0.7521\n",
      "Epoch 4/10, Batch 581/883, Training Loss: 0.7198\n",
      "Epoch 4/10, Batch 582/883, Training Loss: 0.6351\n",
      "Epoch 4/10, Batch 583/883, Training Loss: 0.7002\n",
      "Epoch 4/10, Batch 584/883, Training Loss: 0.7602\n",
      "Epoch 4/10, Batch 585/883, Training Loss: 0.7112\n",
      "Epoch 4/10, Batch 586/883, Training Loss: 1.0761\n",
      "Epoch 4/10, Batch 587/883, Training Loss: 0.7990\n",
      "Epoch 4/10, Batch 588/883, Training Loss: 0.5220\n",
      "Epoch 4/10, Batch 589/883, Training Loss: 0.7812\n",
      "Epoch 4/10, Batch 590/883, Training Loss: 1.1321\n",
      "Epoch 4/10, Batch 591/883, Training Loss: 0.5208\n",
      "Epoch 4/10, Batch 592/883, Training Loss: 0.8392\n",
      "Epoch 4/10, Batch 593/883, Training Loss: 0.8197\n",
      "Epoch 4/10, Batch 594/883, Training Loss: 0.9076\n",
      "Epoch 4/10, Batch 595/883, Training Loss: 0.7172\n",
      "Epoch 4/10, Batch 596/883, Training Loss: 0.8100\n",
      "Epoch 4/10, Batch 597/883, Training Loss: 0.6050\n",
      "Epoch 4/10, Batch 598/883, Training Loss: 0.8324\n",
      "Epoch 4/10, Batch 599/883, Training Loss: 1.0092\n",
      "Epoch 4/10, Batch 600/883, Training Loss: 0.6108\n",
      "Epoch 4/10, Batch 601/883, Training Loss: 0.6813\n",
      "Epoch 4/10, Batch 602/883, Training Loss: 0.7605\n",
      "Epoch 4/10, Batch 603/883, Training Loss: 0.7616\n",
      "Epoch 4/10, Batch 604/883, Training Loss: 0.7856\n",
      "Epoch 4/10, Batch 605/883, Training Loss: 0.6502\n",
      "Epoch 4/10, Batch 606/883, Training Loss: 0.7190\n",
      "Epoch 4/10, Batch 607/883, Training Loss: 0.5645\n",
      "Epoch 4/10, Batch 608/883, Training Loss: 0.9648\n",
      "Epoch 4/10, Batch 609/883, Training Loss: 0.8129\n",
      "Epoch 4/10, Batch 610/883, Training Loss: 0.8348\n",
      "Epoch 4/10, Batch 611/883, Training Loss: 0.6943\n",
      "Epoch 4/10, Batch 612/883, Training Loss: 0.7339\n",
      "Epoch 4/10, Batch 613/883, Training Loss: 0.9859\n",
      "Epoch 4/10, Batch 614/883, Training Loss: 1.0513\n",
      "Epoch 4/10, Batch 615/883, Training Loss: 0.6566\n",
      "Epoch 4/10, Batch 616/883, Training Loss: 0.8272\n",
      "Epoch 4/10, Batch 617/883, Training Loss: 0.7596\n",
      "Epoch 4/10, Batch 618/883, Training Loss: 0.5331\n",
      "Epoch 4/10, Batch 619/883, Training Loss: 0.7768\n",
      "Epoch 4/10, Batch 620/883, Training Loss: 0.7166\n",
      "Epoch 4/10, Batch 621/883, Training Loss: 0.8120\n",
      "Epoch 4/10, Batch 622/883, Training Loss: 0.6911\n",
      "Epoch 4/10, Batch 623/883, Training Loss: 0.6504\n",
      "Epoch 4/10, Batch 624/883, Training Loss: 1.0980\n",
      "Epoch 4/10, Batch 625/883, Training Loss: 0.7224\n",
      "Epoch 4/10, Batch 626/883, Training Loss: 0.6788\n",
      "Epoch 4/10, Batch 627/883, Training Loss: 0.6835\n",
      "Epoch 4/10, Batch 628/883, Training Loss: 0.5828\n",
      "Epoch 4/10, Batch 629/883, Training Loss: 0.5433\n",
      "Epoch 4/10, Batch 630/883, Training Loss: 0.5705\n",
      "Epoch 4/10, Batch 631/883, Training Loss: 0.5069\n",
      "Epoch 4/10, Batch 632/883, Training Loss: 0.8696\n",
      "Epoch 4/10, Batch 633/883, Training Loss: 0.8163\n",
      "Epoch 4/10, Batch 634/883, Training Loss: 0.7712\n",
      "Epoch 4/10, Batch 635/883, Training Loss: 0.8627\n",
      "Epoch 4/10, Batch 636/883, Training Loss: 0.5045\n",
      "Epoch 4/10, Batch 637/883, Training Loss: 1.1626\n",
      "Epoch 4/10, Batch 638/883, Training Loss: 0.7845\n",
      "Epoch 4/10, Batch 639/883, Training Loss: 0.6773\n",
      "Epoch 4/10, Batch 640/883, Training Loss: 0.8807\n",
      "Epoch 4/10, Batch 641/883, Training Loss: 0.8899\n",
      "Epoch 4/10, Batch 642/883, Training Loss: 0.9422\n",
      "Epoch 4/10, Batch 643/883, Training Loss: 0.5689\n",
      "Epoch 4/10, Batch 644/883, Training Loss: 0.8656\n",
      "Epoch 4/10, Batch 645/883, Training Loss: 0.8780\n",
      "Epoch 4/10, Batch 646/883, Training Loss: 0.7389\n",
      "Epoch 4/10, Batch 647/883, Training Loss: 0.8272\n",
      "Epoch 4/10, Batch 648/883, Training Loss: 0.8549\n",
      "Epoch 4/10, Batch 649/883, Training Loss: 0.8622\n",
      "Epoch 4/10, Batch 650/883, Training Loss: 0.7214\n",
      "Epoch 4/10, Batch 651/883, Training Loss: 0.9308\n",
      "Epoch 4/10, Batch 652/883, Training Loss: 0.8209\n",
      "Epoch 4/10, Batch 653/883, Training Loss: 0.8747\n",
      "Epoch 4/10, Batch 654/883, Training Loss: 1.0575\n",
      "Epoch 4/10, Batch 655/883, Training Loss: 0.6500\n",
      "Epoch 4/10, Batch 656/883, Training Loss: 0.8061\n",
      "Epoch 4/10, Batch 657/883, Training Loss: 0.6592\n",
      "Epoch 4/10, Batch 658/883, Training Loss: 0.6182\n",
      "Epoch 4/10, Batch 659/883, Training Loss: 0.8555\n",
      "Epoch 4/10, Batch 660/883, Training Loss: 0.6570\n",
      "Epoch 4/10, Batch 661/883, Training Loss: 0.6754\n",
      "Epoch 4/10, Batch 662/883, Training Loss: 0.7235\n",
      "Epoch 4/10, Batch 663/883, Training Loss: 0.8065\n",
      "Epoch 4/10, Batch 664/883, Training Loss: 1.1466\n",
      "Epoch 4/10, Batch 665/883, Training Loss: 0.4646\n",
      "Epoch 4/10, Batch 666/883, Training Loss: 0.8337\n",
      "Epoch 4/10, Batch 667/883, Training Loss: 0.8776\n",
      "Epoch 4/10, Batch 668/883, Training Loss: 0.5553\n",
      "Epoch 4/10, Batch 669/883, Training Loss: 0.8044\n",
      "Epoch 4/10, Batch 670/883, Training Loss: 0.8467\n",
      "Epoch 4/10, Batch 671/883, Training Loss: 0.8688\n",
      "Epoch 4/10, Batch 672/883, Training Loss: 0.6863\n",
      "Epoch 4/10, Batch 673/883, Training Loss: 0.5357\n",
      "Epoch 4/10, Batch 674/883, Training Loss: 0.7794\n",
      "Epoch 4/10, Batch 675/883, Training Loss: 0.5504\n",
      "Epoch 4/10, Batch 676/883, Training Loss: 0.8992\n",
      "Epoch 4/10, Batch 677/883, Training Loss: 0.6125\n",
      "Epoch 4/10, Batch 678/883, Training Loss: 0.7905\n",
      "Epoch 4/10, Batch 679/883, Training Loss: 0.5785\n",
      "Epoch 4/10, Batch 680/883, Training Loss: 0.7896\n",
      "Epoch 4/10, Batch 681/883, Training Loss: 0.6721\n",
      "Epoch 4/10, Batch 682/883, Training Loss: 0.5676\n",
      "Epoch 4/10, Batch 683/883, Training Loss: 0.7156\n",
      "Epoch 4/10, Batch 684/883, Training Loss: 0.5846\n",
      "Epoch 4/10, Batch 685/883, Training Loss: 0.8580\n",
      "Epoch 4/10, Batch 686/883, Training Loss: 1.1482\n",
      "Epoch 4/10, Batch 687/883, Training Loss: 0.5146\n",
      "Epoch 4/10, Batch 688/883, Training Loss: 0.9536\n",
      "Epoch 4/10, Batch 689/883, Training Loss: 0.4575\n",
      "Epoch 4/10, Batch 690/883, Training Loss: 0.7753\n",
      "Epoch 4/10, Batch 691/883, Training Loss: 0.7074\n",
      "Epoch 4/10, Batch 692/883, Training Loss: 0.8607\n",
      "Epoch 4/10, Batch 693/883, Training Loss: 0.9258\n",
      "Epoch 4/10, Batch 694/883, Training Loss: 0.8275\n",
      "Epoch 4/10, Batch 695/883, Training Loss: 0.6385\n",
      "Epoch 4/10, Batch 696/883, Training Loss: 0.7124\n",
      "Epoch 4/10, Batch 697/883, Training Loss: 0.5764\n",
      "Epoch 4/10, Batch 698/883, Training Loss: 0.6279\n",
      "Epoch 4/10, Batch 699/883, Training Loss: 0.9249\n",
      "Epoch 4/10, Batch 700/883, Training Loss: 1.0268\n",
      "Epoch 4/10, Batch 701/883, Training Loss: 0.6867\n",
      "Epoch 4/10, Batch 702/883, Training Loss: 0.6467\n",
      "Epoch 4/10, Batch 703/883, Training Loss: 0.8012\n",
      "Epoch 4/10, Batch 704/883, Training Loss: 0.6410\n",
      "Epoch 4/10, Batch 705/883, Training Loss: 0.7306\n",
      "Epoch 4/10, Batch 706/883, Training Loss: 0.5994\n",
      "Epoch 4/10, Batch 707/883, Training Loss: 0.7564\n",
      "Epoch 4/10, Batch 708/883, Training Loss: 0.8455\n",
      "Epoch 4/10, Batch 709/883, Training Loss: 0.8895\n",
      "Epoch 4/10, Batch 710/883, Training Loss: 1.1027\n",
      "Epoch 4/10, Batch 711/883, Training Loss: 0.6177\n",
      "Epoch 4/10, Batch 712/883, Training Loss: 0.8988\n",
      "Epoch 4/10, Batch 713/883, Training Loss: 0.9713\n",
      "Epoch 4/10, Batch 714/883, Training Loss: 1.0094\n",
      "Epoch 4/10, Batch 715/883, Training Loss: 0.4280\n",
      "Epoch 4/10, Batch 716/883, Training Loss: 0.9140\n",
      "Epoch 4/10, Batch 717/883, Training Loss: 0.7487\n",
      "Epoch 4/10, Batch 718/883, Training Loss: 0.7973\n",
      "Epoch 4/10, Batch 719/883, Training Loss: 0.7374\n",
      "Epoch 4/10, Batch 720/883, Training Loss: 0.7752\n",
      "Epoch 4/10, Batch 721/883, Training Loss: 0.7912\n",
      "Epoch 4/10, Batch 722/883, Training Loss: 1.0323\n",
      "Epoch 4/10, Batch 723/883, Training Loss: 0.7687\n",
      "Epoch 4/10, Batch 724/883, Training Loss: 0.6591\n",
      "Epoch 4/10, Batch 725/883, Training Loss: 0.8191\n",
      "Epoch 4/10, Batch 726/883, Training Loss: 0.5931\n",
      "Epoch 4/10, Batch 727/883, Training Loss: 0.8600\n",
      "Epoch 4/10, Batch 728/883, Training Loss: 0.6008\n",
      "Epoch 4/10, Batch 729/883, Training Loss: 0.6421\n",
      "Epoch 4/10, Batch 730/883, Training Loss: 0.6771\n",
      "Epoch 4/10, Batch 731/883, Training Loss: 0.4896\n",
      "Epoch 4/10, Batch 732/883, Training Loss: 0.6390\n",
      "Epoch 4/10, Batch 733/883, Training Loss: 0.8260\n",
      "Epoch 4/10, Batch 734/883, Training Loss: 0.5928\n",
      "Epoch 4/10, Batch 735/883, Training Loss: 0.9623\n",
      "Epoch 4/10, Batch 736/883, Training Loss: 0.5715\n",
      "Epoch 4/10, Batch 737/883, Training Loss: 0.5864\n",
      "Epoch 4/10, Batch 738/883, Training Loss: 0.6205\n",
      "Epoch 4/10, Batch 739/883, Training Loss: 0.8048\n",
      "Epoch 4/10, Batch 740/883, Training Loss: 0.8283\n",
      "Epoch 4/10, Batch 741/883, Training Loss: 1.0147\n",
      "Epoch 4/10, Batch 742/883, Training Loss: 0.6497\n",
      "Epoch 4/10, Batch 743/883, Training Loss: 0.7991\n",
      "Epoch 4/10, Batch 744/883, Training Loss: 0.7502\n",
      "Epoch 4/10, Batch 745/883, Training Loss: 0.8762\n",
      "Epoch 4/10, Batch 746/883, Training Loss: 1.3503\n",
      "Epoch 4/10, Batch 747/883, Training Loss: 0.7704\n",
      "Epoch 4/10, Batch 748/883, Training Loss: 0.6809\n",
      "Epoch 4/10, Batch 749/883, Training Loss: 0.9620\n",
      "Epoch 4/10, Batch 750/883, Training Loss: 0.6150\n",
      "Epoch 4/10, Batch 751/883, Training Loss: 0.5403\n",
      "Epoch 4/10, Batch 752/883, Training Loss: 0.6573\n",
      "Epoch 4/10, Batch 753/883, Training Loss: 1.1387\n",
      "Epoch 4/10, Batch 754/883, Training Loss: 0.6053\n",
      "Epoch 4/10, Batch 755/883, Training Loss: 0.6252\n",
      "Epoch 4/10, Batch 756/883, Training Loss: 0.5500\n",
      "Epoch 4/10, Batch 757/883, Training Loss: 0.7559\n",
      "Epoch 4/10, Batch 758/883, Training Loss: 0.5468\n",
      "Epoch 4/10, Batch 759/883, Training Loss: 0.7147\n",
      "Epoch 4/10, Batch 760/883, Training Loss: 0.6856\n",
      "Epoch 4/10, Batch 761/883, Training Loss: 0.6833\n",
      "Epoch 4/10, Batch 762/883, Training Loss: 0.6223\n",
      "Epoch 4/10, Batch 763/883, Training Loss: 0.6261\n",
      "Epoch 4/10, Batch 764/883, Training Loss: 0.7448\n",
      "Epoch 4/10, Batch 765/883, Training Loss: 0.7336\n",
      "Epoch 4/10, Batch 766/883, Training Loss: 0.7791\n",
      "Epoch 4/10, Batch 767/883, Training Loss: 0.6069\n",
      "Epoch 4/10, Batch 768/883, Training Loss: 0.4769\n",
      "Epoch 4/10, Batch 769/883, Training Loss: 0.7832\n",
      "Epoch 4/10, Batch 770/883, Training Loss: 0.7262\n",
      "Epoch 4/10, Batch 771/883, Training Loss: 0.7173\n",
      "Epoch 4/10, Batch 772/883, Training Loss: 0.9214\n",
      "Epoch 4/10, Batch 773/883, Training Loss: 1.0282\n",
      "Epoch 4/10, Batch 774/883, Training Loss: 0.6845\n",
      "Epoch 4/10, Batch 775/883, Training Loss: 0.5927\n",
      "Epoch 4/10, Batch 776/883, Training Loss: 0.6473\n",
      "Epoch 4/10, Batch 777/883, Training Loss: 0.3949\n",
      "Epoch 4/10, Batch 778/883, Training Loss: 0.6625\n",
      "Epoch 4/10, Batch 779/883, Training Loss: 0.6704\n",
      "Epoch 4/10, Batch 780/883, Training Loss: 0.6525\n",
      "Epoch 4/10, Batch 781/883, Training Loss: 0.9904\n",
      "Epoch 4/10, Batch 782/883, Training Loss: 0.7072\n",
      "Epoch 4/10, Batch 783/883, Training Loss: 1.2229\n",
      "Epoch 4/10, Batch 784/883, Training Loss: 0.5215\n",
      "Epoch 4/10, Batch 785/883, Training Loss: 0.4499\n",
      "Epoch 4/10, Batch 786/883, Training Loss: 0.6182\n",
      "Epoch 4/10, Batch 787/883, Training Loss: 0.5020\n",
      "Epoch 4/10, Batch 788/883, Training Loss: 0.7259\n",
      "Epoch 4/10, Batch 789/883, Training Loss: 0.8736\n",
      "Epoch 4/10, Batch 790/883, Training Loss: 0.7755\n",
      "Epoch 4/10, Batch 791/883, Training Loss: 0.8958\n",
      "Epoch 4/10, Batch 792/883, Training Loss: 0.8093\n",
      "Epoch 4/10, Batch 793/883, Training Loss: 0.7233\n",
      "Epoch 4/10, Batch 794/883, Training Loss: 0.7469\n",
      "Epoch 4/10, Batch 795/883, Training Loss: 0.8404\n",
      "Epoch 4/10, Batch 796/883, Training Loss: 0.7040\n",
      "Epoch 4/10, Batch 797/883, Training Loss: 0.8194\n",
      "Epoch 4/10, Batch 798/883, Training Loss: 0.8797\n",
      "Epoch 4/10, Batch 799/883, Training Loss: 0.5345\n",
      "Epoch 4/10, Batch 800/883, Training Loss: 0.6207\n",
      "Epoch 4/10, Batch 801/883, Training Loss: 0.8436\n",
      "Epoch 4/10, Batch 802/883, Training Loss: 0.8745\n",
      "Epoch 4/10, Batch 803/883, Training Loss: 0.7328\n",
      "Epoch 4/10, Batch 804/883, Training Loss: 0.5879\n",
      "Epoch 4/10, Batch 805/883, Training Loss: 0.5903\n",
      "Epoch 4/10, Batch 806/883, Training Loss: 1.0393\n",
      "Epoch 4/10, Batch 807/883, Training Loss: 0.8450\n",
      "Epoch 4/10, Batch 808/883, Training Loss: 0.7600\n",
      "Epoch 4/10, Batch 809/883, Training Loss: 0.8439\n",
      "Epoch 4/10, Batch 810/883, Training Loss: 0.6728\n",
      "Epoch 4/10, Batch 811/883, Training Loss: 0.9724\n",
      "Epoch 4/10, Batch 812/883, Training Loss: 0.8072\n",
      "Epoch 4/10, Batch 813/883, Training Loss: 0.6794\n",
      "Epoch 4/10, Batch 814/883, Training Loss: 0.7175\n",
      "Epoch 4/10, Batch 815/883, Training Loss: 0.9995\n",
      "Epoch 4/10, Batch 816/883, Training Loss: 0.7057\n",
      "Epoch 4/10, Batch 817/883, Training Loss: 0.6437\n",
      "Epoch 4/10, Batch 818/883, Training Loss: 0.6105\n",
      "Epoch 4/10, Batch 819/883, Training Loss: 0.6671\n",
      "Epoch 4/10, Batch 820/883, Training Loss: 0.8254\n",
      "Epoch 4/10, Batch 821/883, Training Loss: 0.6306\n",
      "Epoch 4/10, Batch 822/883, Training Loss: 0.5446\n",
      "Epoch 4/10, Batch 823/883, Training Loss: 0.8794\n",
      "Epoch 4/10, Batch 824/883, Training Loss: 0.9509\n",
      "Epoch 4/10, Batch 825/883, Training Loss: 0.6982\n",
      "Epoch 4/10, Batch 826/883, Training Loss: 1.0195\n",
      "Epoch 4/10, Batch 827/883, Training Loss: 0.6463\n",
      "Epoch 4/10, Batch 828/883, Training Loss: 0.9449\n",
      "Epoch 4/10, Batch 829/883, Training Loss: 0.4100\n",
      "Epoch 4/10, Batch 830/883, Training Loss: 0.6047\n",
      "Epoch 4/10, Batch 831/883, Training Loss: 0.8305\n",
      "Epoch 4/10, Batch 832/883, Training Loss: 0.7355\n",
      "Epoch 4/10, Batch 833/883, Training Loss: 0.7444\n",
      "Epoch 4/10, Batch 834/883, Training Loss: 0.5216\n",
      "Epoch 4/10, Batch 835/883, Training Loss: 0.7542\n",
      "Epoch 4/10, Batch 836/883, Training Loss: 0.7998\n",
      "Epoch 4/10, Batch 837/883, Training Loss: 0.6757\n",
      "Epoch 4/10, Batch 838/883, Training Loss: 0.5747\n",
      "Epoch 4/10, Batch 839/883, Training Loss: 0.8575\n",
      "Epoch 4/10, Batch 840/883, Training Loss: 0.5841\n",
      "Epoch 4/10, Batch 841/883, Training Loss: 0.9155\n",
      "Epoch 4/10, Batch 842/883, Training Loss: 0.7644\n",
      "Epoch 4/10, Batch 843/883, Training Loss: 0.8989\n",
      "Epoch 4/10, Batch 844/883, Training Loss: 1.1407\n",
      "Epoch 4/10, Batch 845/883, Training Loss: 0.7390\n",
      "Epoch 4/10, Batch 846/883, Training Loss: 0.8817\n",
      "Epoch 4/10, Batch 847/883, Training Loss: 0.8004\n",
      "Epoch 4/10, Batch 848/883, Training Loss: 0.9473\n",
      "Epoch 4/10, Batch 849/883, Training Loss: 0.6901\n",
      "Epoch 4/10, Batch 850/883, Training Loss: 0.9573\n",
      "Epoch 4/10, Batch 851/883, Training Loss: 0.7919\n",
      "Epoch 4/10, Batch 852/883, Training Loss: 0.7139\n",
      "Epoch 4/10, Batch 853/883, Training Loss: 0.9788\n",
      "Epoch 4/10, Batch 854/883, Training Loss: 0.6234\n",
      "Epoch 4/10, Batch 855/883, Training Loss: 0.8883\n",
      "Epoch 4/10, Batch 856/883, Training Loss: 0.8263\n",
      "Epoch 4/10, Batch 857/883, Training Loss: 0.7002\n",
      "Epoch 4/10, Batch 858/883, Training Loss: 1.0051\n",
      "Epoch 4/10, Batch 859/883, Training Loss: 0.8069\n",
      "Epoch 4/10, Batch 860/883, Training Loss: 0.9345\n",
      "Epoch 4/10, Batch 861/883, Training Loss: 0.6652\n",
      "Epoch 4/10, Batch 862/883, Training Loss: 0.9179\n",
      "Epoch 4/10, Batch 863/883, Training Loss: 0.7092\n",
      "Epoch 4/10, Batch 864/883, Training Loss: 0.6299\n",
      "Epoch 4/10, Batch 865/883, Training Loss: 0.7511\n",
      "Epoch 4/10, Batch 866/883, Training Loss: 0.7411\n",
      "Epoch 4/10, Batch 867/883, Training Loss: 0.7890\n",
      "Epoch 4/10, Batch 868/883, Training Loss: 0.8097\n",
      "Epoch 4/10, Batch 869/883, Training Loss: 0.7639\n",
      "Epoch 4/10, Batch 870/883, Training Loss: 0.7906\n",
      "Epoch 4/10, Batch 871/883, Training Loss: 0.8590\n",
      "Epoch 4/10, Batch 872/883, Training Loss: 0.6948\n",
      "Epoch 4/10, Batch 873/883, Training Loss: 0.5382\n",
      "Epoch 4/10, Batch 874/883, Training Loss: 0.9090\n",
      "Epoch 4/10, Batch 875/883, Training Loss: 0.7289\n",
      "Epoch 4/10, Batch 876/883, Training Loss: 0.7394\n",
      "Epoch 4/10, Batch 877/883, Training Loss: 0.6628\n",
      "Epoch 4/10, Batch 878/883, Training Loss: 0.8392\n",
      "Epoch 4/10, Batch 879/883, Training Loss: 0.5539\n",
      "Epoch 4/10, Batch 880/883, Training Loss: 0.7618\n",
      "Epoch 4/10, Batch 881/883, Training Loss: 0.7590\n",
      "Epoch 4/10, Batch 882/883, Training Loss: 0.6095\n",
      "Epoch 4/10, Batch 883/883, Training Loss: 1.0621\n",
      "Epoch 4/10, Training Loss: 0.7774, Validation Loss: 0.7572, Validation Accuracy: 0.6302\n",
      "Epoch 5/10, Batch 1/883, Training Loss: 0.7539\n",
      "Epoch 5/10, Batch 2/883, Training Loss: 0.6696\n",
      "Epoch 5/10, Batch 3/883, Training Loss: 0.6319\n",
      "Epoch 5/10, Batch 4/883, Training Loss: 0.6744\n",
      "Epoch 5/10, Batch 5/883, Training Loss: 0.5925\n",
      "Epoch 5/10, Batch 6/883, Training Loss: 0.6359\n",
      "Epoch 5/10, Batch 7/883, Training Loss: 0.6939\n",
      "Epoch 5/10, Batch 8/883, Training Loss: 0.6797\n",
      "Epoch 5/10, Batch 9/883, Training Loss: 0.7663\n",
      "Epoch 5/10, Batch 10/883, Training Loss: 0.7977\n",
      "Epoch 5/10, Batch 11/883, Training Loss: 1.1873\n",
      "Epoch 5/10, Batch 12/883, Training Loss: 0.8022\n",
      "Epoch 5/10, Batch 13/883, Training Loss: 0.8017\n",
      "Epoch 5/10, Batch 14/883, Training Loss: 0.6450\n",
      "Epoch 5/10, Batch 15/883, Training Loss: 0.5170\n",
      "Epoch 5/10, Batch 16/883, Training Loss: 0.8005\n",
      "Epoch 5/10, Batch 17/883, Training Loss: 0.7689\n",
      "Epoch 5/10, Batch 18/883, Training Loss: 0.6687\n",
      "Epoch 5/10, Batch 19/883, Training Loss: 0.5597\n",
      "Epoch 5/10, Batch 20/883, Training Loss: 0.7675\n",
      "Epoch 5/10, Batch 21/883, Training Loss: 0.5777\n",
      "Epoch 5/10, Batch 22/883, Training Loss: 0.5617\n",
      "Epoch 5/10, Batch 23/883, Training Loss: 0.9636\n",
      "Epoch 5/10, Batch 24/883, Training Loss: 0.7208\n",
      "Epoch 5/10, Batch 25/883, Training Loss: 0.7436\n",
      "Epoch 5/10, Batch 26/883, Training Loss: 0.7061\n",
      "Epoch 5/10, Batch 27/883, Training Loss: 0.5501\n",
      "Epoch 5/10, Batch 28/883, Training Loss: 0.8211\n",
      "Epoch 5/10, Batch 29/883, Training Loss: 0.7572\n",
      "Epoch 5/10, Batch 30/883, Training Loss: 0.7298\n",
      "Epoch 5/10, Batch 31/883, Training Loss: 0.8703\n",
      "Epoch 5/10, Batch 32/883, Training Loss: 0.9298\n",
      "Epoch 5/10, Batch 33/883, Training Loss: 0.9588\n",
      "Epoch 5/10, Batch 34/883, Training Loss: 0.8495\n",
      "Epoch 5/10, Batch 35/883, Training Loss: 0.7892\n",
      "Epoch 5/10, Batch 36/883, Training Loss: 0.8194\n",
      "Epoch 5/10, Batch 37/883, Training Loss: 0.6118\n",
      "Epoch 5/10, Batch 38/883, Training Loss: 0.8762\n",
      "Epoch 5/10, Batch 39/883, Training Loss: 0.9098\n",
      "Epoch 5/10, Batch 40/883, Training Loss: 0.5803\n",
      "Epoch 5/10, Batch 41/883, Training Loss: 1.0622\n",
      "Epoch 5/10, Batch 42/883, Training Loss: 1.1080\n",
      "Epoch 5/10, Batch 43/883, Training Loss: 0.9584\n",
      "Epoch 5/10, Batch 44/883, Training Loss: 0.7230\n",
      "Epoch 5/10, Batch 45/883, Training Loss: 0.5878\n",
      "Epoch 5/10, Batch 46/883, Training Loss: 0.8055\n",
      "Epoch 5/10, Batch 47/883, Training Loss: 0.5776\n",
      "Epoch 5/10, Batch 48/883, Training Loss: 0.6151\n",
      "Epoch 5/10, Batch 49/883, Training Loss: 0.5818\n",
      "Epoch 5/10, Batch 50/883, Training Loss: 0.6366\n",
      "Epoch 5/10, Batch 51/883, Training Loss: 0.6588\n",
      "Epoch 5/10, Batch 52/883, Training Loss: 0.6000\n",
      "Epoch 5/10, Batch 53/883, Training Loss: 0.7942\n",
      "Epoch 5/10, Batch 54/883, Training Loss: 0.6161\n",
      "Epoch 5/10, Batch 55/883, Training Loss: 0.7916\n",
      "Epoch 5/10, Batch 56/883, Training Loss: 0.7317\n",
      "Epoch 5/10, Batch 57/883, Training Loss: 0.5915\n",
      "Epoch 5/10, Batch 58/883, Training Loss: 0.5670\n",
      "Epoch 5/10, Batch 59/883, Training Loss: 0.5508\n",
      "Epoch 5/10, Batch 60/883, Training Loss: 0.7144\n",
      "Epoch 5/10, Batch 61/883, Training Loss: 0.7087\n",
      "Epoch 5/10, Batch 62/883, Training Loss: 0.9354\n",
      "Epoch 5/10, Batch 63/883, Training Loss: 0.5434\n",
      "Epoch 5/10, Batch 64/883, Training Loss: 0.9521\n",
      "Epoch 5/10, Batch 65/883, Training Loss: 0.7716\n",
      "Epoch 5/10, Batch 66/883, Training Loss: 0.6937\n",
      "Epoch 5/10, Batch 67/883, Training Loss: 1.0196\n",
      "Epoch 5/10, Batch 68/883, Training Loss: 0.5986\n",
      "Epoch 5/10, Batch 69/883, Training Loss: 0.4438\n",
      "Epoch 5/10, Batch 70/883, Training Loss: 0.7400\n",
      "Epoch 5/10, Batch 71/883, Training Loss: 0.5882\n",
      "Epoch 5/10, Batch 72/883, Training Loss: 0.8004\n",
      "Epoch 5/10, Batch 73/883, Training Loss: 0.6360\n",
      "Epoch 5/10, Batch 74/883, Training Loss: 0.7287\n",
      "Epoch 5/10, Batch 75/883, Training Loss: 0.7581\n",
      "Epoch 5/10, Batch 76/883, Training Loss: 0.8224\n",
      "Epoch 5/10, Batch 77/883, Training Loss: 1.0724\n",
      "Epoch 5/10, Batch 78/883, Training Loss: 0.6726\n",
      "Epoch 5/10, Batch 79/883, Training Loss: 0.6888\n",
      "Epoch 5/10, Batch 80/883, Training Loss: 0.7637\n",
      "Epoch 5/10, Batch 81/883, Training Loss: 0.5286\n",
      "Epoch 5/10, Batch 82/883, Training Loss: 0.6614\n",
      "Epoch 5/10, Batch 83/883, Training Loss: 0.5668\n",
      "Epoch 5/10, Batch 84/883, Training Loss: 0.6968\n",
      "Epoch 5/10, Batch 85/883, Training Loss: 0.7047\n",
      "Epoch 5/10, Batch 86/883, Training Loss: 0.4760\n",
      "Epoch 5/10, Batch 87/883, Training Loss: 0.7275\n",
      "Epoch 5/10, Batch 88/883, Training Loss: 0.5829\n",
      "Epoch 5/10, Batch 89/883, Training Loss: 0.5765\n",
      "Epoch 5/10, Batch 90/883, Training Loss: 0.5513\n",
      "Epoch 5/10, Batch 91/883, Training Loss: 0.3833\n",
      "Epoch 5/10, Batch 92/883, Training Loss: 0.9102\n",
      "Epoch 5/10, Batch 93/883, Training Loss: 0.5648\n",
      "Epoch 5/10, Batch 94/883, Training Loss: 0.9543\n",
      "Epoch 5/10, Batch 95/883, Training Loss: 0.4991\n",
      "Epoch 5/10, Batch 96/883, Training Loss: 0.8598\n",
      "Epoch 5/10, Batch 97/883, Training Loss: 0.9155\n",
      "Epoch 5/10, Batch 98/883, Training Loss: 0.5354\n",
      "Epoch 5/10, Batch 99/883, Training Loss: 0.5674\n",
      "Epoch 5/10, Batch 100/883, Training Loss: 0.5885\n",
      "Epoch 5/10, Batch 101/883, Training Loss: 0.8776\n",
      "Epoch 5/10, Batch 102/883, Training Loss: 0.7992\n",
      "Epoch 5/10, Batch 103/883, Training Loss: 0.6837\n",
      "Epoch 5/10, Batch 104/883, Training Loss: 0.6962\n",
      "Epoch 5/10, Batch 105/883, Training Loss: 0.7227\n",
      "Epoch 5/10, Batch 106/883, Training Loss: 0.5901\n",
      "Epoch 5/10, Batch 107/883, Training Loss: 0.9153\n",
      "Epoch 5/10, Batch 108/883, Training Loss: 0.7877\n",
      "Epoch 5/10, Batch 109/883, Training Loss: 0.8044\n",
      "Epoch 5/10, Batch 110/883, Training Loss: 0.5352\n",
      "Epoch 5/10, Batch 111/883, Training Loss: 1.2396\n",
      "Epoch 5/10, Batch 112/883, Training Loss: 0.4947\n",
      "Epoch 5/10, Batch 113/883, Training Loss: 0.6512\n",
      "Epoch 5/10, Batch 114/883, Training Loss: 0.5006\n",
      "Epoch 5/10, Batch 115/883, Training Loss: 0.9801\n",
      "Epoch 5/10, Batch 116/883, Training Loss: 0.7253\n",
      "Epoch 5/10, Batch 117/883, Training Loss: 1.1239\n",
      "Epoch 5/10, Batch 118/883, Training Loss: 0.5064\n",
      "Epoch 5/10, Batch 119/883, Training Loss: 0.5593\n",
      "Epoch 5/10, Batch 120/883, Training Loss: 0.6606\n",
      "Epoch 5/10, Batch 121/883, Training Loss: 0.6844\n",
      "Epoch 5/10, Batch 122/883, Training Loss: 0.8334\n",
      "Epoch 5/10, Batch 123/883, Training Loss: 0.6934\n",
      "Epoch 5/10, Batch 124/883, Training Loss: 0.7545\n",
      "Epoch 5/10, Batch 125/883, Training Loss: 0.6049\n",
      "Epoch 5/10, Batch 126/883, Training Loss: 0.8221\n",
      "Epoch 5/10, Batch 127/883, Training Loss: 0.9838\n",
      "Epoch 5/10, Batch 128/883, Training Loss: 0.8178\n",
      "Epoch 5/10, Batch 129/883, Training Loss: 0.6542\n",
      "Epoch 5/10, Batch 130/883, Training Loss: 0.4459\n",
      "Epoch 5/10, Batch 131/883, Training Loss: 0.6662\n",
      "Epoch 5/10, Batch 132/883, Training Loss: 0.6038\n",
      "Epoch 5/10, Batch 133/883, Training Loss: 0.5895\n",
      "Epoch 5/10, Batch 134/883, Training Loss: 0.6643\n",
      "Epoch 5/10, Batch 135/883, Training Loss: 0.6661\n",
      "Epoch 5/10, Batch 136/883, Training Loss: 0.8131\n",
      "Epoch 5/10, Batch 137/883, Training Loss: 0.5801\n",
      "Epoch 5/10, Batch 138/883, Training Loss: 0.6200\n",
      "Epoch 5/10, Batch 139/883, Training Loss: 0.8318\n",
      "Epoch 5/10, Batch 140/883, Training Loss: 0.5888\n",
      "Epoch 5/10, Batch 141/883, Training Loss: 0.6857\n",
      "Epoch 5/10, Batch 142/883, Training Loss: 0.5811\n",
      "Epoch 5/10, Batch 143/883, Training Loss: 0.6389\n",
      "Epoch 5/10, Batch 144/883, Training Loss: 0.5154\n",
      "Epoch 5/10, Batch 145/883, Training Loss: 0.9875\n",
      "Epoch 5/10, Batch 146/883, Training Loss: 0.6996\n",
      "Epoch 5/10, Batch 147/883, Training Loss: 0.5693\n",
      "Epoch 5/10, Batch 148/883, Training Loss: 0.9965\n",
      "Epoch 5/10, Batch 149/883, Training Loss: 0.6682\n",
      "Epoch 5/10, Batch 150/883, Training Loss: 0.6209\n",
      "Epoch 5/10, Batch 151/883, Training Loss: 0.5785\n",
      "Epoch 5/10, Batch 152/883, Training Loss: 0.7353\n",
      "Epoch 5/10, Batch 153/883, Training Loss: 0.7098\n",
      "Epoch 5/10, Batch 154/883, Training Loss: 1.2994\n",
      "Epoch 5/10, Batch 155/883, Training Loss: 0.8653\n",
      "Epoch 5/10, Batch 156/883, Training Loss: 1.0931\n",
      "Epoch 5/10, Batch 157/883, Training Loss: 0.6246\n",
      "Epoch 5/10, Batch 158/883, Training Loss: 0.6698\n",
      "Epoch 5/10, Batch 159/883, Training Loss: 0.6772\n",
      "Epoch 5/10, Batch 160/883, Training Loss: 0.8142\n",
      "Epoch 5/10, Batch 161/883, Training Loss: 0.6656\n",
      "Epoch 5/10, Batch 162/883, Training Loss: 0.7812\n",
      "Epoch 5/10, Batch 163/883, Training Loss: 0.7472\n",
      "Epoch 5/10, Batch 164/883, Training Loss: 0.7120\n",
      "Epoch 5/10, Batch 165/883, Training Loss: 0.5313\n",
      "Epoch 5/10, Batch 166/883, Training Loss: 0.7304\n",
      "Epoch 5/10, Batch 167/883, Training Loss: 0.5439\n",
      "Epoch 5/10, Batch 168/883, Training Loss: 0.7671\n",
      "Epoch 5/10, Batch 169/883, Training Loss: 0.8800\n",
      "Epoch 5/10, Batch 170/883, Training Loss: 0.5590\n",
      "Epoch 5/10, Batch 171/883, Training Loss: 0.7706\n",
      "Epoch 5/10, Batch 172/883, Training Loss: 0.7066\n",
      "Epoch 5/10, Batch 173/883, Training Loss: 0.6407\n",
      "Epoch 5/10, Batch 174/883, Training Loss: 0.4849\n",
      "Epoch 5/10, Batch 175/883, Training Loss: 0.8768\n",
      "Epoch 5/10, Batch 176/883, Training Loss: 0.5969\n",
      "Epoch 5/10, Batch 177/883, Training Loss: 0.6478\n",
      "Epoch 5/10, Batch 178/883, Training Loss: 0.7687\n",
      "Epoch 5/10, Batch 179/883, Training Loss: 0.7051\n",
      "Epoch 5/10, Batch 180/883, Training Loss: 0.6754\n",
      "Epoch 5/10, Batch 181/883, Training Loss: 0.8206\n",
      "Epoch 5/10, Batch 182/883, Training Loss: 0.8327\n",
      "Epoch 5/10, Batch 183/883, Training Loss: 0.6480\n",
      "Epoch 5/10, Batch 184/883, Training Loss: 0.7597\n",
      "Epoch 5/10, Batch 185/883, Training Loss: 0.6390\n",
      "Epoch 5/10, Batch 186/883, Training Loss: 0.9638\n",
      "Epoch 5/10, Batch 187/883, Training Loss: 0.5700\n",
      "Epoch 5/10, Batch 188/883, Training Loss: 0.7185\n",
      "Epoch 5/10, Batch 189/883, Training Loss: 0.8522\n",
      "Epoch 5/10, Batch 190/883, Training Loss: 0.6684\n",
      "Epoch 5/10, Batch 191/883, Training Loss: 0.8727\n",
      "Epoch 5/10, Batch 192/883, Training Loss: 0.7380\n",
      "Epoch 5/10, Batch 193/883, Training Loss: 0.6207\n",
      "Epoch 5/10, Batch 194/883, Training Loss: 0.6214\n",
      "Epoch 5/10, Batch 195/883, Training Loss: 1.0650\n",
      "Epoch 5/10, Batch 196/883, Training Loss: 0.7016\n",
      "Epoch 5/10, Batch 197/883, Training Loss: 0.6298\n",
      "Epoch 5/10, Batch 198/883, Training Loss: 0.4955\n",
      "Epoch 5/10, Batch 199/883, Training Loss: 0.7322\n",
      "Epoch 5/10, Batch 200/883, Training Loss: 0.8081\n",
      "Epoch 5/10, Batch 201/883, Training Loss: 0.6654\n",
      "Epoch 5/10, Batch 202/883, Training Loss: 0.6887\n",
      "Epoch 5/10, Batch 203/883, Training Loss: 0.5960\n",
      "Epoch 5/10, Batch 204/883, Training Loss: 0.4596\n",
      "Epoch 5/10, Batch 205/883, Training Loss: 0.5268\n",
      "Epoch 5/10, Batch 206/883, Training Loss: 0.5984\n",
      "Epoch 5/10, Batch 207/883, Training Loss: 0.6468\n",
      "Epoch 5/10, Batch 208/883, Training Loss: 1.0043\n",
      "Epoch 5/10, Batch 209/883, Training Loss: 0.5472\n",
      "Epoch 5/10, Batch 210/883, Training Loss: 0.5780\n",
      "Epoch 5/10, Batch 211/883, Training Loss: 0.7919\n",
      "Epoch 5/10, Batch 212/883, Training Loss: 1.0228\n",
      "Epoch 5/10, Batch 213/883, Training Loss: 0.7686\n",
      "Epoch 5/10, Batch 214/883, Training Loss: 0.7451\n",
      "Epoch 5/10, Batch 215/883, Training Loss: 0.6681\n",
      "Epoch 5/10, Batch 216/883, Training Loss: 0.6712\n",
      "Epoch 5/10, Batch 217/883, Training Loss: 0.6160\n",
      "Epoch 5/10, Batch 218/883, Training Loss: 0.9587\n",
      "Epoch 5/10, Batch 219/883, Training Loss: 0.9172\n",
      "Epoch 5/10, Batch 220/883, Training Loss: 0.8093\n",
      "Epoch 5/10, Batch 221/883, Training Loss: 0.7143\n",
      "Epoch 5/10, Batch 222/883, Training Loss: 0.5587\n",
      "Epoch 5/10, Batch 223/883, Training Loss: 0.5102\n",
      "Epoch 5/10, Batch 224/883, Training Loss: 0.4094\n",
      "Epoch 5/10, Batch 225/883, Training Loss: 0.6555\n",
      "Epoch 5/10, Batch 226/883, Training Loss: 0.9733\n",
      "Epoch 5/10, Batch 227/883, Training Loss: 0.7328\n",
      "Epoch 5/10, Batch 228/883, Training Loss: 1.1283\n",
      "Epoch 5/10, Batch 229/883, Training Loss: 0.8775\n",
      "Epoch 5/10, Batch 230/883, Training Loss: 0.6076\n",
      "Epoch 5/10, Batch 231/883, Training Loss: 0.5517\n",
      "Epoch 5/10, Batch 232/883, Training Loss: 0.6438\n",
      "Epoch 5/10, Batch 233/883, Training Loss: 0.5630\n",
      "Epoch 5/10, Batch 234/883, Training Loss: 0.5534\n",
      "Epoch 5/10, Batch 235/883, Training Loss: 1.1551\n",
      "Epoch 5/10, Batch 236/883, Training Loss: 0.8453\n",
      "Epoch 5/10, Batch 237/883, Training Loss: 0.7967\n",
      "Epoch 5/10, Batch 238/883, Training Loss: 0.5744\n",
      "Epoch 5/10, Batch 239/883, Training Loss: 0.7149\n",
      "Epoch 5/10, Batch 240/883, Training Loss: 1.1451\n",
      "Epoch 5/10, Batch 241/883, Training Loss: 0.7884\n",
      "Epoch 5/10, Batch 242/883, Training Loss: 0.5644\n",
      "Epoch 5/10, Batch 243/883, Training Loss: 0.5854\n",
      "Epoch 5/10, Batch 244/883, Training Loss: 0.6003\n",
      "Epoch 5/10, Batch 245/883, Training Loss: 0.4894\n",
      "Epoch 5/10, Batch 246/883, Training Loss: 0.5688\n",
      "Epoch 5/10, Batch 247/883, Training Loss: 0.7835\n",
      "Epoch 5/10, Batch 248/883, Training Loss: 0.6023\n",
      "Epoch 5/10, Batch 249/883, Training Loss: 0.6415\n",
      "Epoch 5/10, Batch 250/883, Training Loss: 0.7839\n",
      "Epoch 5/10, Batch 251/883, Training Loss: 0.4422\n",
      "Epoch 5/10, Batch 252/883, Training Loss: 0.6698\n",
      "Epoch 5/10, Batch 253/883, Training Loss: 0.6054\n",
      "Epoch 5/10, Batch 254/883, Training Loss: 0.5666\n",
      "Epoch 5/10, Batch 255/883, Training Loss: 0.5028\n",
      "Epoch 5/10, Batch 256/883, Training Loss: 0.5978\n",
      "Epoch 5/10, Batch 257/883, Training Loss: 0.7781\n",
      "Epoch 5/10, Batch 258/883, Training Loss: 0.5105\n",
      "Epoch 5/10, Batch 259/883, Training Loss: 0.6769\n",
      "Epoch 5/10, Batch 260/883, Training Loss: 0.9813\n",
      "Epoch 5/10, Batch 261/883, Training Loss: 0.5755\n",
      "Epoch 5/10, Batch 262/883, Training Loss: 1.2995\n",
      "Epoch 5/10, Batch 263/883, Training Loss: 0.5288\n",
      "Epoch 5/10, Batch 264/883, Training Loss: 0.5096\n",
      "Epoch 5/10, Batch 265/883, Training Loss: 0.6860\n",
      "Epoch 5/10, Batch 266/883, Training Loss: 0.9939\n",
      "Epoch 5/10, Batch 267/883, Training Loss: 0.6254\n",
      "Epoch 5/10, Batch 268/883, Training Loss: 0.4429\n",
      "Epoch 5/10, Batch 269/883, Training Loss: 0.9212\n",
      "Epoch 5/10, Batch 270/883, Training Loss: 0.6701\n",
      "Epoch 5/10, Batch 271/883, Training Loss: 0.8882\n",
      "Epoch 5/10, Batch 272/883, Training Loss: 0.8220\n",
      "Epoch 5/10, Batch 273/883, Training Loss: 0.7364\n",
      "Epoch 5/10, Batch 274/883, Training Loss: 0.7530\n",
      "Epoch 5/10, Batch 275/883, Training Loss: 0.7543\n",
      "Epoch 5/10, Batch 276/883, Training Loss: 0.7776\n",
      "Epoch 5/10, Batch 277/883, Training Loss: 0.8845\n",
      "Epoch 5/10, Batch 278/883, Training Loss: 0.9052\n",
      "Epoch 5/10, Batch 279/883, Training Loss: 0.4280\n",
      "Epoch 5/10, Batch 280/883, Training Loss: 0.5188\n",
      "Epoch 5/10, Batch 281/883, Training Loss: 0.8120\n",
      "Epoch 5/10, Batch 282/883, Training Loss: 0.6732\n",
      "Epoch 5/10, Batch 283/883, Training Loss: 0.8115\n",
      "Epoch 5/10, Batch 284/883, Training Loss: 0.9863\n",
      "Epoch 5/10, Batch 285/883, Training Loss: 0.7132\n",
      "Epoch 5/10, Batch 286/883, Training Loss: 0.8041\n",
      "Epoch 5/10, Batch 287/883, Training Loss: 0.4491\n",
      "Epoch 5/10, Batch 288/883, Training Loss: 0.4969\n",
      "Epoch 5/10, Batch 289/883, Training Loss: 0.5236\n",
      "Epoch 5/10, Batch 290/883, Training Loss: 1.0463\n",
      "Epoch 5/10, Batch 291/883, Training Loss: 0.4173\n",
      "Epoch 5/10, Batch 292/883, Training Loss: 0.4481\n",
      "Epoch 5/10, Batch 293/883, Training Loss: 0.7470\n",
      "Epoch 5/10, Batch 294/883, Training Loss: 0.6595\n",
      "Epoch 5/10, Batch 295/883, Training Loss: 0.4146\n",
      "Epoch 5/10, Batch 296/883, Training Loss: 0.9873\n",
      "Epoch 5/10, Batch 297/883, Training Loss: 0.8228\n",
      "Epoch 5/10, Batch 298/883, Training Loss: 0.8752\n",
      "Epoch 5/10, Batch 299/883, Training Loss: 1.0438\n",
      "Epoch 5/10, Batch 300/883, Training Loss: 0.6558\n",
      "Epoch 5/10, Batch 301/883, Training Loss: 0.7228\n",
      "Epoch 5/10, Batch 302/883, Training Loss: 0.8668\n",
      "Epoch 5/10, Batch 303/883, Training Loss: 0.7360\n",
      "Epoch 5/10, Batch 304/883, Training Loss: 0.8323\n",
      "Epoch 5/10, Batch 305/883, Training Loss: 0.6678\n",
      "Epoch 5/10, Batch 306/883, Training Loss: 0.6745\n",
      "Epoch 5/10, Batch 307/883, Training Loss: 0.7294\n",
      "Epoch 5/10, Batch 308/883, Training Loss: 0.8643\n",
      "Epoch 5/10, Batch 309/883, Training Loss: 0.6391\n",
      "Epoch 5/10, Batch 310/883, Training Loss: 0.5402\n",
      "Epoch 5/10, Batch 311/883, Training Loss: 0.6159\n",
      "Epoch 5/10, Batch 312/883, Training Loss: 0.8057\n",
      "Epoch 5/10, Batch 313/883, Training Loss: 0.8982\n",
      "Epoch 5/10, Batch 314/883, Training Loss: 0.6005\n",
      "Epoch 5/10, Batch 315/883, Training Loss: 0.6072\n",
      "Epoch 5/10, Batch 316/883, Training Loss: 0.5095\n",
      "Epoch 5/10, Batch 317/883, Training Loss: 0.9432\n",
      "Epoch 5/10, Batch 318/883, Training Loss: 0.7402\n",
      "Epoch 5/10, Batch 319/883, Training Loss: 0.6752\n",
      "Epoch 5/10, Batch 320/883, Training Loss: 0.6427\n",
      "Epoch 5/10, Batch 321/883, Training Loss: 0.7046\n",
      "Epoch 5/10, Batch 322/883, Training Loss: 0.6105\n",
      "Epoch 5/10, Batch 323/883, Training Loss: 0.9387\n",
      "Epoch 5/10, Batch 324/883, Training Loss: 1.3540\n",
      "Epoch 5/10, Batch 325/883, Training Loss: 0.7771\n",
      "Epoch 5/10, Batch 326/883, Training Loss: 0.7532\n",
      "Epoch 5/10, Batch 327/883, Training Loss: 0.9286\n",
      "Epoch 5/10, Batch 328/883, Training Loss: 0.5326\n",
      "Epoch 5/10, Batch 329/883, Training Loss: 0.6065\n",
      "Epoch 5/10, Batch 330/883, Training Loss: 0.7045\n",
      "Epoch 5/10, Batch 331/883, Training Loss: 0.8636\n",
      "Epoch 5/10, Batch 332/883, Training Loss: 0.4926\n",
      "Epoch 5/10, Batch 333/883, Training Loss: 0.8644\n",
      "Epoch 5/10, Batch 334/883, Training Loss: 0.5211\n",
      "Epoch 5/10, Batch 335/883, Training Loss: 0.7699\n",
      "Epoch 5/10, Batch 336/883, Training Loss: 1.0493\n",
      "Epoch 5/10, Batch 337/883, Training Loss: 0.5386\n",
      "Epoch 5/10, Batch 338/883, Training Loss: 0.6894\n",
      "Epoch 5/10, Batch 339/883, Training Loss: 0.7952\n",
      "Epoch 5/10, Batch 340/883, Training Loss: 0.5750\n",
      "Epoch 5/10, Batch 341/883, Training Loss: 0.7247\n",
      "Epoch 5/10, Batch 342/883, Training Loss: 0.6507\n",
      "Epoch 5/10, Batch 343/883, Training Loss: 0.8907\n",
      "Epoch 5/10, Batch 344/883, Training Loss: 0.6640\n",
      "Epoch 5/10, Batch 345/883, Training Loss: 0.5984\n",
      "Epoch 5/10, Batch 346/883, Training Loss: 0.6969\n",
      "Epoch 5/10, Batch 347/883, Training Loss: 0.8704\n",
      "Epoch 5/10, Batch 348/883, Training Loss: 0.6195\n",
      "Epoch 5/10, Batch 349/883, Training Loss: 0.6177\n",
      "Epoch 5/10, Batch 350/883, Training Loss: 0.5636\n",
      "Epoch 5/10, Batch 351/883, Training Loss: 0.5819\n",
      "Epoch 5/10, Batch 352/883, Training Loss: 0.6666\n",
      "Epoch 5/10, Batch 353/883, Training Loss: 0.8534\n",
      "Epoch 5/10, Batch 354/883, Training Loss: 0.9230\n",
      "Epoch 5/10, Batch 355/883, Training Loss: 0.6680\n",
      "Epoch 5/10, Batch 356/883, Training Loss: 0.7168\n",
      "Epoch 5/10, Batch 357/883, Training Loss: 0.6070\n",
      "Epoch 5/10, Batch 358/883, Training Loss: 0.9722\n",
      "Epoch 5/10, Batch 359/883, Training Loss: 0.6853\n",
      "Epoch 5/10, Batch 360/883, Training Loss: 0.7323\n",
      "Epoch 5/10, Batch 361/883, Training Loss: 0.5508\n",
      "Epoch 5/10, Batch 362/883, Training Loss: 0.6717\n",
      "Epoch 5/10, Batch 363/883, Training Loss: 0.7952\n",
      "Epoch 5/10, Batch 364/883, Training Loss: 0.8474\n",
      "Epoch 5/10, Batch 365/883, Training Loss: 1.0774\n",
      "Epoch 5/10, Batch 366/883, Training Loss: 0.6949\n",
      "Epoch 5/10, Batch 367/883, Training Loss: 1.0638\n",
      "Epoch 5/10, Batch 368/883, Training Loss: 0.6161\n",
      "Epoch 5/10, Batch 369/883, Training Loss: 0.6732\n",
      "Epoch 5/10, Batch 370/883, Training Loss: 0.8237\n",
      "Epoch 5/10, Batch 371/883, Training Loss: 0.6226\n",
      "Epoch 5/10, Batch 372/883, Training Loss: 0.7865\n",
      "Epoch 5/10, Batch 373/883, Training Loss: 0.5777\n",
      "Epoch 5/10, Batch 374/883, Training Loss: 0.7137\n",
      "Epoch 5/10, Batch 375/883, Training Loss: 0.7475\n",
      "Epoch 5/10, Batch 376/883, Training Loss: 1.0257\n",
      "Epoch 5/10, Batch 377/883, Training Loss: 1.0379\n",
      "Epoch 5/10, Batch 378/883, Training Loss: 0.6764\n",
      "Epoch 5/10, Batch 379/883, Training Loss: 0.8685\n",
      "Epoch 5/10, Batch 380/883, Training Loss: 0.5924\n",
      "Epoch 5/10, Batch 381/883, Training Loss: 0.6627\n",
      "Epoch 5/10, Batch 382/883, Training Loss: 0.6294\n",
      "Epoch 5/10, Batch 383/883, Training Loss: 0.7069\n",
      "Epoch 5/10, Batch 384/883, Training Loss: 0.7133\n",
      "Epoch 5/10, Batch 385/883, Training Loss: 0.6800\n",
      "Epoch 5/10, Batch 386/883, Training Loss: 0.5942\n",
      "Epoch 5/10, Batch 387/883, Training Loss: 0.5737\n",
      "Epoch 5/10, Batch 388/883, Training Loss: 0.5319\n",
      "Epoch 5/10, Batch 389/883, Training Loss: 0.9325\n",
      "Epoch 5/10, Batch 390/883, Training Loss: 0.7645\n",
      "Epoch 5/10, Batch 391/883, Training Loss: 0.4446\n",
      "Epoch 5/10, Batch 392/883, Training Loss: 0.7231\n",
      "Epoch 5/10, Batch 393/883, Training Loss: 0.7819\n",
      "Epoch 5/10, Batch 394/883, Training Loss: 0.6047\n",
      "Epoch 5/10, Batch 395/883, Training Loss: 0.9530\n",
      "Epoch 5/10, Batch 396/883, Training Loss: 0.8950\n",
      "Epoch 5/10, Batch 397/883, Training Loss: 0.7145\n",
      "Epoch 5/10, Batch 398/883, Training Loss: 0.7246\n",
      "Epoch 5/10, Batch 399/883, Training Loss: 0.4656\n",
      "Epoch 5/10, Batch 400/883, Training Loss: 0.7129\n",
      "Epoch 5/10, Batch 401/883, Training Loss: 0.7696\n",
      "Epoch 5/10, Batch 402/883, Training Loss: 0.7875\n",
      "Epoch 5/10, Batch 403/883, Training Loss: 0.7374\n",
      "Epoch 5/10, Batch 404/883, Training Loss: 0.7432\n",
      "Epoch 5/10, Batch 405/883, Training Loss: 0.6926\n",
      "Epoch 5/10, Batch 406/883, Training Loss: 0.6906\n",
      "Epoch 5/10, Batch 407/883, Training Loss: 0.7309\n",
      "Epoch 5/10, Batch 408/883, Training Loss: 0.5995\n",
      "Epoch 5/10, Batch 409/883, Training Loss: 0.6066\n",
      "Epoch 5/10, Batch 410/883, Training Loss: 0.5837\n",
      "Epoch 5/10, Batch 411/883, Training Loss: 0.9710\n",
      "Epoch 5/10, Batch 412/883, Training Loss: 0.7699\n",
      "Epoch 5/10, Batch 413/883, Training Loss: 1.4018\n",
      "Epoch 5/10, Batch 414/883, Training Loss: 0.6125\n",
      "Epoch 5/10, Batch 415/883, Training Loss: 0.6806\n",
      "Epoch 5/10, Batch 416/883, Training Loss: 0.7547\n",
      "Epoch 5/10, Batch 417/883, Training Loss: 0.6725\n",
      "Epoch 5/10, Batch 418/883, Training Loss: 0.5921\n",
      "Epoch 5/10, Batch 419/883, Training Loss: 0.6853\n",
      "Epoch 5/10, Batch 420/883, Training Loss: 0.5122\n",
      "Epoch 5/10, Batch 421/883, Training Loss: 0.7112\n",
      "Epoch 5/10, Batch 422/883, Training Loss: 0.4727\n",
      "Epoch 5/10, Batch 423/883, Training Loss: 0.5817\n",
      "Epoch 5/10, Batch 424/883, Training Loss: 0.6117\n",
      "Epoch 5/10, Batch 425/883, Training Loss: 0.6727\n",
      "Epoch 5/10, Batch 426/883, Training Loss: 0.6208\n",
      "Epoch 5/10, Batch 427/883, Training Loss: 0.5197\n",
      "Epoch 5/10, Batch 428/883, Training Loss: 0.6159\n",
      "Epoch 5/10, Batch 429/883, Training Loss: 0.4840\n",
      "Epoch 5/10, Batch 430/883, Training Loss: 0.6675\n",
      "Epoch 5/10, Batch 431/883, Training Loss: 0.7011\n",
      "Epoch 5/10, Batch 432/883, Training Loss: 0.6016\n",
      "Epoch 5/10, Batch 433/883, Training Loss: 0.5864\n",
      "Epoch 5/10, Batch 434/883, Training Loss: 0.7988\n",
      "Epoch 5/10, Batch 435/883, Training Loss: 0.7968\n",
      "Epoch 5/10, Batch 436/883, Training Loss: 0.9030\n",
      "Epoch 5/10, Batch 437/883, Training Loss: 0.7684\n",
      "Epoch 5/10, Batch 438/883, Training Loss: 0.7228\n",
      "Epoch 5/10, Batch 439/883, Training Loss: 0.6091\n",
      "Epoch 5/10, Batch 440/883, Training Loss: 0.8988\n",
      "Epoch 5/10, Batch 441/883, Training Loss: 0.5571\n",
      "Epoch 5/10, Batch 442/883, Training Loss: 0.8472\n",
      "Epoch 5/10, Batch 443/883, Training Loss: 0.4441\n",
      "Epoch 5/10, Batch 444/883, Training Loss: 0.9740\n",
      "Epoch 5/10, Batch 445/883, Training Loss: 0.9986\n",
      "Epoch 5/10, Batch 446/883, Training Loss: 0.6892\n",
      "Epoch 5/10, Batch 447/883, Training Loss: 0.7709\n",
      "Epoch 5/10, Batch 448/883, Training Loss: 0.6288\n",
      "Epoch 5/10, Batch 449/883, Training Loss: 0.7472\n",
      "Epoch 5/10, Batch 450/883, Training Loss: 0.8330\n",
      "Epoch 5/10, Batch 451/883, Training Loss: 1.0621\n",
      "Epoch 5/10, Batch 452/883, Training Loss: 0.7588\n",
      "Epoch 5/10, Batch 453/883, Training Loss: 1.1645\n",
      "Epoch 5/10, Batch 454/883, Training Loss: 0.6599\n",
      "Epoch 5/10, Batch 455/883, Training Loss: 0.7092\n",
      "Epoch 5/10, Batch 456/883, Training Loss: 0.6723\n",
      "Epoch 5/10, Batch 457/883, Training Loss: 0.7784\n",
      "Epoch 5/10, Batch 458/883, Training Loss: 0.9118\n",
      "Epoch 5/10, Batch 459/883, Training Loss: 0.8432\n",
      "Epoch 5/10, Batch 460/883, Training Loss: 0.7332\n",
      "Epoch 5/10, Batch 461/883, Training Loss: 1.0303\n",
      "Epoch 5/10, Batch 462/883, Training Loss: 0.7720\n",
      "Epoch 5/10, Batch 463/883, Training Loss: 0.6959\n",
      "Epoch 5/10, Batch 464/883, Training Loss: 0.6215\n",
      "Epoch 5/10, Batch 465/883, Training Loss: 0.8060\n",
      "Epoch 5/10, Batch 466/883, Training Loss: 0.8584\n",
      "Epoch 5/10, Batch 467/883, Training Loss: 0.8078\n",
      "Epoch 5/10, Batch 468/883, Training Loss: 0.7431\n",
      "Epoch 5/10, Batch 469/883, Training Loss: 0.8841\n",
      "Epoch 5/10, Batch 470/883, Training Loss: 0.6078\n",
      "Epoch 5/10, Batch 471/883, Training Loss: 0.8087\n",
      "Epoch 5/10, Batch 472/883, Training Loss: 0.4950\n",
      "Epoch 5/10, Batch 473/883, Training Loss: 0.5919\n",
      "Epoch 5/10, Batch 474/883, Training Loss: 0.6984\n",
      "Epoch 5/10, Batch 475/883, Training Loss: 0.9283\n",
      "Epoch 5/10, Batch 476/883, Training Loss: 0.5711\n",
      "Epoch 5/10, Batch 477/883, Training Loss: 0.6416\n",
      "Epoch 5/10, Batch 478/883, Training Loss: 0.8716\n",
      "Epoch 5/10, Batch 479/883, Training Loss: 0.6911\n",
      "Epoch 5/10, Batch 480/883, Training Loss: 0.6820\n",
      "Epoch 5/10, Batch 481/883, Training Loss: 0.7394\n",
      "Epoch 5/10, Batch 482/883, Training Loss: 0.6119\n",
      "Epoch 5/10, Batch 483/883, Training Loss: 0.6423\n",
      "Epoch 5/10, Batch 484/883, Training Loss: 0.7213\n",
      "Epoch 5/10, Batch 485/883, Training Loss: 0.4881\n",
      "Epoch 5/10, Batch 486/883, Training Loss: 0.6854\n",
      "Epoch 5/10, Batch 487/883, Training Loss: 1.1372\n",
      "Epoch 5/10, Batch 488/883, Training Loss: 0.6252\n",
      "Epoch 5/10, Batch 489/883, Training Loss: 0.5222\n",
      "Epoch 5/10, Batch 490/883, Training Loss: 0.9176\n",
      "Epoch 5/10, Batch 491/883, Training Loss: 1.0199\n",
      "Epoch 5/10, Batch 492/883, Training Loss: 0.7568\n",
      "Epoch 5/10, Batch 493/883, Training Loss: 0.9045\n",
      "Epoch 5/10, Batch 494/883, Training Loss: 0.8044\n",
      "Epoch 5/10, Batch 495/883, Training Loss: 0.8905\n",
      "Epoch 5/10, Batch 496/883, Training Loss: 0.7424\n",
      "Epoch 5/10, Batch 497/883, Training Loss: 0.7835\n",
      "Epoch 5/10, Batch 498/883, Training Loss: 0.7651\n",
      "Epoch 5/10, Batch 499/883, Training Loss: 0.5800\n",
      "Epoch 5/10, Batch 500/883, Training Loss: 0.8537\n",
      "Epoch 5/10, Batch 501/883, Training Loss: 0.6834\n",
      "Epoch 5/10, Batch 502/883, Training Loss: 0.6773\n",
      "Epoch 5/10, Batch 503/883, Training Loss: 0.8688\n",
      "Epoch 5/10, Batch 504/883, Training Loss: 0.5267\n",
      "Epoch 5/10, Batch 505/883, Training Loss: 0.8941\n",
      "Epoch 5/10, Batch 506/883, Training Loss: 0.9266\n",
      "Epoch 5/10, Batch 507/883, Training Loss: 0.6908\n",
      "Epoch 5/10, Batch 508/883, Training Loss: 0.7230\n",
      "Epoch 5/10, Batch 509/883, Training Loss: 0.5751\n",
      "Epoch 5/10, Batch 510/883, Training Loss: 0.7701\n",
      "Epoch 5/10, Batch 511/883, Training Loss: 0.5608\n",
      "Epoch 5/10, Batch 512/883, Training Loss: 0.8341\n",
      "Epoch 5/10, Batch 513/883, Training Loss: 0.6308\n",
      "Epoch 5/10, Batch 514/883, Training Loss: 0.9180\n",
      "Epoch 5/10, Batch 515/883, Training Loss: 0.6335\n",
      "Epoch 5/10, Batch 516/883, Training Loss: 1.0094\n",
      "Epoch 5/10, Batch 517/883, Training Loss: 0.5689\n",
      "Epoch 5/10, Batch 518/883, Training Loss: 0.6870\n",
      "Epoch 5/10, Batch 519/883, Training Loss: 0.5774\n",
      "Epoch 5/10, Batch 520/883, Training Loss: 0.5724\n",
      "Epoch 5/10, Batch 521/883, Training Loss: 0.8660\n",
      "Epoch 5/10, Batch 522/883, Training Loss: 0.7104\n",
      "Epoch 5/10, Batch 523/883, Training Loss: 0.8371\n",
      "Epoch 5/10, Batch 524/883, Training Loss: 0.7677\n",
      "Epoch 5/10, Batch 525/883, Training Loss: 0.5818\n",
      "Epoch 5/10, Batch 526/883, Training Loss: 0.7574\n",
      "Epoch 5/10, Batch 527/883, Training Loss: 0.5092\n",
      "Epoch 5/10, Batch 528/883, Training Loss: 0.4816\n",
      "Epoch 5/10, Batch 529/883, Training Loss: 0.8302\n",
      "Epoch 5/10, Batch 530/883, Training Loss: 0.8115\n",
      "Epoch 5/10, Batch 531/883, Training Loss: 0.5980\n",
      "Epoch 5/10, Batch 532/883, Training Loss: 0.6179\n",
      "Epoch 5/10, Batch 533/883, Training Loss: 0.7619\n",
      "Epoch 5/10, Batch 534/883, Training Loss: 0.6619\n",
      "Epoch 5/10, Batch 535/883, Training Loss: 0.5204\n",
      "Epoch 5/10, Batch 536/883, Training Loss: 0.4929\n",
      "Epoch 5/10, Batch 537/883, Training Loss: 0.8616\n",
      "Epoch 5/10, Batch 538/883, Training Loss: 0.7771\n",
      "Epoch 5/10, Batch 539/883, Training Loss: 0.5471\n",
      "Epoch 5/10, Batch 540/883, Training Loss: 0.5438\n",
      "Epoch 5/10, Batch 541/883, Training Loss: 0.8915\n",
      "Epoch 5/10, Batch 542/883, Training Loss: 0.7813\n",
      "Epoch 5/10, Batch 543/883, Training Loss: 0.8498\n",
      "Epoch 5/10, Batch 544/883, Training Loss: 0.8023\n",
      "Epoch 5/10, Batch 545/883, Training Loss: 0.6063\n",
      "Epoch 5/10, Batch 546/883, Training Loss: 0.6790\n",
      "Epoch 5/10, Batch 547/883, Training Loss: 0.8443\n",
      "Epoch 5/10, Batch 548/883, Training Loss: 0.7253\n",
      "Epoch 5/10, Batch 549/883, Training Loss: 0.8800\n",
      "Epoch 5/10, Batch 550/883, Training Loss: 0.6881\n",
      "Epoch 5/10, Batch 551/883, Training Loss: 1.0766\n",
      "Epoch 5/10, Batch 552/883, Training Loss: 0.7905\n",
      "Epoch 5/10, Batch 553/883, Training Loss: 0.5813\n",
      "Epoch 5/10, Batch 554/883, Training Loss: 0.6617\n",
      "Epoch 5/10, Batch 555/883, Training Loss: 0.6446\n",
      "Epoch 5/10, Batch 556/883, Training Loss: 0.8671\n",
      "Epoch 5/10, Batch 557/883, Training Loss: 0.5813\n",
      "Epoch 5/10, Batch 558/883, Training Loss: 0.7760\n",
      "Epoch 5/10, Batch 559/883, Training Loss: 0.5446\n",
      "Epoch 5/10, Batch 560/883, Training Loss: 0.5936\n",
      "Epoch 5/10, Batch 561/883, Training Loss: 0.7066\n",
      "Epoch 5/10, Batch 562/883, Training Loss: 0.9386\n",
      "Epoch 5/10, Batch 563/883, Training Loss: 0.6229\n",
      "Epoch 5/10, Batch 564/883, Training Loss: 0.6776\n",
      "Epoch 5/10, Batch 565/883, Training Loss: 0.6280\n",
      "Epoch 5/10, Batch 566/883, Training Loss: 0.5919\n",
      "Epoch 5/10, Batch 567/883, Training Loss: 0.7636\n",
      "Epoch 5/10, Batch 568/883, Training Loss: 0.7067\n",
      "Epoch 5/10, Batch 569/883, Training Loss: 0.6734\n",
      "Epoch 5/10, Batch 570/883, Training Loss: 0.5388\n",
      "Epoch 5/10, Batch 571/883, Training Loss: 1.0102\n",
      "Epoch 5/10, Batch 572/883, Training Loss: 0.4669\n",
      "Epoch 5/10, Batch 573/883, Training Loss: 0.7744\n",
      "Epoch 5/10, Batch 574/883, Training Loss: 0.5186\n",
      "Epoch 5/10, Batch 575/883, Training Loss: 1.1706\n",
      "Epoch 5/10, Batch 576/883, Training Loss: 0.8640\n",
      "Epoch 5/10, Batch 577/883, Training Loss: 0.5824\n",
      "Epoch 5/10, Batch 578/883, Training Loss: 0.4080\n",
      "Epoch 5/10, Batch 579/883, Training Loss: 0.7197\n",
      "Epoch 5/10, Batch 580/883, Training Loss: 0.4522\n",
      "Epoch 5/10, Batch 581/883, Training Loss: 0.9293\n",
      "Epoch 5/10, Batch 582/883, Training Loss: 0.5172\n",
      "Epoch 5/10, Batch 583/883, Training Loss: 0.8308\n",
      "Epoch 5/10, Batch 584/883, Training Loss: 0.6736\n",
      "Epoch 5/10, Batch 585/883, Training Loss: 0.7052\n",
      "Epoch 5/10, Batch 586/883, Training Loss: 0.4707\n",
      "Epoch 5/10, Batch 587/883, Training Loss: 0.8115\n",
      "Epoch 5/10, Batch 588/883, Training Loss: 0.9393\n",
      "Epoch 5/10, Batch 589/883, Training Loss: 0.7223\n",
      "Epoch 5/10, Batch 590/883, Training Loss: 0.5997\n",
      "Epoch 5/10, Batch 591/883, Training Loss: 0.8809\n",
      "Epoch 5/10, Batch 592/883, Training Loss: 0.6962\n",
      "Epoch 5/10, Batch 593/883, Training Loss: 0.7844\n",
      "Epoch 5/10, Batch 594/883, Training Loss: 0.7032\n",
      "Epoch 5/10, Batch 595/883, Training Loss: 0.7210\n",
      "Epoch 5/10, Batch 596/883, Training Loss: 1.0484\n",
      "Epoch 5/10, Batch 597/883, Training Loss: 0.5647\n",
      "Epoch 5/10, Batch 598/883, Training Loss: 0.6288\n",
      "Epoch 5/10, Batch 599/883, Training Loss: 0.6972\n",
      "Epoch 5/10, Batch 600/883, Training Loss: 0.6796\n",
      "Epoch 5/10, Batch 601/883, Training Loss: 1.0535\n",
      "Epoch 5/10, Batch 602/883, Training Loss: 0.8228\n",
      "Epoch 5/10, Batch 603/883, Training Loss: 0.5716\n",
      "Epoch 5/10, Batch 604/883, Training Loss: 0.7093\n",
      "Epoch 5/10, Batch 605/883, Training Loss: 0.6716\n",
      "Epoch 5/10, Batch 606/883, Training Loss: 0.5597\n",
      "Epoch 5/10, Batch 607/883, Training Loss: 0.6719\n",
      "Epoch 5/10, Batch 608/883, Training Loss: 0.9152\n",
      "Epoch 5/10, Batch 609/883, Training Loss: 0.8061\n",
      "Epoch 5/10, Batch 610/883, Training Loss: 0.4904\n",
      "Epoch 5/10, Batch 611/883, Training Loss: 0.8725\n",
      "Epoch 5/10, Batch 612/883, Training Loss: 0.8089\n",
      "Epoch 5/10, Batch 613/883, Training Loss: 0.4233\n",
      "Epoch 5/10, Batch 614/883, Training Loss: 0.9271\n",
      "Epoch 5/10, Batch 615/883, Training Loss: 0.6930\n",
      "Epoch 5/10, Batch 616/883, Training Loss: 0.4689\n",
      "Epoch 5/10, Batch 617/883, Training Loss: 0.6412\n",
      "Epoch 5/10, Batch 618/883, Training Loss: 0.4657\n",
      "Epoch 5/10, Batch 619/883, Training Loss: 0.9342\n",
      "Epoch 5/10, Batch 620/883, Training Loss: 0.7685\n",
      "Epoch 5/10, Batch 621/883, Training Loss: 0.6745\n",
      "Epoch 5/10, Batch 622/883, Training Loss: 0.5296\n",
      "Epoch 5/10, Batch 623/883, Training Loss: 0.7170\n",
      "Epoch 5/10, Batch 624/883, Training Loss: 0.6812\n",
      "Epoch 5/10, Batch 625/883, Training Loss: 0.5960\n",
      "Epoch 5/10, Batch 626/883, Training Loss: 0.5788\n",
      "Epoch 5/10, Batch 627/883, Training Loss: 0.5615\n",
      "Epoch 5/10, Batch 628/883, Training Loss: 0.6342\n",
      "Epoch 5/10, Batch 629/883, Training Loss: 0.6903\n",
      "Epoch 5/10, Batch 630/883, Training Loss: 0.8387\n",
      "Epoch 5/10, Batch 631/883, Training Loss: 0.6111\n",
      "Epoch 5/10, Batch 632/883, Training Loss: 1.0591\n",
      "Epoch 5/10, Batch 633/883, Training Loss: 0.6149\n",
      "Epoch 5/10, Batch 634/883, Training Loss: 0.6867\n",
      "Epoch 5/10, Batch 635/883, Training Loss: 0.5404\n",
      "Epoch 5/10, Batch 636/883, Training Loss: 0.6607\n",
      "Epoch 5/10, Batch 637/883, Training Loss: 0.4602\n",
      "Epoch 5/10, Batch 638/883, Training Loss: 0.6144\n",
      "Epoch 5/10, Batch 639/883, Training Loss: 0.7528\n",
      "Epoch 5/10, Batch 640/883, Training Loss: 0.6081\n",
      "Epoch 5/10, Batch 641/883, Training Loss: 0.3959\n",
      "Epoch 5/10, Batch 642/883, Training Loss: 0.4472\n",
      "Epoch 5/10, Batch 643/883, Training Loss: 0.7523\n",
      "Epoch 5/10, Batch 644/883, Training Loss: 0.6760\n",
      "Epoch 5/10, Batch 645/883, Training Loss: 0.6545\n",
      "Epoch 5/10, Batch 646/883, Training Loss: 0.6362\n",
      "Epoch 5/10, Batch 647/883, Training Loss: 0.8701\n",
      "Epoch 5/10, Batch 648/883, Training Loss: 0.8108\n",
      "Epoch 5/10, Batch 649/883, Training Loss: 0.7021\n",
      "Epoch 5/10, Batch 650/883, Training Loss: 0.6756\n",
      "Epoch 5/10, Batch 651/883, Training Loss: 0.8710\n",
      "Epoch 5/10, Batch 652/883, Training Loss: 0.5962\n",
      "Epoch 5/10, Batch 653/883, Training Loss: 0.5046\n",
      "Epoch 5/10, Batch 654/883, Training Loss: 0.5829\n",
      "Epoch 5/10, Batch 655/883, Training Loss: 0.5302\n",
      "Epoch 5/10, Batch 656/883, Training Loss: 0.5259\n",
      "Epoch 5/10, Batch 657/883, Training Loss: 0.7344\n",
      "Epoch 5/10, Batch 658/883, Training Loss: 1.4186\n",
      "Epoch 5/10, Batch 659/883, Training Loss: 0.4340\n",
      "Epoch 5/10, Batch 660/883, Training Loss: 0.7977\n",
      "Epoch 5/10, Batch 661/883, Training Loss: 0.6260\n",
      "Epoch 5/10, Batch 662/883, Training Loss: 0.5160\n",
      "Epoch 5/10, Batch 663/883, Training Loss: 0.5621\n",
      "Epoch 5/10, Batch 664/883, Training Loss: 0.9355\n",
      "Epoch 5/10, Batch 665/883, Training Loss: 0.7347\n",
      "Epoch 5/10, Batch 666/883, Training Loss: 0.5906\n",
      "Epoch 5/10, Batch 667/883, Training Loss: 0.5331\n",
      "Epoch 5/10, Batch 668/883, Training Loss: 0.8438\n",
      "Epoch 5/10, Batch 669/883, Training Loss: 0.5605\n",
      "Epoch 5/10, Batch 670/883, Training Loss: 0.6170\n",
      "Epoch 5/10, Batch 671/883, Training Loss: 0.6661\n",
      "Epoch 5/10, Batch 672/883, Training Loss: 0.6266\n",
      "Epoch 5/10, Batch 673/883, Training Loss: 0.6723\n",
      "Epoch 5/10, Batch 674/883, Training Loss: 0.4888\n",
      "Epoch 5/10, Batch 675/883, Training Loss: 0.9707\n",
      "Epoch 5/10, Batch 676/883, Training Loss: 0.7898\n",
      "Epoch 5/10, Batch 677/883, Training Loss: 0.6427\n",
      "Epoch 5/10, Batch 678/883, Training Loss: 0.4509\n",
      "Epoch 5/10, Batch 679/883, Training Loss: 1.2197\n",
      "Epoch 5/10, Batch 680/883, Training Loss: 0.6237\n",
      "Epoch 5/10, Batch 681/883, Training Loss: 0.5473\n",
      "Epoch 5/10, Batch 682/883, Training Loss: 0.6076\n",
      "Epoch 5/10, Batch 683/883, Training Loss: 0.4649\n",
      "Epoch 5/10, Batch 684/883, Training Loss: 1.0522\n",
      "Epoch 5/10, Batch 685/883, Training Loss: 0.6040\n",
      "Epoch 5/10, Batch 686/883, Training Loss: 0.5343\n",
      "Epoch 5/10, Batch 687/883, Training Loss: 0.6099\n",
      "Epoch 5/10, Batch 688/883, Training Loss: 1.0862\n",
      "Epoch 5/10, Batch 689/883, Training Loss: 0.5349\n",
      "Epoch 5/10, Batch 690/883, Training Loss: 0.8774\n",
      "Epoch 5/10, Batch 691/883, Training Loss: 0.6710\n",
      "Epoch 5/10, Batch 692/883, Training Loss: 0.7787\n",
      "Epoch 5/10, Batch 693/883, Training Loss: 0.6238\n",
      "Epoch 5/10, Batch 694/883, Training Loss: 0.6906\n",
      "Epoch 5/10, Batch 695/883, Training Loss: 0.4915\n",
      "Epoch 5/10, Batch 696/883, Training Loss: 0.6552\n",
      "Epoch 5/10, Batch 697/883, Training Loss: 0.4794\n",
      "Epoch 5/10, Batch 698/883, Training Loss: 0.6226\n",
      "Epoch 5/10, Batch 699/883, Training Loss: 0.7427\n",
      "Epoch 5/10, Batch 700/883, Training Loss: 0.5615\n",
      "Epoch 5/10, Batch 701/883, Training Loss: 0.4443\n",
      "Epoch 5/10, Batch 702/883, Training Loss: 0.6727\n",
      "Epoch 5/10, Batch 703/883, Training Loss: 0.8454\n",
      "Epoch 5/10, Batch 704/883, Training Loss: 0.8615\n",
      "Epoch 5/10, Batch 705/883, Training Loss: 0.7404\n",
      "Epoch 5/10, Batch 706/883, Training Loss: 0.5155\n",
      "Epoch 5/10, Batch 707/883, Training Loss: 0.5248\n",
      "Epoch 5/10, Batch 708/883, Training Loss: 0.6205\n",
      "Epoch 5/10, Batch 709/883, Training Loss: 0.5607\n",
      "Epoch 5/10, Batch 710/883, Training Loss: 0.8278\n",
      "Epoch 5/10, Batch 711/883, Training Loss: 0.3770\n",
      "Epoch 5/10, Batch 712/883, Training Loss: 0.7117\n",
      "Epoch 5/10, Batch 713/883, Training Loss: 0.6473\n",
      "Epoch 5/10, Batch 714/883, Training Loss: 0.4158\n",
      "Epoch 5/10, Batch 715/883, Training Loss: 0.7140\n",
      "Epoch 5/10, Batch 716/883, Training Loss: 0.7942\n",
      "Epoch 5/10, Batch 717/883, Training Loss: 0.6643\n",
      "Epoch 5/10, Batch 718/883, Training Loss: 0.7052\n",
      "Epoch 5/10, Batch 719/883, Training Loss: 0.4561\n",
      "Epoch 5/10, Batch 720/883, Training Loss: 0.8278\n",
      "Epoch 5/10, Batch 721/883, Training Loss: 0.6643\n",
      "Epoch 5/10, Batch 722/883, Training Loss: 0.6205\n",
      "Epoch 5/10, Batch 723/883, Training Loss: 0.8644\n",
      "Epoch 5/10, Batch 724/883, Training Loss: 0.8559\n",
      "Epoch 5/10, Batch 725/883, Training Loss: 1.1966\n",
      "Epoch 5/10, Batch 726/883, Training Loss: 0.6236\n",
      "Epoch 5/10, Batch 727/883, Training Loss: 0.6637\n",
      "Epoch 5/10, Batch 728/883, Training Loss: 0.6184\n",
      "Epoch 5/10, Batch 729/883, Training Loss: 0.7211\n",
      "Epoch 5/10, Batch 730/883, Training Loss: 0.6334\n",
      "Epoch 5/10, Batch 731/883, Training Loss: 0.5022\n",
      "Epoch 5/10, Batch 732/883, Training Loss: 0.5961\n",
      "Epoch 5/10, Batch 733/883, Training Loss: 0.5693\n",
      "Epoch 5/10, Batch 734/883, Training Loss: 0.5823\n",
      "Epoch 5/10, Batch 735/883, Training Loss: 0.6514\n",
      "Epoch 5/10, Batch 736/883, Training Loss: 0.9204\n",
      "Epoch 5/10, Batch 737/883, Training Loss: 0.9090\n",
      "Epoch 5/10, Batch 738/883, Training Loss: 0.8207\n",
      "Epoch 5/10, Batch 739/883, Training Loss: 0.6433\n",
      "Epoch 5/10, Batch 740/883, Training Loss: 0.9071\n",
      "Epoch 5/10, Batch 741/883, Training Loss: 1.1841\n",
      "Epoch 5/10, Batch 742/883, Training Loss: 0.5694\n",
      "Epoch 5/10, Batch 743/883, Training Loss: 0.6541\n",
      "Epoch 5/10, Batch 744/883, Training Loss: 0.6269\n",
      "Epoch 5/10, Batch 745/883, Training Loss: 0.8345\n",
      "Epoch 5/10, Batch 746/883, Training Loss: 0.7515\n",
      "Epoch 5/10, Batch 747/883, Training Loss: 0.7525\n",
      "Epoch 5/10, Batch 748/883, Training Loss: 0.4874\n",
      "Epoch 5/10, Batch 749/883, Training Loss: 0.3928\n",
      "Epoch 5/10, Batch 750/883, Training Loss: 0.5351\n",
      "Epoch 5/10, Batch 751/883, Training Loss: 0.4724\n",
      "Epoch 5/10, Batch 752/883, Training Loss: 0.7191\n",
      "Epoch 5/10, Batch 753/883, Training Loss: 0.5443\n",
      "Epoch 5/10, Batch 754/883, Training Loss: 0.6844\n",
      "Epoch 5/10, Batch 755/883, Training Loss: 0.6283\n",
      "Epoch 5/10, Batch 756/883, Training Loss: 0.4906\n",
      "Epoch 5/10, Batch 757/883, Training Loss: 1.0309\n",
      "Epoch 5/10, Batch 758/883, Training Loss: 0.8837\n",
      "Epoch 5/10, Batch 759/883, Training Loss: 0.5703\n",
      "Epoch 5/10, Batch 760/883, Training Loss: 0.9721\n",
      "Epoch 5/10, Batch 761/883, Training Loss: 0.7431\n",
      "Epoch 5/10, Batch 762/883, Training Loss: 1.0317\n",
      "Epoch 5/10, Batch 763/883, Training Loss: 0.4834\n",
      "Epoch 5/10, Batch 764/883, Training Loss: 0.9890\n",
      "Epoch 5/10, Batch 765/883, Training Loss: 0.6370\n",
      "Epoch 5/10, Batch 766/883, Training Loss: 0.8014\n",
      "Epoch 5/10, Batch 767/883, Training Loss: 0.7973\n",
      "Epoch 5/10, Batch 768/883, Training Loss: 0.6875\n",
      "Epoch 5/10, Batch 769/883, Training Loss: 0.7279\n",
      "Epoch 5/10, Batch 770/883, Training Loss: 0.8515\n",
      "Epoch 5/10, Batch 771/883, Training Loss: 0.7925\n",
      "Epoch 5/10, Batch 772/883, Training Loss: 0.8040\n",
      "Epoch 5/10, Batch 773/883, Training Loss: 1.0125\n",
      "Epoch 5/10, Batch 774/883, Training Loss: 0.8925\n",
      "Epoch 5/10, Batch 775/883, Training Loss: 0.7816\n",
      "Epoch 5/10, Batch 776/883, Training Loss: 0.8211\n",
      "Epoch 5/10, Batch 777/883, Training Loss: 0.7264\n",
      "Epoch 5/10, Batch 778/883, Training Loss: 0.8013\n",
      "Epoch 5/10, Batch 779/883, Training Loss: 0.6280\n",
      "Epoch 5/10, Batch 780/883, Training Loss: 0.9834\n",
      "Epoch 5/10, Batch 781/883, Training Loss: 0.6810\n",
      "Epoch 5/10, Batch 782/883, Training Loss: 0.7487\n",
      "Epoch 5/10, Batch 783/883, Training Loss: 0.6408\n",
      "Epoch 5/10, Batch 784/883, Training Loss: 0.6900\n",
      "Epoch 5/10, Batch 785/883, Training Loss: 0.5659\n",
      "Epoch 5/10, Batch 786/883, Training Loss: 0.6048\n",
      "Epoch 5/10, Batch 787/883, Training Loss: 0.8292\n",
      "Epoch 5/10, Batch 788/883, Training Loss: 0.4887\n",
      "Epoch 5/10, Batch 789/883, Training Loss: 0.7399\n",
      "Epoch 5/10, Batch 790/883, Training Loss: 0.5517\n",
      "Epoch 5/10, Batch 791/883, Training Loss: 0.4974\n",
      "Epoch 5/10, Batch 792/883, Training Loss: 1.0182\n",
      "Epoch 5/10, Batch 793/883, Training Loss: 0.7044\n",
      "Epoch 5/10, Batch 794/883, Training Loss: 0.6436\n",
      "Epoch 5/10, Batch 795/883, Training Loss: 0.8458\n",
      "Epoch 5/10, Batch 796/883, Training Loss: 1.2249\n",
      "Epoch 5/10, Batch 797/883, Training Loss: 0.8415\n",
      "Epoch 5/10, Batch 798/883, Training Loss: 0.4998\n",
      "Epoch 5/10, Batch 799/883, Training Loss: 0.6934\n",
      "Epoch 5/10, Batch 800/883, Training Loss: 0.9951\n",
      "Epoch 5/10, Batch 801/883, Training Loss: 0.8097\n",
      "Epoch 5/10, Batch 802/883, Training Loss: 0.6980\n",
      "Epoch 5/10, Batch 803/883, Training Loss: 0.5029\n",
      "Epoch 5/10, Batch 804/883, Training Loss: 0.6660\n",
      "Epoch 5/10, Batch 805/883, Training Loss: 0.4509\n",
      "Epoch 5/10, Batch 806/883, Training Loss: 0.8799\n",
      "Epoch 5/10, Batch 807/883, Training Loss: 0.6405\n",
      "Epoch 5/10, Batch 808/883, Training Loss: 0.4167\n",
      "Epoch 5/10, Batch 809/883, Training Loss: 0.7419\n",
      "Epoch 5/10, Batch 810/883, Training Loss: 0.6790\n",
      "Epoch 5/10, Batch 811/883, Training Loss: 0.6754\n",
      "Epoch 5/10, Batch 812/883, Training Loss: 0.6694\n",
      "Epoch 5/10, Batch 813/883, Training Loss: 0.6647\n",
      "Epoch 5/10, Batch 814/883, Training Loss: 0.5891\n",
      "Epoch 5/10, Batch 815/883, Training Loss: 0.7905\n",
      "Epoch 5/10, Batch 816/883, Training Loss: 0.5594\n",
      "Epoch 5/10, Batch 817/883, Training Loss: 0.7499\n",
      "Epoch 5/10, Batch 818/883, Training Loss: 0.9007\n",
      "Epoch 5/10, Batch 819/883, Training Loss: 0.6155\n",
      "Epoch 5/10, Batch 820/883, Training Loss: 0.8681\n",
      "Epoch 5/10, Batch 821/883, Training Loss: 0.8576\n",
      "Epoch 5/10, Batch 822/883, Training Loss: 0.5288\n",
      "Epoch 5/10, Batch 823/883, Training Loss: 0.7365\n",
      "Epoch 5/10, Batch 824/883, Training Loss: 0.6995\n",
      "Epoch 5/10, Batch 825/883, Training Loss: 0.5629\n",
      "Epoch 5/10, Batch 826/883, Training Loss: 0.6894\n",
      "Epoch 5/10, Batch 827/883, Training Loss: 0.8612\n",
      "Epoch 5/10, Batch 828/883, Training Loss: 0.7564\n",
      "Epoch 5/10, Batch 829/883, Training Loss: 0.6388\n",
      "Epoch 5/10, Batch 830/883, Training Loss: 0.5322\n",
      "Epoch 5/10, Batch 831/883, Training Loss: 0.7469\n",
      "Epoch 5/10, Batch 832/883, Training Loss: 0.5413\n",
      "Epoch 5/10, Batch 833/883, Training Loss: 0.6600\n",
      "Epoch 5/10, Batch 834/883, Training Loss: 0.8686\n",
      "Epoch 5/10, Batch 835/883, Training Loss: 0.6783\n",
      "Epoch 5/10, Batch 836/883, Training Loss: 0.6170\n",
      "Epoch 5/10, Batch 837/883, Training Loss: 0.6364\n",
      "Epoch 5/10, Batch 838/883, Training Loss: 0.6284\n",
      "Epoch 5/10, Batch 839/883, Training Loss: 0.4666\n",
      "Epoch 5/10, Batch 840/883, Training Loss: 0.6057\n",
      "Epoch 5/10, Batch 841/883, Training Loss: 0.3891\n",
      "Epoch 5/10, Batch 842/883, Training Loss: 0.6732\n",
      "Epoch 5/10, Batch 843/883, Training Loss: 0.5828\n",
      "Epoch 5/10, Batch 844/883, Training Loss: 0.6965\n",
      "Epoch 5/10, Batch 845/883, Training Loss: 0.6277\n",
      "Epoch 5/10, Batch 846/883, Training Loss: 0.4456\n",
      "Epoch 5/10, Batch 847/883, Training Loss: 0.6302\n",
      "Epoch 5/10, Batch 848/883, Training Loss: 0.6184\n",
      "Epoch 5/10, Batch 849/883, Training Loss: 1.0881\n",
      "Epoch 5/10, Batch 850/883, Training Loss: 0.5832\n",
      "Epoch 5/10, Batch 851/883, Training Loss: 0.6835\n",
      "Epoch 5/10, Batch 852/883, Training Loss: 0.5540\n",
      "Epoch 5/10, Batch 853/883, Training Loss: 0.6498\n",
      "Epoch 5/10, Batch 854/883, Training Loss: 0.6068\n",
      "Epoch 5/10, Batch 855/883, Training Loss: 0.8754\n",
      "Epoch 5/10, Batch 856/883, Training Loss: 0.5953\n",
      "Epoch 5/10, Batch 857/883, Training Loss: 0.4557\n",
      "Epoch 5/10, Batch 858/883, Training Loss: 0.9791\n",
      "Epoch 5/10, Batch 859/883, Training Loss: 0.5702\n",
      "Epoch 5/10, Batch 860/883, Training Loss: 0.4877\n",
      "Epoch 5/10, Batch 861/883, Training Loss: 0.5869\n",
      "Epoch 5/10, Batch 862/883, Training Loss: 1.1658\n",
      "Epoch 5/10, Batch 863/883, Training Loss: 0.8162\n",
      "Epoch 5/10, Batch 864/883, Training Loss: 0.7619\n",
      "Epoch 5/10, Batch 865/883, Training Loss: 0.7492\n",
      "Epoch 5/10, Batch 866/883, Training Loss: 0.5961\n",
      "Epoch 5/10, Batch 867/883, Training Loss: 0.8139\n",
      "Epoch 5/10, Batch 868/883, Training Loss: 1.1662\n",
      "Epoch 5/10, Batch 869/883, Training Loss: 0.6963\n",
      "Epoch 5/10, Batch 870/883, Training Loss: 0.6499\n",
      "Epoch 5/10, Batch 871/883, Training Loss: 0.8995\n",
      "Epoch 5/10, Batch 872/883, Training Loss: 0.6973\n",
      "Epoch 5/10, Batch 873/883, Training Loss: 0.7040\n",
      "Epoch 5/10, Batch 874/883, Training Loss: 0.4425\n",
      "Epoch 5/10, Batch 875/883, Training Loss: 0.4772\n",
      "Epoch 5/10, Batch 876/883, Training Loss: 0.5516\n",
      "Epoch 5/10, Batch 877/883, Training Loss: 0.6161\n",
      "Epoch 5/10, Batch 878/883, Training Loss: 0.9709\n",
      "Epoch 5/10, Batch 879/883, Training Loss: 0.4739\n",
      "Epoch 5/10, Batch 880/883, Training Loss: 0.6192\n",
      "Epoch 5/10, Batch 881/883, Training Loss: 0.5330\n",
      "Epoch 5/10, Batch 882/883, Training Loss: 0.7912\n",
      "Epoch 5/10, Batch 883/883, Training Loss: 1.0330\n",
      "Epoch 5/10, Training Loss: 0.7123, Validation Loss: 0.7127, Validation Accuracy: 0.6651\n",
      "Epoch 6/10, Batch 1/883, Training Loss: 0.7975\n",
      "Epoch 6/10, Batch 2/883, Training Loss: 0.5944\n",
      "Epoch 6/10, Batch 3/883, Training Loss: 0.7200\n",
      "Epoch 6/10, Batch 4/883, Training Loss: 0.7564\n",
      "Epoch 6/10, Batch 5/883, Training Loss: 0.8470\n",
      "Epoch 6/10, Batch 6/883, Training Loss: 0.8593\n",
      "Epoch 6/10, Batch 7/883, Training Loss: 0.6810\n",
      "Epoch 6/10, Batch 8/883, Training Loss: 0.8327\n",
      "Epoch 6/10, Batch 9/883, Training Loss: 0.4515\n",
      "Epoch 6/10, Batch 10/883, Training Loss: 0.6417\n",
      "Epoch 6/10, Batch 11/883, Training Loss: 0.5250\n",
      "Epoch 6/10, Batch 12/883, Training Loss: 0.5532\n",
      "Epoch 6/10, Batch 13/883, Training Loss: 0.6641\n",
      "Epoch 6/10, Batch 14/883, Training Loss: 0.6187\n",
      "Epoch 6/10, Batch 15/883, Training Loss: 0.5914\n",
      "Epoch 6/10, Batch 16/883, Training Loss: 0.5811\n",
      "Epoch 6/10, Batch 17/883, Training Loss: 0.6690\n",
      "Epoch 6/10, Batch 18/883, Training Loss: 0.6371\n",
      "Epoch 6/10, Batch 19/883, Training Loss: 0.7635\n",
      "Epoch 6/10, Batch 20/883, Training Loss: 0.6631\n",
      "Epoch 6/10, Batch 21/883, Training Loss: 0.7605\n",
      "Epoch 6/10, Batch 22/883, Training Loss: 0.6974\n",
      "Epoch 6/10, Batch 23/883, Training Loss: 0.9976\n",
      "Epoch 6/10, Batch 24/883, Training Loss: 0.6246\n",
      "Epoch 6/10, Batch 25/883, Training Loss: 0.9363\n",
      "Epoch 6/10, Batch 26/883, Training Loss: 0.4404\n",
      "Epoch 6/10, Batch 27/883, Training Loss: 0.4857\n",
      "Epoch 6/10, Batch 28/883, Training Loss: 0.5423\n",
      "Epoch 6/10, Batch 29/883, Training Loss: 0.4720\n",
      "Epoch 6/10, Batch 30/883, Training Loss: 0.3837\n",
      "Epoch 6/10, Batch 31/883, Training Loss: 0.6123\n",
      "Epoch 6/10, Batch 32/883, Training Loss: 0.9869\n",
      "Epoch 6/10, Batch 33/883, Training Loss: 0.5019\n",
      "Epoch 6/10, Batch 34/883, Training Loss: 0.7946\n",
      "Epoch 6/10, Batch 35/883, Training Loss: 0.4207\n",
      "Epoch 6/10, Batch 36/883, Training Loss: 0.8437\n",
      "Epoch 6/10, Batch 37/883, Training Loss: 0.5080\n",
      "Epoch 6/10, Batch 38/883, Training Loss: 0.8330\n",
      "Epoch 6/10, Batch 39/883, Training Loss: 0.4480\n",
      "Epoch 6/10, Batch 40/883, Training Loss: 0.6550\n",
      "Epoch 6/10, Batch 41/883, Training Loss: 0.5016\n",
      "Epoch 6/10, Batch 42/883, Training Loss: 0.6005\n",
      "Epoch 6/10, Batch 43/883, Training Loss: 0.5887\n",
      "Epoch 6/10, Batch 44/883, Training Loss: 0.7254\n",
      "Epoch 6/10, Batch 45/883, Training Loss: 0.5422\n",
      "Epoch 6/10, Batch 46/883, Training Loss: 0.7979\n",
      "Epoch 6/10, Batch 47/883, Training Loss: 0.8051\n",
      "Epoch 6/10, Batch 48/883, Training Loss: 0.6807\n",
      "Epoch 6/10, Batch 49/883, Training Loss: 0.6176\n",
      "Epoch 6/10, Batch 50/883, Training Loss: 0.5233\n",
      "Epoch 6/10, Batch 51/883, Training Loss: 0.6858\n",
      "Epoch 6/10, Batch 52/883, Training Loss: 0.5340\n",
      "Epoch 6/10, Batch 53/883, Training Loss: 0.9231\n",
      "Epoch 6/10, Batch 54/883, Training Loss: 0.5863\n",
      "Epoch 6/10, Batch 55/883, Training Loss: 1.3122\n",
      "Epoch 6/10, Batch 56/883, Training Loss: 0.7499\n",
      "Epoch 6/10, Batch 57/883, Training Loss: 0.4466\n",
      "Epoch 6/10, Batch 58/883, Training Loss: 0.5446\n",
      "Epoch 6/10, Batch 59/883, Training Loss: 0.5579\n",
      "Epoch 6/10, Batch 60/883, Training Loss: 0.7105\n",
      "Epoch 6/10, Batch 61/883, Training Loss: 0.7805\n",
      "Epoch 6/10, Batch 62/883, Training Loss: 0.5127\n",
      "Epoch 6/10, Batch 63/883, Training Loss: 0.5121\n",
      "Epoch 6/10, Batch 64/883, Training Loss: 0.5892\n",
      "Epoch 6/10, Batch 65/883, Training Loss: 0.5943\n",
      "Epoch 6/10, Batch 66/883, Training Loss: 0.5569\n",
      "Epoch 6/10, Batch 67/883, Training Loss: 0.6485\n",
      "Epoch 6/10, Batch 68/883, Training Loss: 0.5369\n",
      "Epoch 6/10, Batch 69/883, Training Loss: 0.6631\n",
      "Epoch 6/10, Batch 70/883, Training Loss: 0.4987\n",
      "Epoch 6/10, Batch 71/883, Training Loss: 1.0868\n",
      "Epoch 6/10, Batch 72/883, Training Loss: 0.6592\n",
      "Epoch 6/10, Batch 73/883, Training Loss: 0.4880\n",
      "Epoch 6/10, Batch 74/883, Training Loss: 0.7658\n",
      "Epoch 6/10, Batch 75/883, Training Loss: 0.5723\n",
      "Epoch 6/10, Batch 76/883, Training Loss: 0.4560\n",
      "Epoch 6/10, Batch 77/883, Training Loss: 0.7938\n",
      "Epoch 6/10, Batch 78/883, Training Loss: 0.6393\n",
      "Epoch 6/10, Batch 79/883, Training Loss: 0.6397\n",
      "Epoch 6/10, Batch 80/883, Training Loss: 0.5430\n",
      "Epoch 6/10, Batch 81/883, Training Loss: 0.6049\n",
      "Epoch 6/10, Batch 82/883, Training Loss: 0.5437\n",
      "Epoch 6/10, Batch 83/883, Training Loss: 0.4079\n",
      "Epoch 6/10, Batch 84/883, Training Loss: 0.5088\n",
      "Epoch 6/10, Batch 85/883, Training Loss: 0.9115\n",
      "Epoch 6/10, Batch 86/883, Training Loss: 0.5032\n",
      "Epoch 6/10, Batch 87/883, Training Loss: 0.8222\n",
      "Epoch 6/10, Batch 88/883, Training Loss: 0.6986\n",
      "Epoch 6/10, Batch 89/883, Training Loss: 1.1086\n",
      "Epoch 6/10, Batch 90/883, Training Loss: 1.0207\n",
      "Epoch 6/10, Batch 91/883, Training Loss: 0.8220\n",
      "Epoch 6/10, Batch 92/883, Training Loss: 0.5904\n",
      "Epoch 6/10, Batch 93/883, Training Loss: 0.5150\n",
      "Epoch 6/10, Batch 94/883, Training Loss: 0.6208\n",
      "Epoch 6/10, Batch 95/883, Training Loss: 0.5982\n",
      "Epoch 6/10, Batch 96/883, Training Loss: 0.5410\n",
      "Epoch 6/10, Batch 97/883, Training Loss: 0.7923\n",
      "Epoch 6/10, Batch 98/883, Training Loss: 0.7179\n",
      "Epoch 6/10, Batch 99/883, Training Loss: 0.9393\n",
      "Epoch 6/10, Batch 100/883, Training Loss: 0.3780\n",
      "Epoch 6/10, Batch 101/883, Training Loss: 0.6608\n",
      "Epoch 6/10, Batch 102/883, Training Loss: 0.7050\n",
      "Epoch 6/10, Batch 103/883, Training Loss: 1.0940\n",
      "Epoch 6/10, Batch 104/883, Training Loss: 0.6786\n",
      "Epoch 6/10, Batch 105/883, Training Loss: 0.6563\n",
      "Epoch 6/10, Batch 106/883, Training Loss: 0.4307\n",
      "Epoch 6/10, Batch 107/883, Training Loss: 0.9096\n",
      "Epoch 6/10, Batch 108/883, Training Loss: 0.5599\n",
      "Epoch 6/10, Batch 109/883, Training Loss: 0.8505\n",
      "Epoch 6/10, Batch 110/883, Training Loss: 1.2139\n",
      "Epoch 6/10, Batch 111/883, Training Loss: 0.9364\n",
      "Epoch 6/10, Batch 112/883, Training Loss: 0.8673\n",
      "Epoch 6/10, Batch 113/883, Training Loss: 0.5996\n",
      "Epoch 6/10, Batch 114/883, Training Loss: 0.6717\n",
      "Epoch 6/10, Batch 115/883, Training Loss: 0.7066\n",
      "Epoch 6/10, Batch 116/883, Training Loss: 0.6982\n",
      "Epoch 6/10, Batch 117/883, Training Loss: 0.4437\n",
      "Epoch 6/10, Batch 118/883, Training Loss: 0.6123\n",
      "Epoch 6/10, Batch 119/883, Training Loss: 0.5978\n",
      "Epoch 6/10, Batch 120/883, Training Loss: 0.6240\n",
      "Epoch 6/10, Batch 121/883, Training Loss: 0.5569\n",
      "Epoch 6/10, Batch 122/883, Training Loss: 0.5793\n",
      "Epoch 6/10, Batch 123/883, Training Loss: 0.6424\n",
      "Epoch 6/10, Batch 124/883, Training Loss: 0.5819\n",
      "Epoch 6/10, Batch 125/883, Training Loss: 0.7961\n",
      "Epoch 6/10, Batch 126/883, Training Loss: 0.7918\n",
      "Epoch 6/10, Batch 127/883, Training Loss: 0.9378\n",
      "Epoch 6/10, Batch 128/883, Training Loss: 1.1070\n",
      "Epoch 6/10, Batch 129/883, Training Loss: 0.3823\n",
      "Epoch 6/10, Batch 130/883, Training Loss: 0.6537\n",
      "Epoch 6/10, Batch 131/883, Training Loss: 0.5212\n",
      "Epoch 6/10, Batch 132/883, Training Loss: 0.3292\n",
      "Epoch 6/10, Batch 133/883, Training Loss: 0.7466\n",
      "Epoch 6/10, Batch 134/883, Training Loss: 1.4683\n",
      "Epoch 6/10, Batch 135/883, Training Loss: 0.6187\n",
      "Epoch 6/10, Batch 136/883, Training Loss: 1.1171\n",
      "Epoch 6/10, Batch 137/883, Training Loss: 1.0343\n",
      "Epoch 6/10, Batch 138/883, Training Loss: 0.5687\n",
      "Epoch 6/10, Batch 139/883, Training Loss: 0.5283\n",
      "Epoch 6/10, Batch 140/883, Training Loss: 0.4300\n",
      "Epoch 6/10, Batch 141/883, Training Loss: 0.7395\n",
      "Epoch 6/10, Batch 142/883, Training Loss: 0.5934\n",
      "Epoch 6/10, Batch 143/883, Training Loss: 0.6256\n",
      "Epoch 6/10, Batch 144/883, Training Loss: 0.4819\n",
      "Epoch 6/10, Batch 145/883, Training Loss: 0.7740\n",
      "Epoch 6/10, Batch 146/883, Training Loss: 0.6565\n",
      "Epoch 6/10, Batch 147/883, Training Loss: 0.7163\n",
      "Epoch 6/10, Batch 148/883, Training Loss: 0.6441\n",
      "Epoch 6/10, Batch 149/883, Training Loss: 0.7446\n",
      "Epoch 6/10, Batch 150/883, Training Loss: 0.5428\n",
      "Epoch 6/10, Batch 151/883, Training Loss: 0.5692\n",
      "Epoch 6/10, Batch 152/883, Training Loss: 0.5291\n",
      "Epoch 6/10, Batch 153/883, Training Loss: 0.4864\n",
      "Epoch 6/10, Batch 154/883, Training Loss: 0.7215\n",
      "Epoch 6/10, Batch 155/883, Training Loss: 0.7296\n",
      "Epoch 6/10, Batch 156/883, Training Loss: 0.5344\n",
      "Epoch 6/10, Batch 157/883, Training Loss: 0.6472\n",
      "Epoch 6/10, Batch 158/883, Training Loss: 0.4841\n",
      "Epoch 6/10, Batch 159/883, Training Loss: 0.4154\n",
      "Epoch 6/10, Batch 160/883, Training Loss: 0.6291\n",
      "Epoch 6/10, Batch 161/883, Training Loss: 0.6842\n",
      "Epoch 6/10, Batch 162/883, Training Loss: 0.7659\n",
      "Epoch 6/10, Batch 163/883, Training Loss: 0.6339\n",
      "Epoch 6/10, Batch 164/883, Training Loss: 0.7571\n",
      "Epoch 6/10, Batch 165/883, Training Loss: 0.6749\n",
      "Epoch 6/10, Batch 166/883, Training Loss: 0.6506\n",
      "Epoch 6/10, Batch 167/883, Training Loss: 0.6501\n",
      "Epoch 6/10, Batch 168/883, Training Loss: 0.5279\n",
      "Epoch 6/10, Batch 169/883, Training Loss: 0.6925\n",
      "Epoch 6/10, Batch 170/883, Training Loss: 0.3778\n",
      "Epoch 6/10, Batch 171/883, Training Loss: 0.9532\n",
      "Epoch 6/10, Batch 172/883, Training Loss: 0.7035\n",
      "Epoch 6/10, Batch 173/883, Training Loss: 0.6121\n",
      "Epoch 6/10, Batch 174/883, Training Loss: 1.0538\n",
      "Epoch 6/10, Batch 175/883, Training Loss: 0.6435\n",
      "Epoch 6/10, Batch 176/883, Training Loss: 0.5422\n",
      "Epoch 6/10, Batch 177/883, Training Loss: 0.6042\n",
      "Epoch 6/10, Batch 178/883, Training Loss: 0.5870\n",
      "Epoch 6/10, Batch 179/883, Training Loss: 0.4150\n",
      "Epoch 6/10, Batch 180/883, Training Loss: 0.7234\n",
      "Epoch 6/10, Batch 181/883, Training Loss: 0.5972\n",
      "Epoch 6/10, Batch 182/883, Training Loss: 0.8346\n",
      "Epoch 6/10, Batch 183/883, Training Loss: 0.8220\n",
      "Epoch 6/10, Batch 184/883, Training Loss: 0.8893\n",
      "Epoch 6/10, Batch 185/883, Training Loss: 0.6891\n",
      "Epoch 6/10, Batch 186/883, Training Loss: 0.5412\n",
      "Epoch 6/10, Batch 187/883, Training Loss: 0.8834\n",
      "Epoch 6/10, Batch 188/883, Training Loss: 0.7504\n",
      "Epoch 6/10, Batch 189/883, Training Loss: 0.4861\n",
      "Epoch 6/10, Batch 190/883, Training Loss: 0.7565\n",
      "Epoch 6/10, Batch 191/883, Training Loss: 0.7161\n",
      "Epoch 6/10, Batch 192/883, Training Loss: 0.9456\n",
      "Epoch 6/10, Batch 193/883, Training Loss: 0.5656\n",
      "Epoch 6/10, Batch 194/883, Training Loss: 0.6650\n",
      "Epoch 6/10, Batch 195/883, Training Loss: 1.0114\n",
      "Epoch 6/10, Batch 196/883, Training Loss: 0.4732\n",
      "Epoch 6/10, Batch 197/883, Training Loss: 0.6744\n",
      "Epoch 6/10, Batch 198/883, Training Loss: 0.8825\n",
      "Epoch 6/10, Batch 199/883, Training Loss: 0.6868\n",
      "Epoch 6/10, Batch 200/883, Training Loss: 0.3976\n",
      "Epoch 6/10, Batch 201/883, Training Loss: 0.4128\n",
      "Epoch 6/10, Batch 202/883, Training Loss: 0.4257\n",
      "Epoch 6/10, Batch 203/883, Training Loss: 0.6316\n",
      "Epoch 6/10, Batch 204/883, Training Loss: 0.6336\n",
      "Epoch 6/10, Batch 205/883, Training Loss: 0.5242\n",
      "Epoch 6/10, Batch 206/883, Training Loss: 0.6926\n",
      "Epoch 6/10, Batch 207/883, Training Loss: 0.5919\n",
      "Epoch 6/10, Batch 208/883, Training Loss: 0.3708\n",
      "Epoch 6/10, Batch 209/883, Training Loss: 0.4942\n",
      "Epoch 6/10, Batch 210/883, Training Loss: 0.6392\n",
      "Epoch 6/10, Batch 211/883, Training Loss: 0.8360\n",
      "Epoch 6/10, Batch 212/883, Training Loss: 0.6649\n",
      "Epoch 6/10, Batch 213/883, Training Loss: 0.6968\n",
      "Epoch 6/10, Batch 214/883, Training Loss: 0.7867\n",
      "Epoch 6/10, Batch 215/883, Training Loss: 0.8164\n",
      "Epoch 6/10, Batch 216/883, Training Loss: 0.8402\n",
      "Epoch 6/10, Batch 217/883, Training Loss: 0.7665\n",
      "Epoch 6/10, Batch 218/883, Training Loss: 0.6686\n",
      "Epoch 6/10, Batch 219/883, Training Loss: 0.4783\n",
      "Epoch 6/10, Batch 220/883, Training Loss: 0.9276\n",
      "Epoch 6/10, Batch 221/883, Training Loss: 0.8482\n",
      "Epoch 6/10, Batch 222/883, Training Loss: 0.6096\n",
      "Epoch 6/10, Batch 223/883, Training Loss: 0.6212\n",
      "Epoch 6/10, Batch 224/883, Training Loss: 0.5067\n",
      "Epoch 6/10, Batch 225/883, Training Loss: 0.6832\n",
      "Epoch 6/10, Batch 226/883, Training Loss: 0.6062\n",
      "Epoch 6/10, Batch 227/883, Training Loss: 0.7600\n",
      "Epoch 6/10, Batch 228/883, Training Loss: 0.5613\n",
      "Epoch 6/10, Batch 229/883, Training Loss: 0.6237\n",
      "Epoch 6/10, Batch 230/883, Training Loss: 0.7456\n",
      "Epoch 6/10, Batch 231/883, Training Loss: 0.7221\n",
      "Epoch 6/10, Batch 232/883, Training Loss: 0.8915\n",
      "Epoch 6/10, Batch 233/883, Training Loss: 0.5894\n",
      "Epoch 6/10, Batch 234/883, Training Loss: 0.7060\n",
      "Epoch 6/10, Batch 235/883, Training Loss: 0.7737\n",
      "Epoch 6/10, Batch 236/883, Training Loss: 0.8932\n",
      "Epoch 6/10, Batch 237/883, Training Loss: 0.7264\n",
      "Epoch 6/10, Batch 238/883, Training Loss: 0.5645\n",
      "Epoch 6/10, Batch 239/883, Training Loss: 0.8971\n",
      "Epoch 6/10, Batch 240/883, Training Loss: 0.6028\n",
      "Epoch 6/10, Batch 241/883, Training Loss: 0.9305\n",
      "Epoch 6/10, Batch 242/883, Training Loss: 0.4076\n",
      "Epoch 6/10, Batch 243/883, Training Loss: 0.6297\n",
      "Epoch 6/10, Batch 244/883, Training Loss: 0.7323\n",
      "Epoch 6/10, Batch 245/883, Training Loss: 1.0686\n",
      "Epoch 6/10, Batch 246/883, Training Loss: 0.6115\n",
      "Epoch 6/10, Batch 247/883, Training Loss: 0.5586\n",
      "Epoch 6/10, Batch 248/883, Training Loss: 0.6087\n",
      "Epoch 6/10, Batch 249/883, Training Loss: 0.6066\n",
      "Epoch 6/10, Batch 250/883, Training Loss: 0.9781\n",
      "Epoch 6/10, Batch 251/883, Training Loss: 0.6290\n",
      "Epoch 6/10, Batch 252/883, Training Loss: 0.8465\n",
      "Epoch 6/10, Batch 253/883, Training Loss: 0.5854\n",
      "Epoch 6/10, Batch 254/883, Training Loss: 0.7957\n",
      "Epoch 6/10, Batch 255/883, Training Loss: 0.6079\n",
      "Epoch 6/10, Batch 256/883, Training Loss: 0.8058\n",
      "Epoch 6/10, Batch 257/883, Training Loss: 0.6160\n",
      "Epoch 6/10, Batch 258/883, Training Loss: 0.6205\n",
      "Epoch 6/10, Batch 259/883, Training Loss: 0.4849\n",
      "Epoch 6/10, Batch 260/883, Training Loss: 0.4079\n",
      "Epoch 6/10, Batch 261/883, Training Loss: 0.5905\n",
      "Epoch 6/10, Batch 262/883, Training Loss: 0.5205\n",
      "Epoch 6/10, Batch 263/883, Training Loss: 0.5939\n",
      "Epoch 6/10, Batch 264/883, Training Loss: 0.8582\n",
      "Epoch 6/10, Batch 265/883, Training Loss: 0.9110\n",
      "Epoch 6/10, Batch 266/883, Training Loss: 0.5905\n",
      "Epoch 6/10, Batch 267/883, Training Loss: 0.8436\n",
      "Epoch 6/10, Batch 268/883, Training Loss: 0.6640\n",
      "Epoch 6/10, Batch 269/883, Training Loss: 0.5935\n",
      "Epoch 6/10, Batch 270/883, Training Loss: 0.4422\n",
      "Epoch 6/10, Batch 271/883, Training Loss: 0.5284\n",
      "Epoch 6/10, Batch 272/883, Training Loss: 0.8928\n",
      "Epoch 6/10, Batch 273/883, Training Loss: 0.7293\n",
      "Epoch 6/10, Batch 274/883, Training Loss: 0.4396\n",
      "Epoch 6/10, Batch 275/883, Training Loss: 0.6723\n",
      "Epoch 6/10, Batch 276/883, Training Loss: 0.3982\n",
      "Epoch 6/10, Batch 277/883, Training Loss: 0.5465\n",
      "Epoch 6/10, Batch 278/883, Training Loss: 0.4711\n",
      "Epoch 6/10, Batch 279/883, Training Loss: 0.8188\n",
      "Epoch 6/10, Batch 280/883, Training Loss: 0.8315\n",
      "Epoch 6/10, Batch 281/883, Training Loss: 0.7530\n",
      "Epoch 6/10, Batch 282/883, Training Loss: 0.6849\n",
      "Epoch 6/10, Batch 283/883, Training Loss: 0.6826\n",
      "Epoch 6/10, Batch 284/883, Training Loss: 0.7431\n",
      "Epoch 6/10, Batch 285/883, Training Loss: 0.8171\n",
      "Epoch 6/10, Batch 286/883, Training Loss: 0.5326\n",
      "Epoch 6/10, Batch 287/883, Training Loss: 0.5832\n",
      "Epoch 6/10, Batch 288/883, Training Loss: 0.6971\n",
      "Epoch 6/10, Batch 289/883, Training Loss: 1.0367\n",
      "Epoch 6/10, Batch 290/883, Training Loss: 0.5103\n",
      "Epoch 6/10, Batch 291/883, Training Loss: 0.7697\n",
      "Epoch 6/10, Batch 292/883, Training Loss: 0.6245\n",
      "Epoch 6/10, Batch 293/883, Training Loss: 0.6307\n",
      "Epoch 6/10, Batch 294/883, Training Loss: 0.5763\n",
      "Epoch 6/10, Batch 295/883, Training Loss: 0.3499\n",
      "Epoch 6/10, Batch 296/883, Training Loss: 0.7378\n",
      "Epoch 6/10, Batch 297/883, Training Loss: 0.4779\n",
      "Epoch 6/10, Batch 298/883, Training Loss: 0.5823\n",
      "Epoch 6/10, Batch 299/883, Training Loss: 0.3806\n",
      "Epoch 6/10, Batch 300/883, Training Loss: 0.5491\n",
      "Epoch 6/10, Batch 301/883, Training Loss: 0.6883\n",
      "Epoch 6/10, Batch 302/883, Training Loss: 1.1155\n",
      "Epoch 6/10, Batch 303/883, Training Loss: 0.6037\n",
      "Epoch 6/10, Batch 304/883, Training Loss: 0.5329\n",
      "Epoch 6/10, Batch 305/883, Training Loss: 0.4601\n",
      "Epoch 6/10, Batch 306/883, Training Loss: 0.6414\n",
      "Epoch 6/10, Batch 307/883, Training Loss: 0.4907\n",
      "Epoch 6/10, Batch 308/883, Training Loss: 0.5901\n",
      "Epoch 6/10, Batch 309/883, Training Loss: 0.5704\n",
      "Epoch 6/10, Batch 310/883, Training Loss: 0.7668\n",
      "Epoch 6/10, Batch 311/883, Training Loss: 0.5555\n",
      "Epoch 6/10, Batch 312/883, Training Loss: 1.0393\n",
      "Epoch 6/10, Batch 313/883, Training Loss: 0.6963\n",
      "Epoch 6/10, Batch 314/883, Training Loss: 0.4917\n",
      "Epoch 6/10, Batch 315/883, Training Loss: 0.7243\n",
      "Epoch 6/10, Batch 316/883, Training Loss: 0.6008\n",
      "Epoch 6/10, Batch 317/883, Training Loss: 0.7465\n",
      "Epoch 6/10, Batch 318/883, Training Loss: 0.7094\n",
      "Epoch 6/10, Batch 319/883, Training Loss: 0.6999\n",
      "Epoch 6/10, Batch 320/883, Training Loss: 0.5938\n",
      "Epoch 6/10, Batch 321/883, Training Loss: 0.4591\n",
      "Epoch 6/10, Batch 322/883, Training Loss: 0.6114\n",
      "Epoch 6/10, Batch 323/883, Training Loss: 0.9261\n",
      "Epoch 6/10, Batch 324/883, Training Loss: 0.3547\n",
      "Epoch 6/10, Batch 325/883, Training Loss: 0.5748\n",
      "Epoch 6/10, Batch 326/883, Training Loss: 1.3027\n",
      "Epoch 6/10, Batch 327/883, Training Loss: 0.4870\n",
      "Epoch 6/10, Batch 328/883, Training Loss: 0.8001\n",
      "Epoch 6/10, Batch 329/883, Training Loss: 0.8179\n",
      "Epoch 6/10, Batch 330/883, Training Loss: 0.9325\n",
      "Epoch 6/10, Batch 331/883, Training Loss: 0.5305\n",
      "Epoch 6/10, Batch 332/883, Training Loss: 0.5020\n",
      "Epoch 6/10, Batch 333/883, Training Loss: 0.8495\n",
      "Epoch 6/10, Batch 334/883, Training Loss: 0.7382\n",
      "Epoch 6/10, Batch 335/883, Training Loss: 0.5602\n",
      "Epoch 6/10, Batch 336/883, Training Loss: 0.6749\n",
      "Epoch 6/10, Batch 337/883, Training Loss: 1.2200\n",
      "Epoch 6/10, Batch 338/883, Training Loss: 0.3743\n",
      "Epoch 6/10, Batch 339/883, Training Loss: 0.4619\n",
      "Epoch 6/10, Batch 340/883, Training Loss: 0.5331\n",
      "Epoch 6/10, Batch 341/883, Training Loss: 0.5336\n",
      "Epoch 6/10, Batch 342/883, Training Loss: 0.4400\n",
      "Epoch 6/10, Batch 343/883, Training Loss: 0.6270\n",
      "Epoch 6/10, Batch 344/883, Training Loss: 0.5100\n",
      "Epoch 6/10, Batch 345/883, Training Loss: 0.5002\n",
      "Epoch 6/10, Batch 346/883, Training Loss: 0.6745\n",
      "Epoch 6/10, Batch 347/883, Training Loss: 0.7058\n",
      "Epoch 6/10, Batch 348/883, Training Loss: 0.8112\n",
      "Epoch 6/10, Batch 349/883, Training Loss: 0.7382\n",
      "Epoch 6/10, Batch 350/883, Training Loss: 0.8429\n",
      "Epoch 6/10, Batch 351/883, Training Loss: 0.4043\n",
      "Epoch 6/10, Batch 352/883, Training Loss: 0.5125\n",
      "Epoch 6/10, Batch 353/883, Training Loss: 0.6538\n",
      "Epoch 6/10, Batch 354/883, Training Loss: 0.6736\n",
      "Epoch 6/10, Batch 355/883, Training Loss: 0.4035\n",
      "Epoch 6/10, Batch 356/883, Training Loss: 0.8441\n",
      "Epoch 6/10, Batch 357/883, Training Loss: 0.5408\n",
      "Epoch 6/10, Batch 358/883, Training Loss: 0.6833\n",
      "Epoch 6/10, Batch 359/883, Training Loss: 0.3463\n",
      "Epoch 6/10, Batch 360/883, Training Loss: 0.4434\n",
      "Epoch 6/10, Batch 361/883, Training Loss: 0.7313\n",
      "Epoch 6/10, Batch 362/883, Training Loss: 0.7898\n",
      "Epoch 6/10, Batch 363/883, Training Loss: 0.7003\n",
      "Epoch 6/10, Batch 364/883, Training Loss: 0.4831\n",
      "Epoch 6/10, Batch 365/883, Training Loss: 0.5177\n",
      "Epoch 6/10, Batch 366/883, Training Loss: 0.6309\n",
      "Epoch 6/10, Batch 367/883, Training Loss: 0.5452\n",
      "Epoch 6/10, Batch 368/883, Training Loss: 0.4356\n",
      "Epoch 6/10, Batch 369/883, Training Loss: 0.9225\n",
      "Epoch 6/10, Batch 370/883, Training Loss: 0.8807\n",
      "Epoch 6/10, Batch 371/883, Training Loss: 0.4926\n",
      "Epoch 6/10, Batch 372/883, Training Loss: 0.6712\n",
      "Epoch 6/10, Batch 373/883, Training Loss: 0.6884\n",
      "Epoch 6/10, Batch 374/883, Training Loss: 0.5876\n",
      "Epoch 6/10, Batch 375/883, Training Loss: 0.7366\n",
      "Epoch 6/10, Batch 376/883, Training Loss: 0.8579\n",
      "Epoch 6/10, Batch 377/883, Training Loss: 0.5625\n",
      "Epoch 6/10, Batch 378/883, Training Loss: 0.5922\n",
      "Epoch 6/10, Batch 379/883, Training Loss: 0.4614\n",
      "Epoch 6/10, Batch 380/883, Training Loss: 0.7667\n",
      "Epoch 6/10, Batch 381/883, Training Loss: 0.5303\n",
      "Epoch 6/10, Batch 382/883, Training Loss: 0.7343\n",
      "Epoch 6/10, Batch 383/883, Training Loss: 0.6754\n",
      "Epoch 6/10, Batch 384/883, Training Loss: 0.6778\n",
      "Epoch 6/10, Batch 385/883, Training Loss: 0.4864\n",
      "Epoch 6/10, Batch 386/883, Training Loss: 0.6068\n",
      "Epoch 6/10, Batch 387/883, Training Loss: 0.6684\n",
      "Epoch 6/10, Batch 388/883, Training Loss: 0.8063\n",
      "Epoch 6/10, Batch 389/883, Training Loss: 0.6745\n",
      "Epoch 6/10, Batch 390/883, Training Loss: 0.8269\n",
      "Epoch 6/10, Batch 391/883, Training Loss: 0.4932\n",
      "Epoch 6/10, Batch 392/883, Training Loss: 0.4629\n",
      "Epoch 6/10, Batch 393/883, Training Loss: 0.6613\n",
      "Epoch 6/10, Batch 394/883, Training Loss: 0.7775\n",
      "Epoch 6/10, Batch 395/883, Training Loss: 0.6805\n",
      "Epoch 6/10, Batch 396/883, Training Loss: 0.8536\n",
      "Epoch 6/10, Batch 397/883, Training Loss: 1.1177\n",
      "Epoch 6/10, Batch 398/883, Training Loss: 1.1471\n",
      "Epoch 6/10, Batch 399/883, Training Loss: 0.7821\n",
      "Epoch 6/10, Batch 400/883, Training Loss: 0.5698\n",
      "Epoch 6/10, Batch 401/883, Training Loss: 0.6427\n",
      "Epoch 6/10, Batch 402/883, Training Loss: 0.6319\n",
      "Epoch 6/10, Batch 403/883, Training Loss: 0.5662\n",
      "Epoch 6/10, Batch 404/883, Training Loss: 0.7809\n",
      "Epoch 6/10, Batch 405/883, Training Loss: 0.6185\n",
      "Epoch 6/10, Batch 406/883, Training Loss: 0.9065\n",
      "Epoch 6/10, Batch 407/883, Training Loss: 0.4405\n",
      "Epoch 6/10, Batch 408/883, Training Loss: 0.7481\n",
      "Epoch 6/10, Batch 409/883, Training Loss: 0.9805\n",
      "Epoch 6/10, Batch 410/883, Training Loss: 0.6134\n",
      "Epoch 6/10, Batch 411/883, Training Loss: 0.5472\n",
      "Epoch 6/10, Batch 412/883, Training Loss: 0.6348\n",
      "Epoch 6/10, Batch 413/883, Training Loss: 0.6297\n",
      "Epoch 6/10, Batch 414/883, Training Loss: 0.8036\n",
      "Epoch 6/10, Batch 415/883, Training Loss: 0.8420\n",
      "Epoch 6/10, Batch 416/883, Training Loss: 1.0131\n",
      "Epoch 6/10, Batch 417/883, Training Loss: 0.6403\n",
      "Epoch 6/10, Batch 418/883, Training Loss: 0.4091\n",
      "Epoch 6/10, Batch 419/883, Training Loss: 0.6284\n",
      "Epoch 6/10, Batch 420/883, Training Loss: 0.7015\n",
      "Epoch 6/10, Batch 421/883, Training Loss: 0.7447\n",
      "Epoch 6/10, Batch 422/883, Training Loss: 0.6149\n",
      "Epoch 6/10, Batch 423/883, Training Loss: 0.9442\n",
      "Epoch 6/10, Batch 424/883, Training Loss: 0.9678\n",
      "Epoch 6/10, Batch 425/883, Training Loss: 0.6684\n",
      "Epoch 6/10, Batch 426/883, Training Loss: 0.6758\n",
      "Epoch 6/10, Batch 427/883, Training Loss: 0.5975\n",
      "Epoch 6/10, Batch 428/883, Training Loss: 0.5212\n",
      "Epoch 6/10, Batch 429/883, Training Loss: 0.9147\n",
      "Epoch 6/10, Batch 430/883, Training Loss: 0.5103\n",
      "Epoch 6/10, Batch 431/883, Training Loss: 0.7549\n",
      "Epoch 6/10, Batch 432/883, Training Loss: 0.6659\n",
      "Epoch 6/10, Batch 433/883, Training Loss: 0.6504\n",
      "Epoch 6/10, Batch 434/883, Training Loss: 1.0625\n",
      "Epoch 6/10, Batch 435/883, Training Loss: 0.7420\n",
      "Epoch 6/10, Batch 436/883, Training Loss: 0.8660\n",
      "Epoch 6/10, Batch 437/883, Training Loss: 0.7534\n",
      "Epoch 6/10, Batch 438/883, Training Loss: 0.6372\n",
      "Epoch 6/10, Batch 439/883, Training Loss: 0.8942\n",
      "Epoch 6/10, Batch 440/883, Training Loss: 0.7073\n",
      "Epoch 6/10, Batch 441/883, Training Loss: 0.5615\n",
      "Epoch 6/10, Batch 442/883, Training Loss: 0.7222\n",
      "Epoch 6/10, Batch 443/883, Training Loss: 0.5370\n",
      "Epoch 6/10, Batch 444/883, Training Loss: 0.6511\n",
      "Epoch 6/10, Batch 445/883, Training Loss: 0.9662\n",
      "Epoch 6/10, Batch 446/883, Training Loss: 0.4684\n",
      "Epoch 6/10, Batch 447/883, Training Loss: 0.7090\n",
      "Epoch 6/10, Batch 448/883, Training Loss: 0.4870\n",
      "Epoch 6/10, Batch 449/883, Training Loss: 0.9267\n",
      "Epoch 6/10, Batch 450/883, Training Loss: 0.8461\n",
      "Epoch 6/10, Batch 451/883, Training Loss: 0.8235\n",
      "Epoch 6/10, Batch 452/883, Training Loss: 0.7640\n",
      "Epoch 6/10, Batch 453/883, Training Loss: 0.5855\n",
      "Epoch 6/10, Batch 454/883, Training Loss: 0.5001\n",
      "Epoch 6/10, Batch 455/883, Training Loss: 0.6253\n",
      "Epoch 6/10, Batch 456/883, Training Loss: 0.5590\n",
      "Epoch 6/10, Batch 457/883, Training Loss: 0.6514\n",
      "Epoch 6/10, Batch 458/883, Training Loss: 0.9995\n",
      "Epoch 6/10, Batch 459/883, Training Loss: 0.5662\n",
      "Epoch 6/10, Batch 460/883, Training Loss: 0.9262\n",
      "Epoch 6/10, Batch 461/883, Training Loss: 1.0933\n",
      "Epoch 6/10, Batch 462/883, Training Loss: 0.6550\n",
      "Epoch 6/10, Batch 463/883, Training Loss: 0.9322\n",
      "Epoch 6/10, Batch 464/883, Training Loss: 0.6623\n",
      "Epoch 6/10, Batch 465/883, Training Loss: 0.6579\n",
      "Epoch 6/10, Batch 466/883, Training Loss: 0.6957\n",
      "Epoch 6/10, Batch 467/883, Training Loss: 0.5519\n",
      "Epoch 6/10, Batch 468/883, Training Loss: 0.6159\n",
      "Epoch 6/10, Batch 469/883, Training Loss: 0.6937\n",
      "Epoch 6/10, Batch 470/883, Training Loss: 0.7550\n",
      "Epoch 6/10, Batch 471/883, Training Loss: 0.6807\n",
      "Epoch 6/10, Batch 472/883, Training Loss: 0.5745\n",
      "Epoch 6/10, Batch 473/883, Training Loss: 0.6221\n",
      "Epoch 6/10, Batch 474/883, Training Loss: 0.9177\n",
      "Epoch 6/10, Batch 475/883, Training Loss: 0.6679\n",
      "Epoch 6/10, Batch 476/883, Training Loss: 0.5683\n",
      "Epoch 6/10, Batch 477/883, Training Loss: 0.5826\n",
      "Epoch 6/10, Batch 478/883, Training Loss: 0.4760\n",
      "Epoch 6/10, Batch 479/883, Training Loss: 0.6675\n",
      "Epoch 6/10, Batch 480/883, Training Loss: 0.6188\n",
      "Epoch 6/10, Batch 481/883, Training Loss: 0.9979\n",
      "Epoch 6/10, Batch 482/883, Training Loss: 0.5538\n",
      "Epoch 6/10, Batch 483/883, Training Loss: 0.6518\n",
      "Epoch 6/10, Batch 484/883, Training Loss: 0.5820\n",
      "Epoch 6/10, Batch 485/883, Training Loss: 0.5610\n",
      "Epoch 6/10, Batch 486/883, Training Loss: 0.7128\n",
      "Epoch 6/10, Batch 487/883, Training Loss: 0.6291\n",
      "Epoch 6/10, Batch 488/883, Training Loss: 0.7901\n",
      "Epoch 6/10, Batch 489/883, Training Loss: 0.5533\n",
      "Epoch 6/10, Batch 490/883, Training Loss: 0.7513\n",
      "Epoch 6/10, Batch 491/883, Training Loss: 0.7066\n",
      "Epoch 6/10, Batch 492/883, Training Loss: 0.8587\n",
      "Epoch 6/10, Batch 493/883, Training Loss: 0.6595\n",
      "Epoch 6/10, Batch 494/883, Training Loss: 0.4833\n",
      "Epoch 6/10, Batch 495/883, Training Loss: 0.5017\n",
      "Epoch 6/10, Batch 496/883, Training Loss: 0.5523\n",
      "Epoch 6/10, Batch 497/883, Training Loss: 0.5621\n",
      "Epoch 6/10, Batch 498/883, Training Loss: 0.7170\n",
      "Epoch 6/10, Batch 499/883, Training Loss: 0.6308\n",
      "Epoch 6/10, Batch 500/883, Training Loss: 0.6160\n",
      "Epoch 6/10, Batch 501/883, Training Loss: 0.5382\n",
      "Epoch 6/10, Batch 502/883, Training Loss: 0.4252\n",
      "Epoch 6/10, Batch 503/883, Training Loss: 0.8007\n",
      "Epoch 6/10, Batch 504/883, Training Loss: 0.8011\n",
      "Epoch 6/10, Batch 505/883, Training Loss: 0.9296\n",
      "Epoch 6/10, Batch 506/883, Training Loss: 0.5999\n",
      "Epoch 6/10, Batch 507/883, Training Loss: 0.6009\n",
      "Epoch 6/10, Batch 508/883, Training Loss: 0.7452\n",
      "Epoch 6/10, Batch 509/883, Training Loss: 0.7000\n",
      "Epoch 6/10, Batch 510/883, Training Loss: 0.8171\n",
      "Epoch 6/10, Batch 511/883, Training Loss: 0.7951\n",
      "Epoch 6/10, Batch 512/883, Training Loss: 0.5691\n",
      "Epoch 6/10, Batch 513/883, Training Loss: 0.4046\n",
      "Epoch 6/10, Batch 514/883, Training Loss: 0.6756\n",
      "Epoch 6/10, Batch 515/883, Training Loss: 0.6283\n",
      "Epoch 6/10, Batch 516/883, Training Loss: 0.7316\n",
      "Epoch 6/10, Batch 517/883, Training Loss: 0.5790\n",
      "Epoch 6/10, Batch 518/883, Training Loss: 0.6452\n",
      "Epoch 6/10, Batch 519/883, Training Loss: 0.6197\n",
      "Epoch 6/10, Batch 520/883, Training Loss: 0.7011\n",
      "Epoch 6/10, Batch 521/883, Training Loss: 0.5631\n",
      "Epoch 6/10, Batch 522/883, Training Loss: 0.5419\n",
      "Epoch 6/10, Batch 523/883, Training Loss: 0.6154\n",
      "Epoch 6/10, Batch 524/883, Training Loss: 0.9197\n",
      "Epoch 6/10, Batch 525/883, Training Loss: 0.7715\n",
      "Epoch 6/10, Batch 526/883, Training Loss: 0.5636\n",
      "Epoch 6/10, Batch 527/883, Training Loss: 0.4859\n",
      "Epoch 6/10, Batch 528/883, Training Loss: 0.5583\n",
      "Epoch 6/10, Batch 529/883, Training Loss: 0.6834\n",
      "Epoch 6/10, Batch 530/883, Training Loss: 0.3327\n",
      "Epoch 6/10, Batch 531/883, Training Loss: 0.4904\n",
      "Epoch 6/10, Batch 532/883, Training Loss: 0.6610\n",
      "Epoch 6/10, Batch 533/883, Training Loss: 1.2374\n",
      "Epoch 6/10, Batch 534/883, Training Loss: 0.4462\n",
      "Epoch 6/10, Batch 535/883, Training Loss: 0.7695\n",
      "Epoch 6/10, Batch 536/883, Training Loss: 0.4621\n",
      "Epoch 6/10, Batch 537/883, Training Loss: 0.7541\n",
      "Epoch 6/10, Batch 538/883, Training Loss: 0.6434\n",
      "Epoch 6/10, Batch 539/883, Training Loss: 0.3962\n",
      "Epoch 6/10, Batch 540/883, Training Loss: 0.8542\n",
      "Epoch 6/10, Batch 541/883, Training Loss: 0.3734\n",
      "Epoch 6/10, Batch 542/883, Training Loss: 0.6921\n",
      "Epoch 6/10, Batch 543/883, Training Loss: 0.8675\n",
      "Epoch 6/10, Batch 544/883, Training Loss: 1.0417\n",
      "Epoch 6/10, Batch 545/883, Training Loss: 0.6403\n",
      "Epoch 6/10, Batch 546/883, Training Loss: 0.6485\n",
      "Epoch 6/10, Batch 547/883, Training Loss: 0.7601\n",
      "Epoch 6/10, Batch 548/883, Training Loss: 0.4849\n",
      "Epoch 6/10, Batch 549/883, Training Loss: 0.4538\n",
      "Epoch 6/10, Batch 550/883, Training Loss: 0.6934\n",
      "Epoch 6/10, Batch 551/883, Training Loss: 0.8001\n",
      "Epoch 6/10, Batch 552/883, Training Loss: 0.4714\n",
      "Epoch 6/10, Batch 553/883, Training Loss: 0.6854\n",
      "Epoch 6/10, Batch 554/883, Training Loss: 0.4882\n",
      "Epoch 6/10, Batch 555/883, Training Loss: 0.9497\n",
      "Epoch 6/10, Batch 556/883, Training Loss: 0.7057\n",
      "Epoch 6/10, Batch 557/883, Training Loss: 0.5036\n",
      "Epoch 6/10, Batch 558/883, Training Loss: 0.7168\n",
      "Epoch 6/10, Batch 559/883, Training Loss: 0.6317\n",
      "Epoch 6/10, Batch 560/883, Training Loss: 0.5904\n",
      "Epoch 6/10, Batch 561/883, Training Loss: 0.5417\n",
      "Epoch 6/10, Batch 562/883, Training Loss: 0.5587\n",
      "Epoch 6/10, Batch 563/883, Training Loss: 0.3474\n",
      "Epoch 6/10, Batch 564/883, Training Loss: 0.5636\n",
      "Epoch 6/10, Batch 565/883, Training Loss: 1.1583\n",
      "Epoch 6/10, Batch 566/883, Training Loss: 0.6598\n",
      "Epoch 6/10, Batch 567/883, Training Loss: 0.5742\n",
      "Epoch 6/10, Batch 568/883, Training Loss: 0.5615\n",
      "Epoch 6/10, Batch 569/883, Training Loss: 0.4509\n",
      "Epoch 6/10, Batch 570/883, Training Loss: 0.5345\n",
      "Epoch 6/10, Batch 571/883, Training Loss: 0.5407\n",
      "Epoch 6/10, Batch 572/883, Training Loss: 0.8057\n",
      "Epoch 6/10, Batch 573/883, Training Loss: 0.4018\n",
      "Epoch 6/10, Batch 574/883, Training Loss: 0.5924\n",
      "Epoch 6/10, Batch 575/883, Training Loss: 0.7156\n",
      "Epoch 6/10, Batch 576/883, Training Loss: 0.4616\n",
      "Epoch 6/10, Batch 577/883, Training Loss: 0.6843\n",
      "Epoch 6/10, Batch 578/883, Training Loss: 0.5049\n",
      "Epoch 6/10, Batch 579/883, Training Loss: 0.4742\n",
      "Epoch 6/10, Batch 580/883, Training Loss: 0.8797\n",
      "Epoch 6/10, Batch 581/883, Training Loss: 0.5965\n",
      "Epoch 6/10, Batch 582/883, Training Loss: 0.5507\n",
      "Epoch 6/10, Batch 583/883, Training Loss: 0.3343\n",
      "Epoch 6/10, Batch 584/883, Training Loss: 1.0787\n",
      "Epoch 6/10, Batch 585/883, Training Loss: 0.4326\n",
      "Epoch 6/10, Batch 586/883, Training Loss: 0.7880\n",
      "Epoch 6/10, Batch 587/883, Training Loss: 0.7328\n",
      "Epoch 6/10, Batch 588/883, Training Loss: 0.5475\n",
      "Epoch 6/10, Batch 589/883, Training Loss: 0.7285\n",
      "Epoch 6/10, Batch 590/883, Training Loss: 0.6560\n",
      "Epoch 6/10, Batch 591/883, Training Loss: 0.4834\n",
      "Epoch 6/10, Batch 592/883, Training Loss: 0.5531\n",
      "Epoch 6/10, Batch 593/883, Training Loss: 0.7235\n",
      "Epoch 6/10, Batch 594/883, Training Loss: 0.7600\n",
      "Epoch 6/10, Batch 595/883, Training Loss: 1.2930\n",
      "Epoch 6/10, Batch 596/883, Training Loss: 0.5497\n",
      "Epoch 6/10, Batch 597/883, Training Loss: 0.6916\n",
      "Epoch 6/10, Batch 598/883, Training Loss: 0.5708\n",
      "Epoch 6/10, Batch 599/883, Training Loss: 0.6086\n",
      "Epoch 6/10, Batch 600/883, Training Loss: 1.0420\n",
      "Epoch 6/10, Batch 601/883, Training Loss: 0.6040\n",
      "Epoch 6/10, Batch 602/883, Training Loss: 0.5122\n",
      "Epoch 6/10, Batch 603/883, Training Loss: 0.6975\n",
      "Epoch 6/10, Batch 604/883, Training Loss: 0.6008\n",
      "Epoch 6/10, Batch 605/883, Training Loss: 0.7226\n",
      "Epoch 6/10, Batch 606/883, Training Loss: 0.5990\n",
      "Epoch 6/10, Batch 607/883, Training Loss: 0.5798\n",
      "Epoch 6/10, Batch 608/883, Training Loss: 0.4485\n",
      "Epoch 6/10, Batch 609/883, Training Loss: 0.8358\n",
      "Epoch 6/10, Batch 610/883, Training Loss: 0.4687\n",
      "Epoch 6/10, Batch 611/883, Training Loss: 0.5941\n",
      "Epoch 6/10, Batch 612/883, Training Loss: 0.4566\n",
      "Epoch 6/10, Batch 613/883, Training Loss: 0.6080\n",
      "Epoch 6/10, Batch 614/883, Training Loss: 0.8654\n",
      "Epoch 6/10, Batch 615/883, Training Loss: 0.4911\n",
      "Epoch 6/10, Batch 616/883, Training Loss: 0.6722\n",
      "Epoch 6/10, Batch 617/883, Training Loss: 0.6005\n",
      "Epoch 6/10, Batch 618/883, Training Loss: 0.5766\n",
      "Epoch 6/10, Batch 619/883, Training Loss: 0.8645\n",
      "Epoch 6/10, Batch 620/883, Training Loss: 0.5262\n",
      "Epoch 6/10, Batch 621/883, Training Loss: 0.7396\n",
      "Epoch 6/10, Batch 622/883, Training Loss: 0.6516\n",
      "Epoch 6/10, Batch 623/883, Training Loss: 0.6461\n",
      "Epoch 6/10, Batch 624/883, Training Loss: 0.9340\n",
      "Epoch 6/10, Batch 625/883, Training Loss: 0.4749\n",
      "Epoch 6/10, Batch 626/883, Training Loss: 0.5910\n",
      "Epoch 6/10, Batch 627/883, Training Loss: 0.3889\n",
      "Epoch 6/10, Batch 628/883, Training Loss: 0.9230\n",
      "Epoch 6/10, Batch 629/883, Training Loss: 0.5779\n",
      "Epoch 6/10, Batch 630/883, Training Loss: 0.4457\n",
      "Epoch 6/10, Batch 631/883, Training Loss: 0.7280\n",
      "Epoch 6/10, Batch 632/883, Training Loss: 0.4778\n",
      "Epoch 6/10, Batch 633/883, Training Loss: 0.4872\n",
      "Epoch 6/10, Batch 634/883, Training Loss: 0.4183\n",
      "Epoch 6/10, Batch 635/883, Training Loss: 0.8506\n",
      "Epoch 6/10, Batch 636/883, Training Loss: 0.4413\n",
      "Epoch 6/10, Batch 637/883, Training Loss: 0.6594\n",
      "Epoch 6/10, Batch 638/883, Training Loss: 0.6352\n",
      "Epoch 6/10, Batch 639/883, Training Loss: 0.6440\n",
      "Epoch 6/10, Batch 640/883, Training Loss: 0.3742\n",
      "Epoch 6/10, Batch 641/883, Training Loss: 0.6997\n",
      "Epoch 6/10, Batch 642/883, Training Loss: 0.6279\n",
      "Epoch 6/10, Batch 643/883, Training Loss: 0.5458\n",
      "Epoch 6/10, Batch 644/883, Training Loss: 0.8651\n",
      "Epoch 6/10, Batch 645/883, Training Loss: 0.5577\n",
      "Epoch 6/10, Batch 646/883, Training Loss: 0.4443\n",
      "Epoch 6/10, Batch 647/883, Training Loss: 0.5051\n",
      "Epoch 6/10, Batch 648/883, Training Loss: 0.5525\n",
      "Epoch 6/10, Batch 649/883, Training Loss: 0.3965\n",
      "Epoch 6/10, Batch 650/883, Training Loss: 0.4748\n",
      "Epoch 6/10, Batch 651/883, Training Loss: 0.6927\n",
      "Epoch 6/10, Batch 652/883, Training Loss: 0.6140\n",
      "Epoch 6/10, Batch 653/883, Training Loss: 0.5459\n",
      "Epoch 6/10, Batch 654/883, Training Loss: 0.8193\n",
      "Epoch 6/10, Batch 655/883, Training Loss: 0.7851\n",
      "Epoch 6/10, Batch 656/883, Training Loss: 0.7592\n",
      "Epoch 6/10, Batch 657/883, Training Loss: 0.6625\n",
      "Epoch 6/10, Batch 658/883, Training Loss: 0.7098\n",
      "Epoch 6/10, Batch 659/883, Training Loss: 0.7210\n",
      "Epoch 6/10, Batch 660/883, Training Loss: 0.6771\n",
      "Epoch 6/10, Batch 661/883, Training Loss: 0.7125\n",
      "Epoch 6/10, Batch 662/883, Training Loss: 0.6101\n",
      "Epoch 6/10, Batch 663/883, Training Loss: 0.9160\n",
      "Epoch 6/10, Batch 664/883, Training Loss: 0.4933\n",
      "Epoch 6/10, Batch 665/883, Training Loss: 0.5359\n",
      "Epoch 6/10, Batch 666/883, Training Loss: 0.4347\n",
      "Epoch 6/10, Batch 667/883, Training Loss: 0.6111\n",
      "Epoch 6/10, Batch 668/883, Training Loss: 0.4743\n",
      "Epoch 6/10, Batch 669/883, Training Loss: 0.5147\n",
      "Epoch 6/10, Batch 670/883, Training Loss: 0.7140\n",
      "Epoch 6/10, Batch 671/883, Training Loss: 0.7144\n",
      "Epoch 6/10, Batch 672/883, Training Loss: 1.1258\n",
      "Epoch 6/10, Batch 673/883, Training Loss: 0.5696\n",
      "Epoch 6/10, Batch 674/883, Training Loss: 0.5470\n",
      "Epoch 6/10, Batch 675/883, Training Loss: 0.4945\n",
      "Epoch 6/10, Batch 676/883, Training Loss: 0.6039\n",
      "Epoch 6/10, Batch 677/883, Training Loss: 0.5130\n",
      "Epoch 6/10, Batch 678/883, Training Loss: 0.4198\n",
      "Epoch 6/10, Batch 679/883, Training Loss: 0.6803\n",
      "Epoch 6/10, Batch 680/883, Training Loss: 0.4111\n",
      "Epoch 6/10, Batch 681/883, Training Loss: 0.7573\n",
      "Epoch 6/10, Batch 682/883, Training Loss: 0.7542\n",
      "Epoch 6/10, Batch 683/883, Training Loss: 1.0399\n",
      "Epoch 6/10, Batch 684/883, Training Loss: 0.5883\n",
      "Epoch 6/10, Batch 685/883, Training Loss: 0.4946\n",
      "Epoch 6/10, Batch 686/883, Training Loss: 0.6239\n",
      "Epoch 6/10, Batch 687/883, Training Loss: 0.6796\n",
      "Epoch 6/10, Batch 688/883, Training Loss: 0.5454\n",
      "Epoch 6/10, Batch 689/883, Training Loss: 0.7154\n",
      "Epoch 6/10, Batch 690/883, Training Loss: 0.6544\n",
      "Epoch 6/10, Batch 691/883, Training Loss: 0.8139\n",
      "Epoch 6/10, Batch 692/883, Training Loss: 0.4448\n",
      "Epoch 6/10, Batch 693/883, Training Loss: 0.5903\n",
      "Epoch 6/10, Batch 694/883, Training Loss: 0.4347\n",
      "Epoch 6/10, Batch 695/883, Training Loss: 0.4985\n",
      "Epoch 6/10, Batch 696/883, Training Loss: 0.2735\n",
      "Epoch 6/10, Batch 697/883, Training Loss: 0.7117\n",
      "Epoch 6/10, Batch 698/883, Training Loss: 0.8474\n",
      "Epoch 6/10, Batch 699/883, Training Loss: 0.3860\n",
      "Epoch 6/10, Batch 700/883, Training Loss: 0.3187\n",
      "Epoch 6/10, Batch 701/883, Training Loss: 0.8526\n",
      "Epoch 6/10, Batch 702/883, Training Loss: 0.5291\n",
      "Epoch 6/10, Batch 703/883, Training Loss: 1.3159\n",
      "Epoch 6/10, Batch 704/883, Training Loss: 0.4239\n",
      "Epoch 6/10, Batch 705/883, Training Loss: 0.5615\n",
      "Epoch 6/10, Batch 706/883, Training Loss: 0.7566\n",
      "Epoch 6/10, Batch 707/883, Training Loss: 0.3421\n",
      "Epoch 6/10, Batch 708/883, Training Loss: 0.4530\n",
      "Epoch 6/10, Batch 709/883, Training Loss: 0.7307\n",
      "Epoch 6/10, Batch 710/883, Training Loss: 0.5451\n",
      "Epoch 6/10, Batch 711/883, Training Loss: 0.4344\n",
      "Epoch 6/10, Batch 712/883, Training Loss: 0.4479\n",
      "Epoch 6/10, Batch 713/883, Training Loss: 0.5836\n",
      "Epoch 6/10, Batch 714/883, Training Loss: 0.8026\n",
      "Epoch 6/10, Batch 715/883, Training Loss: 0.7270\n",
      "Epoch 6/10, Batch 716/883, Training Loss: 0.3616\n",
      "Epoch 6/10, Batch 717/883, Training Loss: 0.6481\n",
      "Epoch 6/10, Batch 718/883, Training Loss: 0.6870\n",
      "Epoch 6/10, Batch 719/883, Training Loss: 0.7206\n",
      "Epoch 6/10, Batch 720/883, Training Loss: 0.5726\n",
      "Epoch 6/10, Batch 721/883, Training Loss: 0.8160\n",
      "Epoch 6/10, Batch 722/883, Training Loss: 0.7679\n",
      "Epoch 6/10, Batch 723/883, Training Loss: 0.8212\n",
      "Epoch 6/10, Batch 724/883, Training Loss: 0.5751\n",
      "Epoch 6/10, Batch 725/883, Training Loss: 0.4698\n",
      "Epoch 6/10, Batch 726/883, Training Loss: 0.5445\n",
      "Epoch 6/10, Batch 727/883, Training Loss: 0.4276\n",
      "Epoch 6/10, Batch 728/883, Training Loss: 0.6741\n",
      "Epoch 6/10, Batch 729/883, Training Loss: 0.6014\n",
      "Epoch 6/10, Batch 730/883, Training Loss: 0.6879\n",
      "Epoch 6/10, Batch 731/883, Training Loss: 0.7639\n",
      "Epoch 6/10, Batch 732/883, Training Loss: 0.3981\n",
      "Epoch 6/10, Batch 733/883, Training Loss: 0.6139\n",
      "Epoch 6/10, Batch 734/883, Training Loss: 0.9267\n",
      "Epoch 6/10, Batch 735/883, Training Loss: 0.7084\n",
      "Epoch 6/10, Batch 736/883, Training Loss: 0.6655\n",
      "Epoch 6/10, Batch 737/883, Training Loss: 0.4183\n",
      "Epoch 6/10, Batch 738/883, Training Loss: 0.6183\n",
      "Epoch 6/10, Batch 739/883, Training Loss: 0.5181\n",
      "Epoch 6/10, Batch 740/883, Training Loss: 0.9239\n",
      "Epoch 6/10, Batch 741/883, Training Loss: 0.5655\n",
      "Epoch 6/10, Batch 742/883, Training Loss: 0.7653\n",
      "Epoch 6/10, Batch 743/883, Training Loss: 0.6347\n",
      "Epoch 6/10, Batch 744/883, Training Loss: 0.3776\n",
      "Epoch 6/10, Batch 745/883, Training Loss: 0.6385\n",
      "Epoch 6/10, Batch 746/883, Training Loss: 0.7809\n",
      "Epoch 6/10, Batch 747/883, Training Loss: 0.4845\n",
      "Epoch 6/10, Batch 748/883, Training Loss: 0.7582\n",
      "Epoch 6/10, Batch 749/883, Training Loss: 0.8085\n",
      "Epoch 6/10, Batch 750/883, Training Loss: 0.6888\n",
      "Epoch 6/10, Batch 751/883, Training Loss: 0.4330\n",
      "Epoch 6/10, Batch 752/883, Training Loss: 0.7232\n",
      "Epoch 6/10, Batch 753/883, Training Loss: 0.5899\n",
      "Epoch 6/10, Batch 754/883, Training Loss: 0.9699\n",
      "Epoch 6/10, Batch 755/883, Training Loss: 0.6022\n",
      "Epoch 6/10, Batch 756/883, Training Loss: 0.6775\n",
      "Epoch 6/10, Batch 757/883, Training Loss: 0.3467\n",
      "Epoch 6/10, Batch 758/883, Training Loss: 0.6362\n",
      "Epoch 6/10, Batch 759/883, Training Loss: 0.5333\n",
      "Epoch 6/10, Batch 760/883, Training Loss: 0.5831\n",
      "Epoch 6/10, Batch 761/883, Training Loss: 1.3835\n",
      "Epoch 6/10, Batch 762/883, Training Loss: 0.6512\n",
      "Epoch 6/10, Batch 763/883, Training Loss: 0.7551\n",
      "Epoch 6/10, Batch 764/883, Training Loss: 0.7291\n",
      "Epoch 6/10, Batch 765/883, Training Loss: 0.5289\n",
      "Epoch 6/10, Batch 766/883, Training Loss: 0.7707\n",
      "Epoch 6/10, Batch 767/883, Training Loss: 0.6477\n",
      "Epoch 6/10, Batch 768/883, Training Loss: 0.5794\n",
      "Epoch 6/10, Batch 769/883, Training Loss: 0.9175\n",
      "Epoch 6/10, Batch 770/883, Training Loss: 0.6499\n",
      "Epoch 6/10, Batch 771/883, Training Loss: 0.7781\n",
      "Epoch 6/10, Batch 772/883, Training Loss: 0.5302\n",
      "Epoch 6/10, Batch 773/883, Training Loss: 0.7369\n",
      "Epoch 6/10, Batch 774/883, Training Loss: 0.8162\n",
      "Epoch 6/10, Batch 775/883, Training Loss: 0.5833\n",
      "Epoch 6/10, Batch 776/883, Training Loss: 0.7835\n",
      "Epoch 6/10, Batch 777/883, Training Loss: 0.4641\n",
      "Epoch 6/10, Batch 778/883, Training Loss: 0.6164\n",
      "Epoch 6/10, Batch 779/883, Training Loss: 0.9037\n",
      "Epoch 6/10, Batch 780/883, Training Loss: 0.4686\n",
      "Epoch 6/10, Batch 781/883, Training Loss: 0.7197\n",
      "Epoch 6/10, Batch 782/883, Training Loss: 0.4555\n",
      "Epoch 6/10, Batch 783/883, Training Loss: 0.5780\n",
      "Epoch 6/10, Batch 784/883, Training Loss: 0.6753\n",
      "Epoch 6/10, Batch 785/883, Training Loss: 0.4432\n",
      "Epoch 6/10, Batch 786/883, Training Loss: 0.4811\n",
      "Epoch 6/10, Batch 787/883, Training Loss: 0.7435\n",
      "Epoch 6/10, Batch 788/883, Training Loss: 0.7622\n",
      "Epoch 6/10, Batch 789/883, Training Loss: 0.4488\n",
      "Epoch 6/10, Batch 790/883, Training Loss: 0.6040\n",
      "Epoch 6/10, Batch 791/883, Training Loss: 0.4855\n",
      "Epoch 6/10, Batch 792/883, Training Loss: 0.4871\n",
      "Epoch 6/10, Batch 793/883, Training Loss: 0.4851\n",
      "Epoch 6/10, Batch 794/883, Training Loss: 0.4992\n",
      "Epoch 6/10, Batch 795/883, Training Loss: 1.1279\n",
      "Epoch 6/10, Batch 796/883, Training Loss: 0.7372\n",
      "Epoch 6/10, Batch 797/883, Training Loss: 0.6710\n",
      "Epoch 6/10, Batch 798/883, Training Loss: 0.7017\n",
      "Epoch 6/10, Batch 799/883, Training Loss: 0.3434\n",
      "Epoch 6/10, Batch 800/883, Training Loss: 0.9366\n",
      "Epoch 6/10, Batch 801/883, Training Loss: 0.5262\n",
      "Epoch 6/10, Batch 802/883, Training Loss: 0.4305\n",
      "Epoch 6/10, Batch 803/883, Training Loss: 0.6543\n",
      "Epoch 6/10, Batch 804/883, Training Loss: 0.6378\n",
      "Epoch 6/10, Batch 805/883, Training Loss: 0.7720\n",
      "Epoch 6/10, Batch 806/883, Training Loss: 0.6158\n",
      "Epoch 6/10, Batch 807/883, Training Loss: 0.7614\n",
      "Epoch 6/10, Batch 808/883, Training Loss: 1.1170\n",
      "Epoch 6/10, Batch 809/883, Training Loss: 0.7412\n",
      "Epoch 6/10, Batch 810/883, Training Loss: 1.0559\n",
      "Epoch 6/10, Batch 811/883, Training Loss: 0.5573\n",
      "Epoch 6/10, Batch 812/883, Training Loss: 0.3713\n",
      "Epoch 6/10, Batch 813/883, Training Loss: 0.7714\n",
      "Epoch 6/10, Batch 814/883, Training Loss: 0.4470\n",
      "Epoch 6/10, Batch 815/883, Training Loss: 0.4943\n",
      "Epoch 6/10, Batch 816/883, Training Loss: 0.5295\n",
      "Epoch 6/10, Batch 817/883, Training Loss: 0.9972\n",
      "Epoch 6/10, Batch 818/883, Training Loss: 0.4673\n",
      "Epoch 6/10, Batch 819/883, Training Loss: 0.5801\n",
      "Epoch 6/10, Batch 820/883, Training Loss: 0.5003\n",
      "Epoch 6/10, Batch 821/883, Training Loss: 0.7265\n",
      "Epoch 6/10, Batch 822/883, Training Loss: 0.4547\n",
      "Epoch 6/10, Batch 823/883, Training Loss: 0.8128\n",
      "Epoch 6/10, Batch 824/883, Training Loss: 0.6053\n",
      "Epoch 6/10, Batch 825/883, Training Loss: 0.5810\n",
      "Epoch 6/10, Batch 826/883, Training Loss: 0.5428\n",
      "Epoch 6/10, Batch 827/883, Training Loss: 0.5765\n",
      "Epoch 6/10, Batch 828/883, Training Loss: 0.7067\n",
      "Epoch 6/10, Batch 829/883, Training Loss: 0.6017\n",
      "Epoch 6/10, Batch 830/883, Training Loss: 0.5117\n",
      "Epoch 6/10, Batch 831/883, Training Loss: 0.6765\n",
      "Epoch 6/10, Batch 832/883, Training Loss: 0.6045\n",
      "Epoch 6/10, Batch 833/883, Training Loss: 0.6996\n",
      "Epoch 6/10, Batch 834/883, Training Loss: 0.4805\n",
      "Epoch 6/10, Batch 835/883, Training Loss: 0.6554\n",
      "Epoch 6/10, Batch 836/883, Training Loss: 0.6280\n",
      "Epoch 6/10, Batch 837/883, Training Loss: 0.4951\n",
      "Epoch 6/10, Batch 838/883, Training Loss: 0.8174\n",
      "Epoch 6/10, Batch 839/883, Training Loss: 0.9376\n",
      "Epoch 6/10, Batch 840/883, Training Loss: 0.5366\n",
      "Epoch 6/10, Batch 841/883, Training Loss: 0.3787\n",
      "Epoch 6/10, Batch 842/883, Training Loss: 0.7829\n",
      "Epoch 6/10, Batch 843/883, Training Loss: 0.5891\n",
      "Epoch 6/10, Batch 844/883, Training Loss: 0.5648\n",
      "Epoch 6/10, Batch 845/883, Training Loss: 0.5086\n",
      "Epoch 6/10, Batch 846/883, Training Loss: 0.6612\n",
      "Epoch 6/10, Batch 847/883, Training Loss: 0.4770\n",
      "Epoch 6/10, Batch 848/883, Training Loss: 0.5731\n",
      "Epoch 6/10, Batch 849/883, Training Loss: 0.6169\n",
      "Epoch 6/10, Batch 850/883, Training Loss: 0.6650\n",
      "Epoch 6/10, Batch 851/883, Training Loss: 0.9201\n",
      "Epoch 6/10, Batch 852/883, Training Loss: 0.4189\n",
      "Epoch 6/10, Batch 853/883, Training Loss: 0.5034\n",
      "Epoch 6/10, Batch 854/883, Training Loss: 0.7683\n",
      "Epoch 6/10, Batch 855/883, Training Loss: 0.9364\n",
      "Epoch 6/10, Batch 856/883, Training Loss: 0.4828\n",
      "Epoch 6/10, Batch 857/883, Training Loss: 0.7020\n",
      "Epoch 6/10, Batch 858/883, Training Loss: 0.5653\n",
      "Epoch 6/10, Batch 859/883, Training Loss: 1.2307\n",
      "Epoch 6/10, Batch 860/883, Training Loss: 0.7836\n",
      "Epoch 6/10, Batch 861/883, Training Loss: 0.7376\n",
      "Epoch 6/10, Batch 862/883, Training Loss: 0.9089\n",
      "Epoch 6/10, Batch 863/883, Training Loss: 0.4844\n",
      "Epoch 6/10, Batch 864/883, Training Loss: 0.6291\n",
      "Epoch 6/10, Batch 865/883, Training Loss: 0.6447\n",
      "Epoch 6/10, Batch 866/883, Training Loss: 0.7623\n",
      "Epoch 6/10, Batch 867/883, Training Loss: 0.8160\n",
      "Epoch 6/10, Batch 868/883, Training Loss: 0.5743\n",
      "Epoch 6/10, Batch 869/883, Training Loss: 0.7803\n",
      "Epoch 6/10, Batch 870/883, Training Loss: 0.5826\n",
      "Epoch 6/10, Batch 871/883, Training Loss: 0.6865\n",
      "Epoch 6/10, Batch 872/883, Training Loss: 0.7493\n",
      "Epoch 6/10, Batch 873/883, Training Loss: 0.6184\n",
      "Epoch 6/10, Batch 874/883, Training Loss: 0.5733\n",
      "Epoch 6/10, Batch 875/883, Training Loss: 1.0846\n",
      "Epoch 6/10, Batch 876/883, Training Loss: 0.7391\n",
      "Epoch 6/10, Batch 877/883, Training Loss: 0.5254\n",
      "Epoch 6/10, Batch 878/883, Training Loss: 0.5997\n",
      "Epoch 6/10, Batch 879/883, Training Loss: 0.8316\n",
      "Epoch 6/10, Batch 880/883, Training Loss: 0.5958\n",
      "Epoch 6/10, Batch 881/883, Training Loss: 0.8933\n",
      "Epoch 6/10, Batch 882/883, Training Loss: 0.8164\n",
      "Epoch 6/10, Batch 883/883, Training Loss: 0.8744\n",
      "Epoch 6/10, Training Loss: 0.6598, Validation Loss: 0.6413, Validation Accuracy: 0.7167\n",
      "Epoch 7/10, Batch 1/883, Training Loss: 0.8161\n",
      "Epoch 7/10, Batch 2/883, Training Loss: 0.5776\n",
      "Epoch 7/10, Batch 3/883, Training Loss: 0.7187\n",
      "Epoch 7/10, Batch 4/883, Training Loss: 0.3607\n",
      "Epoch 7/10, Batch 5/883, Training Loss: 0.6935\n",
      "Epoch 7/10, Batch 6/883, Training Loss: 0.5935\n",
      "Epoch 7/10, Batch 7/883, Training Loss: 0.7840\n",
      "Epoch 7/10, Batch 8/883, Training Loss: 0.7579\n",
      "Epoch 7/10, Batch 9/883, Training Loss: 0.6014\n",
      "Epoch 7/10, Batch 10/883, Training Loss: 0.6255\n",
      "Epoch 7/10, Batch 11/883, Training Loss: 0.4426\n",
      "Epoch 7/10, Batch 12/883, Training Loss: 0.6181\n",
      "Epoch 7/10, Batch 13/883, Training Loss: 0.7579\n",
      "Epoch 7/10, Batch 14/883, Training Loss: 0.5455\n",
      "Epoch 7/10, Batch 15/883, Training Loss: 0.6183\n",
      "Epoch 7/10, Batch 16/883, Training Loss: 0.5273\n",
      "Epoch 7/10, Batch 17/883, Training Loss: 0.7578\n",
      "Epoch 7/10, Batch 18/883, Training Loss: 0.6291\n",
      "Epoch 7/10, Batch 19/883, Training Loss: 0.5280\n",
      "Epoch 7/10, Batch 20/883, Training Loss: 0.5758\n",
      "Epoch 7/10, Batch 21/883, Training Loss: 0.5076\n",
      "Epoch 7/10, Batch 22/883, Training Loss: 0.6122\n",
      "Epoch 7/10, Batch 23/883, Training Loss: 0.5807\n",
      "Epoch 7/10, Batch 24/883, Training Loss: 0.9291\n",
      "Epoch 7/10, Batch 25/883, Training Loss: 0.5047\n",
      "Epoch 7/10, Batch 26/883, Training Loss: 0.7037\n",
      "Epoch 7/10, Batch 27/883, Training Loss: 0.8831\n",
      "Epoch 7/10, Batch 28/883, Training Loss: 0.5643\n",
      "Epoch 7/10, Batch 29/883, Training Loss: 0.6062\n",
      "Epoch 7/10, Batch 30/883, Training Loss: 0.7318\n",
      "Epoch 7/10, Batch 31/883, Training Loss: 0.4704\n",
      "Epoch 7/10, Batch 32/883, Training Loss: 0.4118\n",
      "Epoch 7/10, Batch 33/883, Training Loss: 0.5329\n",
      "Epoch 7/10, Batch 34/883, Training Loss: 0.6442\n",
      "Epoch 7/10, Batch 35/883, Training Loss: 0.9146\n",
      "Epoch 7/10, Batch 36/883, Training Loss: 0.7251\n",
      "Epoch 7/10, Batch 37/883, Training Loss: 0.3592\n",
      "Epoch 7/10, Batch 38/883, Training Loss: 0.5924\n",
      "Epoch 7/10, Batch 39/883, Training Loss: 0.4175\n",
      "Epoch 7/10, Batch 40/883, Training Loss: 0.5930\n",
      "Epoch 7/10, Batch 41/883, Training Loss: 0.6904\n",
      "Epoch 7/10, Batch 42/883, Training Loss: 0.6176\n",
      "Epoch 7/10, Batch 43/883, Training Loss: 0.5366\n",
      "Epoch 7/10, Batch 44/883, Training Loss: 0.9140\n",
      "Epoch 7/10, Batch 45/883, Training Loss: 0.8084\n",
      "Epoch 7/10, Batch 46/883, Training Loss: 0.6608\n",
      "Epoch 7/10, Batch 47/883, Training Loss: 0.6042\n",
      "Epoch 7/10, Batch 48/883, Training Loss: 0.5099\n",
      "Epoch 7/10, Batch 49/883, Training Loss: 0.7983\n",
      "Epoch 7/10, Batch 50/883, Training Loss: 0.6379\n",
      "Epoch 7/10, Batch 51/883, Training Loss: 0.6682\n",
      "Epoch 7/10, Batch 52/883, Training Loss: 0.4656\n",
      "Epoch 7/10, Batch 53/883, Training Loss: 0.5772\n",
      "Epoch 7/10, Batch 54/883, Training Loss: 0.5616\n",
      "Epoch 7/10, Batch 55/883, Training Loss: 0.4104\n",
      "Epoch 7/10, Batch 56/883, Training Loss: 0.8854\n",
      "Epoch 7/10, Batch 57/883, Training Loss: 0.3720\n",
      "Epoch 7/10, Batch 58/883, Training Loss: 0.7754\n",
      "Epoch 7/10, Batch 59/883, Training Loss: 0.7489\n",
      "Epoch 7/10, Batch 60/883, Training Loss: 0.6256\n",
      "Epoch 7/10, Batch 61/883, Training Loss: 0.5355\n",
      "Epoch 7/10, Batch 62/883, Training Loss: 0.4116\n",
      "Epoch 7/10, Batch 63/883, Training Loss: 0.7501\n",
      "Epoch 7/10, Batch 64/883, Training Loss: 0.6103\n",
      "Epoch 7/10, Batch 65/883, Training Loss: 0.7377\n",
      "Epoch 7/10, Batch 66/883, Training Loss: 1.1253\n",
      "Epoch 7/10, Batch 67/883, Training Loss: 0.7224\n",
      "Epoch 7/10, Batch 68/883, Training Loss: 0.5846\n",
      "Epoch 7/10, Batch 69/883, Training Loss: 0.5588\n",
      "Epoch 7/10, Batch 70/883, Training Loss: 0.7337\n",
      "Epoch 7/10, Batch 71/883, Training Loss: 0.4700\n",
      "Epoch 7/10, Batch 72/883, Training Loss: 0.6608\n",
      "Epoch 7/10, Batch 73/883, Training Loss: 0.5196\n",
      "Epoch 7/10, Batch 74/883, Training Loss: 0.5529\n",
      "Epoch 7/10, Batch 75/883, Training Loss: 0.7781\n",
      "Epoch 7/10, Batch 76/883, Training Loss: 0.4477\n",
      "Epoch 7/10, Batch 77/883, Training Loss: 0.7899\n",
      "Epoch 7/10, Batch 78/883, Training Loss: 0.9046\n",
      "Epoch 7/10, Batch 79/883, Training Loss: 0.5527\n",
      "Epoch 7/10, Batch 80/883, Training Loss: 0.3057\n",
      "Epoch 7/10, Batch 81/883, Training Loss: 0.5746\n",
      "Epoch 7/10, Batch 82/883, Training Loss: 0.5599\n",
      "Epoch 7/10, Batch 83/883, Training Loss: 0.8761\n",
      "Epoch 7/10, Batch 84/883, Training Loss: 0.7567\n",
      "Epoch 7/10, Batch 85/883, Training Loss: 0.6418\n",
      "Epoch 7/10, Batch 86/883, Training Loss: 0.5291\n",
      "Epoch 7/10, Batch 87/883, Training Loss: 0.4005\n",
      "Epoch 7/10, Batch 88/883, Training Loss: 0.4306\n",
      "Epoch 7/10, Batch 89/883, Training Loss: 0.3476\n",
      "Epoch 7/10, Batch 90/883, Training Loss: 0.5284\n",
      "Epoch 7/10, Batch 91/883, Training Loss: 0.5842\n",
      "Epoch 7/10, Batch 92/883, Training Loss: 0.6865\n",
      "Epoch 7/10, Batch 93/883, Training Loss: 0.6732\n",
      "Epoch 7/10, Batch 94/883, Training Loss: 0.6821\n",
      "Epoch 7/10, Batch 95/883, Training Loss: 0.3975\n",
      "Epoch 7/10, Batch 96/883, Training Loss: 0.6495\n",
      "Epoch 7/10, Batch 97/883, Training Loss: 0.8955\n",
      "Epoch 7/10, Batch 98/883, Training Loss: 0.3591\n",
      "Epoch 7/10, Batch 99/883, Training Loss: 0.5924\n",
      "Epoch 7/10, Batch 100/883, Training Loss: 0.7575\n",
      "Epoch 7/10, Batch 101/883, Training Loss: 0.6349\n",
      "Epoch 7/10, Batch 102/883, Training Loss: 0.4791\n",
      "Epoch 7/10, Batch 103/883, Training Loss: 0.5249\n",
      "Epoch 7/10, Batch 104/883, Training Loss: 0.6792\n",
      "Epoch 7/10, Batch 105/883, Training Loss: 0.6139\n",
      "Epoch 7/10, Batch 106/883, Training Loss: 0.6825\n",
      "Epoch 7/10, Batch 107/883, Training Loss: 0.4034\n",
      "Epoch 7/10, Batch 108/883, Training Loss: 0.6896\n",
      "Epoch 7/10, Batch 109/883, Training Loss: 0.7146\n",
      "Epoch 7/10, Batch 110/883, Training Loss: 0.4988\n",
      "Epoch 7/10, Batch 111/883, Training Loss: 0.6489\n",
      "Epoch 7/10, Batch 112/883, Training Loss: 0.4913\n",
      "Epoch 7/10, Batch 113/883, Training Loss: 0.7859\n",
      "Epoch 7/10, Batch 114/883, Training Loss: 0.4707\n",
      "Epoch 7/10, Batch 115/883, Training Loss: 0.5172\n",
      "Epoch 7/10, Batch 116/883, Training Loss: 0.5450\n",
      "Epoch 7/10, Batch 117/883, Training Loss: 0.4591\n",
      "Epoch 7/10, Batch 118/883, Training Loss: 0.5305\n",
      "Epoch 7/10, Batch 119/883, Training Loss: 0.5029\n",
      "Epoch 7/10, Batch 120/883, Training Loss: 0.5960\n",
      "Epoch 7/10, Batch 121/883, Training Loss: 0.7929\n",
      "Epoch 7/10, Batch 122/883, Training Loss: 0.6192\n",
      "Epoch 7/10, Batch 123/883, Training Loss: 0.4187\n",
      "Epoch 7/10, Batch 124/883, Training Loss: 0.6938\n",
      "Epoch 7/10, Batch 125/883, Training Loss: 1.0013\n",
      "Epoch 7/10, Batch 126/883, Training Loss: 0.6225\n",
      "Epoch 7/10, Batch 127/883, Training Loss: 0.7495\n",
      "Epoch 7/10, Batch 128/883, Training Loss: 0.5659\n",
      "Epoch 7/10, Batch 129/883, Training Loss: 0.8076\n",
      "Epoch 7/10, Batch 130/883, Training Loss: 0.3792\n",
      "Epoch 7/10, Batch 131/883, Training Loss: 0.3311\n",
      "Epoch 7/10, Batch 132/883, Training Loss: 0.3811\n",
      "Epoch 7/10, Batch 133/883, Training Loss: 0.8356\n",
      "Epoch 7/10, Batch 134/883, Training Loss: 0.4511\n",
      "Epoch 7/10, Batch 135/883, Training Loss: 0.6699\n",
      "Epoch 7/10, Batch 136/883, Training Loss: 0.3573\n",
      "Epoch 7/10, Batch 137/883, Training Loss: 0.5731\n",
      "Epoch 7/10, Batch 138/883, Training Loss: 0.5789\n",
      "Epoch 7/10, Batch 139/883, Training Loss: 0.8192\n",
      "Epoch 7/10, Batch 140/883, Training Loss: 0.5627\n",
      "Epoch 7/10, Batch 141/883, Training Loss: 0.5364\n",
      "Epoch 7/10, Batch 142/883, Training Loss: 0.7250\n",
      "Epoch 7/10, Batch 143/883, Training Loss: 0.7644\n",
      "Epoch 7/10, Batch 144/883, Training Loss: 1.1216\n",
      "Epoch 7/10, Batch 145/883, Training Loss: 0.7832\n",
      "Epoch 7/10, Batch 146/883, Training Loss: 0.4127\n",
      "Epoch 7/10, Batch 147/883, Training Loss: 0.4719\n",
      "Epoch 7/10, Batch 148/883, Training Loss: 0.5998\n",
      "Epoch 7/10, Batch 149/883, Training Loss: 0.5638\n",
      "Epoch 7/10, Batch 150/883, Training Loss: 0.6610\n",
      "Epoch 7/10, Batch 151/883, Training Loss: 0.7642\n",
      "Epoch 7/10, Batch 152/883, Training Loss: 0.7361\n",
      "Epoch 7/10, Batch 153/883, Training Loss: 0.4215\n",
      "Epoch 7/10, Batch 154/883, Training Loss: 1.0638\n",
      "Epoch 7/10, Batch 155/883, Training Loss: 0.5832\n",
      "Epoch 7/10, Batch 156/883, Training Loss: 0.4568\n",
      "Epoch 7/10, Batch 157/883, Training Loss: 0.5807\n",
      "Epoch 7/10, Batch 158/883, Training Loss: 0.5742\n",
      "Epoch 7/10, Batch 159/883, Training Loss: 0.7638\n",
      "Epoch 7/10, Batch 160/883, Training Loss: 0.5693\n",
      "Epoch 7/10, Batch 161/883, Training Loss: 0.7351\n",
      "Epoch 7/10, Batch 162/883, Training Loss: 0.4938\n",
      "Epoch 7/10, Batch 163/883, Training Loss: 0.5595\n",
      "Epoch 7/10, Batch 164/883, Training Loss: 0.4999\n",
      "Epoch 7/10, Batch 165/883, Training Loss: 0.5307\n",
      "Epoch 7/10, Batch 166/883, Training Loss: 0.5760\n",
      "Epoch 7/10, Batch 167/883, Training Loss: 0.4734\n",
      "Epoch 7/10, Batch 168/883, Training Loss: 0.5409\n",
      "Epoch 7/10, Batch 169/883, Training Loss: 0.7380\n",
      "Epoch 7/10, Batch 170/883, Training Loss: 0.5380\n",
      "Epoch 7/10, Batch 171/883, Training Loss: 0.7294\n",
      "Epoch 7/10, Batch 172/883, Training Loss: 0.5310\n",
      "Epoch 7/10, Batch 173/883, Training Loss: 0.7557\n",
      "Epoch 7/10, Batch 174/883, Training Loss: 0.4788\n",
      "Epoch 7/10, Batch 175/883, Training Loss: 0.5956\n",
      "Epoch 7/10, Batch 176/883, Training Loss: 0.4089\n",
      "Epoch 7/10, Batch 177/883, Training Loss: 1.0801\n",
      "Epoch 7/10, Batch 178/883, Training Loss: 0.4999\n",
      "Epoch 7/10, Batch 179/883, Training Loss: 0.5275\n",
      "Epoch 7/10, Batch 180/883, Training Loss: 0.8916\n",
      "Epoch 7/10, Batch 181/883, Training Loss: 0.8346\n",
      "Epoch 7/10, Batch 182/883, Training Loss: 0.6300\n",
      "Epoch 7/10, Batch 183/883, Training Loss: 1.0860\n",
      "Epoch 7/10, Batch 184/883, Training Loss: 0.5760\n",
      "Epoch 7/10, Batch 185/883, Training Loss: 0.7499\n",
      "Epoch 7/10, Batch 186/883, Training Loss: 0.7810\n",
      "Epoch 7/10, Batch 187/883, Training Loss: 0.7911\n",
      "Epoch 7/10, Batch 188/883, Training Loss: 0.6317\n",
      "Epoch 7/10, Batch 189/883, Training Loss: 0.5053\n",
      "Epoch 7/10, Batch 190/883, Training Loss: 0.5625\n",
      "Epoch 7/10, Batch 191/883, Training Loss: 0.5003\n",
      "Epoch 7/10, Batch 192/883, Training Loss: 0.7231\n",
      "Epoch 7/10, Batch 193/883, Training Loss: 0.3404\n",
      "Epoch 7/10, Batch 194/883, Training Loss: 0.7225\n",
      "Epoch 7/10, Batch 195/883, Training Loss: 0.5488\n",
      "Epoch 7/10, Batch 196/883, Training Loss: 1.0381\n",
      "Epoch 7/10, Batch 197/883, Training Loss: 0.7165\n",
      "Epoch 7/10, Batch 198/883, Training Loss: 0.5292\n",
      "Epoch 7/10, Batch 199/883, Training Loss: 0.7154\n",
      "Epoch 7/10, Batch 200/883, Training Loss: 0.5948\n",
      "Epoch 7/10, Batch 201/883, Training Loss: 0.5362\n",
      "Epoch 7/10, Batch 202/883, Training Loss: 0.5353\n",
      "Epoch 7/10, Batch 203/883, Training Loss: 0.4353\n",
      "Epoch 7/10, Batch 204/883, Training Loss: 0.9540\n",
      "Epoch 7/10, Batch 205/883, Training Loss: 0.5818\n",
      "Epoch 7/10, Batch 206/883, Training Loss: 0.4454\n",
      "Epoch 7/10, Batch 207/883, Training Loss: 0.7288\n",
      "Epoch 7/10, Batch 208/883, Training Loss: 0.9562\n",
      "Epoch 7/10, Batch 209/883, Training Loss: 0.4771\n",
      "Epoch 7/10, Batch 210/883, Training Loss: 0.4558\n",
      "Epoch 7/10, Batch 211/883, Training Loss: 0.6859\n",
      "Epoch 7/10, Batch 212/883, Training Loss: 0.8392\n",
      "Epoch 7/10, Batch 213/883, Training Loss: 0.5683\n",
      "Epoch 7/10, Batch 214/883, Training Loss: 0.7055\n",
      "Epoch 7/10, Batch 215/883, Training Loss: 0.6928\n",
      "Epoch 7/10, Batch 216/883, Training Loss: 0.3924\n",
      "Epoch 7/10, Batch 217/883, Training Loss: 0.5179\n",
      "Epoch 7/10, Batch 218/883, Training Loss: 0.6152\n",
      "Epoch 7/10, Batch 219/883, Training Loss: 0.6191\n",
      "Epoch 7/10, Batch 220/883, Training Loss: 0.4887\n",
      "Epoch 7/10, Batch 221/883, Training Loss: 0.6894\n",
      "Epoch 7/10, Batch 222/883, Training Loss: 0.8210\n",
      "Epoch 7/10, Batch 223/883, Training Loss: 0.8692\n",
      "Epoch 7/10, Batch 224/883, Training Loss: 0.8833\n",
      "Epoch 7/10, Batch 225/883, Training Loss: 0.6526\n",
      "Epoch 7/10, Batch 226/883, Training Loss: 0.5804\n",
      "Epoch 7/10, Batch 227/883, Training Loss: 0.6696\n",
      "Epoch 7/10, Batch 228/883, Training Loss: 0.5178\n",
      "Epoch 7/10, Batch 229/883, Training Loss: 0.3465\n",
      "Epoch 7/10, Batch 230/883, Training Loss: 0.5520\n",
      "Epoch 7/10, Batch 231/883, Training Loss: 0.6137\n",
      "Epoch 7/10, Batch 232/883, Training Loss: 0.5450\n",
      "Epoch 7/10, Batch 233/883, Training Loss: 0.6605\n",
      "Epoch 7/10, Batch 234/883, Training Loss: 0.8067\n",
      "Epoch 7/10, Batch 235/883, Training Loss: 0.4773\n",
      "Epoch 7/10, Batch 236/883, Training Loss: 0.5177\n",
      "Epoch 7/10, Batch 237/883, Training Loss: 0.7771\n",
      "Epoch 7/10, Batch 238/883, Training Loss: 0.4996\n",
      "Epoch 7/10, Batch 239/883, Training Loss: 0.6709\n",
      "Epoch 7/10, Batch 240/883, Training Loss: 0.5776\n",
      "Epoch 7/10, Batch 241/883, Training Loss: 0.6708\n",
      "Epoch 7/10, Batch 242/883, Training Loss: 0.6788\n",
      "Epoch 7/10, Batch 243/883, Training Loss: 0.5666\n",
      "Epoch 7/10, Batch 244/883, Training Loss: 0.7193\n",
      "Epoch 7/10, Batch 245/883, Training Loss: 0.5686\n",
      "Epoch 7/10, Batch 246/883, Training Loss: 0.4626\n",
      "Epoch 7/10, Batch 247/883, Training Loss: 1.1088\n",
      "Epoch 7/10, Batch 248/883, Training Loss: 0.6001\n",
      "Epoch 7/10, Batch 249/883, Training Loss: 0.5548\n",
      "Epoch 7/10, Batch 250/883, Training Loss: 0.5224\n",
      "Epoch 7/10, Batch 251/883, Training Loss: 0.8714\n",
      "Epoch 7/10, Batch 252/883, Training Loss: 0.7604\n",
      "Epoch 7/10, Batch 253/883, Training Loss: 0.4498\n",
      "Epoch 7/10, Batch 254/883, Training Loss: 0.5982\n",
      "Epoch 7/10, Batch 255/883, Training Loss: 0.4206\n",
      "Epoch 7/10, Batch 256/883, Training Loss: 0.8735\n",
      "Epoch 7/10, Batch 257/883, Training Loss: 0.6331\n",
      "Epoch 7/10, Batch 258/883, Training Loss: 0.8632\n",
      "Epoch 7/10, Batch 259/883, Training Loss: 0.5650\n",
      "Epoch 7/10, Batch 260/883, Training Loss: 0.4553\n",
      "Epoch 7/10, Batch 261/883, Training Loss: 0.5423\n",
      "Epoch 7/10, Batch 262/883, Training Loss: 0.7836\n",
      "Epoch 7/10, Batch 263/883, Training Loss: 0.6074\n",
      "Epoch 7/10, Batch 264/883, Training Loss: 0.4158\n",
      "Epoch 7/10, Batch 265/883, Training Loss: 0.5883\n",
      "Epoch 7/10, Batch 266/883, Training Loss: 0.7897\n",
      "Epoch 7/10, Batch 267/883, Training Loss: 0.7895\n",
      "Epoch 7/10, Batch 268/883, Training Loss: 0.5519\n",
      "Epoch 7/10, Batch 269/883, Training Loss: 0.3794\n",
      "Epoch 7/10, Batch 270/883, Training Loss: 0.8164\n",
      "Epoch 7/10, Batch 271/883, Training Loss: 0.7743\n",
      "Epoch 7/10, Batch 272/883, Training Loss: 0.5258\n",
      "Epoch 7/10, Batch 273/883, Training Loss: 0.4609\n",
      "Epoch 7/10, Batch 274/883, Training Loss: 0.6012\n",
      "Epoch 7/10, Batch 275/883, Training Loss: 0.6497\n",
      "Epoch 7/10, Batch 276/883, Training Loss: 0.5488\n",
      "Epoch 7/10, Batch 277/883, Training Loss: 0.6984\n",
      "Epoch 7/10, Batch 278/883, Training Loss: 0.6709\n",
      "Epoch 7/10, Batch 279/883, Training Loss: 0.4644\n",
      "Epoch 7/10, Batch 280/883, Training Loss: 0.3589\n",
      "Epoch 7/10, Batch 281/883, Training Loss: 0.4302\n",
      "Epoch 7/10, Batch 282/883, Training Loss: 0.4309\n",
      "Epoch 7/10, Batch 283/883, Training Loss: 0.8908\n",
      "Epoch 7/10, Batch 284/883, Training Loss: 0.6899\n",
      "Epoch 7/10, Batch 285/883, Training Loss: 0.5380\n",
      "Epoch 7/10, Batch 286/883, Training Loss: 0.7661\n",
      "Epoch 7/10, Batch 287/883, Training Loss: 0.4189\n",
      "Epoch 7/10, Batch 288/883, Training Loss: 0.4613\n",
      "Epoch 7/10, Batch 289/883, Training Loss: 1.0331\n",
      "Epoch 7/10, Batch 290/883, Training Loss: 0.7613\n",
      "Epoch 7/10, Batch 291/883, Training Loss: 0.5055\n",
      "Epoch 7/10, Batch 292/883, Training Loss: 0.4580\n",
      "Epoch 7/10, Batch 293/883, Training Loss: 0.5098\n",
      "Epoch 7/10, Batch 294/883, Training Loss: 1.0092\n",
      "Epoch 7/10, Batch 295/883, Training Loss: 0.7344\n",
      "Epoch 7/10, Batch 296/883, Training Loss: 0.4720\n",
      "Epoch 7/10, Batch 297/883, Training Loss: 0.5439\n",
      "Epoch 7/10, Batch 298/883, Training Loss: 0.7483\n",
      "Epoch 7/10, Batch 299/883, Training Loss: 0.6702\n",
      "Epoch 7/10, Batch 300/883, Training Loss: 0.5829\n",
      "Epoch 7/10, Batch 301/883, Training Loss: 0.5898\n",
      "Epoch 7/10, Batch 302/883, Training Loss: 0.6825\n",
      "Epoch 7/10, Batch 303/883, Training Loss: 0.5466\n",
      "Epoch 7/10, Batch 304/883, Training Loss: 0.7089\n",
      "Epoch 7/10, Batch 305/883, Training Loss: 0.8580\n",
      "Epoch 7/10, Batch 306/883, Training Loss: 0.6965\n",
      "Epoch 7/10, Batch 307/883, Training Loss: 0.7768\n",
      "Epoch 7/10, Batch 308/883, Training Loss: 0.7305\n",
      "Epoch 7/10, Batch 309/883, Training Loss: 0.6571\n",
      "Epoch 7/10, Batch 310/883, Training Loss: 0.4455\n",
      "Epoch 7/10, Batch 311/883, Training Loss: 0.3926\n",
      "Epoch 7/10, Batch 312/883, Training Loss: 0.3891\n",
      "Epoch 7/10, Batch 313/883, Training Loss: 0.5663\n",
      "Epoch 7/10, Batch 314/883, Training Loss: 0.5806\n",
      "Epoch 7/10, Batch 315/883, Training Loss: 0.6853\n",
      "Epoch 7/10, Batch 316/883, Training Loss: 0.6863\n",
      "Epoch 7/10, Batch 317/883, Training Loss: 0.5175\n",
      "Epoch 7/10, Batch 318/883, Training Loss: 0.4806\n",
      "Epoch 7/10, Batch 319/883, Training Loss: 0.6211\n",
      "Epoch 7/10, Batch 320/883, Training Loss: 0.4628\n",
      "Epoch 7/10, Batch 321/883, Training Loss: 0.6782\n",
      "Epoch 7/10, Batch 322/883, Training Loss: 0.2978\n",
      "Epoch 7/10, Batch 323/883, Training Loss: 0.5758\n",
      "Epoch 7/10, Batch 324/883, Training Loss: 0.3579\n",
      "Epoch 7/10, Batch 325/883, Training Loss: 0.5445\n",
      "Epoch 7/10, Batch 326/883, Training Loss: 0.8144\n",
      "Epoch 7/10, Batch 327/883, Training Loss: 0.6418\n",
      "Epoch 7/10, Batch 328/883, Training Loss: 0.9879\n",
      "Epoch 7/10, Batch 329/883, Training Loss: 0.7218\n",
      "Epoch 7/10, Batch 330/883, Training Loss: 0.5632\n",
      "Epoch 7/10, Batch 331/883, Training Loss: 0.7077\n",
      "Epoch 7/10, Batch 332/883, Training Loss: 0.5451\n",
      "Epoch 7/10, Batch 333/883, Training Loss: 0.5194\n",
      "Epoch 7/10, Batch 334/883, Training Loss: 0.5548\n",
      "Epoch 7/10, Batch 335/883, Training Loss: 0.9114\n",
      "Epoch 7/10, Batch 336/883, Training Loss: 0.6847\n",
      "Epoch 7/10, Batch 337/883, Training Loss: 0.7433\n",
      "Epoch 7/10, Batch 338/883, Training Loss: 0.6280\n",
      "Epoch 7/10, Batch 339/883, Training Loss: 0.7436\n",
      "Epoch 7/10, Batch 340/883, Training Loss: 0.8210\n",
      "Epoch 7/10, Batch 341/883, Training Loss: 0.3916\n",
      "Epoch 7/10, Batch 342/883, Training Loss: 0.3553\n",
      "Epoch 7/10, Batch 343/883, Training Loss: 0.5755\n",
      "Epoch 7/10, Batch 344/883, Training Loss: 0.7636\n",
      "Epoch 7/10, Batch 345/883, Training Loss: 0.7823\n",
      "Epoch 7/10, Batch 346/883, Training Loss: 0.4574\n",
      "Epoch 7/10, Batch 347/883, Training Loss: 0.4172\n",
      "Epoch 7/10, Batch 348/883, Training Loss: 0.4520\n",
      "Epoch 7/10, Batch 349/883, Training Loss: 0.5209\n",
      "Epoch 7/10, Batch 350/883, Training Loss: 0.7106\n",
      "Epoch 7/10, Batch 351/883, Training Loss: 0.6144\n",
      "Epoch 7/10, Batch 352/883, Training Loss: 0.5088\n",
      "Epoch 7/10, Batch 353/883, Training Loss: 0.7588\n",
      "Epoch 7/10, Batch 354/883, Training Loss: 0.7523\n",
      "Epoch 7/10, Batch 355/883, Training Loss: 0.7136\n",
      "Epoch 7/10, Batch 356/883, Training Loss: 0.6459\n",
      "Epoch 7/10, Batch 357/883, Training Loss: 0.7755\n",
      "Epoch 7/10, Batch 358/883, Training Loss: 0.7573\n",
      "Epoch 7/10, Batch 359/883, Training Loss: 0.5640\n",
      "Epoch 7/10, Batch 360/883, Training Loss: 0.6829\n",
      "Epoch 7/10, Batch 361/883, Training Loss: 0.9730\n",
      "Epoch 7/10, Batch 362/883, Training Loss: 0.8749\n",
      "Epoch 7/10, Batch 363/883, Training Loss: 0.8404\n",
      "Epoch 7/10, Batch 364/883, Training Loss: 0.5303\n",
      "Epoch 7/10, Batch 365/883, Training Loss: 0.7612\n",
      "Epoch 7/10, Batch 366/883, Training Loss: 0.6687\n",
      "Epoch 7/10, Batch 367/883, Training Loss: 0.4683\n",
      "Epoch 7/10, Batch 368/883, Training Loss: 0.6450\n",
      "Epoch 7/10, Batch 369/883, Training Loss: 0.5234\n",
      "Epoch 7/10, Batch 370/883, Training Loss: 0.4994\n",
      "Epoch 7/10, Batch 371/883, Training Loss: 0.5619\n",
      "Epoch 7/10, Batch 372/883, Training Loss: 0.6083\n",
      "Epoch 7/10, Batch 373/883, Training Loss: 0.6193\n",
      "Epoch 7/10, Batch 374/883, Training Loss: 0.4386\n",
      "Epoch 7/10, Batch 375/883, Training Loss: 0.5149\n",
      "Epoch 7/10, Batch 376/883, Training Loss: 0.7988\n",
      "Epoch 7/10, Batch 377/883, Training Loss: 0.7458\n",
      "Epoch 7/10, Batch 378/883, Training Loss: 0.6420\n",
      "Epoch 7/10, Batch 379/883, Training Loss: 0.4505\n",
      "Epoch 7/10, Batch 380/883, Training Loss: 0.3753\n",
      "Epoch 7/10, Batch 381/883, Training Loss: 0.7634\n",
      "Epoch 7/10, Batch 382/883, Training Loss: 0.5298\n",
      "Epoch 7/10, Batch 383/883, Training Loss: 0.5020\n",
      "Epoch 7/10, Batch 384/883, Training Loss: 0.5705\n",
      "Epoch 7/10, Batch 385/883, Training Loss: 0.7143\n",
      "Epoch 7/10, Batch 386/883, Training Loss: 0.8096\n",
      "Epoch 7/10, Batch 387/883, Training Loss: 0.6479\n",
      "Epoch 7/10, Batch 388/883, Training Loss: 0.7763\n",
      "Epoch 7/10, Batch 389/883, Training Loss: 0.5374\n",
      "Epoch 7/10, Batch 390/883, Training Loss: 0.4562\n",
      "Epoch 7/10, Batch 391/883, Training Loss: 0.5688\n",
      "Epoch 7/10, Batch 392/883, Training Loss: 0.4540\n",
      "Epoch 7/10, Batch 393/883, Training Loss: 0.8602\n",
      "Epoch 7/10, Batch 394/883, Training Loss: 0.5591\n",
      "Epoch 7/10, Batch 395/883, Training Loss: 0.7064\n",
      "Epoch 7/10, Batch 396/883, Training Loss: 0.3701\n",
      "Epoch 7/10, Batch 397/883, Training Loss: 0.6148\n",
      "Epoch 7/10, Batch 398/883, Training Loss: 0.8737\n",
      "Epoch 7/10, Batch 399/883, Training Loss: 0.3720\n",
      "Epoch 7/10, Batch 400/883, Training Loss: 0.3731\n",
      "Epoch 7/10, Batch 401/883, Training Loss: 0.5318\n",
      "Epoch 7/10, Batch 402/883, Training Loss: 0.5038\n",
      "Epoch 7/10, Batch 403/883, Training Loss: 0.8044\n",
      "Epoch 7/10, Batch 404/883, Training Loss: 0.5810\n",
      "Epoch 7/10, Batch 405/883, Training Loss: 0.7381\n",
      "Epoch 7/10, Batch 406/883, Training Loss: 0.4121\n",
      "Epoch 7/10, Batch 407/883, Training Loss: 0.5984\n",
      "Epoch 7/10, Batch 408/883, Training Loss: 0.5646\n",
      "Epoch 7/10, Batch 409/883, Training Loss: 0.8873\n",
      "Epoch 7/10, Batch 410/883, Training Loss: 0.6814\n",
      "Epoch 7/10, Batch 411/883, Training Loss: 0.4091\n",
      "Epoch 7/10, Batch 412/883, Training Loss: 0.3759\n",
      "Epoch 7/10, Batch 413/883, Training Loss: 0.7394\n",
      "Epoch 7/10, Batch 414/883, Training Loss: 0.5783\n",
      "Epoch 7/10, Batch 415/883, Training Loss: 0.6440\n",
      "Epoch 7/10, Batch 416/883, Training Loss: 0.4811\n",
      "Epoch 7/10, Batch 417/883, Training Loss: 0.6457\n",
      "Epoch 7/10, Batch 418/883, Training Loss: 0.6826\n",
      "Epoch 7/10, Batch 419/883, Training Loss: 0.4970\n",
      "Epoch 7/10, Batch 420/883, Training Loss: 0.6749\n",
      "Epoch 7/10, Batch 421/883, Training Loss: 0.8392\n",
      "Epoch 7/10, Batch 422/883, Training Loss: 0.3388\n",
      "Epoch 7/10, Batch 423/883, Training Loss: 0.8206\n",
      "Epoch 7/10, Batch 424/883, Training Loss: 0.5020\n",
      "Epoch 7/10, Batch 425/883, Training Loss: 0.6010\n",
      "Epoch 7/10, Batch 426/883, Training Loss: 0.6796\n",
      "Epoch 7/10, Batch 427/883, Training Loss: 0.5353\n",
      "Epoch 7/10, Batch 428/883, Training Loss: 0.4670\n",
      "Epoch 7/10, Batch 429/883, Training Loss: 0.4459\n",
      "Epoch 7/10, Batch 430/883, Training Loss: 0.4436\n",
      "Epoch 7/10, Batch 431/883, Training Loss: 0.5407\n",
      "Epoch 7/10, Batch 432/883, Training Loss: 0.6367\n",
      "Epoch 7/10, Batch 433/883, Training Loss: 0.6813\n",
      "Epoch 7/10, Batch 434/883, Training Loss: 0.7381\n",
      "Epoch 7/10, Batch 435/883, Training Loss: 0.6236\n",
      "Epoch 7/10, Batch 436/883, Training Loss: 0.8495\n",
      "Epoch 7/10, Batch 437/883, Training Loss: 0.4384\n",
      "Epoch 7/10, Batch 438/883, Training Loss: 0.4598\n",
      "Epoch 7/10, Batch 439/883, Training Loss: 0.7955\n",
      "Epoch 7/10, Batch 440/883, Training Loss: 0.3397\n",
      "Epoch 7/10, Batch 441/883, Training Loss: 0.6850\n",
      "Epoch 7/10, Batch 442/883, Training Loss: 0.5804\n",
      "Epoch 7/10, Batch 443/883, Training Loss: 0.8354\n",
      "Epoch 7/10, Batch 444/883, Training Loss: 0.9890\n",
      "Epoch 7/10, Batch 445/883, Training Loss: 0.5231\n",
      "Epoch 7/10, Batch 446/883, Training Loss: 0.6536\n",
      "Epoch 7/10, Batch 447/883, Training Loss: 0.7425\n",
      "Epoch 7/10, Batch 448/883, Training Loss: 0.9278\n",
      "Epoch 7/10, Batch 449/883, Training Loss: 0.5885\n",
      "Epoch 7/10, Batch 450/883, Training Loss: 0.7281\n",
      "Epoch 7/10, Batch 451/883, Training Loss: 0.6314\n",
      "Epoch 7/10, Batch 452/883, Training Loss: 0.7895\n",
      "Epoch 7/10, Batch 453/883, Training Loss: 0.6241\n",
      "Epoch 7/10, Batch 454/883, Training Loss: 0.5175\n",
      "Epoch 7/10, Batch 455/883, Training Loss: 0.4152\n",
      "Epoch 7/10, Batch 456/883, Training Loss: 0.8092\n",
      "Epoch 7/10, Batch 457/883, Training Loss: 0.4228\n",
      "Epoch 7/10, Batch 458/883, Training Loss: 0.6005\n",
      "Epoch 7/10, Batch 459/883, Training Loss: 1.1065\n",
      "Epoch 7/10, Batch 460/883, Training Loss: 0.3710\n",
      "Epoch 7/10, Batch 461/883, Training Loss: 0.4555\n",
      "Epoch 7/10, Batch 462/883, Training Loss: 0.6467\n",
      "Epoch 7/10, Batch 463/883, Training Loss: 0.6531\n",
      "Epoch 7/10, Batch 464/883, Training Loss: 0.4770\n",
      "Epoch 7/10, Batch 465/883, Training Loss: 0.4746\n",
      "Epoch 7/10, Batch 466/883, Training Loss: 0.8071\n",
      "Epoch 7/10, Batch 467/883, Training Loss: 0.3805\n",
      "Epoch 7/10, Batch 468/883, Training Loss: 0.5057\n",
      "Epoch 7/10, Batch 469/883, Training Loss: 0.2689\n",
      "Epoch 7/10, Batch 470/883, Training Loss: 0.5659\n",
      "Epoch 7/10, Batch 471/883, Training Loss: 0.4540\n",
      "Epoch 7/10, Batch 472/883, Training Loss: 0.5840\n",
      "Epoch 7/10, Batch 473/883, Training Loss: 0.4854\n",
      "Epoch 7/10, Batch 474/883, Training Loss: 0.9178\n",
      "Epoch 7/10, Batch 475/883, Training Loss: 0.6279\n",
      "Epoch 7/10, Batch 476/883, Training Loss: 0.5438\n",
      "Epoch 7/10, Batch 477/883, Training Loss: 0.6232\n",
      "Epoch 7/10, Batch 478/883, Training Loss: 0.9986\n",
      "Epoch 7/10, Batch 479/883, Training Loss: 0.5632\n",
      "Epoch 7/10, Batch 480/883, Training Loss: 0.4826\n",
      "Epoch 7/10, Batch 481/883, Training Loss: 0.7896\n",
      "Epoch 7/10, Batch 482/883, Training Loss: 0.5469\n",
      "Epoch 7/10, Batch 483/883, Training Loss: 0.8981\n",
      "Epoch 7/10, Batch 484/883, Training Loss: 0.6264\n",
      "Epoch 7/10, Batch 485/883, Training Loss: 0.5008\n",
      "Epoch 7/10, Batch 486/883, Training Loss: 0.4714\n",
      "Epoch 7/10, Batch 487/883, Training Loss: 0.4911\n",
      "Epoch 7/10, Batch 488/883, Training Loss: 0.4693\n",
      "Epoch 7/10, Batch 489/883, Training Loss: 0.7308\n",
      "Epoch 7/10, Batch 490/883, Training Loss: 0.3619\n",
      "Epoch 7/10, Batch 491/883, Training Loss: 0.6300\n",
      "Epoch 7/10, Batch 492/883, Training Loss: 0.2839\n",
      "Epoch 7/10, Batch 493/883, Training Loss: 0.5031\n",
      "Epoch 7/10, Batch 494/883, Training Loss: 0.8522\n",
      "Epoch 7/10, Batch 495/883, Training Loss: 0.4483\n",
      "Epoch 7/10, Batch 496/883, Training Loss: 0.6415\n",
      "Epoch 7/10, Batch 497/883, Training Loss: 0.7796\n",
      "Epoch 7/10, Batch 498/883, Training Loss: 0.5500\n",
      "Epoch 7/10, Batch 499/883, Training Loss: 0.8469\n",
      "Epoch 7/10, Batch 500/883, Training Loss: 0.6758\n",
      "Epoch 7/10, Batch 501/883, Training Loss: 0.9162\n",
      "Epoch 7/10, Batch 502/883, Training Loss: 0.5179\n",
      "Epoch 7/10, Batch 503/883, Training Loss: 0.4407\n",
      "Epoch 7/10, Batch 504/883, Training Loss: 0.5936\n",
      "Epoch 7/10, Batch 505/883, Training Loss: 0.5446\n",
      "Epoch 7/10, Batch 506/883, Training Loss: 0.4405\n",
      "Epoch 7/10, Batch 507/883, Training Loss: 0.4615\n",
      "Epoch 7/10, Batch 508/883, Training Loss: 0.7288\n",
      "Epoch 7/10, Batch 509/883, Training Loss: 0.8157\n",
      "Epoch 7/10, Batch 510/883, Training Loss: 0.8608\n",
      "Epoch 7/10, Batch 511/883, Training Loss: 0.5619\n",
      "Epoch 7/10, Batch 512/883, Training Loss: 0.5779\n",
      "Epoch 7/10, Batch 513/883, Training Loss: 0.4644\n",
      "Epoch 7/10, Batch 514/883, Training Loss: 0.8814\n",
      "Epoch 7/10, Batch 515/883, Training Loss: 0.8684\n",
      "Epoch 7/10, Batch 516/883, Training Loss: 0.7404\n",
      "Epoch 7/10, Batch 517/883, Training Loss: 0.7345\n",
      "Epoch 7/10, Batch 518/883, Training Loss: 0.7544\n",
      "Epoch 7/10, Batch 519/883, Training Loss: 0.9283\n",
      "Epoch 7/10, Batch 520/883, Training Loss: 0.8039\n",
      "Epoch 7/10, Batch 521/883, Training Loss: 0.4137\n",
      "Epoch 7/10, Batch 522/883, Training Loss: 0.5151\n",
      "Epoch 7/10, Batch 523/883, Training Loss: 0.5508\n",
      "Epoch 7/10, Batch 524/883, Training Loss: 0.6170\n",
      "Epoch 7/10, Batch 525/883, Training Loss: 0.4538\n",
      "Epoch 7/10, Batch 526/883, Training Loss: 0.7217\n",
      "Epoch 7/10, Batch 527/883, Training Loss: 0.4525\n",
      "Epoch 7/10, Batch 528/883, Training Loss: 0.5059\n",
      "Epoch 7/10, Batch 529/883, Training Loss: 0.5012\n",
      "Epoch 7/10, Batch 530/883, Training Loss: 0.4414\n",
      "Epoch 7/10, Batch 531/883, Training Loss: 0.9219\n",
      "Epoch 7/10, Batch 532/883, Training Loss: 1.1188\n",
      "Epoch 7/10, Batch 533/883, Training Loss: 0.4031\n",
      "Epoch 7/10, Batch 534/883, Training Loss: 0.5193\n",
      "Epoch 7/10, Batch 535/883, Training Loss: 0.8611\n",
      "Epoch 7/10, Batch 536/883, Training Loss: 0.6561\n",
      "Epoch 7/10, Batch 537/883, Training Loss: 0.4766\n",
      "Epoch 7/10, Batch 538/883, Training Loss: 0.5422\n",
      "Epoch 7/10, Batch 539/883, Training Loss: 0.5616\n",
      "Epoch 7/10, Batch 540/883, Training Loss: 0.6245\n",
      "Epoch 7/10, Batch 541/883, Training Loss: 0.7910\n",
      "Epoch 7/10, Batch 542/883, Training Loss: 0.4788\n",
      "Epoch 7/10, Batch 543/883, Training Loss: 0.4482\n",
      "Epoch 7/10, Batch 544/883, Training Loss: 0.6239\n",
      "Epoch 7/10, Batch 545/883, Training Loss: 0.6139\n",
      "Epoch 7/10, Batch 546/883, Training Loss: 0.7035\n",
      "Epoch 7/10, Batch 547/883, Training Loss: 0.6256\n",
      "Epoch 7/10, Batch 548/883, Training Loss: 0.5263\n",
      "Epoch 7/10, Batch 549/883, Training Loss: 0.4897\n",
      "Epoch 7/10, Batch 550/883, Training Loss: 0.6868\n",
      "Epoch 7/10, Batch 551/883, Training Loss: 0.3730\n",
      "Epoch 7/10, Batch 552/883, Training Loss: 0.7311\n",
      "Epoch 7/10, Batch 553/883, Training Loss: 0.4407\n",
      "Epoch 7/10, Batch 554/883, Training Loss: 0.4292\n",
      "Epoch 7/10, Batch 555/883, Training Loss: 0.3010\n",
      "Epoch 7/10, Batch 556/883, Training Loss: 0.5791\n",
      "Epoch 7/10, Batch 557/883, Training Loss: 0.5838\n",
      "Epoch 7/10, Batch 558/883, Training Loss: 0.5944\n",
      "Epoch 7/10, Batch 559/883, Training Loss: 0.5650\n",
      "Epoch 7/10, Batch 560/883, Training Loss: 0.5809\n",
      "Epoch 7/10, Batch 561/883, Training Loss: 0.6669\n",
      "Epoch 7/10, Batch 562/883, Training Loss: 0.3720\n",
      "Epoch 7/10, Batch 563/883, Training Loss: 0.5773\n",
      "Epoch 7/10, Batch 564/883, Training Loss: 0.3851\n",
      "Epoch 7/10, Batch 565/883, Training Loss: 0.5206\n",
      "Epoch 7/10, Batch 566/883, Training Loss: 0.7684\n",
      "Epoch 7/10, Batch 567/883, Training Loss: 0.3798\n",
      "Epoch 7/10, Batch 568/883, Training Loss: 0.4001\n",
      "Epoch 7/10, Batch 569/883, Training Loss: 0.4006\n",
      "Epoch 7/10, Batch 570/883, Training Loss: 0.3845\n",
      "Epoch 7/10, Batch 571/883, Training Loss: 0.6398\n",
      "Epoch 7/10, Batch 572/883, Training Loss: 0.8658\n",
      "Epoch 7/10, Batch 573/883, Training Loss: 0.4849\n",
      "Epoch 7/10, Batch 574/883, Training Loss: 0.7483\n",
      "Epoch 7/10, Batch 575/883, Training Loss: 0.5014\n",
      "Epoch 7/10, Batch 576/883, Training Loss: 0.5124\n",
      "Epoch 7/10, Batch 577/883, Training Loss: 0.5419\n",
      "Epoch 7/10, Batch 578/883, Training Loss: 0.5179\n",
      "Epoch 7/10, Batch 579/883, Training Loss: 0.4541\n",
      "Epoch 7/10, Batch 580/883, Training Loss: 0.4726\n",
      "Epoch 7/10, Batch 581/883, Training Loss: 0.5066\n",
      "Epoch 7/10, Batch 582/883, Training Loss: 0.6478\n",
      "Epoch 7/10, Batch 583/883, Training Loss: 0.4955\n",
      "Epoch 7/10, Batch 584/883, Training Loss: 0.4186\n",
      "Epoch 7/10, Batch 585/883, Training Loss: 0.4871\n",
      "Epoch 7/10, Batch 586/883, Training Loss: 0.4795\n",
      "Epoch 7/10, Batch 587/883, Training Loss: 0.5509\n",
      "Epoch 7/10, Batch 588/883, Training Loss: 0.6406\n",
      "Epoch 7/10, Batch 589/883, Training Loss: 0.6564\n",
      "Epoch 7/10, Batch 590/883, Training Loss: 0.5864\n",
      "Epoch 7/10, Batch 591/883, Training Loss: 0.7733\n",
      "Epoch 7/10, Batch 592/883, Training Loss: 0.4261\n",
      "Epoch 7/10, Batch 593/883, Training Loss: 0.6631\n",
      "Epoch 7/10, Batch 594/883, Training Loss: 1.1747\n",
      "Epoch 7/10, Batch 595/883, Training Loss: 0.2891\n",
      "Epoch 7/10, Batch 596/883, Training Loss: 0.4896\n",
      "Epoch 7/10, Batch 597/883, Training Loss: 0.3216\n",
      "Epoch 7/10, Batch 598/883, Training Loss: 0.5516\n",
      "Epoch 7/10, Batch 599/883, Training Loss: 0.5952\n",
      "Epoch 7/10, Batch 600/883, Training Loss: 0.5649\n",
      "Epoch 7/10, Batch 601/883, Training Loss: 0.4265\n",
      "Epoch 7/10, Batch 602/883, Training Loss: 0.8043\n",
      "Epoch 7/10, Batch 603/883, Training Loss: 0.6159\n",
      "Epoch 7/10, Batch 604/883, Training Loss: 0.6935\n",
      "Epoch 7/10, Batch 605/883, Training Loss: 0.5232\n",
      "Epoch 7/10, Batch 606/883, Training Loss: 0.3041\n",
      "Epoch 7/10, Batch 607/883, Training Loss: 0.5547\n",
      "Epoch 7/10, Batch 608/883, Training Loss: 0.9274\n",
      "Epoch 7/10, Batch 609/883, Training Loss: 0.5177\n",
      "Epoch 7/10, Batch 610/883, Training Loss: 0.6617\n",
      "Epoch 7/10, Batch 611/883, Training Loss: 1.1103\n",
      "Epoch 7/10, Batch 612/883, Training Loss: 0.4718\n",
      "Epoch 7/10, Batch 613/883, Training Loss: 0.6263\n",
      "Epoch 7/10, Batch 614/883, Training Loss: 0.5518\n",
      "Epoch 7/10, Batch 615/883, Training Loss: 0.5870\n",
      "Epoch 7/10, Batch 616/883, Training Loss: 0.5162\n",
      "Epoch 7/10, Batch 617/883, Training Loss: 0.9705\n",
      "Epoch 7/10, Batch 618/883, Training Loss: 0.3812\n",
      "Epoch 7/10, Batch 619/883, Training Loss: 0.5838\n",
      "Epoch 7/10, Batch 620/883, Training Loss: 0.6022\n",
      "Epoch 7/10, Batch 621/883, Training Loss: 0.4197\n",
      "Epoch 7/10, Batch 622/883, Training Loss: 0.6343\n",
      "Epoch 7/10, Batch 623/883, Training Loss: 0.8055\n",
      "Epoch 7/10, Batch 624/883, Training Loss: 0.5702\n",
      "Epoch 7/10, Batch 625/883, Training Loss: 0.2652\n",
      "Epoch 7/10, Batch 626/883, Training Loss: 0.5286\n",
      "Epoch 7/10, Batch 627/883, Training Loss: 0.5788\n",
      "Epoch 7/10, Batch 628/883, Training Loss: 0.6645\n",
      "Epoch 7/10, Batch 629/883, Training Loss: 0.6226\n",
      "Epoch 7/10, Batch 630/883, Training Loss: 0.4763\n",
      "Epoch 7/10, Batch 631/883, Training Loss: 0.6461\n",
      "Epoch 7/10, Batch 632/883, Training Loss: 0.3654\n",
      "Epoch 7/10, Batch 633/883, Training Loss: 0.6497\n",
      "Epoch 7/10, Batch 634/883, Training Loss: 1.1845\n",
      "Epoch 7/10, Batch 635/883, Training Loss: 0.7080\n",
      "Epoch 7/10, Batch 636/883, Training Loss: 0.7136\n",
      "Epoch 7/10, Batch 637/883, Training Loss: 1.0674\n",
      "Epoch 7/10, Batch 638/883, Training Loss: 0.5693\n",
      "Epoch 7/10, Batch 639/883, Training Loss: 0.4110\n",
      "Epoch 7/10, Batch 640/883, Training Loss: 0.9593\n",
      "Epoch 7/10, Batch 641/883, Training Loss: 0.4186\n",
      "Epoch 7/10, Batch 642/883, Training Loss: 0.6633\n",
      "Epoch 7/10, Batch 643/883, Training Loss: 0.4072\n",
      "Epoch 7/10, Batch 644/883, Training Loss: 0.5119\n",
      "Epoch 7/10, Batch 645/883, Training Loss: 0.4991\n",
      "Epoch 7/10, Batch 646/883, Training Loss: 0.5628\n",
      "Epoch 7/10, Batch 647/883, Training Loss: 0.4765\n",
      "Epoch 7/10, Batch 648/883, Training Loss: 0.5162\n",
      "Epoch 7/10, Batch 649/883, Training Loss: 0.6350\n",
      "Epoch 7/10, Batch 650/883, Training Loss: 0.6713\n",
      "Epoch 7/10, Batch 651/883, Training Loss: 0.7100\n",
      "Epoch 7/10, Batch 652/883, Training Loss: 0.9373\n",
      "Epoch 7/10, Batch 653/883, Training Loss: 0.4626\n",
      "Epoch 7/10, Batch 654/883, Training Loss: 0.5591\n",
      "Epoch 7/10, Batch 655/883, Training Loss: 0.4475\n",
      "Epoch 7/10, Batch 656/883, Training Loss: 0.9886\n",
      "Epoch 7/10, Batch 657/883, Training Loss: 0.9764\n",
      "Epoch 7/10, Batch 658/883, Training Loss: 0.4663\n",
      "Epoch 7/10, Batch 659/883, Training Loss: 0.4935\n",
      "Epoch 7/10, Batch 660/883, Training Loss: 0.6628\n",
      "Epoch 7/10, Batch 661/883, Training Loss: 0.5926\n",
      "Epoch 7/10, Batch 662/883, Training Loss: 0.7812\n",
      "Epoch 7/10, Batch 663/883, Training Loss: 0.7031\n",
      "Epoch 7/10, Batch 664/883, Training Loss: 0.7674\n",
      "Epoch 7/10, Batch 665/883, Training Loss: 0.7221\n",
      "Epoch 7/10, Batch 666/883, Training Loss: 0.7349\n",
      "Epoch 7/10, Batch 667/883, Training Loss: 0.3833\n",
      "Epoch 7/10, Batch 668/883, Training Loss: 0.6540\n",
      "Epoch 7/10, Batch 669/883, Training Loss: 0.3268\n",
      "Epoch 7/10, Batch 670/883, Training Loss: 0.4622\n",
      "Epoch 7/10, Batch 671/883, Training Loss: 0.7577\n",
      "Epoch 7/10, Batch 672/883, Training Loss: 0.9661\n",
      "Epoch 7/10, Batch 673/883, Training Loss: 0.4211\n",
      "Epoch 7/10, Batch 674/883, Training Loss: 0.6377\n",
      "Epoch 7/10, Batch 675/883, Training Loss: 0.6484\n",
      "Epoch 7/10, Batch 676/883, Training Loss: 0.5689\n",
      "Epoch 7/10, Batch 677/883, Training Loss: 0.5528\n",
      "Epoch 7/10, Batch 678/883, Training Loss: 0.8139\n",
      "Epoch 7/10, Batch 679/883, Training Loss: 0.5265\n",
      "Epoch 7/10, Batch 680/883, Training Loss: 0.4119\n",
      "Epoch 7/10, Batch 681/883, Training Loss: 0.3906\n",
      "Epoch 7/10, Batch 682/883, Training Loss: 0.4584\n",
      "Epoch 7/10, Batch 683/883, Training Loss: 0.8523\n",
      "Epoch 7/10, Batch 684/883, Training Loss: 0.8738\n",
      "Epoch 7/10, Batch 685/883, Training Loss: 0.5803\n",
      "Epoch 7/10, Batch 686/883, Training Loss: 0.4286\n",
      "Epoch 7/10, Batch 687/883, Training Loss: 0.3975\n",
      "Epoch 7/10, Batch 688/883, Training Loss: 0.5254\n",
      "Epoch 7/10, Batch 689/883, Training Loss: 0.4860\n",
      "Epoch 7/10, Batch 690/883, Training Loss: 0.6530\n",
      "Epoch 7/10, Batch 691/883, Training Loss: 0.3902\n",
      "Epoch 7/10, Batch 692/883, Training Loss: 0.6597\n",
      "Epoch 7/10, Batch 693/883, Training Loss: 0.7588\n",
      "Epoch 7/10, Batch 694/883, Training Loss: 0.4030\n",
      "Epoch 7/10, Batch 695/883, Training Loss: 0.7498\n",
      "Epoch 7/10, Batch 696/883, Training Loss: 0.5488\n",
      "Epoch 7/10, Batch 697/883, Training Loss: 0.6441\n",
      "Epoch 7/10, Batch 698/883, Training Loss: 0.3983\n",
      "Epoch 7/10, Batch 699/883, Training Loss: 0.5610\n",
      "Epoch 7/10, Batch 700/883, Training Loss: 0.5320\n",
      "Epoch 7/10, Batch 701/883, Training Loss: 0.6035\n",
      "Epoch 7/10, Batch 702/883, Training Loss: 0.6674\n",
      "Epoch 7/10, Batch 703/883, Training Loss: 0.5380\n",
      "Epoch 7/10, Batch 704/883, Training Loss: 0.3731\n",
      "Epoch 7/10, Batch 705/883, Training Loss: 0.5117\n",
      "Epoch 7/10, Batch 706/883, Training Loss: 0.4826\n",
      "Epoch 7/10, Batch 707/883, Training Loss: 0.4363\n",
      "Epoch 7/10, Batch 708/883, Training Loss: 0.4511\n",
      "Epoch 7/10, Batch 709/883, Training Loss: 0.3443\n",
      "Epoch 7/10, Batch 710/883, Training Loss: 0.5472\n",
      "Epoch 7/10, Batch 711/883, Training Loss: 0.6783\n",
      "Epoch 7/10, Batch 712/883, Training Loss: 0.5667\n",
      "Epoch 7/10, Batch 713/883, Training Loss: 0.6889\n",
      "Epoch 7/10, Batch 714/883, Training Loss: 0.8067\n",
      "Epoch 7/10, Batch 715/883, Training Loss: 0.4007\n",
      "Epoch 7/10, Batch 716/883, Training Loss: 0.4109\n",
      "Epoch 7/10, Batch 717/883, Training Loss: 0.2886\n",
      "Epoch 7/10, Batch 718/883, Training Loss: 0.7260\n",
      "Epoch 7/10, Batch 719/883, Training Loss: 0.5047\n",
      "Epoch 7/10, Batch 720/883, Training Loss: 0.5415\n",
      "Epoch 7/10, Batch 721/883, Training Loss: 0.5598\n",
      "Epoch 7/10, Batch 722/883, Training Loss: 0.7827\n",
      "Epoch 7/10, Batch 723/883, Training Loss: 0.7646\n",
      "Epoch 7/10, Batch 724/883, Training Loss: 0.6186\n",
      "Epoch 7/10, Batch 725/883, Training Loss: 0.5015\n",
      "Epoch 7/10, Batch 726/883, Training Loss: 0.4254\n",
      "Epoch 7/10, Batch 727/883, Training Loss: 0.3653\n",
      "Epoch 7/10, Batch 728/883, Training Loss: 0.4609\n",
      "Epoch 7/10, Batch 729/883, Training Loss: 0.7422\n",
      "Epoch 7/10, Batch 730/883, Training Loss: 0.6354\n",
      "Epoch 7/10, Batch 731/883, Training Loss: 1.0767\n",
      "Epoch 7/10, Batch 732/883, Training Loss: 0.5435\n",
      "Epoch 7/10, Batch 733/883, Training Loss: 0.7547\n",
      "Epoch 7/10, Batch 734/883, Training Loss: 0.8430\n",
      "Epoch 7/10, Batch 735/883, Training Loss: 0.7138\n",
      "Epoch 7/10, Batch 736/883, Training Loss: 0.6802\n",
      "Epoch 7/10, Batch 737/883, Training Loss: 0.4818\n",
      "Epoch 7/10, Batch 738/883, Training Loss: 0.4083\n",
      "Epoch 7/10, Batch 739/883, Training Loss: 0.7051\n",
      "Epoch 7/10, Batch 740/883, Training Loss: 0.6507\n",
      "Epoch 7/10, Batch 741/883, Training Loss: 0.8098\n",
      "Epoch 7/10, Batch 742/883, Training Loss: 0.5679\n",
      "Epoch 7/10, Batch 743/883, Training Loss: 0.5092\n",
      "Epoch 7/10, Batch 744/883, Training Loss: 0.5003\n",
      "Epoch 7/10, Batch 745/883, Training Loss: 0.6095\n",
      "Epoch 7/10, Batch 746/883, Training Loss: 0.6127\n",
      "Epoch 7/10, Batch 747/883, Training Loss: 0.4650\n",
      "Epoch 7/10, Batch 748/883, Training Loss: 0.6018\n",
      "Epoch 7/10, Batch 749/883, Training Loss: 0.5263\n",
      "Epoch 7/10, Batch 750/883, Training Loss: 0.5241\n",
      "Epoch 7/10, Batch 751/883, Training Loss: 0.8370\n",
      "Epoch 7/10, Batch 752/883, Training Loss: 1.1826\n",
      "Epoch 7/10, Batch 753/883, Training Loss: 1.1832\n",
      "Epoch 7/10, Batch 754/883, Training Loss: 0.6883\n",
      "Epoch 7/10, Batch 755/883, Training Loss: 0.5053\n",
      "Epoch 7/10, Batch 756/883, Training Loss: 0.4615\n",
      "Epoch 7/10, Batch 757/883, Training Loss: 0.5033\n",
      "Epoch 7/10, Batch 758/883, Training Loss: 0.7176\n",
      "Epoch 7/10, Batch 759/883, Training Loss: 0.5573\n",
      "Epoch 7/10, Batch 760/883, Training Loss: 0.6059\n",
      "Epoch 7/10, Batch 761/883, Training Loss: 0.6362\n",
      "Epoch 7/10, Batch 762/883, Training Loss: 0.6385\n",
      "Epoch 7/10, Batch 763/883, Training Loss: 0.7905\n",
      "Epoch 7/10, Batch 764/883, Training Loss: 0.5068\n",
      "Epoch 7/10, Batch 765/883, Training Loss: 0.5664\n",
      "Epoch 7/10, Batch 766/883, Training Loss: 0.5210\n",
      "Epoch 7/10, Batch 767/883, Training Loss: 0.7066\n",
      "Epoch 7/10, Batch 768/883, Training Loss: 0.6995\n",
      "Epoch 7/10, Batch 769/883, Training Loss: 0.5029\n",
      "Epoch 7/10, Batch 770/883, Training Loss: 0.5815\n",
      "Epoch 7/10, Batch 771/883, Training Loss: 0.5403\n",
      "Epoch 7/10, Batch 772/883, Training Loss: 0.6216\n",
      "Epoch 7/10, Batch 773/883, Training Loss: 0.7289\n",
      "Epoch 7/10, Batch 774/883, Training Loss: 0.3689\n",
      "Epoch 7/10, Batch 775/883, Training Loss: 1.0828\n",
      "Epoch 7/10, Batch 776/883, Training Loss: 0.8326\n",
      "Epoch 7/10, Batch 777/883, Training Loss: 0.6726\n",
      "Epoch 7/10, Batch 778/883, Training Loss: 0.7530\n",
      "Epoch 7/10, Batch 779/883, Training Loss: 0.5150\n",
      "Epoch 7/10, Batch 780/883, Training Loss: 0.4716\n",
      "Epoch 7/10, Batch 781/883, Training Loss: 0.7170\n",
      "Epoch 7/10, Batch 782/883, Training Loss: 0.6654\n",
      "Epoch 7/10, Batch 783/883, Training Loss: 0.6390\n",
      "Epoch 7/10, Batch 784/883, Training Loss: 0.7616\n",
      "Epoch 7/10, Batch 785/883, Training Loss: 0.3855\n",
      "Epoch 7/10, Batch 786/883, Training Loss: 0.4991\n",
      "Epoch 7/10, Batch 787/883, Training Loss: 0.6209\n",
      "Epoch 7/10, Batch 788/883, Training Loss: 0.5126\n",
      "Epoch 7/10, Batch 789/883, Training Loss: 0.5422\n",
      "Epoch 7/10, Batch 790/883, Training Loss: 0.4090\n",
      "Epoch 7/10, Batch 791/883, Training Loss: 0.5506\n",
      "Epoch 7/10, Batch 792/883, Training Loss: 0.6198\n",
      "Epoch 7/10, Batch 793/883, Training Loss: 0.7994\n",
      "Epoch 7/10, Batch 794/883, Training Loss: 0.2849\n",
      "Epoch 7/10, Batch 795/883, Training Loss: 0.3795\n",
      "Epoch 7/10, Batch 796/883, Training Loss: 0.8325\n",
      "Epoch 7/10, Batch 797/883, Training Loss: 0.6319\n",
      "Epoch 7/10, Batch 798/883, Training Loss: 0.5246\n",
      "Epoch 7/10, Batch 799/883, Training Loss: 0.3590\n",
      "Epoch 7/10, Batch 800/883, Training Loss: 0.6048\n",
      "Epoch 7/10, Batch 801/883, Training Loss: 0.6720\n",
      "Epoch 7/10, Batch 802/883, Training Loss: 0.9532\n",
      "Epoch 7/10, Batch 803/883, Training Loss: 0.6924\n",
      "Epoch 7/10, Batch 804/883, Training Loss: 0.3939\n",
      "Epoch 7/10, Batch 805/883, Training Loss: 0.8637\n",
      "Epoch 7/10, Batch 806/883, Training Loss: 0.7506\n",
      "Epoch 7/10, Batch 807/883, Training Loss: 0.3986\n",
      "Epoch 7/10, Batch 808/883, Training Loss: 0.5117\n",
      "Epoch 7/10, Batch 809/883, Training Loss: 0.4574\n",
      "Epoch 7/10, Batch 810/883, Training Loss: 0.5332\n",
      "Epoch 7/10, Batch 811/883, Training Loss: 0.3399\n",
      "Epoch 7/10, Batch 812/883, Training Loss: 0.5151\n",
      "Epoch 7/10, Batch 813/883, Training Loss: 0.5445\n",
      "Epoch 7/10, Batch 814/883, Training Loss: 0.6903\n",
      "Epoch 7/10, Batch 815/883, Training Loss: 0.6640\n",
      "Epoch 7/10, Batch 816/883, Training Loss: 0.3301\n",
      "Epoch 7/10, Batch 817/883, Training Loss: 0.6672\n",
      "Epoch 7/10, Batch 818/883, Training Loss: 0.5202\n",
      "Epoch 7/10, Batch 819/883, Training Loss: 0.7561\n",
      "Epoch 7/10, Batch 820/883, Training Loss: 0.5424\n",
      "Epoch 7/10, Batch 821/883, Training Loss: 0.5544\n",
      "Epoch 7/10, Batch 822/883, Training Loss: 0.5613\n",
      "Epoch 7/10, Batch 823/883, Training Loss: 0.8211\n",
      "Epoch 7/10, Batch 824/883, Training Loss: 0.5771\n",
      "Epoch 7/10, Batch 825/883, Training Loss: 0.7670\n",
      "Epoch 7/10, Batch 826/883, Training Loss: 0.8998\n",
      "Epoch 7/10, Batch 827/883, Training Loss: 0.4813\n",
      "Epoch 7/10, Batch 828/883, Training Loss: 0.5295\n",
      "Epoch 7/10, Batch 829/883, Training Loss: 0.4354\n",
      "Epoch 7/10, Batch 830/883, Training Loss: 0.5945\n",
      "Epoch 7/10, Batch 831/883, Training Loss: 0.8869\n",
      "Epoch 7/10, Batch 832/883, Training Loss: 0.3303\n",
      "Epoch 7/10, Batch 833/883, Training Loss: 0.5437\n",
      "Epoch 7/10, Batch 834/883, Training Loss: 0.5882\n",
      "Epoch 7/10, Batch 835/883, Training Loss: 0.7079\n",
      "Epoch 7/10, Batch 836/883, Training Loss: 0.5488\n",
      "Epoch 7/10, Batch 837/883, Training Loss: 0.4280\n",
      "Epoch 7/10, Batch 838/883, Training Loss: 0.2978\n",
      "Epoch 7/10, Batch 839/883, Training Loss: 0.4289\n",
      "Epoch 7/10, Batch 840/883, Training Loss: 0.9048\n",
      "Epoch 7/10, Batch 841/883, Training Loss: 0.7646\n",
      "Epoch 7/10, Batch 842/883, Training Loss: 0.4181\n",
      "Epoch 7/10, Batch 843/883, Training Loss: 0.7304\n",
      "Epoch 7/10, Batch 844/883, Training Loss: 0.4416\n",
      "Epoch 7/10, Batch 845/883, Training Loss: 0.8777\n",
      "Epoch 7/10, Batch 846/883, Training Loss: 0.4048\n",
      "Epoch 7/10, Batch 847/883, Training Loss: 0.6755\n",
      "Epoch 7/10, Batch 848/883, Training Loss: 0.5924\n",
      "Epoch 7/10, Batch 849/883, Training Loss: 0.6032\n",
      "Epoch 7/10, Batch 850/883, Training Loss: 0.4835\n",
      "Epoch 7/10, Batch 851/883, Training Loss: 0.6346\n",
      "Epoch 7/10, Batch 852/883, Training Loss: 1.1777\n",
      "Epoch 7/10, Batch 853/883, Training Loss: 0.7459\n",
      "Epoch 7/10, Batch 854/883, Training Loss: 0.4083\n",
      "Epoch 7/10, Batch 855/883, Training Loss: 0.7622\n",
      "Epoch 7/10, Batch 856/883, Training Loss: 0.6399\n",
      "Epoch 7/10, Batch 857/883, Training Loss: 0.6707\n",
      "Epoch 7/10, Batch 858/883, Training Loss: 0.3980\n",
      "Epoch 7/10, Batch 859/883, Training Loss: 0.4792\n",
      "Epoch 7/10, Batch 860/883, Training Loss: 0.6924\n",
      "Epoch 7/10, Batch 861/883, Training Loss: 0.7555\n",
      "Epoch 7/10, Batch 862/883, Training Loss: 0.5851\n",
      "Epoch 7/10, Batch 863/883, Training Loss: 0.8594\n",
      "Epoch 7/10, Batch 864/883, Training Loss: 0.7044\n",
      "Epoch 7/10, Batch 865/883, Training Loss: 0.5167\n",
      "Epoch 7/10, Batch 866/883, Training Loss: 0.6187\n",
      "Epoch 7/10, Batch 867/883, Training Loss: 0.4939\n",
      "Epoch 7/10, Batch 868/883, Training Loss: 0.5118\n",
      "Epoch 7/10, Batch 869/883, Training Loss: 0.4430\n",
      "Epoch 7/10, Batch 870/883, Training Loss: 0.5913\n",
      "Epoch 7/10, Batch 871/883, Training Loss: 0.6214\n",
      "Epoch 7/10, Batch 872/883, Training Loss: 0.4211\n",
      "Epoch 7/10, Batch 873/883, Training Loss: 0.5413\n",
      "Epoch 7/10, Batch 874/883, Training Loss: 0.6037\n",
      "Epoch 7/10, Batch 875/883, Training Loss: 0.3753\n",
      "Epoch 7/10, Batch 876/883, Training Loss: 0.4756\n",
      "Epoch 7/10, Batch 877/883, Training Loss: 0.8294\n",
      "Epoch 7/10, Batch 878/883, Training Loss: 0.3063\n",
      "Epoch 7/10, Batch 879/883, Training Loss: 0.5360\n",
      "Epoch 7/10, Batch 880/883, Training Loss: 0.4572\n",
      "Epoch 7/10, Batch 881/883, Training Loss: 0.5948\n",
      "Epoch 7/10, Batch 882/883, Training Loss: 0.4694\n",
      "Epoch 7/10, Batch 883/883, Training Loss: 0.4803\n",
      "Epoch 7/10, Training Loss: 0.6109, Validation Loss: 0.5438, Validation Accuracy: 0.7680\n",
      "Epoch 8/10, Batch 1/883, Training Loss: 0.4803\n",
      "Epoch 8/10, Batch 2/883, Training Loss: 1.0791\n",
      "Epoch 8/10, Batch 3/883, Training Loss: 0.5777\n",
      "Epoch 8/10, Batch 4/883, Training Loss: 0.4831\n",
      "Epoch 8/10, Batch 5/883, Training Loss: 0.6425\n",
      "Epoch 8/10, Batch 6/883, Training Loss: 0.5150\n",
      "Epoch 8/10, Batch 7/883, Training Loss: 0.6408\n",
      "Epoch 8/10, Batch 8/883, Training Loss: 0.6284\n",
      "Epoch 8/10, Batch 9/883, Training Loss: 0.7759\n",
      "Epoch 8/10, Batch 10/883, Training Loss: 0.3231\n",
      "Epoch 8/10, Batch 11/883, Training Loss: 0.5574\n",
      "Epoch 8/10, Batch 12/883, Training Loss: 0.5873\n",
      "Epoch 8/10, Batch 13/883, Training Loss: 0.4767\n",
      "Epoch 8/10, Batch 14/883, Training Loss: 0.4545\n",
      "Epoch 8/10, Batch 15/883, Training Loss: 0.7357\n",
      "Epoch 8/10, Batch 16/883, Training Loss: 0.4649\n",
      "Epoch 8/10, Batch 17/883, Training Loss: 0.6572\n",
      "Epoch 8/10, Batch 18/883, Training Loss: 0.5531\n",
      "Epoch 8/10, Batch 19/883, Training Loss: 0.4532\n",
      "Epoch 8/10, Batch 20/883, Training Loss: 0.8811\n",
      "Epoch 8/10, Batch 21/883, Training Loss: 0.3126\n",
      "Epoch 8/10, Batch 22/883, Training Loss: 0.4811\n",
      "Epoch 8/10, Batch 23/883, Training Loss: 0.3946\n",
      "Epoch 8/10, Batch 24/883, Training Loss: 0.3226\n",
      "Epoch 8/10, Batch 25/883, Training Loss: 0.5945\n",
      "Epoch 8/10, Batch 26/883, Training Loss: 0.7979\n",
      "Epoch 8/10, Batch 27/883, Training Loss: 0.5108\n",
      "Epoch 8/10, Batch 28/883, Training Loss: 0.4507\n",
      "Epoch 8/10, Batch 29/883, Training Loss: 0.2707\n",
      "Epoch 8/10, Batch 30/883, Training Loss: 0.6390\n",
      "Epoch 8/10, Batch 31/883, Training Loss: 0.8056\n",
      "Epoch 8/10, Batch 32/883, Training Loss: 0.3874\n",
      "Epoch 8/10, Batch 33/883, Training Loss: 0.7179\n",
      "Epoch 8/10, Batch 34/883, Training Loss: 0.6497\n",
      "Epoch 8/10, Batch 35/883, Training Loss: 0.6552\n",
      "Epoch 8/10, Batch 36/883, Training Loss: 0.5706\n",
      "Epoch 8/10, Batch 37/883, Training Loss: 0.5876\n",
      "Epoch 8/10, Batch 38/883, Training Loss: 0.7269\n",
      "Epoch 8/10, Batch 39/883, Training Loss: 0.3429\n",
      "Epoch 8/10, Batch 40/883, Training Loss: 0.9149\n",
      "Epoch 8/10, Batch 41/883, Training Loss: 0.5543\n",
      "Epoch 8/10, Batch 42/883, Training Loss: 0.6967\n",
      "Epoch 8/10, Batch 43/883, Training Loss: 0.5647\n",
      "Epoch 8/10, Batch 44/883, Training Loss: 0.5776\n",
      "Epoch 8/10, Batch 45/883, Training Loss: 0.4974\n",
      "Epoch 8/10, Batch 46/883, Training Loss: 0.3820\n",
      "Epoch 8/10, Batch 47/883, Training Loss: 0.6070\n",
      "Epoch 8/10, Batch 48/883, Training Loss: 0.5402\n",
      "Epoch 8/10, Batch 49/883, Training Loss: 0.3361\n",
      "Epoch 8/10, Batch 50/883, Training Loss: 0.4822\n",
      "Epoch 8/10, Batch 51/883, Training Loss: 0.6293\n",
      "Epoch 8/10, Batch 52/883, Training Loss: 0.4710\n",
      "Epoch 8/10, Batch 53/883, Training Loss: 0.3243\n",
      "Epoch 8/10, Batch 54/883, Training Loss: 0.7393\n",
      "Epoch 8/10, Batch 55/883, Training Loss: 0.9886\n",
      "Epoch 8/10, Batch 56/883, Training Loss: 0.5854\n",
      "Epoch 8/10, Batch 57/883, Training Loss: 0.5137\n",
      "Epoch 8/10, Batch 58/883, Training Loss: 0.4758\n",
      "Epoch 8/10, Batch 59/883, Training Loss: 0.7101\n",
      "Epoch 8/10, Batch 60/883, Training Loss: 0.4898\n",
      "Epoch 8/10, Batch 61/883, Training Loss: 0.3287\n",
      "Epoch 8/10, Batch 62/883, Training Loss: 0.7492\n",
      "Epoch 8/10, Batch 63/883, Training Loss: 0.5225\n",
      "Epoch 8/10, Batch 64/883, Training Loss: 0.9812\n",
      "Epoch 8/10, Batch 65/883, Training Loss: 0.2759\n",
      "Epoch 8/10, Batch 66/883, Training Loss: 0.6070\n",
      "Epoch 8/10, Batch 67/883, Training Loss: 0.6158\n",
      "Epoch 8/10, Batch 68/883, Training Loss: 0.4854\n",
      "Epoch 8/10, Batch 69/883, Training Loss: 0.5801\n",
      "Epoch 8/10, Batch 70/883, Training Loss: 1.3265\n",
      "Epoch 8/10, Batch 71/883, Training Loss: 0.7254\n",
      "Epoch 8/10, Batch 72/883, Training Loss: 0.3957\n",
      "Epoch 8/10, Batch 73/883, Training Loss: 0.7707\n",
      "Epoch 8/10, Batch 74/883, Training Loss: 0.6696\n",
      "Epoch 8/10, Batch 75/883, Training Loss: 0.4772\n",
      "Epoch 8/10, Batch 76/883, Training Loss: 0.5186\n",
      "Epoch 8/10, Batch 77/883, Training Loss: 0.7773\n",
      "Epoch 8/10, Batch 78/883, Training Loss: 0.4607\n",
      "Epoch 8/10, Batch 79/883, Training Loss: 0.5460\n",
      "Epoch 8/10, Batch 80/883, Training Loss: 0.5648\n",
      "Epoch 8/10, Batch 81/883, Training Loss: 0.5534\n",
      "Epoch 8/10, Batch 82/883, Training Loss: 0.7441\n",
      "Epoch 8/10, Batch 83/883, Training Loss: 0.6517\n",
      "Epoch 8/10, Batch 84/883, Training Loss: 0.3817\n",
      "Epoch 8/10, Batch 85/883, Training Loss: 0.4735\n",
      "Epoch 8/10, Batch 86/883, Training Loss: 0.7814\n",
      "Epoch 8/10, Batch 87/883, Training Loss: 0.4719\n",
      "Epoch 8/10, Batch 88/883, Training Loss: 0.5834\n",
      "Epoch 8/10, Batch 89/883, Training Loss: 0.5712\n",
      "Epoch 8/10, Batch 90/883, Training Loss: 0.4829\n",
      "Epoch 8/10, Batch 91/883, Training Loss: 0.5992\n",
      "Epoch 8/10, Batch 92/883, Training Loss: 0.4916\n",
      "Epoch 8/10, Batch 93/883, Training Loss: 0.4970\n",
      "Epoch 8/10, Batch 94/883, Training Loss: 0.7348\n",
      "Epoch 8/10, Batch 95/883, Training Loss: 0.4672\n",
      "Epoch 8/10, Batch 96/883, Training Loss: 0.5765\n",
      "Epoch 8/10, Batch 97/883, Training Loss: 0.5886\n",
      "Epoch 8/10, Batch 98/883, Training Loss: 0.3920\n",
      "Epoch 8/10, Batch 99/883, Training Loss: 0.7261\n",
      "Epoch 8/10, Batch 100/883, Training Loss: 0.8362\n",
      "Epoch 8/10, Batch 101/883, Training Loss: 0.6295\n",
      "Epoch 8/10, Batch 102/883, Training Loss: 0.6596\n",
      "Epoch 8/10, Batch 103/883, Training Loss: 0.7811\n",
      "Epoch 8/10, Batch 104/883, Training Loss: 0.6563\n",
      "Epoch 8/10, Batch 105/883, Training Loss: 0.6133\n",
      "Epoch 8/10, Batch 106/883, Training Loss: 0.3862\n",
      "Epoch 8/10, Batch 107/883, Training Loss: 0.5187\n",
      "Epoch 8/10, Batch 108/883, Training Loss: 0.4296\n",
      "Epoch 8/10, Batch 109/883, Training Loss: 0.7625\n",
      "Epoch 8/10, Batch 110/883, Training Loss: 0.5846\n",
      "Epoch 8/10, Batch 111/883, Training Loss: 0.3846\n",
      "Epoch 8/10, Batch 112/883, Training Loss: 0.6984\n",
      "Epoch 8/10, Batch 113/883, Training Loss: 0.5007\n",
      "Epoch 8/10, Batch 114/883, Training Loss: 0.6349\n",
      "Epoch 8/10, Batch 115/883, Training Loss: 0.6734\n",
      "Epoch 8/10, Batch 116/883, Training Loss: 0.4557\n",
      "Epoch 8/10, Batch 117/883, Training Loss: 0.7072\n",
      "Epoch 8/10, Batch 118/883, Training Loss: 0.5205\n",
      "Epoch 8/10, Batch 119/883, Training Loss: 0.4102\n",
      "Epoch 8/10, Batch 120/883, Training Loss: 0.5522\n",
      "Epoch 8/10, Batch 121/883, Training Loss: 0.4559\n",
      "Epoch 8/10, Batch 122/883, Training Loss: 0.6873\n",
      "Epoch 8/10, Batch 123/883, Training Loss: 0.7415\n",
      "Epoch 8/10, Batch 124/883, Training Loss: 0.6313\n",
      "Epoch 8/10, Batch 125/883, Training Loss: 0.2463\n",
      "Epoch 8/10, Batch 126/883, Training Loss: 0.4422\n",
      "Epoch 8/10, Batch 127/883, Training Loss: 0.6005\n",
      "Epoch 8/10, Batch 128/883, Training Loss: 0.9077\n",
      "Epoch 8/10, Batch 129/883, Training Loss: 0.4561\n",
      "Epoch 8/10, Batch 130/883, Training Loss: 0.3956\n",
      "Epoch 8/10, Batch 131/883, Training Loss: 0.5659\n",
      "Epoch 8/10, Batch 132/883, Training Loss: 0.4378\n",
      "Epoch 8/10, Batch 133/883, Training Loss: 0.2887\n",
      "Epoch 8/10, Batch 134/883, Training Loss: 0.5428\n",
      "Epoch 8/10, Batch 135/883, Training Loss: 0.6401\n",
      "Epoch 8/10, Batch 136/883, Training Loss: 0.6858\n",
      "Epoch 8/10, Batch 137/883, Training Loss: 0.8281\n",
      "Epoch 8/10, Batch 138/883, Training Loss: 0.6376\n",
      "Epoch 8/10, Batch 139/883, Training Loss: 0.8348\n",
      "Epoch 8/10, Batch 140/883, Training Loss: 0.6388\n",
      "Epoch 8/10, Batch 141/883, Training Loss: 0.7608\n",
      "Epoch 8/10, Batch 142/883, Training Loss: 0.5179\n",
      "Epoch 8/10, Batch 143/883, Training Loss: 0.2600\n",
      "Epoch 8/10, Batch 144/883, Training Loss: 0.6223\n",
      "Epoch 8/10, Batch 145/883, Training Loss: 0.4028\n",
      "Epoch 8/10, Batch 146/883, Training Loss: 0.4409\n",
      "Epoch 8/10, Batch 147/883, Training Loss: 0.4676\n",
      "Epoch 8/10, Batch 148/883, Training Loss: 0.5332\n",
      "Epoch 8/10, Batch 149/883, Training Loss: 0.3066\n",
      "Epoch 8/10, Batch 150/883, Training Loss: 0.5288\n",
      "Epoch 8/10, Batch 151/883, Training Loss: 0.5861\n",
      "Epoch 8/10, Batch 152/883, Training Loss: 0.5847\n",
      "Epoch 8/10, Batch 153/883, Training Loss: 0.3533\n",
      "Epoch 8/10, Batch 154/883, Training Loss: 0.3847\n",
      "Epoch 8/10, Batch 155/883, Training Loss: 0.3779\n",
      "Epoch 8/10, Batch 156/883, Training Loss: 0.5443\n",
      "Epoch 8/10, Batch 157/883, Training Loss: 0.7197\n",
      "Epoch 8/10, Batch 158/883, Training Loss: 0.4542\n",
      "Epoch 8/10, Batch 159/883, Training Loss: 0.4455\n",
      "Epoch 8/10, Batch 160/883, Training Loss: 0.7794\n",
      "Epoch 8/10, Batch 161/883, Training Loss: 0.6989\n",
      "Epoch 8/10, Batch 162/883, Training Loss: 0.5229\n",
      "Epoch 8/10, Batch 163/883, Training Loss: 0.8210\n",
      "Epoch 8/10, Batch 164/883, Training Loss: 0.6673\n",
      "Epoch 8/10, Batch 165/883, Training Loss: 0.5314\n",
      "Epoch 8/10, Batch 166/883, Training Loss: 0.3840\n",
      "Epoch 8/10, Batch 167/883, Training Loss: 0.3428\n",
      "Epoch 8/10, Batch 168/883, Training Loss: 0.7585\n",
      "Epoch 8/10, Batch 169/883, Training Loss: 0.3976\n",
      "Epoch 8/10, Batch 170/883, Training Loss: 0.5024\n",
      "Epoch 8/10, Batch 171/883, Training Loss: 0.9304\n",
      "Epoch 8/10, Batch 172/883, Training Loss: 0.8049\n",
      "Epoch 8/10, Batch 173/883, Training Loss: 0.4656\n",
      "Epoch 8/10, Batch 174/883, Training Loss: 0.5134\n",
      "Epoch 8/10, Batch 175/883, Training Loss: 0.4671\n",
      "Epoch 8/10, Batch 176/883, Training Loss: 0.4424\n",
      "Epoch 8/10, Batch 177/883, Training Loss: 0.5876\n",
      "Epoch 8/10, Batch 178/883, Training Loss: 0.4373\n",
      "Epoch 8/10, Batch 179/883, Training Loss: 0.3151\n",
      "Epoch 8/10, Batch 180/883, Training Loss: 0.4821\n",
      "Epoch 8/10, Batch 181/883, Training Loss: 0.3920\n",
      "Epoch 8/10, Batch 182/883, Training Loss: 0.6627\n",
      "Epoch 8/10, Batch 183/883, Training Loss: 0.3998\n",
      "Epoch 8/10, Batch 184/883, Training Loss: 0.4537\n",
      "Epoch 8/10, Batch 185/883, Training Loss: 0.4775\n",
      "Epoch 8/10, Batch 186/883, Training Loss: 0.3551\n",
      "Epoch 8/10, Batch 187/883, Training Loss: 0.2854\n",
      "Epoch 8/10, Batch 188/883, Training Loss: 0.6305\n",
      "Epoch 8/10, Batch 189/883, Training Loss: 0.5531\n",
      "Epoch 8/10, Batch 190/883, Training Loss: 0.4335\n",
      "Epoch 8/10, Batch 191/883, Training Loss: 0.7002\n",
      "Epoch 8/10, Batch 192/883, Training Loss: 0.3340\n",
      "Epoch 8/10, Batch 193/883, Training Loss: 0.4925\n",
      "Epoch 8/10, Batch 194/883, Training Loss: 0.7254\n",
      "Epoch 8/10, Batch 195/883, Training Loss: 0.4849\n",
      "Epoch 8/10, Batch 196/883, Training Loss: 0.3722\n",
      "Epoch 8/10, Batch 197/883, Training Loss: 0.6890\n",
      "Epoch 8/10, Batch 198/883, Training Loss: 0.4370\n",
      "Epoch 8/10, Batch 199/883, Training Loss: 0.5739\n",
      "Epoch 8/10, Batch 200/883, Training Loss: 0.6222\n",
      "Epoch 8/10, Batch 201/883, Training Loss: 0.4528\n",
      "Epoch 8/10, Batch 202/883, Training Loss: 0.8683\n",
      "Epoch 8/10, Batch 203/883, Training Loss: 0.5546\n",
      "Epoch 8/10, Batch 204/883, Training Loss: 0.3438\n",
      "Epoch 8/10, Batch 205/883, Training Loss: 0.3120\n",
      "Epoch 8/10, Batch 206/883, Training Loss: 0.1495\n",
      "Epoch 8/10, Batch 207/883, Training Loss: 0.4617\n",
      "Epoch 8/10, Batch 208/883, Training Loss: 0.4318\n",
      "Epoch 8/10, Batch 209/883, Training Loss: 0.5696\n",
      "Epoch 8/10, Batch 210/883, Training Loss: 0.8650\n",
      "Epoch 8/10, Batch 211/883, Training Loss: 0.3535\n",
      "Epoch 8/10, Batch 212/883, Training Loss: 0.6937\n",
      "Epoch 8/10, Batch 213/883, Training Loss: 0.3361\n",
      "Epoch 8/10, Batch 214/883, Training Loss: 0.5103\n",
      "Epoch 8/10, Batch 215/883, Training Loss: 0.3249\n",
      "Epoch 8/10, Batch 216/883, Training Loss: 0.8953\n",
      "Epoch 8/10, Batch 217/883, Training Loss: 0.6604\n",
      "Epoch 8/10, Batch 218/883, Training Loss: 0.3940\n",
      "Epoch 8/10, Batch 219/883, Training Loss: 0.6260\n",
      "Epoch 8/10, Batch 220/883, Training Loss: 0.4388\n",
      "Epoch 8/10, Batch 221/883, Training Loss: 0.4976\n",
      "Epoch 8/10, Batch 222/883, Training Loss: 0.7609\n",
      "Epoch 8/10, Batch 223/883, Training Loss: 0.4791\n",
      "Epoch 8/10, Batch 224/883, Training Loss: 0.5292\n",
      "Epoch 8/10, Batch 225/883, Training Loss: 0.8000\n",
      "Epoch 8/10, Batch 226/883, Training Loss: 0.5848\n",
      "Epoch 8/10, Batch 227/883, Training Loss: 0.2407\n",
      "Epoch 8/10, Batch 228/883, Training Loss: 0.4897\n",
      "Epoch 8/10, Batch 229/883, Training Loss: 0.7613\n",
      "Epoch 8/10, Batch 230/883, Training Loss: 0.7174\n",
      "Epoch 8/10, Batch 231/883, Training Loss: 0.2885\n",
      "Epoch 8/10, Batch 232/883, Training Loss: 0.3768\n",
      "Epoch 8/10, Batch 233/883, Training Loss: 0.6154\n",
      "Epoch 8/10, Batch 234/883, Training Loss: 0.5861\n",
      "Epoch 8/10, Batch 235/883, Training Loss: 1.1012\n",
      "Epoch 8/10, Batch 236/883, Training Loss: 1.1510\n",
      "Epoch 8/10, Batch 237/883, Training Loss: 0.4983\n",
      "Epoch 8/10, Batch 238/883, Training Loss: 0.4725\n",
      "Epoch 8/10, Batch 239/883, Training Loss: 0.5929\n",
      "Epoch 8/10, Batch 240/883, Training Loss: 0.4621\n",
      "Epoch 8/10, Batch 241/883, Training Loss: 0.4790\n",
      "Epoch 8/10, Batch 242/883, Training Loss: 0.5679\n",
      "Epoch 8/10, Batch 243/883, Training Loss: 0.3966\n",
      "Epoch 8/10, Batch 244/883, Training Loss: 0.4274\n",
      "Epoch 8/10, Batch 245/883, Training Loss: 0.8952\n",
      "Epoch 8/10, Batch 246/883, Training Loss: 0.6039\n",
      "Epoch 8/10, Batch 247/883, Training Loss: 0.8298\n",
      "Epoch 8/10, Batch 248/883, Training Loss: 0.4733\n",
      "Epoch 8/10, Batch 249/883, Training Loss: 0.5114\n",
      "Epoch 8/10, Batch 250/883, Training Loss: 1.1624\n",
      "Epoch 8/10, Batch 251/883, Training Loss: 0.8081\n",
      "Epoch 8/10, Batch 252/883, Training Loss: 0.5561\n",
      "Epoch 8/10, Batch 253/883, Training Loss: 0.4819\n",
      "Epoch 8/10, Batch 254/883, Training Loss: 0.4706\n",
      "Epoch 8/10, Batch 255/883, Training Loss: 0.6693\n",
      "Epoch 8/10, Batch 256/883, Training Loss: 0.4053\n",
      "Epoch 8/10, Batch 257/883, Training Loss: 0.5836\n",
      "Epoch 8/10, Batch 258/883, Training Loss: 0.4345\n",
      "Epoch 8/10, Batch 259/883, Training Loss: 0.6162\n",
      "Epoch 8/10, Batch 260/883, Training Loss: 0.4415\n",
      "Epoch 8/10, Batch 261/883, Training Loss: 0.7546\n",
      "Epoch 8/10, Batch 262/883, Training Loss: 0.5257\n",
      "Epoch 8/10, Batch 263/883, Training Loss: 0.7459\n",
      "Epoch 8/10, Batch 264/883, Training Loss: 0.8099\n",
      "Epoch 8/10, Batch 265/883, Training Loss: 0.5590\n",
      "Epoch 8/10, Batch 266/883, Training Loss: 0.7549\n",
      "Epoch 8/10, Batch 267/883, Training Loss: 1.0027\n",
      "Epoch 8/10, Batch 268/883, Training Loss: 0.5207\n",
      "Epoch 8/10, Batch 269/883, Training Loss: 0.8906\n",
      "Epoch 8/10, Batch 270/883, Training Loss: 0.6116\n",
      "Epoch 8/10, Batch 271/883, Training Loss: 0.5247\n",
      "Epoch 8/10, Batch 272/883, Training Loss: 0.5800\n",
      "Epoch 8/10, Batch 273/883, Training Loss: 0.6257\n",
      "Epoch 8/10, Batch 274/883, Training Loss: 0.9161\n",
      "Epoch 8/10, Batch 275/883, Training Loss: 0.5847\n",
      "Epoch 8/10, Batch 276/883, Training Loss: 0.6316\n",
      "Epoch 8/10, Batch 277/883, Training Loss: 0.4910\n",
      "Epoch 8/10, Batch 278/883, Training Loss: 0.3145\n",
      "Epoch 8/10, Batch 279/883, Training Loss: 0.9164\n",
      "Epoch 8/10, Batch 280/883, Training Loss: 0.6268\n",
      "Epoch 8/10, Batch 281/883, Training Loss: 0.6285\n",
      "Epoch 8/10, Batch 282/883, Training Loss: 0.4274\n",
      "Epoch 8/10, Batch 283/883, Training Loss: 0.5999\n",
      "Epoch 8/10, Batch 284/883, Training Loss: 0.6210\n",
      "Epoch 8/10, Batch 285/883, Training Loss: 0.5115\n",
      "Epoch 8/10, Batch 286/883, Training Loss: 0.8217\n",
      "Epoch 8/10, Batch 287/883, Training Loss: 0.6510\n",
      "Epoch 8/10, Batch 288/883, Training Loss: 0.7609\n",
      "Epoch 8/10, Batch 289/883, Training Loss: 0.4674\n",
      "Epoch 8/10, Batch 290/883, Training Loss: 0.4015\n",
      "Epoch 8/10, Batch 291/883, Training Loss: 0.6915\n",
      "Epoch 8/10, Batch 292/883, Training Loss: 0.3247\n",
      "Epoch 8/10, Batch 293/883, Training Loss: 0.6368\n",
      "Epoch 8/10, Batch 294/883, Training Loss: 0.9367\n",
      "Epoch 8/10, Batch 295/883, Training Loss: 0.5584\n",
      "Epoch 8/10, Batch 296/883, Training Loss: 0.5033\n",
      "Epoch 8/10, Batch 297/883, Training Loss: 0.2777\n",
      "Epoch 8/10, Batch 298/883, Training Loss: 0.3699\n",
      "Epoch 8/10, Batch 299/883, Training Loss: 0.4951\n",
      "Epoch 8/10, Batch 300/883, Training Loss: 0.4693\n",
      "Epoch 8/10, Batch 301/883, Training Loss: 0.6521\n",
      "Epoch 8/10, Batch 302/883, Training Loss: 0.4222\n",
      "Epoch 8/10, Batch 303/883, Training Loss: 0.5315\n",
      "Epoch 8/10, Batch 304/883, Training Loss: 0.4045\n",
      "Epoch 8/10, Batch 305/883, Training Loss: 0.7569\n",
      "Epoch 8/10, Batch 306/883, Training Loss: 0.5083\n",
      "Epoch 8/10, Batch 307/883, Training Loss: 0.9468\n",
      "Epoch 8/10, Batch 308/883, Training Loss: 0.4040\n",
      "Epoch 8/10, Batch 309/883, Training Loss: 0.5223\n",
      "Epoch 8/10, Batch 310/883, Training Loss: 0.4711\n",
      "Epoch 8/10, Batch 311/883, Training Loss: 0.6199\n",
      "Epoch 8/10, Batch 312/883, Training Loss: 0.1934\n",
      "Epoch 8/10, Batch 313/883, Training Loss: 0.4887\n",
      "Epoch 8/10, Batch 314/883, Training Loss: 0.6100\n",
      "Epoch 8/10, Batch 315/883, Training Loss: 0.2460\n",
      "Epoch 8/10, Batch 316/883, Training Loss: 0.4480\n",
      "Epoch 8/10, Batch 317/883, Training Loss: 0.3096\n",
      "Epoch 8/10, Batch 318/883, Training Loss: 0.4393\n",
      "Epoch 8/10, Batch 319/883, Training Loss: 0.4350\n",
      "Epoch 8/10, Batch 320/883, Training Loss: 0.6524\n",
      "Epoch 8/10, Batch 321/883, Training Loss: 0.7849\n",
      "Epoch 8/10, Batch 322/883, Training Loss: 0.7156\n",
      "Epoch 8/10, Batch 323/883, Training Loss: 0.6056\n",
      "Epoch 8/10, Batch 324/883, Training Loss: 0.7117\n",
      "Epoch 8/10, Batch 325/883, Training Loss: 0.4335\n",
      "Epoch 8/10, Batch 326/883, Training Loss: 0.6250\n",
      "Epoch 8/10, Batch 327/883, Training Loss: 0.3655\n",
      "Epoch 8/10, Batch 328/883, Training Loss: 0.5687\n",
      "Epoch 8/10, Batch 329/883, Training Loss: 0.6614\n",
      "Epoch 8/10, Batch 330/883, Training Loss: 0.5758\n",
      "Epoch 8/10, Batch 331/883, Training Loss: 0.6345\n",
      "Epoch 8/10, Batch 332/883, Training Loss: 0.4498\n",
      "Epoch 8/10, Batch 333/883, Training Loss: 0.4831\n",
      "Epoch 8/10, Batch 334/883, Training Loss: 0.3838\n",
      "Epoch 8/10, Batch 335/883, Training Loss: 0.3873\n",
      "Epoch 8/10, Batch 336/883, Training Loss: 0.4323\n",
      "Epoch 8/10, Batch 337/883, Training Loss: 0.5350\n",
      "Epoch 8/10, Batch 338/883, Training Loss: 0.4468\n",
      "Epoch 8/10, Batch 339/883, Training Loss: 0.5502\n",
      "Epoch 8/10, Batch 340/883, Training Loss: 0.5851\n",
      "Epoch 8/10, Batch 341/883, Training Loss: 1.1656\n",
      "Epoch 8/10, Batch 342/883, Training Loss: 0.5183\n",
      "Epoch 8/10, Batch 343/883, Training Loss: 0.4870\n",
      "Epoch 8/10, Batch 344/883, Training Loss: 0.3839\n",
      "Epoch 8/10, Batch 345/883, Training Loss: 0.2431\n",
      "Epoch 8/10, Batch 346/883, Training Loss: 0.4482\n",
      "Epoch 8/10, Batch 347/883, Training Loss: 0.2904\n",
      "Epoch 8/10, Batch 348/883, Training Loss: 0.3869\n",
      "Epoch 8/10, Batch 349/883, Training Loss: 0.4670\n",
      "Epoch 8/10, Batch 350/883, Training Loss: 0.3272\n",
      "Epoch 8/10, Batch 351/883, Training Loss: 0.4936\n",
      "Epoch 8/10, Batch 352/883, Training Loss: 0.5830\n",
      "Epoch 8/10, Batch 353/883, Training Loss: 0.4813\n",
      "Epoch 8/10, Batch 354/883, Training Loss: 0.3340\n",
      "Epoch 8/10, Batch 355/883, Training Loss: 1.0884\n",
      "Epoch 8/10, Batch 356/883, Training Loss: 0.7191\n",
      "Epoch 8/10, Batch 357/883, Training Loss: 0.8484\n",
      "Epoch 8/10, Batch 358/883, Training Loss: 0.4197\n",
      "Epoch 8/10, Batch 359/883, Training Loss: 0.5531\n",
      "Epoch 8/10, Batch 360/883, Training Loss: 0.6253\n",
      "Epoch 8/10, Batch 361/883, Training Loss: 0.4715\n",
      "Epoch 8/10, Batch 362/883, Training Loss: 0.9865\n",
      "Epoch 8/10, Batch 363/883, Training Loss: 0.3576\n",
      "Epoch 8/10, Batch 364/883, Training Loss: 0.2324\n",
      "Epoch 8/10, Batch 365/883, Training Loss: 0.7831\n",
      "Epoch 8/10, Batch 366/883, Training Loss: 0.5578\n",
      "Epoch 8/10, Batch 367/883, Training Loss: 0.3677\n",
      "Epoch 8/10, Batch 368/883, Training Loss: 0.7703\n",
      "Epoch 8/10, Batch 369/883, Training Loss: 0.6428\n",
      "Epoch 8/10, Batch 370/883, Training Loss: 0.6686\n",
      "Epoch 8/10, Batch 371/883, Training Loss: 0.7338\n",
      "Epoch 8/10, Batch 372/883, Training Loss: 0.5799\n",
      "Epoch 8/10, Batch 373/883, Training Loss: 0.5184\n",
      "Epoch 8/10, Batch 374/883, Training Loss: 0.7101\n",
      "Epoch 8/10, Batch 375/883, Training Loss: 0.6062\n",
      "Epoch 8/10, Batch 376/883, Training Loss: 0.3200\n",
      "Epoch 8/10, Batch 377/883, Training Loss: 0.6377\n",
      "Epoch 8/10, Batch 378/883, Training Loss: 0.4709\n",
      "Epoch 8/10, Batch 379/883, Training Loss: 0.6920\n",
      "Epoch 8/10, Batch 380/883, Training Loss: 0.4618\n",
      "Epoch 8/10, Batch 381/883, Training Loss: 0.5516\n",
      "Epoch 8/10, Batch 382/883, Training Loss: 0.5062\n",
      "Epoch 8/10, Batch 383/883, Training Loss: 0.6253\n",
      "Epoch 8/10, Batch 384/883, Training Loss: 0.5669\n",
      "Epoch 8/10, Batch 385/883, Training Loss: 0.7436\n",
      "Epoch 8/10, Batch 386/883, Training Loss: 0.4996\n",
      "Epoch 8/10, Batch 387/883, Training Loss: 0.5080\n",
      "Epoch 8/10, Batch 388/883, Training Loss: 0.6664\n",
      "Epoch 8/10, Batch 389/883, Training Loss: 0.5324\n",
      "Epoch 8/10, Batch 390/883, Training Loss: 0.5914\n",
      "Epoch 8/10, Batch 391/883, Training Loss: 0.7918\n",
      "Epoch 8/10, Batch 392/883, Training Loss: 0.5978\n",
      "Epoch 8/10, Batch 393/883, Training Loss: 0.4527\n",
      "Epoch 8/10, Batch 394/883, Training Loss: 0.5441\n",
      "Epoch 8/10, Batch 395/883, Training Loss: 0.9128\n",
      "Epoch 8/10, Batch 396/883, Training Loss: 0.4782\n",
      "Epoch 8/10, Batch 397/883, Training Loss: 0.7828\n",
      "Epoch 8/10, Batch 398/883, Training Loss: 0.5830\n",
      "Epoch 8/10, Batch 399/883, Training Loss: 0.3720\n",
      "Epoch 8/10, Batch 400/883, Training Loss: 0.5206\n",
      "Epoch 8/10, Batch 401/883, Training Loss: 0.7109\n",
      "Epoch 8/10, Batch 402/883, Training Loss: 0.5881\n",
      "Epoch 8/10, Batch 403/883, Training Loss: 0.4867\n",
      "Epoch 8/10, Batch 404/883, Training Loss: 0.5984\n",
      "Epoch 8/10, Batch 405/883, Training Loss: 0.4467\n",
      "Epoch 8/10, Batch 406/883, Training Loss: 0.6470\n",
      "Epoch 8/10, Batch 407/883, Training Loss: 0.5413\n",
      "Epoch 8/10, Batch 408/883, Training Loss: 0.4107\n",
      "Epoch 8/10, Batch 409/883, Training Loss: 0.4391\n",
      "Epoch 8/10, Batch 410/883, Training Loss: 0.3806\n",
      "Epoch 8/10, Batch 411/883, Training Loss: 0.5489\n",
      "Epoch 8/10, Batch 412/883, Training Loss: 0.9723\n",
      "Epoch 8/10, Batch 413/883, Training Loss: 0.8189\n",
      "Epoch 8/10, Batch 414/883, Training Loss: 0.6896\n",
      "Epoch 8/10, Batch 415/883, Training Loss: 0.5724\n",
      "Epoch 8/10, Batch 416/883, Training Loss: 1.0089\n",
      "Epoch 8/10, Batch 417/883, Training Loss: 0.3580\n",
      "Epoch 8/10, Batch 418/883, Training Loss: 0.3991\n",
      "Epoch 8/10, Batch 419/883, Training Loss: 0.5426\n",
      "Epoch 8/10, Batch 420/883, Training Loss: 0.6208\n",
      "Epoch 8/10, Batch 421/883, Training Loss: 0.3582\n",
      "Epoch 8/10, Batch 422/883, Training Loss: 0.4336\n",
      "Epoch 8/10, Batch 423/883, Training Loss: 0.5873\n",
      "Epoch 8/10, Batch 424/883, Training Loss: 0.5189\n",
      "Epoch 8/10, Batch 425/883, Training Loss: 0.8658\n",
      "Epoch 8/10, Batch 426/883, Training Loss: 0.4528\n",
      "Epoch 8/10, Batch 427/883, Training Loss: 0.7320\n",
      "Epoch 8/10, Batch 428/883, Training Loss: 0.9827\n",
      "Epoch 8/10, Batch 429/883, Training Loss: 0.6145\n",
      "Epoch 8/10, Batch 430/883, Training Loss: 0.6255\n",
      "Epoch 8/10, Batch 431/883, Training Loss: 0.7999\n",
      "Epoch 8/10, Batch 432/883, Training Loss: 0.5167\n",
      "Epoch 8/10, Batch 433/883, Training Loss: 0.4312\n",
      "Epoch 8/10, Batch 434/883, Training Loss: 0.2846\n",
      "Epoch 8/10, Batch 435/883, Training Loss: 0.7414\n",
      "Epoch 8/10, Batch 436/883, Training Loss: 0.4610\n",
      "Epoch 8/10, Batch 437/883, Training Loss: 0.7897\n",
      "Epoch 8/10, Batch 438/883, Training Loss: 0.7365\n",
      "Epoch 8/10, Batch 439/883, Training Loss: 0.4171\n",
      "Epoch 8/10, Batch 440/883, Training Loss: 0.6923\n",
      "Epoch 8/10, Batch 441/883, Training Loss: 0.7992\n",
      "Epoch 8/10, Batch 442/883, Training Loss: 0.5963\n",
      "Epoch 8/10, Batch 443/883, Training Loss: 0.3462\n",
      "Epoch 8/10, Batch 444/883, Training Loss: 0.3881\n",
      "Epoch 8/10, Batch 445/883, Training Loss: 0.4152\n",
      "Epoch 8/10, Batch 446/883, Training Loss: 0.4495\n",
      "Epoch 8/10, Batch 447/883, Training Loss: 0.3968\n",
      "Epoch 8/10, Batch 448/883, Training Loss: 0.6483\n",
      "Epoch 8/10, Batch 449/883, Training Loss: 0.7177\n",
      "Epoch 8/10, Batch 450/883, Training Loss: 0.8365\n",
      "Epoch 8/10, Batch 451/883, Training Loss: 0.6289\n",
      "Epoch 8/10, Batch 452/883, Training Loss: 0.6658\n",
      "Epoch 8/10, Batch 453/883, Training Loss: 0.5169\n",
      "Epoch 8/10, Batch 454/883, Training Loss: 0.3938\n",
      "Epoch 8/10, Batch 455/883, Training Loss: 0.4130\n",
      "Epoch 8/10, Batch 456/883, Training Loss: 0.9481\n",
      "Epoch 8/10, Batch 457/883, Training Loss: 0.4173\n",
      "Epoch 8/10, Batch 458/883, Training Loss: 0.5813\n",
      "Epoch 8/10, Batch 459/883, Training Loss: 0.4673\n",
      "Epoch 8/10, Batch 460/883, Training Loss: 0.5746\n",
      "Epoch 8/10, Batch 461/883, Training Loss: 0.4219\n",
      "Epoch 8/10, Batch 462/883, Training Loss: 0.5658\n",
      "Epoch 8/10, Batch 463/883, Training Loss: 0.6895\n",
      "Epoch 8/10, Batch 464/883, Training Loss: 0.5454\n",
      "Epoch 8/10, Batch 465/883, Training Loss: 0.5544\n",
      "Epoch 8/10, Batch 466/883, Training Loss: 0.5441\n",
      "Epoch 8/10, Batch 467/883, Training Loss: 0.9669\n",
      "Epoch 8/10, Batch 468/883, Training Loss: 0.5201\n",
      "Epoch 8/10, Batch 469/883, Training Loss: 0.4674\n",
      "Epoch 8/10, Batch 470/883, Training Loss: 0.6378\n",
      "Epoch 8/10, Batch 471/883, Training Loss: 0.4188\n",
      "Epoch 8/10, Batch 472/883, Training Loss: 0.4183\n",
      "Epoch 8/10, Batch 473/883, Training Loss: 0.5200\n",
      "Epoch 8/10, Batch 474/883, Training Loss: 0.9155\n",
      "Epoch 8/10, Batch 475/883, Training Loss: 0.4641\n",
      "Epoch 8/10, Batch 476/883, Training Loss: 0.2751\n",
      "Epoch 8/10, Batch 477/883, Training Loss: 0.4005\n",
      "Epoch 8/10, Batch 478/883, Training Loss: 0.5155\n",
      "Epoch 8/10, Batch 479/883, Training Loss: 0.7309\n",
      "Epoch 8/10, Batch 480/883, Training Loss: 0.5627\n",
      "Epoch 8/10, Batch 481/883, Training Loss: 0.5568\n",
      "Epoch 8/10, Batch 482/883, Training Loss: 0.5173\n",
      "Epoch 8/10, Batch 483/883, Training Loss: 1.0040\n",
      "Epoch 8/10, Batch 484/883, Training Loss: 0.6220\n",
      "Epoch 8/10, Batch 485/883, Training Loss: 0.3664\n",
      "Epoch 8/10, Batch 486/883, Training Loss: 0.6444\n",
      "Epoch 8/10, Batch 487/883, Training Loss: 0.6205\n",
      "Epoch 8/10, Batch 488/883, Training Loss: 0.4543\n",
      "Epoch 8/10, Batch 489/883, Training Loss: 0.4055\n",
      "Epoch 8/10, Batch 490/883, Training Loss: 0.4252\n",
      "Epoch 8/10, Batch 491/883, Training Loss: 0.4232\n",
      "Epoch 8/10, Batch 492/883, Training Loss: 0.6883\n",
      "Epoch 8/10, Batch 493/883, Training Loss: 0.7019\n",
      "Epoch 8/10, Batch 494/883, Training Loss: 0.7448\n",
      "Epoch 8/10, Batch 495/883, Training Loss: 0.6076\n",
      "Epoch 8/10, Batch 496/883, Training Loss: 0.2731\n",
      "Epoch 8/10, Batch 497/883, Training Loss: 0.3963\n",
      "Epoch 8/10, Batch 498/883, Training Loss: 0.4107\n",
      "Epoch 8/10, Batch 499/883, Training Loss: 0.4003\n",
      "Epoch 8/10, Batch 500/883, Training Loss: 0.4494\n",
      "Epoch 8/10, Batch 501/883, Training Loss: 0.5996\n",
      "Epoch 8/10, Batch 502/883, Training Loss: 0.5010\n",
      "Epoch 8/10, Batch 503/883, Training Loss: 0.5182\n",
      "Epoch 8/10, Batch 504/883, Training Loss: 0.4462\n",
      "Epoch 8/10, Batch 505/883, Training Loss: 0.4359\n",
      "Epoch 8/10, Batch 506/883, Training Loss: 0.4002\n",
      "Epoch 8/10, Batch 507/883, Training Loss: 0.3934\n",
      "Epoch 8/10, Batch 508/883, Training Loss: 0.6162\n",
      "Epoch 8/10, Batch 509/883, Training Loss: 0.4574\n",
      "Epoch 8/10, Batch 510/883, Training Loss: 0.5660\n",
      "Epoch 8/10, Batch 511/883, Training Loss: 0.4454\n",
      "Epoch 8/10, Batch 512/883, Training Loss: 0.5089\n",
      "Epoch 8/10, Batch 513/883, Training Loss: 0.3220\n",
      "Epoch 8/10, Batch 514/883, Training Loss: 0.5394\n",
      "Epoch 8/10, Batch 515/883, Training Loss: 0.8964\n",
      "Epoch 8/10, Batch 516/883, Training Loss: 0.4711\n",
      "Epoch 8/10, Batch 517/883, Training Loss: 0.5726\n",
      "Epoch 8/10, Batch 518/883, Training Loss: 0.7967\n",
      "Epoch 8/10, Batch 519/883, Training Loss: 0.6785\n",
      "Epoch 8/10, Batch 520/883, Training Loss: 0.3912\n",
      "Epoch 8/10, Batch 521/883, Training Loss: 0.5611\n",
      "Epoch 8/10, Batch 522/883, Training Loss: 0.5227\n",
      "Epoch 8/10, Batch 523/883, Training Loss: 0.4289\n",
      "Epoch 8/10, Batch 524/883, Training Loss: 0.4136\n",
      "Epoch 8/10, Batch 525/883, Training Loss: 0.6923\n",
      "Epoch 8/10, Batch 526/883, Training Loss: 0.4458\n",
      "Epoch 8/10, Batch 527/883, Training Loss: 0.6378\n",
      "Epoch 8/10, Batch 528/883, Training Loss: 0.8832\n",
      "Epoch 8/10, Batch 529/883, Training Loss: 0.8253\n",
      "Epoch 8/10, Batch 530/883, Training Loss: 1.0311\n",
      "Epoch 8/10, Batch 531/883, Training Loss: 0.6937\n",
      "Epoch 8/10, Batch 532/883, Training Loss: 0.6827\n",
      "Epoch 8/10, Batch 533/883, Training Loss: 0.4616\n",
      "Epoch 8/10, Batch 534/883, Training Loss: 0.5729\n",
      "Epoch 8/10, Batch 535/883, Training Loss: 0.4661\n",
      "Epoch 8/10, Batch 536/883, Training Loss: 0.8622\n",
      "Epoch 8/10, Batch 537/883, Training Loss: 0.8317\n",
      "Epoch 8/10, Batch 538/883, Training Loss: 0.2605\n",
      "Epoch 8/10, Batch 539/883, Training Loss: 0.5041\n",
      "Epoch 8/10, Batch 540/883, Training Loss: 0.4808\n",
      "Epoch 8/10, Batch 541/883, Training Loss: 0.6639\n",
      "Epoch 8/10, Batch 542/883, Training Loss: 0.6800\n",
      "Epoch 8/10, Batch 543/883, Training Loss: 0.8791\n",
      "Epoch 8/10, Batch 544/883, Training Loss: 0.5357\n",
      "Epoch 8/10, Batch 545/883, Training Loss: 0.4943\n",
      "Epoch 8/10, Batch 546/883, Training Loss: 0.7135\n",
      "Epoch 8/10, Batch 547/883, Training Loss: 0.5485\n",
      "Epoch 8/10, Batch 548/883, Training Loss: 0.6722\n",
      "Epoch 8/10, Batch 549/883, Training Loss: 0.3516\n",
      "Epoch 8/10, Batch 550/883, Training Loss: 0.3358\n",
      "Epoch 8/10, Batch 551/883, Training Loss: 0.8018\n",
      "Epoch 8/10, Batch 552/883, Training Loss: 0.2343\n",
      "Epoch 8/10, Batch 553/883, Training Loss: 0.2596\n",
      "Epoch 8/10, Batch 554/883, Training Loss: 0.3017\n",
      "Epoch 8/10, Batch 555/883, Training Loss: 0.3771\n",
      "Epoch 8/10, Batch 556/883, Training Loss: 0.6075\n",
      "Epoch 8/10, Batch 557/883, Training Loss: 0.6433\n",
      "Epoch 8/10, Batch 558/883, Training Loss: 0.3481\n",
      "Epoch 8/10, Batch 559/883, Training Loss: 0.7034\n",
      "Epoch 8/10, Batch 560/883, Training Loss: 0.6183\n",
      "Epoch 8/10, Batch 561/883, Training Loss: 0.5504\n",
      "Epoch 8/10, Batch 562/883, Training Loss: 0.7838\n",
      "Epoch 8/10, Batch 563/883, Training Loss: 0.2146\n",
      "Epoch 8/10, Batch 564/883, Training Loss: 0.3583\n",
      "Epoch 8/10, Batch 565/883, Training Loss: 0.6407\n",
      "Epoch 8/10, Batch 566/883, Training Loss: 0.6282\n",
      "Epoch 8/10, Batch 567/883, Training Loss: 0.4242\n",
      "Epoch 8/10, Batch 568/883, Training Loss: 0.7451\n",
      "Epoch 8/10, Batch 569/883, Training Loss: 0.3152\n",
      "Epoch 8/10, Batch 570/883, Training Loss: 0.3025\n",
      "Epoch 8/10, Batch 571/883, Training Loss: 0.5993\n",
      "Epoch 8/10, Batch 572/883, Training Loss: 0.5704\n",
      "Epoch 8/10, Batch 573/883, Training Loss: 0.4009\n",
      "Epoch 8/10, Batch 574/883, Training Loss: 0.5408\n",
      "Epoch 8/10, Batch 575/883, Training Loss: 0.4430\n",
      "Epoch 8/10, Batch 576/883, Training Loss: 0.4206\n",
      "Epoch 8/10, Batch 577/883, Training Loss: 0.4783\n",
      "Epoch 8/10, Batch 578/883, Training Loss: 0.5331\n",
      "Epoch 8/10, Batch 579/883, Training Loss: 0.5889\n",
      "Epoch 8/10, Batch 580/883, Training Loss: 0.7757\n",
      "Epoch 8/10, Batch 581/883, Training Loss: 0.6358\n",
      "Epoch 8/10, Batch 582/883, Training Loss: 0.4912\n",
      "Epoch 8/10, Batch 583/883, Training Loss: 0.5719\n",
      "Epoch 8/10, Batch 584/883, Training Loss: 0.7362\n",
      "Epoch 8/10, Batch 585/883, Training Loss: 0.6568\n",
      "Epoch 8/10, Batch 586/883, Training Loss: 1.0801\n",
      "Epoch 8/10, Batch 587/883, Training Loss: 0.5543\n",
      "Epoch 8/10, Batch 588/883, Training Loss: 1.0800\n",
      "Epoch 8/10, Batch 589/883, Training Loss: 1.1315\n",
      "Epoch 8/10, Batch 590/883, Training Loss: 0.7340\n",
      "Epoch 8/10, Batch 591/883, Training Loss: 0.4311\n",
      "Epoch 8/10, Batch 592/883, Training Loss: 0.4215\n",
      "Epoch 8/10, Batch 593/883, Training Loss: 0.5539\n",
      "Epoch 8/10, Batch 594/883, Training Loss: 0.3643\n",
      "Epoch 8/10, Batch 595/883, Training Loss: 0.6293\n",
      "Epoch 8/10, Batch 596/883, Training Loss: 0.5711\n",
      "Epoch 8/10, Batch 597/883, Training Loss: 0.8011\n",
      "Epoch 8/10, Batch 598/883, Training Loss: 0.8703\n",
      "Epoch 8/10, Batch 599/883, Training Loss: 0.5789\n",
      "Epoch 8/10, Batch 600/883, Training Loss: 0.4560\n",
      "Epoch 8/10, Batch 601/883, Training Loss: 0.5572\n",
      "Epoch 8/10, Batch 602/883, Training Loss: 0.4169\n",
      "Epoch 8/10, Batch 603/883, Training Loss: 0.5725\n",
      "Epoch 8/10, Batch 604/883, Training Loss: 0.6303\n",
      "Epoch 8/10, Batch 605/883, Training Loss: 0.2806\n",
      "Epoch 8/10, Batch 606/883, Training Loss: 0.3693\n",
      "Epoch 8/10, Batch 607/883, Training Loss: 0.4348\n",
      "Epoch 8/10, Batch 608/883, Training Loss: 0.5828\n",
      "Epoch 8/10, Batch 609/883, Training Loss: 0.4123\n",
      "Epoch 8/10, Batch 610/883, Training Loss: 0.4738\n",
      "Epoch 8/10, Batch 611/883, Training Loss: 0.4718\n",
      "Epoch 8/10, Batch 612/883, Training Loss: 0.5597\n",
      "Epoch 8/10, Batch 613/883, Training Loss: 0.6162\n",
      "Epoch 8/10, Batch 614/883, Training Loss: 0.4959\n",
      "Epoch 8/10, Batch 615/883, Training Loss: 0.4686\n",
      "Epoch 8/10, Batch 616/883, Training Loss: 0.6280\n",
      "Epoch 8/10, Batch 617/883, Training Loss: 0.7476\n",
      "Epoch 8/10, Batch 618/883, Training Loss: 0.3684\n",
      "Epoch 8/10, Batch 619/883, Training Loss: 0.8675\n",
      "Epoch 8/10, Batch 620/883, Training Loss: 0.5308\n",
      "Epoch 8/10, Batch 621/883, Training Loss: 0.5704\n",
      "Epoch 8/10, Batch 622/883, Training Loss: 0.6084\n",
      "Epoch 8/10, Batch 623/883, Training Loss: 0.6437\n",
      "Epoch 8/10, Batch 624/883, Training Loss: 0.9870\n",
      "Epoch 8/10, Batch 625/883, Training Loss: 0.5450\n",
      "Epoch 8/10, Batch 626/883, Training Loss: 0.6235\n",
      "Epoch 8/10, Batch 627/883, Training Loss: 0.6005\n",
      "Epoch 8/10, Batch 628/883, Training Loss: 0.6599\n",
      "Epoch 8/10, Batch 629/883, Training Loss: 0.5696\n",
      "Epoch 8/10, Batch 630/883, Training Loss: 1.0624\n",
      "Epoch 8/10, Batch 631/883, Training Loss: 0.7151\n",
      "Epoch 8/10, Batch 632/883, Training Loss: 0.6984\n",
      "Epoch 8/10, Batch 633/883, Training Loss: 0.5222\n",
      "Epoch 8/10, Batch 634/883, Training Loss: 0.5690\n",
      "Epoch 8/10, Batch 635/883, Training Loss: 0.5209\n",
      "Epoch 8/10, Batch 636/883, Training Loss: 0.3890\n",
      "Epoch 8/10, Batch 637/883, Training Loss: 0.3551\n",
      "Epoch 8/10, Batch 638/883, Training Loss: 0.5961\n",
      "Epoch 8/10, Batch 639/883, Training Loss: 0.6582\n",
      "Epoch 8/10, Batch 640/883, Training Loss: 0.7099\n",
      "Epoch 8/10, Batch 641/883, Training Loss: 0.4056\n",
      "Epoch 8/10, Batch 642/883, Training Loss: 0.4720\n",
      "Epoch 8/10, Batch 643/883, Training Loss: 0.4075\n",
      "Epoch 8/10, Batch 644/883, Training Loss: 0.6443\n",
      "Epoch 8/10, Batch 645/883, Training Loss: 0.4471\n",
      "Epoch 8/10, Batch 646/883, Training Loss: 0.7022\n",
      "Epoch 8/10, Batch 647/883, Training Loss: 0.7343\n",
      "Epoch 8/10, Batch 648/883, Training Loss: 0.6593\n",
      "Epoch 8/10, Batch 649/883, Training Loss: 0.3890\n",
      "Epoch 8/10, Batch 650/883, Training Loss: 0.7554\n",
      "Epoch 8/10, Batch 651/883, Training Loss: 0.2973\n",
      "Epoch 8/10, Batch 652/883, Training Loss: 0.3892\n",
      "Epoch 8/10, Batch 653/883, Training Loss: 0.7107\n",
      "Epoch 8/10, Batch 654/883, Training Loss: 0.4518\n",
      "Epoch 8/10, Batch 655/883, Training Loss: 0.2589\n",
      "Epoch 8/10, Batch 656/883, Training Loss: 0.4405\n",
      "Epoch 8/10, Batch 657/883, Training Loss: 0.4007\n",
      "Epoch 8/10, Batch 658/883, Training Loss: 0.5626\n",
      "Epoch 8/10, Batch 659/883, Training Loss: 0.4439\n",
      "Epoch 8/10, Batch 660/883, Training Loss: 0.5420\n",
      "Epoch 8/10, Batch 661/883, Training Loss: 0.5460\n",
      "Epoch 8/10, Batch 662/883, Training Loss: 0.8655\n",
      "Epoch 8/10, Batch 663/883, Training Loss: 0.6769\n",
      "Epoch 8/10, Batch 664/883, Training Loss: 0.4111\n",
      "Epoch 8/10, Batch 665/883, Training Loss: 0.8463\n",
      "Epoch 8/10, Batch 666/883, Training Loss: 0.6643\n",
      "Epoch 8/10, Batch 667/883, Training Loss: 0.3190\n",
      "Epoch 8/10, Batch 668/883, Training Loss: 0.8906\n",
      "Epoch 8/10, Batch 669/883, Training Loss: 0.6121\n",
      "Epoch 8/10, Batch 670/883, Training Loss: 0.6312\n",
      "Epoch 8/10, Batch 671/883, Training Loss: 0.3854\n",
      "Epoch 8/10, Batch 672/883, Training Loss: 0.7877\n",
      "Epoch 8/10, Batch 673/883, Training Loss: 0.6056\n",
      "Epoch 8/10, Batch 674/883, Training Loss: 0.6083\n",
      "Epoch 8/10, Batch 675/883, Training Loss: 0.3458\n",
      "Epoch 8/10, Batch 676/883, Training Loss: 0.6203\n",
      "Epoch 8/10, Batch 677/883, Training Loss: 0.4014\n",
      "Epoch 8/10, Batch 678/883, Training Loss: 0.3714\n",
      "Epoch 8/10, Batch 679/883, Training Loss: 0.6460\n",
      "Epoch 8/10, Batch 680/883, Training Loss: 0.5508\n",
      "Epoch 8/10, Batch 681/883, Training Loss: 0.8366\n",
      "Epoch 8/10, Batch 682/883, Training Loss: 0.8804\n",
      "Epoch 8/10, Batch 683/883, Training Loss: 0.4328\n",
      "Epoch 8/10, Batch 684/883, Training Loss: 0.4593\n",
      "Epoch 8/10, Batch 685/883, Training Loss: 0.8271\n",
      "Epoch 8/10, Batch 686/883, Training Loss: 0.2921\n",
      "Epoch 8/10, Batch 687/883, Training Loss: 0.6943\n",
      "Epoch 8/10, Batch 688/883, Training Loss: 0.7000\n",
      "Epoch 8/10, Batch 689/883, Training Loss: 0.7969\n",
      "Epoch 8/10, Batch 690/883, Training Loss: 0.6689\n",
      "Epoch 8/10, Batch 691/883, Training Loss: 0.7234\n",
      "Epoch 8/10, Batch 692/883, Training Loss: 0.3327\n",
      "Epoch 8/10, Batch 693/883, Training Loss: 0.5093\n",
      "Epoch 8/10, Batch 694/883, Training Loss: 0.4025\n",
      "Epoch 8/10, Batch 695/883, Training Loss: 0.3131\n",
      "Epoch 8/10, Batch 696/883, Training Loss: 0.4792\n",
      "Epoch 8/10, Batch 697/883, Training Loss: 0.5537\n",
      "Epoch 8/10, Batch 698/883, Training Loss: 0.4714\n",
      "Epoch 8/10, Batch 699/883, Training Loss: 0.5728\n",
      "Epoch 8/10, Batch 700/883, Training Loss: 0.5750\n",
      "Epoch 8/10, Batch 701/883, Training Loss: 0.4150\n",
      "Epoch 8/10, Batch 702/883, Training Loss: 0.5149\n",
      "Epoch 8/10, Batch 703/883, Training Loss: 0.5224\n",
      "Epoch 8/10, Batch 704/883, Training Loss: 0.6422\n",
      "Epoch 8/10, Batch 705/883, Training Loss: 0.3753\n",
      "Epoch 8/10, Batch 706/883, Training Loss: 0.3321\n",
      "Epoch 8/10, Batch 707/883, Training Loss: 0.2615\n",
      "Epoch 8/10, Batch 708/883, Training Loss: 0.7739\n",
      "Epoch 8/10, Batch 709/883, Training Loss: 0.6298\n",
      "Epoch 8/10, Batch 710/883, Training Loss: 0.8136\n",
      "Epoch 8/10, Batch 711/883, Training Loss: 0.5795\n",
      "Epoch 8/10, Batch 712/883, Training Loss: 0.4567\n",
      "Epoch 8/10, Batch 713/883, Training Loss: 0.2505\n",
      "Epoch 8/10, Batch 714/883, Training Loss: 0.6047\n",
      "Epoch 8/10, Batch 715/883, Training Loss: 0.3834\n",
      "Epoch 8/10, Batch 716/883, Training Loss: 0.5886\n",
      "Epoch 8/10, Batch 717/883, Training Loss: 0.3533\n",
      "Epoch 8/10, Batch 718/883, Training Loss: 0.3851\n",
      "Epoch 8/10, Batch 719/883, Training Loss: 0.3289\n",
      "Epoch 8/10, Batch 720/883, Training Loss: 0.3787\n",
      "Epoch 8/10, Batch 721/883, Training Loss: 0.3647\n",
      "Epoch 8/10, Batch 722/883, Training Loss: 0.4769\n",
      "Epoch 8/10, Batch 723/883, Training Loss: 0.5711\n",
      "Epoch 8/10, Batch 724/883, Training Loss: 0.5800\n",
      "Epoch 8/10, Batch 725/883, Training Loss: 0.6487\n",
      "Epoch 8/10, Batch 726/883, Training Loss: 0.4384\n",
      "Epoch 8/10, Batch 727/883, Training Loss: 0.5927\n",
      "Epoch 8/10, Batch 728/883, Training Loss: 0.3093\n",
      "Epoch 8/10, Batch 729/883, Training Loss: 0.9087\n",
      "Epoch 8/10, Batch 730/883, Training Loss: 0.4982\n",
      "Epoch 8/10, Batch 731/883, Training Loss: 1.0833\n",
      "Epoch 8/10, Batch 732/883, Training Loss: 0.5209\n",
      "Epoch 8/10, Batch 733/883, Training Loss: 0.3619\n",
      "Epoch 8/10, Batch 734/883, Training Loss: 0.3502\n",
      "Epoch 8/10, Batch 735/883, Training Loss: 0.7462\n",
      "Epoch 8/10, Batch 736/883, Training Loss: 0.4966\n",
      "Epoch 8/10, Batch 737/883, Training Loss: 0.2971\n",
      "Epoch 8/10, Batch 738/883, Training Loss: 0.9231\n",
      "Epoch 8/10, Batch 739/883, Training Loss: 0.2207\n",
      "Epoch 8/10, Batch 740/883, Training Loss: 0.3789\n",
      "Epoch 8/10, Batch 741/883, Training Loss: 0.8438\n",
      "Epoch 8/10, Batch 742/883, Training Loss: 0.5177\n",
      "Epoch 8/10, Batch 743/883, Training Loss: 0.6163\n",
      "Epoch 8/10, Batch 744/883, Training Loss: 0.7756\n",
      "Epoch 8/10, Batch 745/883, Training Loss: 0.9916\n",
      "Epoch 8/10, Batch 746/883, Training Loss: 0.9862\n",
      "Epoch 8/10, Batch 747/883, Training Loss: 0.7932\n",
      "Epoch 8/10, Batch 748/883, Training Loss: 0.3368\n",
      "Epoch 8/10, Batch 749/883, Training Loss: 0.3295\n",
      "Epoch 8/10, Batch 750/883, Training Loss: 0.4469\n",
      "Epoch 8/10, Batch 751/883, Training Loss: 0.5331\n",
      "Epoch 8/10, Batch 752/883, Training Loss: 0.5540\n",
      "Epoch 8/10, Batch 753/883, Training Loss: 0.5696\n",
      "Epoch 8/10, Batch 754/883, Training Loss: 0.7415\n",
      "Epoch 8/10, Batch 755/883, Training Loss: 0.6646\n",
      "Epoch 8/10, Batch 756/883, Training Loss: 0.3538\n",
      "Epoch 8/10, Batch 757/883, Training Loss: 0.4471\n",
      "Epoch 8/10, Batch 758/883, Training Loss: 0.6413\n",
      "Epoch 8/10, Batch 759/883, Training Loss: 0.4943\n",
      "Epoch 8/10, Batch 760/883, Training Loss: 0.4620\n",
      "Epoch 8/10, Batch 761/883, Training Loss: 0.3987\n",
      "Epoch 8/10, Batch 762/883, Training Loss: 0.5511\n",
      "Epoch 8/10, Batch 763/883, Training Loss: 0.6295\n",
      "Epoch 8/10, Batch 764/883, Training Loss: 0.5951\n",
      "Epoch 8/10, Batch 765/883, Training Loss: 0.6840\n",
      "Epoch 8/10, Batch 766/883, Training Loss: 0.6692\n",
      "Epoch 8/10, Batch 767/883, Training Loss: 0.9770\n",
      "Epoch 8/10, Batch 768/883, Training Loss: 0.3517\n",
      "Epoch 8/10, Batch 769/883, Training Loss: 0.2509\n",
      "Epoch 8/10, Batch 770/883, Training Loss: 0.6300\n",
      "Epoch 8/10, Batch 771/883, Training Loss: 0.6451\n",
      "Epoch 8/10, Batch 772/883, Training Loss: 0.6026\n",
      "Epoch 8/10, Batch 773/883, Training Loss: 0.3956\n",
      "Epoch 8/10, Batch 774/883, Training Loss: 0.7608\n",
      "Epoch 8/10, Batch 775/883, Training Loss: 0.9063\n",
      "Epoch 8/10, Batch 776/883, Training Loss: 0.5180\n",
      "Epoch 8/10, Batch 777/883, Training Loss: 0.4108\n",
      "Epoch 8/10, Batch 778/883, Training Loss: 0.6699\n",
      "Epoch 8/10, Batch 779/883, Training Loss: 0.6536\n",
      "Epoch 8/10, Batch 780/883, Training Loss: 0.5995\n",
      "Epoch 8/10, Batch 781/883, Training Loss: 0.8572\n",
      "Epoch 8/10, Batch 782/883, Training Loss: 0.5173\n",
      "Epoch 8/10, Batch 783/883, Training Loss: 0.3045\n",
      "Epoch 8/10, Batch 784/883, Training Loss: 0.6354\n",
      "Epoch 8/10, Batch 785/883, Training Loss: 0.5339\n",
      "Epoch 8/10, Batch 786/883, Training Loss: 0.6839\n",
      "Epoch 8/10, Batch 787/883, Training Loss: 0.5758\n",
      "Epoch 8/10, Batch 788/883, Training Loss: 0.3915\n",
      "Epoch 8/10, Batch 789/883, Training Loss: 0.6033\n",
      "Epoch 8/10, Batch 790/883, Training Loss: 0.4056\n",
      "Epoch 8/10, Batch 791/883, Training Loss: 0.3965\n",
      "Epoch 8/10, Batch 792/883, Training Loss: 0.5868\n",
      "Epoch 8/10, Batch 793/883, Training Loss: 0.5739\n",
      "Epoch 8/10, Batch 794/883, Training Loss: 0.5338\n",
      "Epoch 8/10, Batch 795/883, Training Loss: 0.5934\n",
      "Epoch 8/10, Batch 796/883, Training Loss: 0.6372\n",
      "Epoch 8/10, Batch 797/883, Training Loss: 0.3786\n",
      "Epoch 8/10, Batch 798/883, Training Loss: 0.6475\n",
      "Epoch 8/10, Batch 799/883, Training Loss: 0.5225\n",
      "Epoch 8/10, Batch 800/883, Training Loss: 0.3039\n",
      "Epoch 8/10, Batch 801/883, Training Loss: 0.7455\n",
      "Epoch 8/10, Batch 802/883, Training Loss: 0.6743\n",
      "Epoch 8/10, Batch 803/883, Training Loss: 0.2827\n",
      "Epoch 8/10, Batch 804/883, Training Loss: 0.4242\n",
      "Epoch 8/10, Batch 805/883, Training Loss: 0.5999\n",
      "Epoch 8/10, Batch 806/883, Training Loss: 0.3197\n",
      "Epoch 8/10, Batch 807/883, Training Loss: 0.5313\n",
      "Epoch 8/10, Batch 808/883, Training Loss: 0.4906\n",
      "Epoch 8/10, Batch 809/883, Training Loss: 0.8806\n",
      "Epoch 8/10, Batch 810/883, Training Loss: 0.6321\n",
      "Epoch 8/10, Batch 811/883, Training Loss: 0.4620\n",
      "Epoch 8/10, Batch 812/883, Training Loss: 0.3741\n",
      "Epoch 8/10, Batch 813/883, Training Loss: 0.5487\n",
      "Epoch 8/10, Batch 814/883, Training Loss: 0.5018\n",
      "Epoch 8/10, Batch 815/883, Training Loss: 0.4212\n",
      "Epoch 8/10, Batch 816/883, Training Loss: 0.6611\n",
      "Epoch 8/10, Batch 817/883, Training Loss: 0.6720\n",
      "Epoch 8/10, Batch 818/883, Training Loss: 0.5623\n",
      "Epoch 8/10, Batch 819/883, Training Loss: 0.9138\n",
      "Epoch 8/10, Batch 820/883, Training Loss: 0.7483\n",
      "Epoch 8/10, Batch 821/883, Training Loss: 0.2757\n",
      "Epoch 8/10, Batch 822/883, Training Loss: 0.5168\n",
      "Epoch 8/10, Batch 823/883, Training Loss: 0.6489\n",
      "Epoch 8/10, Batch 824/883, Training Loss: 0.5616\n",
      "Epoch 8/10, Batch 825/883, Training Loss: 0.2934\n",
      "Epoch 8/10, Batch 826/883, Training Loss: 0.4621\n",
      "Epoch 8/10, Batch 827/883, Training Loss: 0.4880\n",
      "Epoch 8/10, Batch 828/883, Training Loss: 0.4135\n",
      "Epoch 8/10, Batch 829/883, Training Loss: 0.8697\n",
      "Epoch 8/10, Batch 830/883, Training Loss: 0.5190\n",
      "Epoch 8/10, Batch 831/883, Training Loss: 0.3230\n",
      "Epoch 8/10, Batch 832/883, Training Loss: 0.3976\n",
      "Epoch 8/10, Batch 833/883, Training Loss: 0.4457\n",
      "Epoch 8/10, Batch 834/883, Training Loss: 0.7207\n",
      "Epoch 8/10, Batch 835/883, Training Loss: 0.5135\n",
      "Epoch 8/10, Batch 836/883, Training Loss: 0.4532\n",
      "Epoch 8/10, Batch 837/883, Training Loss: 0.5449\n",
      "Epoch 8/10, Batch 838/883, Training Loss: 0.4979\n",
      "Epoch 8/10, Batch 839/883, Training Loss: 0.5832\n",
      "Epoch 8/10, Batch 840/883, Training Loss: 0.7039\n",
      "Epoch 8/10, Batch 841/883, Training Loss: 0.6700\n",
      "Epoch 8/10, Batch 842/883, Training Loss: 0.4728\n",
      "Epoch 8/10, Batch 843/883, Training Loss: 0.4537\n",
      "Epoch 8/10, Batch 844/883, Training Loss: 0.4685\n",
      "Epoch 8/10, Batch 845/883, Training Loss: 0.9928\n",
      "Epoch 8/10, Batch 846/883, Training Loss: 0.4956\n",
      "Epoch 8/10, Batch 847/883, Training Loss: 0.6479\n",
      "Epoch 8/10, Batch 848/883, Training Loss: 0.6807\n",
      "Epoch 8/10, Batch 849/883, Training Loss: 0.5535\n",
      "Epoch 8/10, Batch 850/883, Training Loss: 0.3755\n",
      "Epoch 8/10, Batch 851/883, Training Loss: 0.9159\n",
      "Epoch 8/10, Batch 852/883, Training Loss: 0.4721\n",
      "Epoch 8/10, Batch 853/883, Training Loss: 0.4879\n",
      "Epoch 8/10, Batch 854/883, Training Loss: 0.6260\n",
      "Epoch 8/10, Batch 855/883, Training Loss: 0.3968\n",
      "Epoch 8/10, Batch 856/883, Training Loss: 1.0856\n",
      "Epoch 8/10, Batch 857/883, Training Loss: 0.7652\n",
      "Epoch 8/10, Batch 858/883, Training Loss: 0.6160\n",
      "Epoch 8/10, Batch 859/883, Training Loss: 0.4303\n",
      "Epoch 8/10, Batch 860/883, Training Loss: 0.4727\n",
      "Epoch 8/10, Batch 861/883, Training Loss: 0.5376\n",
      "Epoch 8/10, Batch 862/883, Training Loss: 0.6437\n",
      "Epoch 8/10, Batch 863/883, Training Loss: 0.6289\n",
      "Epoch 8/10, Batch 864/883, Training Loss: 0.7778\n",
      "Epoch 8/10, Batch 865/883, Training Loss: 0.5179\n",
      "Epoch 8/10, Batch 866/883, Training Loss: 0.5550\n",
      "Epoch 8/10, Batch 867/883, Training Loss: 0.3712\n",
      "Epoch 8/10, Batch 868/883, Training Loss: 0.5677\n",
      "Epoch 8/10, Batch 869/883, Training Loss: 0.6218\n",
      "Epoch 8/10, Batch 870/883, Training Loss: 0.4519\n",
      "Epoch 8/10, Batch 871/883, Training Loss: 0.8766\n",
      "Epoch 8/10, Batch 872/883, Training Loss: 0.3946\n",
      "Epoch 8/10, Batch 873/883, Training Loss: 0.5191\n",
      "Epoch 8/10, Batch 874/883, Training Loss: 0.3842\n",
      "Epoch 8/10, Batch 875/883, Training Loss: 0.5154\n",
      "Epoch 8/10, Batch 876/883, Training Loss: 0.6029\n",
      "Epoch 8/10, Batch 877/883, Training Loss: 0.4989\n",
      "Epoch 8/10, Batch 878/883, Training Loss: 0.7471\n",
      "Epoch 8/10, Batch 879/883, Training Loss: 0.3375\n",
      "Epoch 8/10, Batch 880/883, Training Loss: 0.3442\n",
      "Epoch 8/10, Batch 881/883, Training Loss: 0.6212\n",
      "Epoch 8/10, Batch 882/883, Training Loss: 0.7825\n",
      "Epoch 8/10, Batch 883/883, Training Loss: 0.5881\n",
      "Epoch 8/10, Training Loss: 0.5650, Validation Loss: 0.5351, Validation Accuracy: 0.7635\n",
      "Epoch 9/10, Batch 1/883, Training Loss: 0.6816\n",
      "Epoch 9/10, Batch 2/883, Training Loss: 0.4310\n",
      "Epoch 9/10, Batch 3/883, Training Loss: 0.5692\n",
      "Epoch 9/10, Batch 4/883, Training Loss: 0.4752\n",
      "Epoch 9/10, Batch 5/883, Training Loss: 0.6070\n",
      "Epoch 9/10, Batch 6/883, Training Loss: 0.7381\n",
      "Epoch 9/10, Batch 7/883, Training Loss: 0.5315\n",
      "Epoch 9/10, Batch 8/883, Training Loss: 0.3646\n",
      "Epoch 9/10, Batch 9/883, Training Loss: 0.3668\n",
      "Epoch 9/10, Batch 10/883, Training Loss: 0.6299\n",
      "Epoch 9/10, Batch 11/883, Training Loss: 0.4473\n",
      "Epoch 9/10, Batch 12/883, Training Loss: 0.2297\n",
      "Epoch 9/10, Batch 13/883, Training Loss: 0.7385\n",
      "Epoch 9/10, Batch 14/883, Training Loss: 0.3900\n",
      "Epoch 9/10, Batch 15/883, Training Loss: 0.6622\n",
      "Epoch 9/10, Batch 16/883, Training Loss: 0.7253\n",
      "Epoch 9/10, Batch 17/883, Training Loss: 0.6563\n",
      "Epoch 9/10, Batch 18/883, Training Loss: 0.5135\n",
      "Epoch 9/10, Batch 19/883, Training Loss: 0.2996\n",
      "Epoch 9/10, Batch 20/883, Training Loss: 0.4343\n",
      "Epoch 9/10, Batch 21/883, Training Loss: 0.4606\n",
      "Epoch 9/10, Batch 22/883, Training Loss: 0.3587\n",
      "Epoch 9/10, Batch 23/883, Training Loss: 0.6831\n",
      "Epoch 9/10, Batch 24/883, Training Loss: 0.8456\n",
      "Epoch 9/10, Batch 25/883, Training Loss: 0.4488\n",
      "Epoch 9/10, Batch 26/883, Training Loss: 0.5064\n",
      "Epoch 9/10, Batch 27/883, Training Loss: 0.5760\n",
      "Epoch 9/10, Batch 28/883, Training Loss: 0.4037\n",
      "Epoch 9/10, Batch 29/883, Training Loss: 0.7028\n",
      "Epoch 9/10, Batch 30/883, Training Loss: 0.7269\n",
      "Epoch 9/10, Batch 31/883, Training Loss: 0.3676\n",
      "Epoch 9/10, Batch 32/883, Training Loss: 0.4906\n",
      "Epoch 9/10, Batch 33/883, Training Loss: 0.9463\n",
      "Epoch 9/10, Batch 34/883, Training Loss: 0.7281\n",
      "Epoch 9/10, Batch 35/883, Training Loss: 0.5138\n",
      "Epoch 9/10, Batch 36/883, Training Loss: 0.5463\n",
      "Epoch 9/10, Batch 37/883, Training Loss: 0.6748\n",
      "Epoch 9/10, Batch 38/883, Training Loss: 0.5676\n",
      "Epoch 9/10, Batch 39/883, Training Loss: 0.5999\n",
      "Epoch 9/10, Batch 40/883, Training Loss: 0.8771\n",
      "Epoch 9/10, Batch 41/883, Training Loss: 0.4894\n",
      "Epoch 9/10, Batch 42/883, Training Loss: 0.3732\n",
      "Epoch 9/10, Batch 43/883, Training Loss: 0.5533\n",
      "Epoch 9/10, Batch 44/883, Training Loss: 0.4497\n",
      "Epoch 9/10, Batch 45/883, Training Loss: 0.2887\n",
      "Epoch 9/10, Batch 46/883, Training Loss: 0.3080\n",
      "Epoch 9/10, Batch 47/883, Training Loss: 0.5344\n",
      "Epoch 9/10, Batch 48/883, Training Loss: 0.4079\n",
      "Epoch 9/10, Batch 49/883, Training Loss: 0.6797\n",
      "Epoch 9/10, Batch 50/883, Training Loss: 0.6165\n",
      "Epoch 9/10, Batch 51/883, Training Loss: 0.6049\n",
      "Epoch 9/10, Batch 52/883, Training Loss: 0.3156\n",
      "Epoch 9/10, Batch 53/883, Training Loss: 0.7507\n",
      "Epoch 9/10, Batch 54/883, Training Loss: 0.7074\n",
      "Epoch 9/10, Batch 55/883, Training Loss: 0.4042\n",
      "Epoch 9/10, Batch 56/883, Training Loss: 0.4171\n",
      "Epoch 9/10, Batch 57/883, Training Loss: 0.8075\n",
      "Epoch 9/10, Batch 58/883, Training Loss: 0.5842\n",
      "Epoch 9/10, Batch 59/883, Training Loss: 0.5271\n",
      "Epoch 9/10, Batch 60/883, Training Loss: 0.9285\n",
      "Epoch 9/10, Batch 61/883, Training Loss: 0.6267\n",
      "Epoch 9/10, Batch 62/883, Training Loss: 0.6070\n",
      "Epoch 9/10, Batch 63/883, Training Loss: 0.5996\n",
      "Epoch 9/10, Batch 64/883, Training Loss: 0.4610\n",
      "Epoch 9/10, Batch 65/883, Training Loss: 0.3578\n",
      "Epoch 9/10, Batch 66/883, Training Loss: 0.5198\n",
      "Epoch 9/10, Batch 67/883, Training Loss: 0.7208\n",
      "Epoch 9/10, Batch 68/883, Training Loss: 0.3799\n",
      "Epoch 9/10, Batch 69/883, Training Loss: 0.4453\n",
      "Epoch 9/10, Batch 70/883, Training Loss: 0.3088\n",
      "Epoch 9/10, Batch 71/883, Training Loss: 0.6597\n",
      "Epoch 9/10, Batch 72/883, Training Loss: 0.5276\n",
      "Epoch 9/10, Batch 73/883, Training Loss: 0.4830\n",
      "Epoch 9/10, Batch 74/883, Training Loss: 0.6376\n",
      "Epoch 9/10, Batch 75/883, Training Loss: 0.7733\n",
      "Epoch 9/10, Batch 76/883, Training Loss: 0.3798\n",
      "Epoch 9/10, Batch 77/883, Training Loss: 0.4550\n",
      "Epoch 9/10, Batch 78/883, Training Loss: 0.7979\n",
      "Epoch 9/10, Batch 79/883, Training Loss: 0.5991\n",
      "Epoch 9/10, Batch 80/883, Training Loss: 0.6004\n",
      "Epoch 9/10, Batch 81/883, Training Loss: 0.3093\n",
      "Epoch 9/10, Batch 82/883, Training Loss: 0.4788\n",
      "Epoch 9/10, Batch 83/883, Training Loss: 0.4483\n",
      "Epoch 9/10, Batch 84/883, Training Loss: 0.4418\n",
      "Epoch 9/10, Batch 85/883, Training Loss: 0.7099\n",
      "Epoch 9/10, Batch 86/883, Training Loss: 0.4165\n",
      "Epoch 9/10, Batch 87/883, Training Loss: 0.4829\n",
      "Epoch 9/10, Batch 88/883, Training Loss: 0.3510\n",
      "Epoch 9/10, Batch 89/883, Training Loss: 0.3967\n",
      "Epoch 9/10, Batch 90/883, Training Loss: 0.6191\n",
      "Epoch 9/10, Batch 91/883, Training Loss: 0.5682\n",
      "Epoch 9/10, Batch 92/883, Training Loss: 0.3160\n",
      "Epoch 9/10, Batch 93/883, Training Loss: 0.6652\n",
      "Epoch 9/10, Batch 94/883, Training Loss: 0.4977\n",
      "Epoch 9/10, Batch 95/883, Training Loss: 0.5032\n",
      "Epoch 9/10, Batch 96/883, Training Loss: 0.4421\n",
      "Epoch 9/10, Batch 97/883, Training Loss: 1.0303\n",
      "Epoch 9/10, Batch 98/883, Training Loss: 0.3416\n",
      "Epoch 9/10, Batch 99/883, Training Loss: 0.3222\n",
      "Epoch 9/10, Batch 100/883, Training Loss: 0.4230\n",
      "Epoch 9/10, Batch 101/883, Training Loss: 0.4506\n",
      "Epoch 9/10, Batch 102/883, Training Loss: 0.5268\n",
      "Epoch 9/10, Batch 103/883, Training Loss: 0.3098\n",
      "Epoch 9/10, Batch 104/883, Training Loss: 0.3149\n",
      "Epoch 9/10, Batch 105/883, Training Loss: 0.4241\n",
      "Epoch 9/10, Batch 106/883, Training Loss: 0.4696\n",
      "Epoch 9/10, Batch 107/883, Training Loss: 0.3917\n",
      "Epoch 9/10, Batch 108/883, Training Loss: 0.4627\n",
      "Epoch 9/10, Batch 109/883, Training Loss: 0.4803\n",
      "Epoch 9/10, Batch 110/883, Training Loss: 0.2808\n",
      "Epoch 9/10, Batch 111/883, Training Loss: 0.4375\n",
      "Epoch 9/10, Batch 112/883, Training Loss: 0.4310\n",
      "Epoch 9/10, Batch 113/883, Training Loss: 0.6523\n",
      "Epoch 9/10, Batch 114/883, Training Loss: 0.4643\n",
      "Epoch 9/10, Batch 115/883, Training Loss: 0.6075\n",
      "Epoch 9/10, Batch 116/883, Training Loss: 0.3036\n",
      "Epoch 9/10, Batch 117/883, Training Loss: 0.8340\n",
      "Epoch 9/10, Batch 118/883, Training Loss: 0.7686\n",
      "Epoch 9/10, Batch 119/883, Training Loss: 0.3147\n",
      "Epoch 9/10, Batch 120/883, Training Loss: 0.5075\n",
      "Epoch 9/10, Batch 121/883, Training Loss: 0.5729\n",
      "Epoch 9/10, Batch 122/883, Training Loss: 0.6727\n",
      "Epoch 9/10, Batch 123/883, Training Loss: 0.4002\n",
      "Epoch 9/10, Batch 124/883, Training Loss: 0.7351\n",
      "Epoch 9/10, Batch 125/883, Training Loss: 0.5972\n",
      "Epoch 9/10, Batch 126/883, Training Loss: 0.4218\n",
      "Epoch 9/10, Batch 127/883, Training Loss: 0.3745\n",
      "Epoch 9/10, Batch 128/883, Training Loss: 0.3288\n",
      "Epoch 9/10, Batch 129/883, Training Loss: 0.4902\n",
      "Epoch 9/10, Batch 130/883, Training Loss: 0.5068\n",
      "Epoch 9/10, Batch 131/883, Training Loss: 0.2981\n",
      "Epoch 9/10, Batch 132/883, Training Loss: 0.4425\n",
      "Epoch 9/10, Batch 133/883, Training Loss: 0.4733\n",
      "Epoch 9/10, Batch 134/883, Training Loss: 0.2302\n",
      "Epoch 9/10, Batch 135/883, Training Loss: 0.4451\n",
      "Epoch 9/10, Batch 136/883, Training Loss: 0.8314\n",
      "Epoch 9/10, Batch 137/883, Training Loss: 0.6204\n",
      "Epoch 9/10, Batch 138/883, Training Loss: 0.4761\n",
      "Epoch 9/10, Batch 139/883, Training Loss: 0.5058\n",
      "Epoch 9/10, Batch 140/883, Training Loss: 1.5188\n",
      "Epoch 9/10, Batch 141/883, Training Loss: 0.6475\n",
      "Epoch 9/10, Batch 142/883, Training Loss: 0.5001\n",
      "Epoch 9/10, Batch 143/883, Training Loss: 0.3123\n",
      "Epoch 9/10, Batch 144/883, Training Loss: 0.2629\n",
      "Epoch 9/10, Batch 145/883, Training Loss: 0.4800\n",
      "Epoch 9/10, Batch 146/883, Training Loss: 0.7680\n",
      "Epoch 9/10, Batch 147/883, Training Loss: 0.3870\n",
      "Epoch 9/10, Batch 148/883, Training Loss: 0.7526\n",
      "Epoch 9/10, Batch 149/883, Training Loss: 0.5537\n",
      "Epoch 9/10, Batch 150/883, Training Loss: 0.4209\n",
      "Epoch 9/10, Batch 151/883, Training Loss: 0.9565\n",
      "Epoch 9/10, Batch 152/883, Training Loss: 0.5169\n",
      "Epoch 9/10, Batch 153/883, Training Loss: 0.6586\n",
      "Epoch 9/10, Batch 154/883, Training Loss: 0.2985\n",
      "Epoch 9/10, Batch 155/883, Training Loss: 1.0721\n",
      "Epoch 9/10, Batch 156/883, Training Loss: 0.4770\n",
      "Epoch 9/10, Batch 157/883, Training Loss: 0.4907\n",
      "Epoch 9/10, Batch 158/883, Training Loss: 0.4027\n",
      "Epoch 9/10, Batch 159/883, Training Loss: 0.5751\n",
      "Epoch 9/10, Batch 160/883, Training Loss: 0.6069\n",
      "Epoch 9/10, Batch 161/883, Training Loss: 0.5103\n",
      "Epoch 9/10, Batch 162/883, Training Loss: 0.3339\n",
      "Epoch 9/10, Batch 163/883, Training Loss: 0.3205\n",
      "Epoch 9/10, Batch 164/883, Training Loss: 0.6487\n",
      "Epoch 9/10, Batch 165/883, Training Loss: 0.4706\n",
      "Epoch 9/10, Batch 166/883, Training Loss: 0.4534\n",
      "Epoch 9/10, Batch 167/883, Training Loss: 0.6168\n",
      "Epoch 9/10, Batch 168/883, Training Loss: 0.4021\n",
      "Epoch 9/10, Batch 169/883, Training Loss: 0.3810\n",
      "Epoch 9/10, Batch 170/883, Training Loss: 0.6103\n",
      "Epoch 9/10, Batch 171/883, Training Loss: 0.3176\n",
      "Epoch 9/10, Batch 172/883, Training Loss: 0.5291\n",
      "Epoch 9/10, Batch 173/883, Training Loss: 0.6061\n",
      "Epoch 9/10, Batch 174/883, Training Loss: 0.2448\n",
      "Epoch 9/10, Batch 175/883, Training Loss: 0.4086\n",
      "Epoch 9/10, Batch 176/883, Training Loss: 0.3129\n",
      "Epoch 9/10, Batch 177/883, Training Loss: 0.4171\n",
      "Epoch 9/10, Batch 178/883, Training Loss: 0.5498\n",
      "Epoch 9/10, Batch 179/883, Training Loss: 0.6388\n",
      "Epoch 9/10, Batch 180/883, Training Loss: 0.3833\n",
      "Epoch 9/10, Batch 181/883, Training Loss: 0.5288\n",
      "Epoch 9/10, Batch 182/883, Training Loss: 0.7990\n",
      "Epoch 9/10, Batch 183/883, Training Loss: 0.5636\n",
      "Epoch 9/10, Batch 184/883, Training Loss: 0.5813\n",
      "Epoch 9/10, Batch 185/883, Training Loss: 0.4856\n",
      "Epoch 9/10, Batch 186/883, Training Loss: 0.6380\n",
      "Epoch 9/10, Batch 187/883, Training Loss: 0.7561\n",
      "Epoch 9/10, Batch 188/883, Training Loss: 0.5481\n",
      "Epoch 9/10, Batch 189/883, Training Loss: 0.4798\n",
      "Epoch 9/10, Batch 190/883, Training Loss: 0.5474\n",
      "Epoch 9/10, Batch 191/883, Training Loss: 0.5790\n",
      "Epoch 9/10, Batch 192/883, Training Loss: 0.5170\n",
      "Epoch 9/10, Batch 193/883, Training Loss: 0.7644\n",
      "Epoch 9/10, Batch 194/883, Training Loss: 0.4489\n",
      "Epoch 9/10, Batch 195/883, Training Loss: 0.5012\n",
      "Epoch 9/10, Batch 196/883, Training Loss: 0.3256\n",
      "Epoch 9/10, Batch 197/883, Training Loss: 0.5582\n",
      "Epoch 9/10, Batch 198/883, Training Loss: 0.5714\n",
      "Epoch 9/10, Batch 199/883, Training Loss: 0.5073\n",
      "Epoch 9/10, Batch 200/883, Training Loss: 0.4634\n",
      "Epoch 9/10, Batch 201/883, Training Loss: 0.5238\n",
      "Epoch 9/10, Batch 202/883, Training Loss: 0.6365\n",
      "Epoch 9/10, Batch 203/883, Training Loss: 0.4506\n",
      "Epoch 9/10, Batch 204/883, Training Loss: 0.3387\n",
      "Epoch 9/10, Batch 205/883, Training Loss: 0.4528\n",
      "Epoch 9/10, Batch 206/883, Training Loss: 0.6332\n",
      "Epoch 9/10, Batch 207/883, Training Loss: 0.4960\n",
      "Epoch 9/10, Batch 208/883, Training Loss: 0.6460\n",
      "Epoch 9/10, Batch 209/883, Training Loss: 0.4017\n",
      "Epoch 9/10, Batch 210/883, Training Loss: 0.3058\n",
      "Epoch 9/10, Batch 211/883, Training Loss: 0.6037\n",
      "Epoch 9/10, Batch 212/883, Training Loss: 0.8111\n",
      "Epoch 9/10, Batch 213/883, Training Loss: 0.4759\n",
      "Epoch 9/10, Batch 214/883, Training Loss: 0.5543\n",
      "Epoch 9/10, Batch 215/883, Training Loss: 0.3070\n",
      "Epoch 9/10, Batch 216/883, Training Loss: 0.6699\n",
      "Epoch 9/10, Batch 217/883, Training Loss: 0.5161\n",
      "Epoch 9/10, Batch 218/883, Training Loss: 0.5364\n",
      "Epoch 9/10, Batch 219/883, Training Loss: 0.5446\n",
      "Epoch 9/10, Batch 220/883, Training Loss: 0.4698\n",
      "Epoch 9/10, Batch 221/883, Training Loss: 0.6425\n",
      "Epoch 9/10, Batch 222/883, Training Loss: 0.6776\n",
      "Epoch 9/10, Batch 223/883, Training Loss: 0.7903\n",
      "Epoch 9/10, Batch 224/883, Training Loss: 0.4069\n",
      "Epoch 9/10, Batch 225/883, Training Loss: 0.4648\n",
      "Epoch 9/10, Batch 226/883, Training Loss: 0.3194\n",
      "Epoch 9/10, Batch 227/883, Training Loss: 0.7138\n",
      "Epoch 9/10, Batch 228/883, Training Loss: 0.5234\n",
      "Epoch 9/10, Batch 229/883, Training Loss: 0.3456\n",
      "Epoch 9/10, Batch 230/883, Training Loss: 0.2544\n",
      "Epoch 9/10, Batch 231/883, Training Loss: 0.8342\n",
      "Epoch 9/10, Batch 232/883, Training Loss: 0.6487\n",
      "Epoch 9/10, Batch 233/883, Training Loss: 0.7698\n",
      "Epoch 9/10, Batch 234/883, Training Loss: 0.3576\n",
      "Epoch 9/10, Batch 235/883, Training Loss: 1.0335\n",
      "Epoch 9/10, Batch 236/883, Training Loss: 0.4486\n",
      "Epoch 9/10, Batch 237/883, Training Loss: 0.6294\n",
      "Epoch 9/10, Batch 238/883, Training Loss: 0.6704\n",
      "Epoch 9/10, Batch 239/883, Training Loss: 0.4298\n",
      "Epoch 9/10, Batch 240/883, Training Loss: 0.3802\n",
      "Epoch 9/10, Batch 241/883, Training Loss: 0.4836\n",
      "Epoch 9/10, Batch 242/883, Training Loss: 0.7257\n",
      "Epoch 9/10, Batch 243/883, Training Loss: 0.7893\n",
      "Epoch 9/10, Batch 244/883, Training Loss: 0.5967\n",
      "Epoch 9/10, Batch 245/883, Training Loss: 0.8239\n",
      "Epoch 9/10, Batch 246/883, Training Loss: 0.3920\n",
      "Epoch 9/10, Batch 247/883, Training Loss: 0.5120\n",
      "Epoch 9/10, Batch 248/883, Training Loss: 0.6671\n",
      "Epoch 9/10, Batch 249/883, Training Loss: 0.8553\n",
      "Epoch 9/10, Batch 250/883, Training Loss: 0.3559\n",
      "Epoch 9/10, Batch 251/883, Training Loss: 0.6262\n",
      "Epoch 9/10, Batch 252/883, Training Loss: 0.6850\n",
      "Epoch 9/10, Batch 253/883, Training Loss: 0.7077\n",
      "Epoch 9/10, Batch 254/883, Training Loss: 0.5341\n",
      "Epoch 9/10, Batch 255/883, Training Loss: 0.2795\n",
      "Epoch 9/10, Batch 256/883, Training Loss: 0.5828\n",
      "Epoch 9/10, Batch 257/883, Training Loss: 0.3221\n",
      "Epoch 9/10, Batch 258/883, Training Loss: 0.3250\n",
      "Epoch 9/10, Batch 259/883, Training Loss: 0.4398\n",
      "Epoch 9/10, Batch 260/883, Training Loss: 0.4202\n",
      "Epoch 9/10, Batch 261/883, Training Loss: 0.5456\n",
      "Epoch 9/10, Batch 262/883, Training Loss: 0.5665\n",
      "Epoch 9/10, Batch 263/883, Training Loss: 0.6910\n",
      "Epoch 9/10, Batch 264/883, Training Loss: 0.6290\n",
      "Epoch 9/10, Batch 265/883, Training Loss: 0.5854\n",
      "Epoch 9/10, Batch 266/883, Training Loss: 0.5083\n",
      "Epoch 9/10, Batch 267/883, Training Loss: 0.6276\n",
      "Epoch 9/10, Batch 268/883, Training Loss: 0.2988\n",
      "Epoch 9/10, Batch 269/883, Training Loss: 0.5829\n",
      "Epoch 9/10, Batch 270/883, Training Loss: 0.4099\n",
      "Epoch 9/10, Batch 271/883, Training Loss: 0.3370\n",
      "Epoch 9/10, Batch 272/883, Training Loss: 0.3976\n",
      "Epoch 9/10, Batch 273/883, Training Loss: 0.4802\n",
      "Epoch 9/10, Batch 274/883, Training Loss: 0.9552\n",
      "Epoch 9/10, Batch 275/883, Training Loss: 0.4908\n",
      "Epoch 9/10, Batch 276/883, Training Loss: 0.9461\n",
      "Epoch 9/10, Batch 277/883, Training Loss: 0.2822\n",
      "Epoch 9/10, Batch 278/883, Training Loss: 0.2932\n",
      "Epoch 9/10, Batch 279/883, Training Loss: 0.4727\n",
      "Epoch 9/10, Batch 280/883, Training Loss: 0.3443\n",
      "Epoch 9/10, Batch 281/883, Training Loss: 0.8166\n",
      "Epoch 9/10, Batch 282/883, Training Loss: 0.7260\n",
      "Epoch 9/10, Batch 283/883, Training Loss: 0.4154\n",
      "Epoch 9/10, Batch 284/883, Training Loss: 0.4482\n",
      "Epoch 9/10, Batch 285/883, Training Loss: 0.5793\n",
      "Epoch 9/10, Batch 286/883, Training Loss: 0.3518\n",
      "Epoch 9/10, Batch 287/883, Training Loss: 0.6043\n",
      "Epoch 9/10, Batch 288/883, Training Loss: 0.3509\n",
      "Epoch 9/10, Batch 289/883, Training Loss: 0.2935\n",
      "Epoch 9/10, Batch 290/883, Training Loss: 0.3760\n",
      "Epoch 9/10, Batch 291/883, Training Loss: 0.5770\n",
      "Epoch 9/10, Batch 292/883, Training Loss: 0.7050\n",
      "Epoch 9/10, Batch 293/883, Training Loss: 0.8008\n",
      "Epoch 9/10, Batch 294/883, Training Loss: 0.6154\n",
      "Epoch 9/10, Batch 295/883, Training Loss: 0.9182\n",
      "Epoch 9/10, Batch 296/883, Training Loss: 0.6515\n",
      "Epoch 9/10, Batch 297/883, Training Loss: 0.4546\n",
      "Epoch 9/10, Batch 298/883, Training Loss: 0.2369\n",
      "Epoch 9/10, Batch 299/883, Training Loss: 0.4002\n",
      "Epoch 9/10, Batch 300/883, Training Loss: 0.5188\n",
      "Epoch 9/10, Batch 301/883, Training Loss: 0.9000\n",
      "Epoch 9/10, Batch 302/883, Training Loss: 0.5691\n",
      "Epoch 9/10, Batch 303/883, Training Loss: 0.7310\n",
      "Epoch 9/10, Batch 304/883, Training Loss: 0.6492\n",
      "Epoch 9/10, Batch 305/883, Training Loss: 0.4933\n",
      "Epoch 9/10, Batch 306/883, Training Loss: 0.9083\n",
      "Epoch 9/10, Batch 307/883, Training Loss: 0.3605\n",
      "Epoch 9/10, Batch 308/883, Training Loss: 1.1174\n",
      "Epoch 9/10, Batch 309/883, Training Loss: 0.5273\n",
      "Epoch 9/10, Batch 310/883, Training Loss: 0.3840\n",
      "Epoch 9/10, Batch 311/883, Training Loss: 0.2672\n",
      "Epoch 9/10, Batch 312/883, Training Loss: 0.4341\n",
      "Epoch 9/10, Batch 313/883, Training Loss: 0.4757\n",
      "Epoch 9/10, Batch 314/883, Training Loss: 0.6209\n",
      "Epoch 9/10, Batch 315/883, Training Loss: 0.5943\n",
      "Epoch 9/10, Batch 316/883, Training Loss: 0.3183\n",
      "Epoch 9/10, Batch 317/883, Training Loss: 0.5885\n",
      "Epoch 9/10, Batch 318/883, Training Loss: 0.5769\n",
      "Epoch 9/10, Batch 319/883, Training Loss: 0.5911\n",
      "Epoch 9/10, Batch 320/883, Training Loss: 0.6151\n",
      "Epoch 9/10, Batch 321/883, Training Loss: 0.3729\n",
      "Epoch 9/10, Batch 322/883, Training Loss: 0.6599\n",
      "Epoch 9/10, Batch 323/883, Training Loss: 0.5237\n",
      "Epoch 9/10, Batch 324/883, Training Loss: 0.3819\n",
      "Epoch 9/10, Batch 325/883, Training Loss: 0.3754\n",
      "Epoch 9/10, Batch 326/883, Training Loss: 0.4944\n",
      "Epoch 9/10, Batch 327/883, Training Loss: 0.6690\n",
      "Epoch 9/10, Batch 328/883, Training Loss: 0.7112\n",
      "Epoch 9/10, Batch 329/883, Training Loss: 0.5195\n",
      "Epoch 9/10, Batch 330/883, Training Loss: 0.6373\n",
      "Epoch 9/10, Batch 331/883, Training Loss: 0.5508\n",
      "Epoch 9/10, Batch 332/883, Training Loss: 0.4838\n",
      "Epoch 9/10, Batch 333/883, Training Loss: 0.4665\n",
      "Epoch 9/10, Batch 334/883, Training Loss: 0.2762\n",
      "Epoch 9/10, Batch 335/883, Training Loss: 0.3228\n",
      "Epoch 9/10, Batch 336/883, Training Loss: 1.1637\n",
      "Epoch 9/10, Batch 337/883, Training Loss: 0.5002\n",
      "Epoch 9/10, Batch 338/883, Training Loss: 0.5586\n",
      "Epoch 9/10, Batch 339/883, Training Loss: 0.4550\n",
      "Epoch 9/10, Batch 340/883, Training Loss: 0.6885\n",
      "Epoch 9/10, Batch 341/883, Training Loss: 0.3191\n",
      "Epoch 9/10, Batch 342/883, Training Loss: 0.3556\n",
      "Epoch 9/10, Batch 343/883, Training Loss: 0.8172\n",
      "Epoch 9/10, Batch 344/883, Training Loss: 0.4243\n",
      "Epoch 9/10, Batch 345/883, Training Loss: 0.4842\n",
      "Epoch 9/10, Batch 346/883, Training Loss: 0.5227\n",
      "Epoch 9/10, Batch 347/883, Training Loss: 0.7842\n",
      "Epoch 9/10, Batch 348/883, Training Loss: 0.5714\n",
      "Epoch 9/10, Batch 349/883, Training Loss: 0.3805\n",
      "Epoch 9/10, Batch 350/883, Training Loss: 0.2788\n",
      "Epoch 9/10, Batch 351/883, Training Loss: 0.7248\n",
      "Epoch 9/10, Batch 352/883, Training Loss: 0.4124\n",
      "Epoch 9/10, Batch 353/883, Training Loss: 0.3713\n",
      "Epoch 9/10, Batch 354/883, Training Loss: 0.4173\n",
      "Epoch 9/10, Batch 355/883, Training Loss: 0.6058\n",
      "Epoch 9/10, Batch 356/883, Training Loss: 0.6034\n",
      "Epoch 9/10, Batch 357/883, Training Loss: 0.5533\n",
      "Epoch 9/10, Batch 358/883, Training Loss: 0.5231\n",
      "Epoch 9/10, Batch 359/883, Training Loss: 0.5311\n",
      "Epoch 9/10, Batch 360/883, Training Loss: 0.6767\n",
      "Epoch 9/10, Batch 361/883, Training Loss: 0.4102\n",
      "Epoch 9/10, Batch 362/883, Training Loss: 0.3398\n",
      "Epoch 9/10, Batch 363/883, Training Loss: 0.4755\n",
      "Epoch 9/10, Batch 364/883, Training Loss: 0.8718\n",
      "Epoch 9/10, Batch 365/883, Training Loss: 0.3431\n",
      "Epoch 9/10, Batch 366/883, Training Loss: 0.5258\n",
      "Epoch 9/10, Batch 367/883, Training Loss: 0.2506\n",
      "Epoch 9/10, Batch 368/883, Training Loss: 0.4078\n",
      "Epoch 9/10, Batch 369/883, Training Loss: 0.4213\n",
      "Epoch 9/10, Batch 370/883, Training Loss: 0.3296\n",
      "Epoch 9/10, Batch 371/883, Training Loss: 0.5948\n",
      "Epoch 9/10, Batch 372/883, Training Loss: 0.4857\n",
      "Epoch 9/10, Batch 373/883, Training Loss: 0.6899\n",
      "Epoch 9/10, Batch 374/883, Training Loss: 0.3784\n",
      "Epoch 9/10, Batch 375/883, Training Loss: 0.4514\n",
      "Epoch 9/10, Batch 376/883, Training Loss: 0.3687\n",
      "Epoch 9/10, Batch 377/883, Training Loss: 0.6416\n",
      "Epoch 9/10, Batch 378/883, Training Loss: 0.6166\n",
      "Epoch 9/10, Batch 379/883, Training Loss: 0.2264\n",
      "Epoch 9/10, Batch 380/883, Training Loss: 0.7724\n",
      "Epoch 9/10, Batch 381/883, Training Loss: 0.5886\n",
      "Epoch 9/10, Batch 382/883, Training Loss: 0.3591\n",
      "Epoch 9/10, Batch 383/883, Training Loss: 0.3253\n",
      "Epoch 9/10, Batch 384/883, Training Loss: 0.7683\n",
      "Epoch 9/10, Batch 385/883, Training Loss: 0.8678\n",
      "Epoch 9/10, Batch 386/883, Training Loss: 0.4839\n",
      "Epoch 9/10, Batch 387/883, Training Loss: 0.3987\n",
      "Epoch 9/10, Batch 388/883, Training Loss: 0.6246\n",
      "Epoch 9/10, Batch 389/883, Training Loss: 0.5195\n",
      "Epoch 9/10, Batch 390/883, Training Loss: 0.4696\n",
      "Epoch 9/10, Batch 391/883, Training Loss: 0.4213\n",
      "Epoch 9/10, Batch 392/883, Training Loss: 0.4682\n",
      "Epoch 9/10, Batch 393/883, Training Loss: 0.5915\n",
      "Epoch 9/10, Batch 394/883, Training Loss: 0.2795\n",
      "Epoch 9/10, Batch 395/883, Training Loss: 0.3548\n",
      "Epoch 9/10, Batch 396/883, Training Loss: 0.4096\n",
      "Epoch 9/10, Batch 397/883, Training Loss: 0.8420\n",
      "Epoch 9/10, Batch 398/883, Training Loss: 0.3751\n",
      "Epoch 9/10, Batch 399/883, Training Loss: 0.4597\n",
      "Epoch 9/10, Batch 400/883, Training Loss: 0.6405\n",
      "Epoch 9/10, Batch 401/883, Training Loss: 0.4744\n",
      "Epoch 9/10, Batch 402/883, Training Loss: 0.5310\n",
      "Epoch 9/10, Batch 403/883, Training Loss: 0.4480\n",
      "Epoch 9/10, Batch 404/883, Training Loss: 0.6883\n",
      "Epoch 9/10, Batch 405/883, Training Loss: 0.5369\n",
      "Epoch 9/10, Batch 406/883, Training Loss: 0.9297\n",
      "Epoch 9/10, Batch 407/883, Training Loss: 0.3322\n",
      "Epoch 9/10, Batch 408/883, Training Loss: 0.4276\n",
      "Epoch 9/10, Batch 409/883, Training Loss: 0.3518\n",
      "Epoch 9/10, Batch 410/883, Training Loss: 0.3664\n",
      "Epoch 9/10, Batch 411/883, Training Loss: 0.6623\n",
      "Epoch 9/10, Batch 412/883, Training Loss: 0.5229\n",
      "Epoch 9/10, Batch 413/883, Training Loss: 0.4240\n",
      "Epoch 9/10, Batch 414/883, Training Loss: 0.5286\n",
      "Epoch 9/10, Batch 415/883, Training Loss: 0.5800\n",
      "Epoch 9/10, Batch 416/883, Training Loss: 0.3171\n",
      "Epoch 9/10, Batch 417/883, Training Loss: 0.3095\n",
      "Epoch 9/10, Batch 418/883, Training Loss: 0.8683\n",
      "Epoch 9/10, Batch 419/883, Training Loss: 0.6936\n",
      "Epoch 9/10, Batch 420/883, Training Loss: 0.4274\n",
      "Epoch 9/10, Batch 421/883, Training Loss: 0.5575\n",
      "Epoch 9/10, Batch 422/883, Training Loss: 0.8531\n",
      "Epoch 9/10, Batch 423/883, Training Loss: 0.4301\n",
      "Epoch 9/10, Batch 424/883, Training Loss: 0.3545\n",
      "Epoch 9/10, Batch 425/883, Training Loss: 0.5897\n",
      "Epoch 9/10, Batch 426/883, Training Loss: 0.6559\n",
      "Epoch 9/10, Batch 427/883, Training Loss: 0.6848\n",
      "Epoch 9/10, Batch 428/883, Training Loss: 0.2693\n",
      "Epoch 9/10, Batch 429/883, Training Loss: 0.5181\n",
      "Epoch 9/10, Batch 430/883, Training Loss: 0.4731\n",
      "Epoch 9/10, Batch 431/883, Training Loss: 0.4614\n",
      "Epoch 9/10, Batch 432/883, Training Loss: 0.3888\n",
      "Epoch 9/10, Batch 433/883, Training Loss: 0.5194\n",
      "Epoch 9/10, Batch 434/883, Training Loss: 0.6422\n",
      "Epoch 9/10, Batch 435/883, Training Loss: 0.5421\n",
      "Epoch 9/10, Batch 436/883, Training Loss: 0.3542\n",
      "Epoch 9/10, Batch 437/883, Training Loss: 0.5115\n",
      "Epoch 9/10, Batch 438/883, Training Loss: 0.4601\n",
      "Epoch 9/10, Batch 439/883, Training Loss: 0.6127\n",
      "Epoch 9/10, Batch 440/883, Training Loss: 0.4750\n",
      "Epoch 9/10, Batch 441/883, Training Loss: 0.5379\n",
      "Epoch 9/10, Batch 442/883, Training Loss: 0.8256\n",
      "Epoch 9/10, Batch 443/883, Training Loss: 0.7640\n",
      "Epoch 9/10, Batch 444/883, Training Loss: 0.6051\n",
      "Epoch 9/10, Batch 445/883, Training Loss: 0.4123\n",
      "Epoch 9/10, Batch 446/883, Training Loss: 0.7978\n",
      "Epoch 9/10, Batch 447/883, Training Loss: 0.2092\n",
      "Epoch 9/10, Batch 448/883, Training Loss: 1.4120\n",
      "Epoch 9/10, Batch 449/883, Training Loss: 0.5077\n",
      "Epoch 9/10, Batch 450/883, Training Loss: 0.5890\n",
      "Epoch 9/10, Batch 451/883, Training Loss: 0.5160\n",
      "Epoch 9/10, Batch 452/883, Training Loss: 0.4954\n",
      "Epoch 9/10, Batch 453/883, Training Loss: 0.6391\n",
      "Epoch 9/10, Batch 454/883, Training Loss: 0.5717\n",
      "Epoch 9/10, Batch 455/883, Training Loss: 0.6109\n",
      "Epoch 9/10, Batch 456/883, Training Loss: 0.4587\n",
      "Epoch 9/10, Batch 457/883, Training Loss: 0.4644\n",
      "Epoch 9/10, Batch 458/883, Training Loss: 0.5114\n",
      "Epoch 9/10, Batch 459/883, Training Loss: 0.3329\n",
      "Epoch 9/10, Batch 460/883, Training Loss: 0.5553\n",
      "Epoch 9/10, Batch 461/883, Training Loss: 0.6366\n",
      "Epoch 9/10, Batch 462/883, Training Loss: 0.3582\n",
      "Epoch 9/10, Batch 463/883, Training Loss: 0.4142\n",
      "Epoch 9/10, Batch 464/883, Training Loss: 0.2830\n",
      "Epoch 9/10, Batch 465/883, Training Loss: 0.6071\n",
      "Epoch 9/10, Batch 466/883, Training Loss: 0.4433\n",
      "Epoch 9/10, Batch 467/883, Training Loss: 0.3792\n",
      "Epoch 9/10, Batch 468/883, Training Loss: 0.3316\n",
      "Epoch 9/10, Batch 469/883, Training Loss: 0.4331\n",
      "Epoch 9/10, Batch 470/883, Training Loss: 0.7464\n",
      "Epoch 9/10, Batch 471/883, Training Loss: 0.6424\n",
      "Epoch 9/10, Batch 472/883, Training Loss: 0.6213\n",
      "Epoch 9/10, Batch 473/883, Training Loss: 0.2923\n",
      "Epoch 9/10, Batch 474/883, Training Loss: 0.2789\n",
      "Epoch 9/10, Batch 475/883, Training Loss: 0.6159\n",
      "Epoch 9/10, Batch 476/883, Training Loss: 0.4559\n",
      "Epoch 9/10, Batch 477/883, Training Loss: 0.3655\n",
      "Epoch 9/10, Batch 478/883, Training Loss: 0.3743\n",
      "Epoch 9/10, Batch 479/883, Training Loss: 0.1541\n",
      "Epoch 9/10, Batch 480/883, Training Loss: 0.9235\n",
      "Epoch 9/10, Batch 481/883, Training Loss: 0.6171\n",
      "Epoch 9/10, Batch 482/883, Training Loss: 0.7566\n",
      "Epoch 9/10, Batch 483/883, Training Loss: 0.3923\n",
      "Epoch 9/10, Batch 484/883, Training Loss: 0.5548\n",
      "Epoch 9/10, Batch 485/883, Training Loss: 0.9192\n",
      "Epoch 9/10, Batch 486/883, Training Loss: 0.9655\n",
      "Epoch 9/10, Batch 487/883, Training Loss: 0.4766\n",
      "Epoch 9/10, Batch 488/883, Training Loss: 0.5943\n",
      "Epoch 9/10, Batch 489/883, Training Loss: 0.3092\n",
      "Epoch 9/10, Batch 490/883, Training Loss: 0.7010\n",
      "Epoch 9/10, Batch 491/883, Training Loss: 0.3515\n",
      "Epoch 9/10, Batch 492/883, Training Loss: 0.7117\n",
      "Epoch 9/10, Batch 493/883, Training Loss: 0.5212\n",
      "Epoch 9/10, Batch 494/883, Training Loss: 0.8649\n",
      "Epoch 9/10, Batch 495/883, Training Loss: 0.4418\n",
      "Epoch 9/10, Batch 496/883, Training Loss: 0.3771\n",
      "Epoch 9/10, Batch 497/883, Training Loss: 0.4066\n",
      "Epoch 9/10, Batch 498/883, Training Loss: 0.5436\n",
      "Epoch 9/10, Batch 499/883, Training Loss: 0.5942\n",
      "Epoch 9/10, Batch 500/883, Training Loss: 0.4786\n",
      "Epoch 9/10, Batch 501/883, Training Loss: 0.8904\n",
      "Epoch 9/10, Batch 502/883, Training Loss: 0.6654\n",
      "Epoch 9/10, Batch 503/883, Training Loss: 0.3663\n",
      "Epoch 9/10, Batch 504/883, Training Loss: 0.5224\n",
      "Epoch 9/10, Batch 505/883, Training Loss: 0.6017\n",
      "Epoch 9/10, Batch 506/883, Training Loss: 0.3306\n",
      "Epoch 9/10, Batch 507/883, Training Loss: 0.3175\n",
      "Epoch 9/10, Batch 508/883, Training Loss: 0.5624\n",
      "Epoch 9/10, Batch 509/883, Training Loss: 0.3670\n",
      "Epoch 9/10, Batch 510/883, Training Loss: 0.4591\n",
      "Epoch 9/10, Batch 511/883, Training Loss: 0.2887\n",
      "Epoch 9/10, Batch 512/883, Training Loss: 0.5787\n",
      "Epoch 9/10, Batch 513/883, Training Loss: 0.2512\n",
      "Epoch 9/10, Batch 514/883, Training Loss: 0.4919\n",
      "Epoch 9/10, Batch 515/883, Training Loss: 0.6594\n",
      "Epoch 9/10, Batch 516/883, Training Loss: 0.5352\n",
      "Epoch 9/10, Batch 517/883, Training Loss: 0.7565\n",
      "Epoch 9/10, Batch 518/883, Training Loss: 0.5445\n",
      "Epoch 9/10, Batch 519/883, Training Loss: 1.1073\n",
      "Epoch 9/10, Batch 520/883, Training Loss: 0.4161\n",
      "Epoch 9/10, Batch 521/883, Training Loss: 0.4788\n",
      "Epoch 9/10, Batch 522/883, Training Loss: 0.7609\n",
      "Epoch 9/10, Batch 523/883, Training Loss: 0.5488\n",
      "Epoch 9/10, Batch 524/883, Training Loss: 0.2675\n",
      "Epoch 9/10, Batch 525/883, Training Loss: 0.5000\n",
      "Epoch 9/10, Batch 526/883, Training Loss: 0.5775\n",
      "Epoch 9/10, Batch 527/883, Training Loss: 0.5948\n",
      "Epoch 9/10, Batch 528/883, Training Loss: 0.3059\n",
      "Epoch 9/10, Batch 529/883, Training Loss: 0.8354\n",
      "Epoch 9/10, Batch 530/883, Training Loss: 0.6556\n",
      "Epoch 9/10, Batch 531/883, Training Loss: 0.5676\n",
      "Epoch 9/10, Batch 532/883, Training Loss: 0.8482\n",
      "Epoch 9/10, Batch 533/883, Training Loss: 0.7590\n",
      "Epoch 9/10, Batch 534/883, Training Loss: 0.5768\n",
      "Epoch 9/10, Batch 535/883, Training Loss: 0.4142\n",
      "Epoch 9/10, Batch 536/883, Training Loss: 0.5700\n",
      "Epoch 9/10, Batch 537/883, Training Loss: 0.3362\n",
      "Epoch 9/10, Batch 538/883, Training Loss: 0.5550\n",
      "Epoch 9/10, Batch 539/883, Training Loss: 0.4954\n",
      "Epoch 9/10, Batch 540/883, Training Loss: 0.5435\n",
      "Epoch 9/10, Batch 541/883, Training Loss: 0.8067\n",
      "Epoch 9/10, Batch 542/883, Training Loss: 0.6837\n",
      "Epoch 9/10, Batch 543/883, Training Loss: 0.6059\n",
      "Epoch 9/10, Batch 544/883, Training Loss: 0.1717\n",
      "Epoch 9/10, Batch 545/883, Training Loss: 0.5650\n",
      "Epoch 9/10, Batch 546/883, Training Loss: 0.6956\n",
      "Epoch 9/10, Batch 547/883, Training Loss: 0.5153\n",
      "Epoch 9/10, Batch 548/883, Training Loss: 0.7557\n",
      "Epoch 9/10, Batch 549/883, Training Loss: 0.4003\n",
      "Epoch 9/10, Batch 550/883, Training Loss: 0.6614\n",
      "Epoch 9/10, Batch 551/883, Training Loss: 0.6583\n",
      "Epoch 9/10, Batch 552/883, Training Loss: 0.4367\n",
      "Epoch 9/10, Batch 553/883, Training Loss: 0.3392\n",
      "Epoch 9/10, Batch 554/883, Training Loss: 0.6093\n",
      "Epoch 9/10, Batch 555/883, Training Loss: 0.6871\n",
      "Epoch 9/10, Batch 556/883, Training Loss: 0.4863\n",
      "Epoch 9/10, Batch 557/883, Training Loss: 0.7696\n",
      "Epoch 9/10, Batch 558/883, Training Loss: 0.5956\n",
      "Epoch 9/10, Batch 559/883, Training Loss: 0.6357\n",
      "Epoch 9/10, Batch 560/883, Training Loss: 0.5122\n",
      "Epoch 9/10, Batch 561/883, Training Loss: 0.4458\n",
      "Epoch 9/10, Batch 562/883, Training Loss: 0.4595\n",
      "Epoch 9/10, Batch 563/883, Training Loss: 0.6286\n",
      "Epoch 9/10, Batch 564/883, Training Loss: 0.4155\n",
      "Epoch 9/10, Batch 565/883, Training Loss: 0.7833\n",
      "Epoch 9/10, Batch 566/883, Training Loss: 0.8927\n",
      "Epoch 9/10, Batch 567/883, Training Loss: 0.3483\n",
      "Epoch 9/10, Batch 568/883, Training Loss: 0.5665\n",
      "Epoch 9/10, Batch 569/883, Training Loss: 0.6805\n",
      "Epoch 9/10, Batch 570/883, Training Loss: 0.4468\n",
      "Epoch 9/10, Batch 571/883, Training Loss: 0.5707\n",
      "Epoch 9/10, Batch 572/883, Training Loss: 0.5757\n",
      "Epoch 9/10, Batch 573/883, Training Loss: 0.7387\n",
      "Epoch 9/10, Batch 574/883, Training Loss: 0.4320\n",
      "Epoch 9/10, Batch 575/883, Training Loss: 0.5107\n",
      "Epoch 9/10, Batch 576/883, Training Loss: 0.5936\n",
      "Epoch 9/10, Batch 577/883, Training Loss: 0.8204\n",
      "Epoch 9/10, Batch 578/883, Training Loss: 0.4808\n",
      "Epoch 9/10, Batch 579/883, Training Loss: 0.4290\n",
      "Epoch 9/10, Batch 580/883, Training Loss: 0.6819\n",
      "Epoch 9/10, Batch 581/883, Training Loss: 0.3765\n",
      "Epoch 9/10, Batch 582/883, Training Loss: 0.8921\n",
      "Epoch 9/10, Batch 583/883, Training Loss: 0.8347\n",
      "Epoch 9/10, Batch 584/883, Training Loss: 0.2480\n",
      "Epoch 9/10, Batch 585/883, Training Loss: 0.6173\n",
      "Epoch 9/10, Batch 586/883, Training Loss: 0.2987\n",
      "Epoch 9/10, Batch 587/883, Training Loss: 0.9759\n",
      "Epoch 9/10, Batch 588/883, Training Loss: 0.6635\n",
      "Epoch 9/10, Batch 589/883, Training Loss: 0.4946\n",
      "Epoch 9/10, Batch 590/883, Training Loss: 0.6645\n",
      "Epoch 9/10, Batch 591/883, Training Loss: 0.5700\n",
      "Epoch 9/10, Batch 592/883, Training Loss: 0.6254\n",
      "Epoch 9/10, Batch 593/883, Training Loss: 0.6454\n",
      "Epoch 9/10, Batch 594/883, Training Loss: 0.4194\n",
      "Epoch 9/10, Batch 595/883, Training Loss: 0.7052\n",
      "Epoch 9/10, Batch 596/883, Training Loss: 0.6495\n",
      "Epoch 9/10, Batch 597/883, Training Loss: 0.5704\n",
      "Epoch 9/10, Batch 598/883, Training Loss: 0.2951\n",
      "Epoch 9/10, Batch 599/883, Training Loss: 1.1971\n",
      "Epoch 9/10, Batch 600/883, Training Loss: 0.3938\n",
      "Epoch 9/10, Batch 601/883, Training Loss: 0.4144\n",
      "Epoch 9/10, Batch 602/883, Training Loss: 0.3943\n",
      "Epoch 9/10, Batch 603/883, Training Loss: 0.6548\n",
      "Epoch 9/10, Batch 604/883, Training Loss: 0.3173\n",
      "Epoch 9/10, Batch 605/883, Training Loss: 0.6043\n",
      "Epoch 9/10, Batch 606/883, Training Loss: 0.5182\n",
      "Epoch 9/10, Batch 607/883, Training Loss: 0.6387\n",
      "Epoch 9/10, Batch 608/883, Training Loss: 0.3646\n",
      "Epoch 9/10, Batch 609/883, Training Loss: 0.5018\n",
      "Epoch 9/10, Batch 610/883, Training Loss: 0.4927\n",
      "Epoch 9/10, Batch 611/883, Training Loss: 0.3940\n",
      "Epoch 9/10, Batch 612/883, Training Loss: 0.4737\n",
      "Epoch 9/10, Batch 613/883, Training Loss: 0.4478\n",
      "Epoch 9/10, Batch 614/883, Training Loss: 0.9140\n",
      "Epoch 9/10, Batch 615/883, Training Loss: 1.0212\n",
      "Epoch 9/10, Batch 616/883, Training Loss: 0.7126\n",
      "Epoch 9/10, Batch 617/883, Training Loss: 0.5069\n",
      "Epoch 9/10, Batch 618/883, Training Loss: 0.4466\n",
      "Epoch 9/10, Batch 619/883, Training Loss: 0.2813\n",
      "Epoch 9/10, Batch 620/883, Training Loss: 0.5227\n",
      "Epoch 9/10, Batch 621/883, Training Loss: 0.5213\n",
      "Epoch 9/10, Batch 622/883, Training Loss: 0.6634\n",
      "Epoch 9/10, Batch 623/883, Training Loss: 0.7324\n",
      "Epoch 9/10, Batch 624/883, Training Loss: 0.9230\n",
      "Epoch 9/10, Batch 625/883, Training Loss: 0.6573\n",
      "Epoch 9/10, Batch 626/883, Training Loss: 0.6873\n",
      "Epoch 9/10, Batch 627/883, Training Loss: 0.4114\n",
      "Epoch 9/10, Batch 628/883, Training Loss: 0.3388\n",
      "Epoch 9/10, Batch 629/883, Training Loss: 0.6132\n",
      "Epoch 9/10, Batch 630/883, Training Loss: 0.5505\n",
      "Epoch 9/10, Batch 631/883, Training Loss: 0.2627\n",
      "Epoch 9/10, Batch 632/883, Training Loss: 0.5761\n",
      "Epoch 9/10, Batch 633/883, Training Loss: 0.4297\n",
      "Epoch 9/10, Batch 634/883, Training Loss: 0.4575\n",
      "Epoch 9/10, Batch 635/883, Training Loss: 0.7235\n",
      "Epoch 9/10, Batch 636/883, Training Loss: 0.5384\n",
      "Epoch 9/10, Batch 637/883, Training Loss: 0.2983\n",
      "Epoch 9/10, Batch 638/883, Training Loss: 0.6543\n",
      "Epoch 9/10, Batch 639/883, Training Loss: 0.7808\n",
      "Epoch 9/10, Batch 640/883, Training Loss: 0.5607\n",
      "Epoch 9/10, Batch 641/883, Training Loss: 0.4645\n",
      "Epoch 9/10, Batch 642/883, Training Loss: 0.4255\n",
      "Epoch 9/10, Batch 643/883, Training Loss: 0.6668\n",
      "Epoch 9/10, Batch 644/883, Training Loss: 0.4270\n",
      "Epoch 9/10, Batch 645/883, Training Loss: 0.9440\n",
      "Epoch 9/10, Batch 646/883, Training Loss: 0.5133\n",
      "Epoch 9/10, Batch 647/883, Training Loss: 0.3491\n",
      "Epoch 9/10, Batch 648/883, Training Loss: 0.5710\n",
      "Epoch 9/10, Batch 649/883, Training Loss: 0.4388\n",
      "Epoch 9/10, Batch 650/883, Training Loss: 0.6721\n",
      "Epoch 9/10, Batch 651/883, Training Loss: 0.5943\n",
      "Epoch 9/10, Batch 652/883, Training Loss: 0.4366\n",
      "Epoch 9/10, Batch 653/883, Training Loss: 0.7367\n",
      "Epoch 9/10, Batch 654/883, Training Loss: 0.5642\n",
      "Epoch 9/10, Batch 655/883, Training Loss: 0.4494\n",
      "Epoch 9/10, Batch 656/883, Training Loss: 0.4266\n",
      "Epoch 9/10, Batch 657/883, Training Loss: 0.2794\n",
      "Epoch 9/10, Batch 658/883, Training Loss: 0.8562\n",
      "Epoch 9/10, Batch 659/883, Training Loss: 0.7695\n",
      "Epoch 9/10, Batch 660/883, Training Loss: 0.5029\n",
      "Epoch 9/10, Batch 661/883, Training Loss: 0.4702\n",
      "Epoch 9/10, Batch 662/883, Training Loss: 0.7230\n",
      "Epoch 9/10, Batch 663/883, Training Loss: 0.6780\n",
      "Epoch 9/10, Batch 664/883, Training Loss: 0.5677\n",
      "Epoch 9/10, Batch 665/883, Training Loss: 0.7026\n",
      "Epoch 9/10, Batch 666/883, Training Loss: 0.6989\n",
      "Epoch 9/10, Batch 667/883, Training Loss: 0.6303\n",
      "Epoch 9/10, Batch 668/883, Training Loss: 0.3681\n",
      "Epoch 9/10, Batch 669/883, Training Loss: 0.5179\n",
      "Epoch 9/10, Batch 670/883, Training Loss: 0.2908\n",
      "Epoch 9/10, Batch 671/883, Training Loss: 0.6161\n",
      "Epoch 9/10, Batch 672/883, Training Loss: 0.6318\n",
      "Epoch 9/10, Batch 673/883, Training Loss: 0.3244\n",
      "Epoch 9/10, Batch 674/883, Training Loss: 0.6380\n",
      "Epoch 9/10, Batch 675/883, Training Loss: 0.4188\n",
      "Epoch 9/10, Batch 676/883, Training Loss: 0.4680\n",
      "Epoch 9/10, Batch 677/883, Training Loss: 0.4664\n",
      "Epoch 9/10, Batch 678/883, Training Loss: 0.5100\n",
      "Epoch 9/10, Batch 679/883, Training Loss: 0.8063\n",
      "Epoch 9/10, Batch 680/883, Training Loss: 0.4264\n",
      "Epoch 9/10, Batch 681/883, Training Loss: 0.5566\n",
      "Epoch 9/10, Batch 682/883, Training Loss: 0.3654\n",
      "Epoch 9/10, Batch 683/883, Training Loss: 0.4417\n",
      "Epoch 9/10, Batch 684/883, Training Loss: 0.5205\n",
      "Epoch 9/10, Batch 685/883, Training Loss: 0.2164\n",
      "Epoch 9/10, Batch 686/883, Training Loss: 0.7966\n",
      "Epoch 9/10, Batch 687/883, Training Loss: 0.3063\n",
      "Epoch 9/10, Batch 688/883, Training Loss: 0.7382\n",
      "Epoch 9/10, Batch 689/883, Training Loss: 0.6303\n",
      "Epoch 9/10, Batch 690/883, Training Loss: 0.5656\n",
      "Epoch 9/10, Batch 691/883, Training Loss: 0.4085\n",
      "Epoch 9/10, Batch 692/883, Training Loss: 0.6812\n",
      "Epoch 9/10, Batch 693/883, Training Loss: 0.3158\n",
      "Epoch 9/10, Batch 694/883, Training Loss: 0.5642\n",
      "Epoch 9/10, Batch 695/883, Training Loss: 0.5308\n",
      "Epoch 9/10, Batch 696/883, Training Loss: 0.2691\n",
      "Epoch 9/10, Batch 697/883, Training Loss: 0.6788\n",
      "Epoch 9/10, Batch 698/883, Training Loss: 0.2515\n",
      "Epoch 9/10, Batch 699/883, Training Loss: 0.7436\n",
      "Epoch 9/10, Batch 700/883, Training Loss: 0.2345\n",
      "Epoch 9/10, Batch 701/883, Training Loss: 0.2663\n",
      "Epoch 9/10, Batch 702/883, Training Loss: 0.5235\n",
      "Epoch 9/10, Batch 703/883, Training Loss: 0.3189\n",
      "Epoch 9/10, Batch 704/883, Training Loss: 0.3655\n",
      "Epoch 9/10, Batch 705/883, Training Loss: 0.5653\n",
      "Epoch 9/10, Batch 706/883, Training Loss: 0.2404\n",
      "Epoch 9/10, Batch 707/883, Training Loss: 0.4940\n",
      "Epoch 9/10, Batch 708/883, Training Loss: 0.6529\n",
      "Epoch 9/10, Batch 709/883, Training Loss: 0.6936\n",
      "Epoch 9/10, Batch 710/883, Training Loss: 0.6993\n",
      "Epoch 9/10, Batch 711/883, Training Loss: 0.2602\n",
      "Epoch 9/10, Batch 712/883, Training Loss: 0.5382\n",
      "Epoch 9/10, Batch 713/883, Training Loss: 0.4107\n",
      "Epoch 9/10, Batch 714/883, Training Loss: 0.3326\n",
      "Epoch 9/10, Batch 715/883, Training Loss: 0.5697\n",
      "Epoch 9/10, Batch 716/883, Training Loss: 0.4311\n",
      "Epoch 9/10, Batch 717/883, Training Loss: 0.6824\n",
      "Epoch 9/10, Batch 718/883, Training Loss: 0.3542\n",
      "Epoch 9/10, Batch 719/883, Training Loss: 0.3539\n",
      "Epoch 9/10, Batch 720/883, Training Loss: 0.4818\n",
      "Epoch 9/10, Batch 721/883, Training Loss: 0.3429\n",
      "Epoch 9/10, Batch 722/883, Training Loss: 0.4843\n",
      "Epoch 9/10, Batch 723/883, Training Loss: 0.8711\n",
      "Epoch 9/10, Batch 724/883, Training Loss: 0.4230\n",
      "Epoch 9/10, Batch 725/883, Training Loss: 0.3355\n",
      "Epoch 9/10, Batch 726/883, Training Loss: 0.6744\n",
      "Epoch 9/10, Batch 727/883, Training Loss: 0.4360\n",
      "Epoch 9/10, Batch 728/883, Training Loss: 0.4456\n",
      "Epoch 9/10, Batch 729/883, Training Loss: 0.6595\n",
      "Epoch 9/10, Batch 730/883, Training Loss: 0.7344\n",
      "Epoch 9/10, Batch 731/883, Training Loss: 0.5257\n",
      "Epoch 9/10, Batch 732/883, Training Loss: 0.5131\n",
      "Epoch 9/10, Batch 733/883, Training Loss: 0.3624\n",
      "Epoch 9/10, Batch 734/883, Training Loss: 0.6150\n",
      "Epoch 9/10, Batch 735/883, Training Loss: 0.8712\n",
      "Epoch 9/10, Batch 736/883, Training Loss: 0.6636\n",
      "Epoch 9/10, Batch 737/883, Training Loss: 0.2932\n",
      "Epoch 9/10, Batch 738/883, Training Loss: 1.1971\n",
      "Epoch 9/10, Batch 739/883, Training Loss: 0.6348\n",
      "Epoch 9/10, Batch 740/883, Training Loss: 0.6924\n",
      "Epoch 9/10, Batch 741/883, Training Loss: 0.3277\n",
      "Epoch 9/10, Batch 742/883, Training Loss: 0.9455\n",
      "Epoch 9/10, Batch 743/883, Training Loss: 0.6552\n",
      "Epoch 9/10, Batch 744/883, Training Loss: 0.7757\n",
      "Epoch 9/10, Batch 745/883, Training Loss: 0.6666\n",
      "Epoch 9/10, Batch 746/883, Training Loss: 0.4928\n",
      "Epoch 9/10, Batch 747/883, Training Loss: 1.0145\n",
      "Epoch 9/10, Batch 748/883, Training Loss: 0.5585\n",
      "Epoch 9/10, Batch 749/883, Training Loss: 0.4214\n",
      "Epoch 9/10, Batch 750/883, Training Loss: 0.3333\n",
      "Epoch 9/10, Batch 751/883, Training Loss: 0.4668\n",
      "Epoch 9/10, Batch 752/883, Training Loss: 0.7341\n",
      "Epoch 9/10, Batch 753/883, Training Loss: 0.6887\n",
      "Epoch 9/10, Batch 754/883, Training Loss: 0.3910\n",
      "Epoch 9/10, Batch 755/883, Training Loss: 0.3063\n",
      "Epoch 9/10, Batch 756/883, Training Loss: 0.8883\n",
      "Epoch 9/10, Batch 757/883, Training Loss: 0.3254\n",
      "Epoch 9/10, Batch 758/883, Training Loss: 0.4954\n",
      "Epoch 9/10, Batch 759/883, Training Loss: 0.5189\n",
      "Epoch 9/10, Batch 760/883, Training Loss: 0.5446\n",
      "Epoch 9/10, Batch 761/883, Training Loss: 0.3633\n",
      "Epoch 9/10, Batch 762/883, Training Loss: 0.5819\n",
      "Epoch 9/10, Batch 763/883, Training Loss: 0.4555\n",
      "Epoch 9/10, Batch 764/883, Training Loss: 0.3453\n",
      "Epoch 9/10, Batch 765/883, Training Loss: 0.4509\n",
      "Epoch 9/10, Batch 766/883, Training Loss: 0.6385\n",
      "Epoch 9/10, Batch 767/883, Training Loss: 0.6765\n",
      "Epoch 9/10, Batch 768/883, Training Loss: 0.6130\n",
      "Epoch 9/10, Batch 769/883, Training Loss: 0.2897\n",
      "Epoch 9/10, Batch 770/883, Training Loss: 0.3032\n",
      "Epoch 9/10, Batch 771/883, Training Loss: 0.7825\n",
      "Epoch 9/10, Batch 772/883, Training Loss: 0.3642\n",
      "Epoch 9/10, Batch 773/883, Training Loss: 0.6333\n",
      "Epoch 9/10, Batch 774/883, Training Loss: 0.9404\n",
      "Epoch 9/10, Batch 775/883, Training Loss: 0.7279\n",
      "Epoch 9/10, Batch 776/883, Training Loss: 0.1709\n",
      "Epoch 9/10, Batch 777/883, Training Loss: 0.6802\n",
      "Epoch 9/10, Batch 778/883, Training Loss: 0.4737\n",
      "Epoch 9/10, Batch 779/883, Training Loss: 0.3150\n",
      "Epoch 9/10, Batch 780/883, Training Loss: 0.6911\n",
      "Epoch 9/10, Batch 781/883, Training Loss: 0.3101\n",
      "Epoch 9/10, Batch 782/883, Training Loss: 0.6981\n",
      "Epoch 9/10, Batch 783/883, Training Loss: 0.2980\n",
      "Epoch 9/10, Batch 784/883, Training Loss: 0.3751\n",
      "Epoch 9/10, Batch 785/883, Training Loss: 0.5163\n",
      "Epoch 9/10, Batch 786/883, Training Loss: 0.4598\n",
      "Epoch 9/10, Batch 787/883, Training Loss: 0.4636\n",
      "Epoch 9/10, Batch 788/883, Training Loss: 0.5934\n",
      "Epoch 9/10, Batch 789/883, Training Loss: 0.2674\n",
      "Epoch 9/10, Batch 790/883, Training Loss: 0.7161\n",
      "Epoch 9/10, Batch 791/883, Training Loss: 0.4643\n",
      "Epoch 9/10, Batch 792/883, Training Loss: 0.7999\n",
      "Epoch 9/10, Batch 793/883, Training Loss: 0.3781\n",
      "Epoch 9/10, Batch 794/883, Training Loss: 0.3759\n",
      "Epoch 9/10, Batch 795/883, Training Loss: 0.5095\n",
      "Epoch 9/10, Batch 796/883, Training Loss: 0.6269\n",
      "Epoch 9/10, Batch 797/883, Training Loss: 0.6507\n",
      "Epoch 9/10, Batch 798/883, Training Loss: 0.4928\n",
      "Epoch 9/10, Batch 799/883, Training Loss: 0.4908\n",
      "Epoch 9/10, Batch 800/883, Training Loss: 0.8514\n",
      "Epoch 9/10, Batch 801/883, Training Loss: 0.3305\n",
      "Epoch 9/10, Batch 802/883, Training Loss: 0.4745\n",
      "Epoch 9/10, Batch 803/883, Training Loss: 0.6262\n",
      "Epoch 9/10, Batch 804/883, Training Loss: 0.3763\n",
      "Epoch 9/10, Batch 805/883, Training Loss: 0.4767\n",
      "Epoch 9/10, Batch 806/883, Training Loss: 0.6625\n",
      "Epoch 9/10, Batch 807/883, Training Loss: 0.3054\n",
      "Epoch 9/10, Batch 808/883, Training Loss: 0.7078\n",
      "Epoch 9/10, Batch 809/883, Training Loss: 0.3700\n",
      "Epoch 9/10, Batch 810/883, Training Loss: 0.3485\n",
      "Epoch 9/10, Batch 811/883, Training Loss: 0.4595\n",
      "Epoch 9/10, Batch 812/883, Training Loss: 0.2750\n",
      "Epoch 9/10, Batch 813/883, Training Loss: 0.7335\n",
      "Epoch 9/10, Batch 814/883, Training Loss: 0.4266\n",
      "Epoch 9/10, Batch 815/883, Training Loss: 0.2781\n",
      "Epoch 9/10, Batch 816/883, Training Loss: 0.3684\n",
      "Epoch 9/10, Batch 817/883, Training Loss: 0.4548\n",
      "Epoch 9/10, Batch 818/883, Training Loss: 0.2134\n",
      "Epoch 9/10, Batch 819/883, Training Loss: 0.6626\n",
      "Epoch 9/10, Batch 820/883, Training Loss: 0.3059\n",
      "Epoch 9/10, Batch 821/883, Training Loss: 0.6663\n",
      "Epoch 9/10, Batch 822/883, Training Loss: 0.2421\n",
      "Epoch 9/10, Batch 823/883, Training Loss: 0.7108\n",
      "Epoch 9/10, Batch 824/883, Training Loss: 0.4368\n",
      "Epoch 9/10, Batch 825/883, Training Loss: 0.7316\n",
      "Epoch 9/10, Batch 826/883, Training Loss: 0.5285\n",
      "Epoch 9/10, Batch 827/883, Training Loss: 0.3558\n",
      "Epoch 9/10, Batch 828/883, Training Loss: 0.2394\n",
      "Epoch 9/10, Batch 829/883, Training Loss: 0.4191\n",
      "Epoch 9/10, Batch 830/883, Training Loss: 0.9261\n",
      "Epoch 9/10, Batch 831/883, Training Loss: 0.4143\n",
      "Epoch 9/10, Batch 832/883, Training Loss: 0.4991\n",
      "Epoch 9/10, Batch 833/883, Training Loss: 0.5018\n",
      "Epoch 9/10, Batch 834/883, Training Loss: 0.2176\n",
      "Epoch 9/10, Batch 835/883, Training Loss: 0.5603\n",
      "Epoch 9/10, Batch 836/883, Training Loss: 0.7617\n",
      "Epoch 9/10, Batch 837/883, Training Loss: 0.5338\n",
      "Epoch 9/10, Batch 838/883, Training Loss: 0.6900\n",
      "Epoch 9/10, Batch 839/883, Training Loss: 0.3619\n",
      "Epoch 9/10, Batch 840/883, Training Loss: 0.4262\n",
      "Epoch 9/10, Batch 841/883, Training Loss: 0.4260\n",
      "Epoch 9/10, Batch 842/883, Training Loss: 0.3965\n",
      "Epoch 9/10, Batch 843/883, Training Loss: 0.6398\n",
      "Epoch 9/10, Batch 844/883, Training Loss: 0.6833\n",
      "Epoch 9/10, Batch 845/883, Training Loss: 0.3384\n",
      "Epoch 9/10, Batch 846/883, Training Loss: 0.5825\n",
      "Epoch 9/10, Batch 847/883, Training Loss: 0.2937\n",
      "Epoch 9/10, Batch 848/883, Training Loss: 0.6149\n",
      "Epoch 9/10, Batch 849/883, Training Loss: 1.3088\n",
      "Epoch 9/10, Batch 850/883, Training Loss: 0.6417\n",
      "Epoch 9/10, Batch 851/883, Training Loss: 0.3415\n",
      "Epoch 9/10, Batch 852/883, Training Loss: 0.6116\n",
      "Epoch 9/10, Batch 853/883, Training Loss: 0.2190\n",
      "Epoch 9/10, Batch 854/883, Training Loss: 0.7724\n",
      "Epoch 9/10, Batch 855/883, Training Loss: 0.7162\n",
      "Epoch 9/10, Batch 856/883, Training Loss: 0.3627\n",
      "Epoch 9/10, Batch 857/883, Training Loss: 0.1724\n",
      "Epoch 9/10, Batch 858/883, Training Loss: 0.6618\n",
      "Epoch 9/10, Batch 859/883, Training Loss: 0.4182\n",
      "Epoch 9/10, Batch 860/883, Training Loss: 0.4447\n",
      "Epoch 9/10, Batch 861/883, Training Loss: 0.3218\n",
      "Epoch 9/10, Batch 862/883, Training Loss: 0.3841\n",
      "Epoch 9/10, Batch 863/883, Training Loss: 0.3043\n",
      "Epoch 9/10, Batch 864/883, Training Loss: 0.3740\n",
      "Epoch 9/10, Batch 865/883, Training Loss: 0.3419\n",
      "Epoch 9/10, Batch 866/883, Training Loss: 0.6479\n",
      "Epoch 9/10, Batch 867/883, Training Loss: 1.1710\n",
      "Epoch 9/10, Batch 868/883, Training Loss: 0.4846\n",
      "Epoch 9/10, Batch 869/883, Training Loss: 0.5618\n",
      "Epoch 9/10, Batch 870/883, Training Loss: 0.3245\n",
      "Epoch 9/10, Batch 871/883, Training Loss: 0.3882\n",
      "Epoch 9/10, Batch 872/883, Training Loss: 0.4574\n",
      "Epoch 9/10, Batch 873/883, Training Loss: 0.5554\n",
      "Epoch 9/10, Batch 874/883, Training Loss: 0.5018\n",
      "Epoch 9/10, Batch 875/883, Training Loss: 0.4304\n",
      "Epoch 9/10, Batch 876/883, Training Loss: 0.2530\n",
      "Epoch 9/10, Batch 877/883, Training Loss: 0.4306\n",
      "Epoch 9/10, Batch 878/883, Training Loss: 0.2554\n",
      "Epoch 9/10, Batch 879/883, Training Loss: 0.5861\n",
      "Epoch 9/10, Batch 880/883, Training Loss: 0.4287\n",
      "Epoch 9/10, Batch 881/883, Training Loss: 0.6843\n",
      "Epoch 9/10, Batch 882/883, Training Loss: 0.7893\n",
      "Epoch 9/10, Batch 883/883, Training Loss: 0.4372\n",
      "Epoch 9/10, Training Loss: 0.5339, Validation Loss: 0.5395, Validation Accuracy: 0.7751\n",
      "Epoch 10/10, Batch 1/883, Training Loss: 0.4911\n",
      "Epoch 10/10, Batch 2/883, Training Loss: 0.5951\n",
      "Epoch 10/10, Batch 3/883, Training Loss: 0.5705\n",
      "Epoch 10/10, Batch 4/883, Training Loss: 0.4477\n",
      "Epoch 10/10, Batch 5/883, Training Loss: 0.3084\n",
      "Epoch 10/10, Batch 6/883, Training Loss: 0.3799\n",
      "Epoch 10/10, Batch 7/883, Training Loss: 1.0152\n",
      "Epoch 10/10, Batch 8/883, Training Loss: 0.5508\n",
      "Epoch 10/10, Batch 9/883, Training Loss: 0.2887\n",
      "Epoch 10/10, Batch 10/883, Training Loss: 0.5617\n",
      "Epoch 10/10, Batch 11/883, Training Loss: 0.5272\n",
      "Epoch 10/10, Batch 12/883, Training Loss: 0.5122\n",
      "Epoch 10/10, Batch 13/883, Training Loss: 0.4411\n",
      "Epoch 10/10, Batch 14/883, Training Loss: 0.5164\n",
      "Epoch 10/10, Batch 15/883, Training Loss: 0.5967\n",
      "Epoch 10/10, Batch 16/883, Training Loss: 0.5171\n",
      "Epoch 10/10, Batch 17/883, Training Loss: 0.2863\n",
      "Epoch 10/10, Batch 18/883, Training Loss: 0.5739\n",
      "Epoch 10/10, Batch 19/883, Training Loss: 0.2487\n",
      "Epoch 10/10, Batch 20/883, Training Loss: 0.7569\n",
      "Epoch 10/10, Batch 21/883, Training Loss: 0.5614\n",
      "Epoch 10/10, Batch 22/883, Training Loss: 0.5171\n",
      "Epoch 10/10, Batch 23/883, Training Loss: 0.3376\n",
      "Epoch 10/10, Batch 24/883, Training Loss: 0.3631\n",
      "Epoch 10/10, Batch 25/883, Training Loss: 0.4514\n",
      "Epoch 10/10, Batch 26/883, Training Loss: 0.3222\n",
      "Epoch 10/10, Batch 27/883, Training Loss: 0.4551\n",
      "Epoch 10/10, Batch 28/883, Training Loss: 0.5083\n",
      "Epoch 10/10, Batch 29/883, Training Loss: 0.8684\n",
      "Epoch 10/10, Batch 30/883, Training Loss: 0.8445\n",
      "Epoch 10/10, Batch 31/883, Training Loss: 0.3711\n",
      "Epoch 10/10, Batch 32/883, Training Loss: 0.6054\n",
      "Epoch 10/10, Batch 33/883, Training Loss: 0.6551\n",
      "Epoch 10/10, Batch 34/883, Training Loss: 0.5559\n",
      "Epoch 10/10, Batch 35/883, Training Loss: 0.3825\n",
      "Epoch 10/10, Batch 36/883, Training Loss: 0.6451\n",
      "Epoch 10/10, Batch 37/883, Training Loss: 0.2058\n",
      "Epoch 10/10, Batch 38/883, Training Loss: 0.3824\n",
      "Epoch 10/10, Batch 39/883, Training Loss: 0.5481\n",
      "Epoch 10/10, Batch 40/883, Training Loss: 0.8014\n",
      "Epoch 10/10, Batch 41/883, Training Loss: 0.5211\n",
      "Epoch 10/10, Batch 42/883, Training Loss: 0.4666\n",
      "Epoch 10/10, Batch 43/883, Training Loss: 0.7768\n",
      "Epoch 10/10, Batch 44/883, Training Loss: 0.4425\n",
      "Epoch 10/10, Batch 45/883, Training Loss: 0.3954\n",
      "Epoch 10/10, Batch 46/883, Training Loss: 0.3655\n",
      "Epoch 10/10, Batch 47/883, Training Loss: 0.2686\n",
      "Epoch 10/10, Batch 48/883, Training Loss: 0.3413\n",
      "Epoch 10/10, Batch 49/883, Training Loss: 0.3186\n",
      "Epoch 10/10, Batch 50/883, Training Loss: 0.2476\n",
      "Epoch 10/10, Batch 51/883, Training Loss: 0.4884\n",
      "Epoch 10/10, Batch 52/883, Training Loss: 0.3779\n",
      "Epoch 10/10, Batch 53/883, Training Loss: 0.5464\n",
      "Epoch 10/10, Batch 54/883, Training Loss: 0.2930\n",
      "Epoch 10/10, Batch 55/883, Training Loss: 0.4717\n",
      "Epoch 10/10, Batch 56/883, Training Loss: 0.4588\n",
      "Epoch 10/10, Batch 57/883, Training Loss: 0.3281\n",
      "Epoch 10/10, Batch 58/883, Training Loss: 0.4987\n",
      "Epoch 10/10, Batch 59/883, Training Loss: 0.4052\n",
      "Epoch 10/10, Batch 60/883, Training Loss: 0.9462\n",
      "Epoch 10/10, Batch 61/883, Training Loss: 0.4596\n",
      "Epoch 10/10, Batch 62/883, Training Loss: 0.4160\n",
      "Epoch 10/10, Batch 63/883, Training Loss: 0.7006\n",
      "Epoch 10/10, Batch 64/883, Training Loss: 0.3848\n",
      "Epoch 10/10, Batch 65/883, Training Loss: 0.4912\n",
      "Epoch 10/10, Batch 66/883, Training Loss: 0.3908\n",
      "Epoch 10/10, Batch 67/883, Training Loss: 0.2911\n",
      "Epoch 10/10, Batch 68/883, Training Loss: 0.4069\n",
      "Epoch 10/10, Batch 69/883, Training Loss: 0.2258\n",
      "Epoch 10/10, Batch 70/883, Training Loss: 0.7488\n",
      "Epoch 10/10, Batch 71/883, Training Loss: 0.2322\n",
      "Epoch 10/10, Batch 72/883, Training Loss: 0.3917\n",
      "Epoch 10/10, Batch 73/883, Training Loss: 0.2946\n",
      "Epoch 10/10, Batch 74/883, Training Loss: 0.4669\n",
      "Epoch 10/10, Batch 75/883, Training Loss: 0.4429\n",
      "Epoch 10/10, Batch 76/883, Training Loss: 0.7854\n",
      "Epoch 10/10, Batch 77/883, Training Loss: 0.4261\n",
      "Epoch 10/10, Batch 78/883, Training Loss: 0.7019\n",
      "Epoch 10/10, Batch 79/883, Training Loss: 1.0291\n",
      "Epoch 10/10, Batch 80/883, Training Loss: 0.4364\n",
      "Epoch 10/10, Batch 81/883, Training Loss: 0.3797\n",
      "Epoch 10/10, Batch 82/883, Training Loss: 0.4739\n",
      "Epoch 10/10, Batch 83/883, Training Loss: 0.4517\n",
      "Epoch 10/10, Batch 84/883, Training Loss: 0.3831\n",
      "Epoch 10/10, Batch 85/883, Training Loss: 0.4979\n",
      "Epoch 10/10, Batch 86/883, Training Loss: 0.3611\n",
      "Epoch 10/10, Batch 87/883, Training Loss: 0.7939\n",
      "Epoch 10/10, Batch 88/883, Training Loss: 0.4361\n",
      "Epoch 10/10, Batch 89/883, Training Loss: 0.6213\n",
      "Epoch 10/10, Batch 90/883, Training Loss: 0.7146\n",
      "Epoch 10/10, Batch 91/883, Training Loss: 0.3657\n",
      "Epoch 10/10, Batch 92/883, Training Loss: 0.6092\n",
      "Epoch 10/10, Batch 93/883, Training Loss: 0.5269\n",
      "Epoch 10/10, Batch 94/883, Training Loss: 0.5102\n",
      "Epoch 10/10, Batch 95/883, Training Loss: 0.8305\n",
      "Epoch 10/10, Batch 96/883, Training Loss: 0.6021\n",
      "Epoch 10/10, Batch 97/883, Training Loss: 0.3840\n",
      "Epoch 10/10, Batch 98/883, Training Loss: 0.5388\n",
      "Epoch 10/10, Batch 99/883, Training Loss: 0.3193\n",
      "Epoch 10/10, Batch 100/883, Training Loss: 0.5173\n",
      "Epoch 10/10, Batch 101/883, Training Loss: 0.5078\n",
      "Epoch 10/10, Batch 102/883, Training Loss: 0.8237\n",
      "Epoch 10/10, Batch 103/883, Training Loss: 0.4653\n",
      "Epoch 10/10, Batch 104/883, Training Loss: 0.4146\n",
      "Epoch 10/10, Batch 105/883, Training Loss: 0.6409\n",
      "Epoch 10/10, Batch 106/883, Training Loss: 0.7814\n",
      "Epoch 10/10, Batch 107/883, Training Loss: 0.3958\n",
      "Epoch 10/10, Batch 108/883, Training Loss: 0.6874\n",
      "Epoch 10/10, Batch 109/883, Training Loss: 0.2885\n",
      "Epoch 10/10, Batch 110/883, Training Loss: 0.6132\n",
      "Epoch 10/10, Batch 111/883, Training Loss: 0.7960\n",
      "Epoch 10/10, Batch 112/883, Training Loss: 0.5603\n",
      "Epoch 10/10, Batch 113/883, Training Loss: 0.3543\n",
      "Epoch 10/10, Batch 114/883, Training Loss: 0.5549\n",
      "Epoch 10/10, Batch 115/883, Training Loss: 0.8373\n",
      "Epoch 10/10, Batch 116/883, Training Loss: 0.6116\n",
      "Epoch 10/10, Batch 117/883, Training Loss: 0.5228\n",
      "Epoch 10/10, Batch 118/883, Training Loss: 0.3633\n",
      "Epoch 10/10, Batch 119/883, Training Loss: 0.6339\n",
      "Epoch 10/10, Batch 120/883, Training Loss: 0.4598\n",
      "Epoch 10/10, Batch 121/883, Training Loss: 0.4148\n",
      "Epoch 10/10, Batch 122/883, Training Loss: 0.4955\n",
      "Epoch 10/10, Batch 123/883, Training Loss: 0.6172\n",
      "Epoch 10/10, Batch 124/883, Training Loss: 0.4357\n",
      "Epoch 10/10, Batch 125/883, Training Loss: 0.8601\n",
      "Epoch 10/10, Batch 126/883, Training Loss: 0.8132\n",
      "Epoch 10/10, Batch 127/883, Training Loss: 0.2789\n",
      "Epoch 10/10, Batch 128/883, Training Loss: 0.4429\n",
      "Epoch 10/10, Batch 129/883, Training Loss: 0.3889\n",
      "Epoch 10/10, Batch 130/883, Training Loss: 0.7290\n",
      "Epoch 10/10, Batch 131/883, Training Loss: 0.2639\n",
      "Epoch 10/10, Batch 132/883, Training Loss: 0.4987\n",
      "Epoch 10/10, Batch 133/883, Training Loss: 0.3027\n",
      "Epoch 10/10, Batch 134/883, Training Loss: 0.4400\n",
      "Epoch 10/10, Batch 135/883, Training Loss: 0.5917\n",
      "Epoch 10/10, Batch 136/883, Training Loss: 0.4740\n",
      "Epoch 10/10, Batch 137/883, Training Loss: 0.6245\n",
      "Epoch 10/10, Batch 138/883, Training Loss: 0.6228\n",
      "Epoch 10/10, Batch 139/883, Training Loss: 0.5088\n",
      "Epoch 10/10, Batch 140/883, Training Loss: 0.4099\n",
      "Epoch 10/10, Batch 141/883, Training Loss: 0.3244\n",
      "Epoch 10/10, Batch 142/883, Training Loss: 0.6167\n",
      "Epoch 10/10, Batch 143/883, Training Loss: 1.0067\n",
      "Epoch 10/10, Batch 144/883, Training Loss: 0.4764\n",
      "Epoch 10/10, Batch 145/883, Training Loss: 0.6214\n",
      "Epoch 10/10, Batch 146/883, Training Loss: 0.5514\n",
      "Epoch 10/10, Batch 147/883, Training Loss: 0.5111\n",
      "Epoch 10/10, Batch 148/883, Training Loss: 0.6466\n",
      "Epoch 10/10, Batch 149/883, Training Loss: 0.5321\n",
      "Epoch 10/10, Batch 150/883, Training Loss: 0.3771\n",
      "Epoch 10/10, Batch 151/883, Training Loss: 0.4395\n",
      "Epoch 10/10, Batch 152/883, Training Loss: 0.4221\n",
      "Epoch 10/10, Batch 153/883, Training Loss: 0.4159\n",
      "Epoch 10/10, Batch 154/883, Training Loss: 0.5403\n",
      "Epoch 10/10, Batch 155/883, Training Loss: 0.4280\n",
      "Epoch 10/10, Batch 156/883, Training Loss: 0.3877\n",
      "Epoch 10/10, Batch 157/883, Training Loss: 0.4699\n",
      "Epoch 10/10, Batch 158/883, Training Loss: 0.5307\n",
      "Epoch 10/10, Batch 159/883, Training Loss: 0.6935\n",
      "Epoch 10/10, Batch 160/883, Training Loss: 0.6726\n",
      "Epoch 10/10, Batch 161/883, Training Loss: 0.4284\n",
      "Epoch 10/10, Batch 162/883, Training Loss: 0.5752\n",
      "Epoch 10/10, Batch 163/883, Training Loss: 0.5880\n",
      "Epoch 10/10, Batch 164/883, Training Loss: 0.4150\n",
      "Epoch 10/10, Batch 165/883, Training Loss: 0.3941\n",
      "Epoch 10/10, Batch 166/883, Training Loss: 0.4297\n",
      "Epoch 10/10, Batch 167/883, Training Loss: 0.5069\n",
      "Epoch 10/10, Batch 168/883, Training Loss: 0.4008\n",
      "Epoch 10/10, Batch 169/883, Training Loss: 0.2111\n",
      "Epoch 10/10, Batch 170/883, Training Loss: 0.5762\n",
      "Epoch 10/10, Batch 171/883, Training Loss: 0.4471\n",
      "Epoch 10/10, Batch 172/883, Training Loss: 0.4629\n",
      "Epoch 10/10, Batch 173/883, Training Loss: 0.6757\n",
      "Epoch 10/10, Batch 174/883, Training Loss: 0.3828\n",
      "Epoch 10/10, Batch 175/883, Training Loss: 0.4010\n",
      "Epoch 10/10, Batch 176/883, Training Loss: 0.5875\n",
      "Epoch 10/10, Batch 177/883, Training Loss: 0.5548\n",
      "Epoch 10/10, Batch 178/883, Training Loss: 0.5222\n",
      "Epoch 10/10, Batch 179/883, Training Loss: 0.9657\n",
      "Epoch 10/10, Batch 180/883, Training Loss: 0.4836\n",
      "Epoch 10/10, Batch 181/883, Training Loss: 0.3166\n",
      "Epoch 10/10, Batch 182/883, Training Loss: 0.8618\n",
      "Epoch 10/10, Batch 183/883, Training Loss: 0.4437\n",
      "Epoch 10/10, Batch 184/883, Training Loss: 0.5851\n",
      "Epoch 10/10, Batch 185/883, Training Loss: 0.4717\n",
      "Epoch 10/10, Batch 186/883, Training Loss: 0.2853\n",
      "Epoch 10/10, Batch 187/883, Training Loss: 0.5072\n",
      "Epoch 10/10, Batch 188/883, Training Loss: 0.3986\n",
      "Epoch 10/10, Batch 189/883, Training Loss: 0.4728\n",
      "Epoch 10/10, Batch 190/883, Training Loss: 0.5783\n",
      "Epoch 10/10, Batch 191/883, Training Loss: 0.3341\n",
      "Epoch 10/10, Batch 192/883, Training Loss: 0.4661\n",
      "Epoch 10/10, Batch 193/883, Training Loss: 0.6819\n",
      "Epoch 10/10, Batch 194/883, Training Loss: 0.5198\n",
      "Epoch 10/10, Batch 195/883, Training Loss: 0.6316\n",
      "Epoch 10/10, Batch 196/883, Training Loss: 0.6449\n",
      "Epoch 10/10, Batch 197/883, Training Loss: 0.3869\n",
      "Epoch 10/10, Batch 198/883, Training Loss: 0.5644\n",
      "Epoch 10/10, Batch 199/883, Training Loss: 0.4845\n",
      "Epoch 10/10, Batch 200/883, Training Loss: 0.2125\n",
      "Epoch 10/10, Batch 201/883, Training Loss: 0.4387\n",
      "Epoch 10/10, Batch 202/883, Training Loss: 0.6354\n",
      "Epoch 10/10, Batch 203/883, Training Loss: 0.6597\n",
      "Epoch 10/10, Batch 204/883, Training Loss: 0.4891\n",
      "Epoch 10/10, Batch 205/883, Training Loss: 0.4463\n",
      "Epoch 10/10, Batch 206/883, Training Loss: 0.5554\n",
      "Epoch 10/10, Batch 207/883, Training Loss: 0.4550\n",
      "Epoch 10/10, Batch 208/883, Training Loss: 0.8504\n",
      "Epoch 10/10, Batch 209/883, Training Loss: 0.6476\n",
      "Epoch 10/10, Batch 210/883, Training Loss: 0.5600\n",
      "Epoch 10/10, Batch 211/883, Training Loss: 0.5209\n",
      "Epoch 10/10, Batch 212/883, Training Loss: 0.6779\n",
      "Epoch 10/10, Batch 213/883, Training Loss: 0.3599\n",
      "Epoch 10/10, Batch 214/883, Training Loss: 0.4166\n",
      "Epoch 10/10, Batch 215/883, Training Loss: 0.6191\n",
      "Epoch 10/10, Batch 216/883, Training Loss: 0.4669\n",
      "Epoch 10/10, Batch 217/883, Training Loss: 0.5584\n",
      "Epoch 10/10, Batch 218/883, Training Loss: 0.6062\n",
      "Epoch 10/10, Batch 219/883, Training Loss: 0.5272\n",
      "Epoch 10/10, Batch 220/883, Training Loss: 0.5200\n",
      "Epoch 10/10, Batch 221/883, Training Loss: 0.6040\n",
      "Epoch 10/10, Batch 222/883, Training Loss: 0.2448\n",
      "Epoch 10/10, Batch 223/883, Training Loss: 0.4434\n",
      "Epoch 10/10, Batch 224/883, Training Loss: 0.2480\n",
      "Epoch 10/10, Batch 225/883, Training Loss: 0.3336\n",
      "Epoch 10/10, Batch 226/883, Training Loss: 0.4130\n",
      "Epoch 10/10, Batch 227/883, Training Loss: 0.5667\n",
      "Epoch 10/10, Batch 228/883, Training Loss: 0.3074\n",
      "Epoch 10/10, Batch 229/883, Training Loss: 0.7702\n",
      "Epoch 10/10, Batch 230/883, Training Loss: 0.4427\n",
      "Epoch 10/10, Batch 231/883, Training Loss: 0.4551\n",
      "Epoch 10/10, Batch 232/883, Training Loss: 0.4787\n",
      "Epoch 10/10, Batch 233/883, Training Loss: 0.6228\n",
      "Epoch 10/10, Batch 234/883, Training Loss: 1.0142\n",
      "Epoch 10/10, Batch 235/883, Training Loss: 0.3753\n",
      "Epoch 10/10, Batch 236/883, Training Loss: 0.3880\n",
      "Epoch 10/10, Batch 237/883, Training Loss: 0.7832\n",
      "Epoch 10/10, Batch 238/883, Training Loss: 0.4711\n",
      "Epoch 10/10, Batch 239/883, Training Loss: 0.3365\n",
      "Epoch 10/10, Batch 240/883, Training Loss: 0.3757\n",
      "Epoch 10/10, Batch 241/883, Training Loss: 0.4842\n",
      "Epoch 10/10, Batch 242/883, Training Loss: 0.5105\n",
      "Epoch 10/10, Batch 243/883, Training Loss: 0.4436\n",
      "Epoch 10/10, Batch 244/883, Training Loss: 0.4966\n",
      "Epoch 10/10, Batch 245/883, Training Loss: 0.2959\n",
      "Epoch 10/10, Batch 246/883, Training Loss: 0.3560\n",
      "Epoch 10/10, Batch 247/883, Training Loss: 0.6504\n",
      "Epoch 10/10, Batch 248/883, Training Loss: 0.6170\n",
      "Epoch 10/10, Batch 249/883, Training Loss: 0.9360\n",
      "Epoch 10/10, Batch 250/883, Training Loss: 0.4116\n",
      "Epoch 10/10, Batch 251/883, Training Loss: 0.4248\n",
      "Epoch 10/10, Batch 252/883, Training Loss: 0.3960\n",
      "Epoch 10/10, Batch 253/883, Training Loss: 0.4461\n",
      "Epoch 10/10, Batch 254/883, Training Loss: 0.7467\n",
      "Epoch 10/10, Batch 255/883, Training Loss: 0.2272\n",
      "Epoch 10/10, Batch 256/883, Training Loss: 0.8415\n",
      "Epoch 10/10, Batch 257/883, Training Loss: 0.3360\n",
      "Epoch 10/10, Batch 258/883, Training Loss: 0.3752\n",
      "Epoch 10/10, Batch 259/883, Training Loss: 1.0800\n",
      "Epoch 10/10, Batch 260/883, Training Loss: 0.4433\n",
      "Epoch 10/10, Batch 261/883, Training Loss: 0.4444\n",
      "Epoch 10/10, Batch 262/883, Training Loss: 0.3735\n",
      "Epoch 10/10, Batch 263/883, Training Loss: 0.3095\n",
      "Epoch 10/10, Batch 264/883, Training Loss: 0.2958\n",
      "Epoch 10/10, Batch 265/883, Training Loss: 0.4251\n",
      "Epoch 10/10, Batch 266/883, Training Loss: 0.7656\n",
      "Epoch 10/10, Batch 267/883, Training Loss: 0.8582\n",
      "Epoch 10/10, Batch 268/883, Training Loss: 0.3024\n",
      "Epoch 10/10, Batch 269/883, Training Loss: 0.5326\n",
      "Epoch 10/10, Batch 270/883, Training Loss: 0.8836\n",
      "Epoch 10/10, Batch 271/883, Training Loss: 0.4073\n",
      "Epoch 10/10, Batch 272/883, Training Loss: 0.6736\n",
      "Epoch 10/10, Batch 273/883, Training Loss: 0.5337\n",
      "Epoch 10/10, Batch 274/883, Training Loss: 0.4670\n",
      "Epoch 10/10, Batch 275/883, Training Loss: 0.4712\n",
      "Epoch 10/10, Batch 276/883, Training Loss: 0.3954\n",
      "Epoch 10/10, Batch 277/883, Training Loss: 0.6568\n",
      "Epoch 10/10, Batch 278/883, Training Loss: 0.6739\n",
      "Epoch 10/10, Batch 279/883, Training Loss: 0.6836\n",
      "Epoch 10/10, Batch 280/883, Training Loss: 0.3136\n",
      "Epoch 10/10, Batch 281/883, Training Loss: 0.4890\n",
      "Epoch 10/10, Batch 282/883, Training Loss: 0.5828\n",
      "Epoch 10/10, Batch 283/883, Training Loss: 0.3379\n",
      "Epoch 10/10, Batch 284/883, Training Loss: 0.5309\n",
      "Epoch 10/10, Batch 285/883, Training Loss: 0.6300\n",
      "Epoch 10/10, Batch 286/883, Training Loss: 0.2408\n",
      "Epoch 10/10, Batch 287/883, Training Loss: 0.2678\n",
      "Epoch 10/10, Batch 288/883, Training Loss: 0.4892\n",
      "Epoch 10/10, Batch 289/883, Training Loss: 0.4615\n",
      "Epoch 10/10, Batch 290/883, Training Loss: 0.3809\n",
      "Epoch 10/10, Batch 291/883, Training Loss: 0.4087\n",
      "Epoch 10/10, Batch 292/883, Training Loss: 0.3141\n",
      "Epoch 10/10, Batch 293/883, Training Loss: 0.4474\n",
      "Epoch 10/10, Batch 294/883, Training Loss: 0.4669\n",
      "Epoch 10/10, Batch 295/883, Training Loss: 0.4519\n",
      "Epoch 10/10, Batch 296/883, Training Loss: 0.5463\n",
      "Epoch 10/10, Batch 297/883, Training Loss: 0.6477\n",
      "Epoch 10/10, Batch 298/883, Training Loss: 0.3466\n",
      "Epoch 10/10, Batch 299/883, Training Loss: 1.0263\n",
      "Epoch 10/10, Batch 300/883, Training Loss: 0.7417\n",
      "Epoch 10/10, Batch 301/883, Training Loss: 0.2348\n",
      "Epoch 10/10, Batch 302/883, Training Loss: 0.9755\n",
      "Epoch 10/10, Batch 303/883, Training Loss: 0.4015\n",
      "Epoch 10/10, Batch 304/883, Training Loss: 0.7423\n",
      "Epoch 10/10, Batch 305/883, Training Loss: 0.2892\n",
      "Epoch 10/10, Batch 306/883, Training Loss: 0.4140\n",
      "Epoch 10/10, Batch 307/883, Training Loss: 0.2709\n",
      "Epoch 10/10, Batch 308/883, Training Loss: 0.5770\n",
      "Epoch 10/10, Batch 309/883, Training Loss: 0.3508\n",
      "Epoch 10/10, Batch 310/883, Training Loss: 0.4191\n",
      "Epoch 10/10, Batch 311/883, Training Loss: 0.4237\n",
      "Epoch 10/10, Batch 312/883, Training Loss: 0.6668\n",
      "Epoch 10/10, Batch 313/883, Training Loss: 0.5019\n",
      "Epoch 10/10, Batch 314/883, Training Loss: 0.7611\n",
      "Epoch 10/10, Batch 315/883, Training Loss: 0.2802\n",
      "Epoch 10/10, Batch 316/883, Training Loss: 0.8334\n",
      "Epoch 10/10, Batch 317/883, Training Loss: 0.5722\n",
      "Epoch 10/10, Batch 318/883, Training Loss: 0.4958\n",
      "Epoch 10/10, Batch 319/883, Training Loss: 0.6044\n",
      "Epoch 10/10, Batch 320/883, Training Loss: 0.8700\n",
      "Epoch 10/10, Batch 321/883, Training Loss: 0.9612\n",
      "Epoch 10/10, Batch 322/883, Training Loss: 0.7642\n",
      "Epoch 10/10, Batch 323/883, Training Loss: 0.4270\n",
      "Epoch 10/10, Batch 324/883, Training Loss: 0.5235\n",
      "Epoch 10/10, Batch 325/883, Training Loss: 0.4569\n",
      "Epoch 10/10, Batch 326/883, Training Loss: 0.3577\n",
      "Epoch 10/10, Batch 327/883, Training Loss: 0.6119\n",
      "Epoch 10/10, Batch 328/883, Training Loss: 0.7564\n",
      "Epoch 10/10, Batch 329/883, Training Loss: 0.4218\n",
      "Epoch 10/10, Batch 330/883, Training Loss: 0.4719\n",
      "Epoch 10/10, Batch 331/883, Training Loss: 0.3341\n",
      "Epoch 10/10, Batch 332/883, Training Loss: 0.5927\n",
      "Epoch 10/10, Batch 333/883, Training Loss: 0.3568\n",
      "Epoch 10/10, Batch 334/883, Training Loss: 0.4356\n",
      "Epoch 10/10, Batch 335/883, Training Loss: 0.5496\n",
      "Epoch 10/10, Batch 336/883, Training Loss: 0.5182\n",
      "Epoch 10/10, Batch 337/883, Training Loss: 0.4128\n",
      "Epoch 10/10, Batch 338/883, Training Loss: 0.3373\n",
      "Epoch 10/10, Batch 339/883, Training Loss: 0.2936\n",
      "Epoch 10/10, Batch 340/883, Training Loss: 0.7014\n",
      "Epoch 10/10, Batch 341/883, Training Loss: 0.7588\n",
      "Epoch 10/10, Batch 342/883, Training Loss: 0.3714\n",
      "Epoch 10/10, Batch 343/883, Training Loss: 0.2348\n",
      "Epoch 10/10, Batch 344/883, Training Loss: 1.0912\n",
      "Epoch 10/10, Batch 345/883, Training Loss: 0.7466\n",
      "Epoch 10/10, Batch 346/883, Training Loss: 0.6271\n",
      "Epoch 10/10, Batch 347/883, Training Loss: 0.5255\n",
      "Epoch 10/10, Batch 348/883, Training Loss: 0.5212\n",
      "Epoch 10/10, Batch 349/883, Training Loss: 0.4938\n",
      "Epoch 10/10, Batch 350/883, Training Loss: 0.3835\n",
      "Epoch 10/10, Batch 351/883, Training Loss: 0.5682\n",
      "Epoch 10/10, Batch 352/883, Training Loss: 0.5244\n",
      "Epoch 10/10, Batch 353/883, Training Loss: 0.5679\n",
      "Epoch 10/10, Batch 354/883, Training Loss: 0.4878\n",
      "Epoch 10/10, Batch 355/883, Training Loss: 0.5060\n",
      "Epoch 10/10, Batch 356/883, Training Loss: 0.3680\n",
      "Epoch 10/10, Batch 357/883, Training Loss: 0.4617\n",
      "Epoch 10/10, Batch 358/883, Training Loss: 0.4509\n",
      "Epoch 10/10, Batch 359/883, Training Loss: 0.3981\n",
      "Epoch 10/10, Batch 360/883, Training Loss: 0.6123\n",
      "Epoch 10/10, Batch 361/883, Training Loss: 0.4923\n",
      "Epoch 10/10, Batch 362/883, Training Loss: 0.4283\n",
      "Epoch 10/10, Batch 363/883, Training Loss: 0.7964\n",
      "Epoch 10/10, Batch 364/883, Training Loss: 0.4689\n",
      "Epoch 10/10, Batch 365/883, Training Loss: 0.3458\n",
      "Epoch 10/10, Batch 366/883, Training Loss: 0.2918\n",
      "Epoch 10/10, Batch 367/883, Training Loss: 0.7239\n",
      "Epoch 10/10, Batch 368/883, Training Loss: 0.5027\n",
      "Epoch 10/10, Batch 369/883, Training Loss: 0.5768\n",
      "Epoch 10/10, Batch 370/883, Training Loss: 0.3378\n",
      "Epoch 10/10, Batch 371/883, Training Loss: 0.5529\n",
      "Epoch 10/10, Batch 372/883, Training Loss: 0.5183\n",
      "Epoch 10/10, Batch 373/883, Training Loss: 0.4002\n",
      "Epoch 10/10, Batch 374/883, Training Loss: 0.5111\n",
      "Epoch 10/10, Batch 375/883, Training Loss: 0.3547\n",
      "Epoch 10/10, Batch 376/883, Training Loss: 0.3695\n",
      "Epoch 10/10, Batch 377/883, Training Loss: 0.5371\n",
      "Epoch 10/10, Batch 378/883, Training Loss: 0.3544\n",
      "Epoch 10/10, Batch 379/883, Training Loss: 0.5824\n",
      "Epoch 10/10, Batch 380/883, Training Loss: 0.4953\n",
      "Epoch 10/10, Batch 381/883, Training Loss: 0.8450\n",
      "Epoch 10/10, Batch 382/883, Training Loss: 0.4451\n",
      "Epoch 10/10, Batch 383/883, Training Loss: 0.5502\n",
      "Epoch 10/10, Batch 384/883, Training Loss: 0.2979\n",
      "Epoch 10/10, Batch 385/883, Training Loss: 0.5590\n",
      "Epoch 10/10, Batch 386/883, Training Loss: 0.6178\n",
      "Epoch 10/10, Batch 387/883, Training Loss: 0.3329\n",
      "Epoch 10/10, Batch 388/883, Training Loss: 0.4169\n",
      "Epoch 10/10, Batch 389/883, Training Loss: 0.5560\n",
      "Epoch 10/10, Batch 390/883, Training Loss: 0.4227\n",
      "Epoch 10/10, Batch 391/883, Training Loss: 0.9799\n",
      "Epoch 10/10, Batch 392/883, Training Loss: 0.5131\n",
      "Epoch 10/10, Batch 393/883, Training Loss: 0.5562\n",
      "Epoch 10/10, Batch 394/883, Training Loss: 0.4119\n",
      "Epoch 10/10, Batch 395/883, Training Loss: 0.4422\n",
      "Epoch 10/10, Batch 396/883, Training Loss: 0.7317\n",
      "Epoch 10/10, Batch 397/883, Training Loss: 0.7412\n",
      "Epoch 10/10, Batch 398/883, Training Loss: 0.3211\n",
      "Epoch 10/10, Batch 399/883, Training Loss: 0.5213\n",
      "Epoch 10/10, Batch 400/883, Training Loss: 0.4113\n",
      "Epoch 10/10, Batch 401/883, Training Loss: 0.4022\n",
      "Epoch 10/10, Batch 402/883, Training Loss: 0.4166\n",
      "Epoch 10/10, Batch 403/883, Training Loss: 0.4197\n",
      "Epoch 10/10, Batch 404/883, Training Loss: 0.3734\n",
      "Epoch 10/10, Batch 405/883, Training Loss: 0.5281\n",
      "Epoch 10/10, Batch 406/883, Training Loss: 0.4150\n",
      "Epoch 10/10, Batch 407/883, Training Loss: 0.3812\n",
      "Epoch 10/10, Batch 408/883, Training Loss: 0.5968\n",
      "Epoch 10/10, Batch 409/883, Training Loss: 0.6141\n",
      "Epoch 10/10, Batch 410/883, Training Loss: 0.4414\n",
      "Epoch 10/10, Batch 411/883, Training Loss: 0.7095\n",
      "Epoch 10/10, Batch 412/883, Training Loss: 0.3742\n",
      "Epoch 10/10, Batch 413/883, Training Loss: 0.2938\n",
      "Epoch 10/10, Batch 414/883, Training Loss: 0.2948\n",
      "Epoch 10/10, Batch 415/883, Training Loss: 0.2801\n",
      "Epoch 10/10, Batch 416/883, Training Loss: 0.3719\n",
      "Epoch 10/10, Batch 417/883, Training Loss: 0.2653\n",
      "Epoch 10/10, Batch 418/883, Training Loss: 0.4580\n",
      "Epoch 10/10, Batch 419/883, Training Loss: 0.4993\n",
      "Epoch 10/10, Batch 420/883, Training Loss: 0.3730\n",
      "Epoch 10/10, Batch 421/883, Training Loss: 0.3704\n",
      "Epoch 10/10, Batch 422/883, Training Loss: 0.6050\n",
      "Epoch 10/10, Batch 423/883, Training Loss: 0.4523\n",
      "Epoch 10/10, Batch 424/883, Training Loss: 0.8086\n",
      "Epoch 10/10, Batch 425/883, Training Loss: 0.3531\n",
      "Epoch 10/10, Batch 426/883, Training Loss: 0.2380\n",
      "Epoch 10/10, Batch 427/883, Training Loss: 0.5708\n",
      "Epoch 10/10, Batch 428/883, Training Loss: 0.3615\n",
      "Epoch 10/10, Batch 429/883, Training Loss: 0.4872\n",
      "Epoch 10/10, Batch 430/883, Training Loss: 0.6181\n",
      "Epoch 10/10, Batch 431/883, Training Loss: 0.2845\n",
      "Epoch 10/10, Batch 432/883, Training Loss: 0.7096\n",
      "Epoch 10/10, Batch 433/883, Training Loss: 0.2796\n",
      "Epoch 10/10, Batch 434/883, Training Loss: 0.6629\n",
      "Epoch 10/10, Batch 435/883, Training Loss: 0.3909\n",
      "Epoch 10/10, Batch 436/883, Training Loss: 0.1991\n",
      "Epoch 10/10, Batch 437/883, Training Loss: 0.4889\n",
      "Epoch 10/10, Batch 438/883, Training Loss: 0.3333\n",
      "Epoch 10/10, Batch 439/883, Training Loss: 0.4024\n",
      "Epoch 10/10, Batch 440/883, Training Loss: 0.3273\n",
      "Epoch 10/10, Batch 441/883, Training Loss: 0.8207\n",
      "Epoch 10/10, Batch 442/883, Training Loss: 0.3577\n",
      "Epoch 10/10, Batch 443/883, Training Loss: 0.7235\n",
      "Epoch 10/10, Batch 444/883, Training Loss: 0.2910\n",
      "Epoch 10/10, Batch 445/883, Training Loss: 0.7621\n",
      "Epoch 10/10, Batch 446/883, Training Loss: 0.3213\n",
      "Epoch 10/10, Batch 447/883, Training Loss: 0.2891\n",
      "Epoch 10/10, Batch 448/883, Training Loss: 0.9318\n",
      "Epoch 10/10, Batch 449/883, Training Loss: 0.7255\n",
      "Epoch 10/10, Batch 450/883, Training Loss: 0.2862\n",
      "Epoch 10/10, Batch 451/883, Training Loss: 0.5819\n",
      "Epoch 10/10, Batch 452/883, Training Loss: 0.6829\n",
      "Epoch 10/10, Batch 453/883, Training Loss: 0.3780\n",
      "Epoch 10/10, Batch 454/883, Training Loss: 0.3937\n",
      "Epoch 10/10, Batch 455/883, Training Loss: 0.4336\n",
      "Epoch 10/10, Batch 456/883, Training Loss: 0.4119\n",
      "Epoch 10/10, Batch 457/883, Training Loss: 0.3185\n",
      "Epoch 10/10, Batch 458/883, Training Loss: 0.5397\n",
      "Epoch 10/10, Batch 459/883, Training Loss: 0.4057\n",
      "Epoch 10/10, Batch 460/883, Training Loss: 0.4692\n",
      "Epoch 10/10, Batch 461/883, Training Loss: 0.2369\n",
      "Epoch 10/10, Batch 462/883, Training Loss: 0.2235\n",
      "Epoch 10/10, Batch 463/883, Training Loss: 0.5258\n",
      "Epoch 10/10, Batch 464/883, Training Loss: 0.5680\n",
      "Epoch 10/10, Batch 465/883, Training Loss: 0.4804\n",
      "Epoch 10/10, Batch 466/883, Training Loss: 0.5043\n",
      "Epoch 10/10, Batch 467/883, Training Loss: 0.3052\n",
      "Epoch 10/10, Batch 468/883, Training Loss: 0.5301\n",
      "Epoch 10/10, Batch 469/883, Training Loss: 0.3875\n",
      "Epoch 10/10, Batch 470/883, Training Loss: 0.9901\n",
      "Epoch 10/10, Batch 471/883, Training Loss: 0.3808\n",
      "Epoch 10/10, Batch 472/883, Training Loss: 0.5822\n",
      "Epoch 10/10, Batch 473/883, Training Loss: 0.2807\n",
      "Epoch 10/10, Batch 474/883, Training Loss: 0.6309\n",
      "Epoch 10/10, Batch 475/883, Training Loss: 0.2907\n",
      "Epoch 10/10, Batch 476/883, Training Loss: 0.6714\n",
      "Epoch 10/10, Batch 477/883, Training Loss: 0.4112\n",
      "Epoch 10/10, Batch 478/883, Training Loss: 0.4666\n",
      "Epoch 10/10, Batch 479/883, Training Loss: 0.4787\n",
      "Epoch 10/10, Batch 480/883, Training Loss: 0.6771\n",
      "Epoch 10/10, Batch 481/883, Training Loss: 0.4399\n",
      "Epoch 10/10, Batch 482/883, Training Loss: 0.2712\n",
      "Epoch 10/10, Batch 483/883, Training Loss: 0.6721\n",
      "Epoch 10/10, Batch 484/883, Training Loss: 0.6370\n",
      "Epoch 10/10, Batch 485/883, Training Loss: 0.3513\n",
      "Epoch 10/10, Batch 486/883, Training Loss: 0.6021\n",
      "Epoch 10/10, Batch 487/883, Training Loss: 0.5857\n",
      "Epoch 10/10, Batch 488/883, Training Loss: 0.4401\n",
      "Epoch 10/10, Batch 489/883, Training Loss: 0.6066\n",
      "Epoch 10/10, Batch 490/883, Training Loss: 0.4589\n",
      "Epoch 10/10, Batch 491/883, Training Loss: 0.4053\n",
      "Epoch 10/10, Batch 492/883, Training Loss: 0.2416\n",
      "Epoch 10/10, Batch 493/883, Training Loss: 0.5801\n",
      "Epoch 10/10, Batch 494/883, Training Loss: 0.5037\n",
      "Epoch 10/10, Batch 495/883, Training Loss: 0.4807\n",
      "Epoch 10/10, Batch 496/883, Training Loss: 0.6142\n",
      "Epoch 10/10, Batch 497/883, Training Loss: 0.5893\n",
      "Epoch 10/10, Batch 498/883, Training Loss: 0.3835\n",
      "Epoch 10/10, Batch 499/883, Training Loss: 0.4571\n",
      "Epoch 10/10, Batch 500/883, Training Loss: 0.3363\n",
      "Epoch 10/10, Batch 501/883, Training Loss: 0.3439\n",
      "Epoch 10/10, Batch 502/883, Training Loss: 0.4835\n",
      "Epoch 10/10, Batch 503/883, Training Loss: 0.6357\n",
      "Epoch 10/10, Batch 504/883, Training Loss: 0.4108\n",
      "Epoch 10/10, Batch 505/883, Training Loss: 0.3842\n",
      "Epoch 10/10, Batch 506/883, Training Loss: 0.8527\n",
      "Epoch 10/10, Batch 507/883, Training Loss: 0.5202\n",
      "Epoch 10/10, Batch 508/883, Training Loss: 0.6649\n",
      "Epoch 10/10, Batch 509/883, Training Loss: 0.4951\n",
      "Epoch 10/10, Batch 510/883, Training Loss: 0.8760\n",
      "Epoch 10/10, Batch 511/883, Training Loss: 0.2990\n",
      "Epoch 10/10, Batch 512/883, Training Loss: 0.7630\n",
      "Epoch 10/10, Batch 513/883, Training Loss: 0.4124\n",
      "Epoch 10/10, Batch 514/883, Training Loss: 0.7253\n",
      "Epoch 10/10, Batch 515/883, Training Loss: 0.4316\n",
      "Epoch 10/10, Batch 516/883, Training Loss: 0.6780\n",
      "Epoch 10/10, Batch 517/883, Training Loss: 0.2565\n",
      "Epoch 10/10, Batch 518/883, Training Loss: 0.7355\n",
      "Epoch 10/10, Batch 519/883, Training Loss: 0.6069\n",
      "Epoch 10/10, Batch 520/883, Training Loss: 0.3156\n",
      "Epoch 10/10, Batch 521/883, Training Loss: 0.1736\n",
      "Epoch 10/10, Batch 522/883, Training Loss: 0.7670\n",
      "Epoch 10/10, Batch 523/883, Training Loss: 0.5852\n",
      "Epoch 10/10, Batch 524/883, Training Loss: 0.7021\n",
      "Epoch 10/10, Batch 525/883, Training Loss: 0.5736\n",
      "Epoch 10/10, Batch 526/883, Training Loss: 0.7423\n",
      "Epoch 10/10, Batch 527/883, Training Loss: 0.5734\n",
      "Epoch 10/10, Batch 528/883, Training Loss: 0.4144\n",
      "Epoch 10/10, Batch 529/883, Training Loss: 0.8211\n",
      "Epoch 10/10, Batch 530/883, Training Loss: 0.4045\n",
      "Epoch 10/10, Batch 531/883, Training Loss: 0.7352\n",
      "Epoch 10/10, Batch 532/883, Training Loss: 0.3613\n",
      "Epoch 10/10, Batch 533/883, Training Loss: 0.8981\n",
      "Epoch 10/10, Batch 534/883, Training Loss: 0.4312\n",
      "Epoch 10/10, Batch 535/883, Training Loss: 0.4791\n",
      "Epoch 10/10, Batch 536/883, Training Loss: 0.5168\n",
      "Epoch 10/10, Batch 537/883, Training Loss: 0.4047\n",
      "Epoch 10/10, Batch 538/883, Training Loss: 0.5642\n",
      "Epoch 10/10, Batch 539/883, Training Loss: 0.5781\n",
      "Epoch 10/10, Batch 540/883, Training Loss: 0.3912\n",
      "Epoch 10/10, Batch 541/883, Training Loss: 0.6025\n",
      "Epoch 10/10, Batch 542/883, Training Loss: 0.6066\n",
      "Epoch 10/10, Batch 543/883, Training Loss: 0.5740\n",
      "Epoch 10/10, Batch 544/883, Training Loss: 0.6344\n",
      "Epoch 10/10, Batch 545/883, Training Loss: 0.8746\n",
      "Epoch 10/10, Batch 546/883, Training Loss: 0.3152\n",
      "Epoch 10/10, Batch 547/883, Training Loss: 0.5973\n",
      "Epoch 10/10, Batch 548/883, Training Loss: 0.3454\n",
      "Epoch 10/10, Batch 549/883, Training Loss: 0.7544\n",
      "Epoch 10/10, Batch 550/883, Training Loss: 0.6893\n",
      "Epoch 10/10, Batch 551/883, Training Loss: 0.5954\n",
      "Epoch 10/10, Batch 552/883, Training Loss: 0.3024\n",
      "Epoch 10/10, Batch 553/883, Training Loss: 0.4925\n",
      "Epoch 10/10, Batch 554/883, Training Loss: 0.4235\n",
      "Epoch 10/10, Batch 555/883, Training Loss: 0.5667\n",
      "Epoch 10/10, Batch 556/883, Training Loss: 0.5859\n",
      "Epoch 10/10, Batch 557/883, Training Loss: 0.5792\n",
      "Epoch 10/10, Batch 558/883, Training Loss: 0.3915\n",
      "Epoch 10/10, Batch 559/883, Training Loss: 0.3386\n",
      "Epoch 10/10, Batch 560/883, Training Loss: 0.5426\n",
      "Epoch 10/10, Batch 561/883, Training Loss: 0.5489\n",
      "Epoch 10/10, Batch 562/883, Training Loss: 0.4490\n",
      "Epoch 10/10, Batch 563/883, Training Loss: 0.3533\n",
      "Epoch 10/10, Batch 564/883, Training Loss: 0.8499\n",
      "Epoch 10/10, Batch 565/883, Training Loss: 0.5404\n",
      "Epoch 10/10, Batch 566/883, Training Loss: 0.5781\n",
      "Epoch 10/10, Batch 567/883, Training Loss: 0.7494\n",
      "Epoch 10/10, Batch 568/883, Training Loss: 0.6012\n",
      "Epoch 10/10, Batch 569/883, Training Loss: 0.4422\n",
      "Epoch 10/10, Batch 570/883, Training Loss: 0.4568\n",
      "Epoch 10/10, Batch 571/883, Training Loss: 0.3605\n",
      "Epoch 10/10, Batch 572/883, Training Loss: 0.7354\n",
      "Epoch 10/10, Batch 573/883, Training Loss: 0.5048\n",
      "Epoch 10/10, Batch 574/883, Training Loss: 0.5947\n",
      "Epoch 10/10, Batch 575/883, Training Loss: 0.2524\n",
      "Epoch 10/10, Batch 576/883, Training Loss: 0.4953\n",
      "Epoch 10/10, Batch 577/883, Training Loss: 0.3280\n",
      "Epoch 10/10, Batch 578/883, Training Loss: 0.4855\n",
      "Epoch 10/10, Batch 579/883, Training Loss: 0.3897\n",
      "Epoch 10/10, Batch 580/883, Training Loss: 0.2833\n",
      "Epoch 10/10, Batch 581/883, Training Loss: 0.4020\n",
      "Epoch 10/10, Batch 582/883, Training Loss: 0.4798\n",
      "Epoch 10/10, Batch 583/883, Training Loss: 0.3928\n",
      "Epoch 10/10, Batch 584/883, Training Loss: 0.5660\n",
      "Epoch 10/10, Batch 585/883, Training Loss: 0.4933\n",
      "Epoch 10/10, Batch 586/883, Training Loss: 0.6317\n",
      "Epoch 10/10, Batch 587/883, Training Loss: 0.9061\n",
      "Epoch 10/10, Batch 588/883, Training Loss: 0.3480\n",
      "Epoch 10/10, Batch 589/883, Training Loss: 0.3286\n",
      "Epoch 10/10, Batch 590/883, Training Loss: 0.3430\n",
      "Epoch 10/10, Batch 591/883, Training Loss: 0.6007\n",
      "Epoch 10/10, Batch 592/883, Training Loss: 0.8196\n",
      "Epoch 10/10, Batch 593/883, Training Loss: 0.5559\n",
      "Epoch 10/10, Batch 594/883, Training Loss: 0.5438\n",
      "Epoch 10/10, Batch 595/883, Training Loss: 0.7066\n",
      "Epoch 10/10, Batch 596/883, Training Loss: 0.5167\n",
      "Epoch 10/10, Batch 597/883, Training Loss: 1.1594\n",
      "Epoch 10/10, Batch 598/883, Training Loss: 0.3612\n",
      "Epoch 10/10, Batch 599/883, Training Loss: 0.2771\n",
      "Epoch 10/10, Batch 600/883, Training Loss: 0.3190\n",
      "Epoch 10/10, Batch 601/883, Training Loss: 0.3719\n",
      "Epoch 10/10, Batch 602/883, Training Loss: 0.4552\n",
      "Epoch 10/10, Batch 603/883, Training Loss: 0.3891\n",
      "Epoch 10/10, Batch 604/883, Training Loss: 0.5039\n",
      "Epoch 10/10, Batch 605/883, Training Loss: 0.7674\n",
      "Epoch 10/10, Batch 606/883, Training Loss: 0.4203\n",
      "Epoch 10/10, Batch 607/883, Training Loss: 0.5237\n",
      "Epoch 10/10, Batch 608/883, Training Loss: 0.4565\n",
      "Epoch 10/10, Batch 609/883, Training Loss: 0.3589\n",
      "Epoch 10/10, Batch 610/883, Training Loss: 0.3880\n",
      "Epoch 10/10, Batch 611/883, Training Loss: 0.3022\n",
      "Epoch 10/10, Batch 612/883, Training Loss: 0.2662\n",
      "Epoch 10/10, Batch 613/883, Training Loss: 0.3450\n",
      "Epoch 10/10, Batch 614/883, Training Loss: 0.3802\n",
      "Epoch 10/10, Batch 615/883, Training Loss: 0.5452\n",
      "Epoch 10/10, Batch 616/883, Training Loss: 0.3660\n",
      "Epoch 10/10, Batch 617/883, Training Loss: 0.6307\n",
      "Epoch 10/10, Batch 618/883, Training Loss: 0.4335\n",
      "Epoch 10/10, Batch 619/883, Training Loss: 0.2009\n",
      "Epoch 10/10, Batch 620/883, Training Loss: 0.3316\n",
      "Epoch 10/10, Batch 621/883, Training Loss: 0.3598\n",
      "Epoch 10/10, Batch 622/883, Training Loss: 0.2549\n",
      "Epoch 10/10, Batch 623/883, Training Loss: 0.3319\n",
      "Epoch 10/10, Batch 624/883, Training Loss: 0.5496\n",
      "Epoch 10/10, Batch 625/883, Training Loss: 0.5121\n",
      "Epoch 10/10, Batch 626/883, Training Loss: 0.4190\n",
      "Epoch 10/10, Batch 627/883, Training Loss: 0.4303\n",
      "Epoch 10/10, Batch 628/883, Training Loss: 0.7148\n",
      "Epoch 10/10, Batch 629/883, Training Loss: 0.5204\n",
      "Epoch 10/10, Batch 630/883, Training Loss: 0.6969\n",
      "Epoch 10/10, Batch 631/883, Training Loss: 0.5110\n",
      "Epoch 10/10, Batch 632/883, Training Loss: 0.5565\n",
      "Epoch 10/10, Batch 633/883, Training Loss: 0.4511\n",
      "Epoch 10/10, Batch 634/883, Training Loss: 1.1117\n",
      "Epoch 10/10, Batch 635/883, Training Loss: 0.3034\n",
      "Epoch 10/10, Batch 636/883, Training Loss: 0.2411\n",
      "Epoch 10/10, Batch 637/883, Training Loss: 0.7703\n",
      "Epoch 10/10, Batch 638/883, Training Loss: 0.4650\n",
      "Epoch 10/10, Batch 639/883, Training Loss: 0.8371\n",
      "Epoch 10/10, Batch 640/883, Training Loss: 0.3660\n",
      "Epoch 10/10, Batch 641/883, Training Loss: 0.3993\n",
      "Epoch 10/10, Batch 642/883, Training Loss: 0.4468\n",
      "Epoch 10/10, Batch 643/883, Training Loss: 0.6615\n",
      "Epoch 10/10, Batch 644/883, Training Loss: 0.4761\n",
      "Epoch 10/10, Batch 645/883, Training Loss: 0.4162\n",
      "Epoch 10/10, Batch 646/883, Training Loss: 0.4618\n",
      "Epoch 10/10, Batch 647/883, Training Loss: 0.9293\n",
      "Epoch 10/10, Batch 648/883, Training Loss: 0.5779\n",
      "Epoch 10/10, Batch 649/883, Training Loss: 0.5136\n",
      "Epoch 10/10, Batch 650/883, Training Loss: 0.4014\n",
      "Epoch 10/10, Batch 651/883, Training Loss: 0.6108\n",
      "Epoch 10/10, Batch 652/883, Training Loss: 1.0194\n",
      "Epoch 10/10, Batch 653/883, Training Loss: 0.8827\n",
      "Epoch 10/10, Batch 654/883, Training Loss: 0.3810\n",
      "Epoch 10/10, Batch 655/883, Training Loss: 0.3388\n",
      "Epoch 10/10, Batch 656/883, Training Loss: 0.2568\n",
      "Epoch 10/10, Batch 657/883, Training Loss: 0.5313\n",
      "Epoch 10/10, Batch 658/883, Training Loss: 0.2783\n",
      "Epoch 10/10, Batch 659/883, Training Loss: 0.8169\n",
      "Epoch 10/10, Batch 660/883, Training Loss: 0.4797\n",
      "Epoch 10/10, Batch 661/883, Training Loss: 0.5303\n",
      "Epoch 10/10, Batch 662/883, Training Loss: 0.6349\n",
      "Epoch 10/10, Batch 663/883, Training Loss: 0.5090\n",
      "Epoch 10/10, Batch 664/883, Training Loss: 0.3321\n",
      "Epoch 10/10, Batch 665/883, Training Loss: 0.6341\n",
      "Epoch 10/10, Batch 666/883, Training Loss: 0.4566\n",
      "Epoch 10/10, Batch 667/883, Training Loss: 0.3938\n",
      "Epoch 10/10, Batch 668/883, Training Loss: 0.3789\n",
      "Epoch 10/10, Batch 669/883, Training Loss: 0.3708\n",
      "Epoch 10/10, Batch 670/883, Training Loss: 0.4584\n",
      "Epoch 10/10, Batch 671/883, Training Loss: 0.4669\n",
      "Epoch 10/10, Batch 672/883, Training Loss: 0.4894\n",
      "Epoch 10/10, Batch 673/883, Training Loss: 0.4862\n",
      "Epoch 10/10, Batch 674/883, Training Loss: 0.3328\n",
      "Epoch 10/10, Batch 675/883, Training Loss: 0.6709\n",
      "Epoch 10/10, Batch 676/883, Training Loss: 0.4346\n",
      "Epoch 10/10, Batch 677/883, Training Loss: 0.6396\n",
      "Epoch 10/10, Batch 678/883, Training Loss: 0.2808\n",
      "Epoch 10/10, Batch 679/883, Training Loss: 0.4007\n",
      "Epoch 10/10, Batch 680/883, Training Loss: 0.5407\n",
      "Epoch 10/10, Batch 681/883, Training Loss: 0.3416\n",
      "Epoch 10/10, Batch 682/883, Training Loss: 0.4641\n",
      "Epoch 10/10, Batch 683/883, Training Loss: 0.4389\n",
      "Epoch 10/10, Batch 684/883, Training Loss: 0.4812\n",
      "Epoch 10/10, Batch 685/883, Training Loss: 0.8524\n",
      "Epoch 10/10, Batch 686/883, Training Loss: 0.3235\n",
      "Epoch 10/10, Batch 687/883, Training Loss: 0.5010\n",
      "Epoch 10/10, Batch 688/883, Training Loss: 0.6703\n",
      "Epoch 10/10, Batch 689/883, Training Loss: 0.3466\n",
      "Epoch 10/10, Batch 690/883, Training Loss: 0.5725\n",
      "Epoch 10/10, Batch 691/883, Training Loss: 0.6098\n",
      "Epoch 10/10, Batch 692/883, Training Loss: 0.4848\n",
      "Epoch 10/10, Batch 693/883, Training Loss: 0.5029\n",
      "Epoch 10/10, Batch 694/883, Training Loss: 0.3330\n",
      "Epoch 10/10, Batch 695/883, Training Loss: 0.3831\n",
      "Epoch 10/10, Batch 696/883, Training Loss: 0.3862\n",
      "Epoch 10/10, Batch 697/883, Training Loss: 0.7092\n",
      "Epoch 10/10, Batch 698/883, Training Loss: 0.3478\n",
      "Epoch 10/10, Batch 699/883, Training Loss: 0.4323\n",
      "Epoch 10/10, Batch 700/883, Training Loss: 0.5913\n",
      "Epoch 10/10, Batch 701/883, Training Loss: 0.3980\n",
      "Epoch 10/10, Batch 702/883, Training Loss: 0.5350\n",
      "Epoch 10/10, Batch 703/883, Training Loss: 0.7361\n",
      "Epoch 10/10, Batch 704/883, Training Loss: 0.5987\n",
      "Epoch 10/10, Batch 705/883, Training Loss: 0.6713\n",
      "Epoch 10/10, Batch 706/883, Training Loss: 0.3584\n",
      "Epoch 10/10, Batch 707/883, Training Loss: 0.4896\n",
      "Epoch 10/10, Batch 708/883, Training Loss: 0.6449\n",
      "Epoch 10/10, Batch 709/883, Training Loss: 0.3908\n",
      "Epoch 10/10, Batch 710/883, Training Loss: 0.6394\n",
      "Epoch 10/10, Batch 711/883, Training Loss: 0.6543\n",
      "Epoch 10/10, Batch 712/883, Training Loss: 0.4836\n",
      "Epoch 10/10, Batch 713/883, Training Loss: 0.5873\n",
      "Epoch 10/10, Batch 714/883, Training Loss: 0.5248\n",
      "Epoch 10/10, Batch 715/883, Training Loss: 0.3365\n",
      "Epoch 10/10, Batch 716/883, Training Loss: 0.2120\n",
      "Epoch 10/10, Batch 717/883, Training Loss: 0.7592\n",
      "Epoch 10/10, Batch 718/883, Training Loss: 0.5126\n",
      "Epoch 10/10, Batch 719/883, Training Loss: 0.3171\n",
      "Epoch 10/10, Batch 720/883, Training Loss: 0.5390\n",
      "Epoch 10/10, Batch 721/883, Training Loss: 0.8718\n",
      "Epoch 10/10, Batch 722/883, Training Loss: 0.7763\n",
      "Epoch 10/10, Batch 723/883, Training Loss: 0.4504\n",
      "Epoch 10/10, Batch 724/883, Training Loss: 0.3234\n",
      "Epoch 10/10, Batch 725/883, Training Loss: 0.5491\n",
      "Epoch 10/10, Batch 726/883, Training Loss: 0.5152\n",
      "Epoch 10/10, Batch 727/883, Training Loss: 0.3434\n",
      "Epoch 10/10, Batch 728/883, Training Loss: 0.3678\n",
      "Epoch 10/10, Batch 729/883, Training Loss: 0.9368\n",
      "Epoch 10/10, Batch 730/883, Training Loss: 0.6301\n",
      "Epoch 10/10, Batch 731/883, Training Loss: 0.7309\n",
      "Epoch 10/10, Batch 732/883, Training Loss: 0.3540\n",
      "Epoch 10/10, Batch 733/883, Training Loss: 0.2677\n",
      "Epoch 10/10, Batch 734/883, Training Loss: 0.5081\n",
      "Epoch 10/10, Batch 735/883, Training Loss: 0.3645\n",
      "Epoch 10/10, Batch 736/883, Training Loss: 0.7792\n",
      "Epoch 10/10, Batch 737/883, Training Loss: 0.3520\n",
      "Epoch 10/10, Batch 738/883, Training Loss: 0.6245\n",
      "Epoch 10/10, Batch 739/883, Training Loss: 0.5493\n",
      "Epoch 10/10, Batch 740/883, Training Loss: 0.3193\n",
      "Epoch 10/10, Batch 741/883, Training Loss: 0.3300\n",
      "Epoch 10/10, Batch 742/883, Training Loss: 0.5834\n",
      "Epoch 10/10, Batch 743/883, Training Loss: 0.3746\n",
      "Epoch 10/10, Batch 744/883, Training Loss: 0.3135\n",
      "Epoch 10/10, Batch 745/883, Training Loss: 0.5059\n",
      "Epoch 10/10, Batch 746/883, Training Loss: 0.6514\n",
      "Epoch 10/10, Batch 747/883, Training Loss: 0.4980\n",
      "Epoch 10/10, Batch 748/883, Training Loss: 0.5141\n",
      "Epoch 10/10, Batch 749/883, Training Loss: 0.5715\n",
      "Epoch 10/10, Batch 750/883, Training Loss: 0.5032\n",
      "Epoch 10/10, Batch 751/883, Training Loss: 0.6575\n",
      "Epoch 10/10, Batch 752/883, Training Loss: 0.3554\n",
      "Epoch 10/10, Batch 753/883, Training Loss: 0.2549\n",
      "Epoch 10/10, Batch 754/883, Training Loss: 0.4181\n",
      "Epoch 10/10, Batch 755/883, Training Loss: 0.3390\n",
      "Epoch 10/10, Batch 756/883, Training Loss: 0.6460\n",
      "Epoch 10/10, Batch 757/883, Training Loss: 0.2476\n",
      "Epoch 10/10, Batch 758/883, Training Loss: 0.2680\n",
      "Epoch 10/10, Batch 759/883, Training Loss: 0.3977\n",
      "Epoch 10/10, Batch 760/883, Training Loss: 0.2860\n",
      "Epoch 10/10, Batch 761/883, Training Loss: 0.4117\n",
      "Epoch 10/10, Batch 762/883, Training Loss: 0.2342\n",
      "Epoch 10/10, Batch 763/883, Training Loss: 0.4596\n",
      "Epoch 10/10, Batch 764/883, Training Loss: 0.2925\n",
      "Epoch 10/10, Batch 765/883, Training Loss: 0.4409\n",
      "Epoch 10/10, Batch 766/883, Training Loss: 0.7927\n",
      "Epoch 10/10, Batch 767/883, Training Loss: 0.4560\n",
      "Epoch 10/10, Batch 768/883, Training Loss: 0.4346\n",
      "Epoch 10/10, Batch 769/883, Training Loss: 0.5741\n",
      "Epoch 10/10, Batch 770/883, Training Loss: 0.4391\n",
      "Epoch 10/10, Batch 771/883, Training Loss: 0.7464\n",
      "Epoch 10/10, Batch 772/883, Training Loss: 0.6903\n",
      "Epoch 10/10, Batch 773/883, Training Loss: 0.2360\n",
      "Epoch 10/10, Batch 774/883, Training Loss: 0.2591\n",
      "Epoch 10/10, Batch 775/883, Training Loss: 0.3754\n",
      "Epoch 10/10, Batch 776/883, Training Loss: 0.5502\n",
      "Epoch 10/10, Batch 777/883, Training Loss: 0.2542\n",
      "Epoch 10/10, Batch 778/883, Training Loss: 0.8730\n",
      "Epoch 10/10, Batch 779/883, Training Loss: 0.3372\n",
      "Epoch 10/10, Batch 780/883, Training Loss: 0.4070\n",
      "Epoch 10/10, Batch 781/883, Training Loss: 0.4477\n",
      "Epoch 10/10, Batch 782/883, Training Loss: 0.4377\n",
      "Epoch 10/10, Batch 783/883, Training Loss: 0.5808\n",
      "Epoch 10/10, Batch 784/883, Training Loss: 0.6368\n",
      "Epoch 10/10, Batch 785/883, Training Loss: 0.7752\n",
      "Epoch 10/10, Batch 786/883, Training Loss: 0.3253\n",
      "Epoch 10/10, Batch 787/883, Training Loss: 0.3211\n",
      "Epoch 10/10, Batch 788/883, Training Loss: 0.8089\n",
      "Epoch 10/10, Batch 789/883, Training Loss: 0.4882\n",
      "Epoch 10/10, Batch 790/883, Training Loss: 0.6397\n",
      "Epoch 10/10, Batch 791/883, Training Loss: 0.5169\n",
      "Epoch 10/10, Batch 792/883, Training Loss: 0.4671\n",
      "Epoch 10/10, Batch 793/883, Training Loss: 0.4538\n",
      "Epoch 10/10, Batch 794/883, Training Loss: 0.3275\n",
      "Epoch 10/10, Batch 795/883, Training Loss: 0.5141\n",
      "Epoch 10/10, Batch 796/883, Training Loss: 0.5883\n",
      "Epoch 10/10, Batch 797/883, Training Loss: 0.5822\n",
      "Epoch 10/10, Batch 798/883, Training Loss: 0.2989\n",
      "Epoch 10/10, Batch 799/883, Training Loss: 0.3736\n",
      "Epoch 10/10, Batch 800/883, Training Loss: 0.7041\n",
      "Epoch 10/10, Batch 801/883, Training Loss: 0.4167\n",
      "Epoch 10/10, Batch 802/883, Training Loss: 0.3195\n",
      "Epoch 10/10, Batch 803/883, Training Loss: 0.4107\n",
      "Epoch 10/10, Batch 804/883, Training Loss: 0.4634\n",
      "Epoch 10/10, Batch 805/883, Training Loss: 0.3930\n",
      "Epoch 10/10, Batch 806/883, Training Loss: 0.2148\n",
      "Epoch 10/10, Batch 807/883, Training Loss: 0.2906\n",
      "Epoch 10/10, Batch 808/883, Training Loss: 0.5339\n",
      "Epoch 10/10, Batch 809/883, Training Loss: 0.4103\n",
      "Epoch 10/10, Batch 810/883, Training Loss: 0.7542\n",
      "Epoch 10/10, Batch 811/883, Training Loss: 0.5556\n",
      "Epoch 10/10, Batch 812/883, Training Loss: 0.2301\n",
      "Epoch 10/10, Batch 813/883, Training Loss: 0.5283\n",
      "Epoch 10/10, Batch 814/883, Training Loss: 0.9541\n",
      "Epoch 10/10, Batch 815/883, Training Loss: 0.4089\n",
      "Epoch 10/10, Batch 816/883, Training Loss: 0.4792\n",
      "Epoch 10/10, Batch 817/883, Training Loss: 0.4577\n",
      "Epoch 10/10, Batch 818/883, Training Loss: 0.5167\n",
      "Epoch 10/10, Batch 819/883, Training Loss: 0.5687\n",
      "Epoch 10/10, Batch 820/883, Training Loss: 0.6872\n",
      "Epoch 10/10, Batch 821/883, Training Loss: 0.1974\n",
      "Epoch 10/10, Batch 822/883, Training Loss: 0.3530\n",
      "Epoch 10/10, Batch 823/883, Training Loss: 0.5827\n",
      "Epoch 10/10, Batch 824/883, Training Loss: 0.5303\n",
      "Epoch 10/10, Batch 825/883, Training Loss: 0.6738\n",
      "Epoch 10/10, Batch 826/883, Training Loss: 0.2938\n",
      "Epoch 10/10, Batch 827/883, Training Loss: 0.4983\n",
      "Epoch 10/10, Batch 828/883, Training Loss: 0.3110\n",
      "Epoch 10/10, Batch 829/883, Training Loss: 0.3360\n",
      "Epoch 10/10, Batch 830/883, Training Loss: 0.4140\n",
      "Epoch 10/10, Batch 831/883, Training Loss: 0.4340\n",
      "Epoch 10/10, Batch 832/883, Training Loss: 0.6294\n",
      "Epoch 10/10, Batch 833/883, Training Loss: 0.2734\n",
      "Epoch 10/10, Batch 834/883, Training Loss: 0.6268\n",
      "Epoch 10/10, Batch 835/883, Training Loss: 0.7585\n",
      "Epoch 10/10, Batch 836/883, Training Loss: 0.4957\n",
      "Epoch 10/10, Batch 837/883, Training Loss: 0.4423\n",
      "Epoch 10/10, Batch 838/883, Training Loss: 0.1773\n",
      "Epoch 10/10, Batch 839/883, Training Loss: 0.4189\n",
      "Epoch 10/10, Batch 840/883, Training Loss: 0.5931\n",
      "Epoch 10/10, Batch 841/883, Training Loss: 0.6618\n",
      "Epoch 10/10, Batch 842/883, Training Loss: 0.6841\n",
      "Epoch 10/10, Batch 843/883, Training Loss: 0.4270\n",
      "Epoch 10/10, Batch 844/883, Training Loss: 0.6347\n",
      "Epoch 10/10, Batch 845/883, Training Loss: 0.2897\n",
      "Epoch 10/10, Batch 846/883, Training Loss: 0.5276\n",
      "Epoch 10/10, Batch 847/883, Training Loss: 0.6415\n",
      "Epoch 10/10, Batch 848/883, Training Loss: 0.4899\n",
      "Epoch 10/10, Batch 849/883, Training Loss: 0.9345\n",
      "Epoch 10/10, Batch 850/883, Training Loss: 0.5370\n",
      "Epoch 10/10, Batch 851/883, Training Loss: 0.4160\n",
      "Epoch 10/10, Batch 852/883, Training Loss: 0.4917\n",
      "Epoch 10/10, Batch 853/883, Training Loss: 0.6444\n",
      "Epoch 10/10, Batch 854/883, Training Loss: 0.5270\n",
      "Epoch 10/10, Batch 855/883, Training Loss: 0.5684\n",
      "Epoch 10/10, Batch 856/883, Training Loss: 0.8132\n",
      "Epoch 10/10, Batch 857/883, Training Loss: 0.7808\n",
      "Epoch 10/10, Batch 858/883, Training Loss: 0.7803\n",
      "Epoch 10/10, Batch 859/883, Training Loss: 0.5879\n",
      "Epoch 10/10, Batch 860/883, Training Loss: 0.3655\n",
      "Epoch 10/10, Batch 861/883, Training Loss: 0.3949\n",
      "Epoch 10/10, Batch 862/883, Training Loss: 0.4420\n",
      "Epoch 10/10, Batch 863/883, Training Loss: 0.2257\n",
      "Epoch 10/10, Batch 864/883, Training Loss: 0.5689\n",
      "Epoch 10/10, Batch 865/883, Training Loss: 0.5038\n",
      "Epoch 10/10, Batch 866/883, Training Loss: 0.8734\n",
      "Epoch 10/10, Batch 867/883, Training Loss: 0.6745\n",
      "Epoch 10/10, Batch 868/883, Training Loss: 0.3632\n",
      "Epoch 10/10, Batch 869/883, Training Loss: 0.2574\n",
      "Epoch 10/10, Batch 870/883, Training Loss: 1.1791\n",
      "Epoch 10/10, Batch 871/883, Training Loss: 0.3572\n",
      "Epoch 10/10, Batch 872/883, Training Loss: 0.6575\n",
      "Epoch 10/10, Batch 873/883, Training Loss: 0.6054\n",
      "Epoch 10/10, Batch 874/883, Training Loss: 0.4353\n",
      "Epoch 10/10, Batch 875/883, Training Loss: 0.5079\n",
      "Epoch 10/10, Batch 876/883, Training Loss: 0.2797\n",
      "Epoch 10/10, Batch 877/883, Training Loss: 0.4653\n",
      "Epoch 10/10, Batch 878/883, Training Loss: 0.3791\n",
      "Epoch 10/10, Batch 879/883, Training Loss: 0.4489\n",
      "Epoch 10/10, Batch 880/883, Training Loss: 0.6557\n",
      "Epoch 10/10, Batch 881/883, Training Loss: 0.4151\n",
      "Epoch 10/10, Batch 882/883, Training Loss: 0.2304\n",
      "Epoch 10/10, Batch 883/883, Training Loss: 0.4184\n",
      "Epoch 10/10, Training Loss: 0.5045, Validation Loss: 0.4842, Validation Accuracy: 0.8045\n",
      "Test Loss: 0.4861, Test Accuracy: 0.7982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-27 20:44:53,349] Trial 3 finished with value: 0.484209561086207 and parameters: {'batch_size': 16, 'learning_rate': 0.007188326518439874, 'weight_decay': 5.235169847231183e-06}. Best is trial 0 with value: 0.2196991708036512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/111, Training Loss: 1.1284\n",
      "Epoch 1/10, Batch 2/111, Training Loss: 2.4873\n",
      "Epoch 1/10, Batch 3/111, Training Loss: 2.0630\n",
      "Epoch 1/10, Batch 4/111, Training Loss: 1.6952\n",
      "Epoch 1/10, Batch 5/111, Training Loss: 1.6865\n",
      "Epoch 1/10, Batch 6/111, Training Loss: 1.0415\n",
      "Epoch 1/10, Batch 7/111, Training Loss: 1.2857\n",
      "Epoch 1/10, Batch 8/111, Training Loss: 1.6115\n",
      "Epoch 1/10, Batch 9/111, Training Loss: 1.1464\n",
      "Epoch 1/10, Batch 10/111, Training Loss: 1.0288\n",
      "Epoch 1/10, Batch 11/111, Training Loss: 1.5675\n",
      "Epoch 1/10, Batch 12/111, Training Loss: 1.1233\n",
      "Epoch 1/10, Batch 13/111, Training Loss: 1.0533\n",
      "Epoch 1/10, Batch 14/111, Training Loss: 1.0696\n",
      "Epoch 1/10, Batch 15/111, Training Loss: 1.0273\n",
      "Epoch 1/10, Batch 16/111, Training Loss: 1.5196\n",
      "Epoch 1/10, Batch 17/111, Training Loss: 1.2469\n",
      "Epoch 1/10, Batch 18/111, Training Loss: 1.3240\n",
      "Epoch 1/10, Batch 19/111, Training Loss: 1.2535\n",
      "Epoch 1/10, Batch 20/111, Training Loss: 1.0707\n",
      "Epoch 1/10, Batch 21/111, Training Loss: 1.1785\n",
      "Epoch 1/10, Batch 22/111, Training Loss: 1.2585\n",
      "Epoch 1/10, Batch 23/111, Training Loss: 1.1477\n",
      "Epoch 1/10, Batch 24/111, Training Loss: 1.0861\n",
      "Epoch 1/10, Batch 25/111, Training Loss: 1.0766\n",
      "Epoch 1/10, Batch 26/111, Training Loss: 1.0566\n",
      "Epoch 1/10, Batch 27/111, Training Loss: 1.0186\n",
      "Epoch 1/10, Batch 28/111, Training Loss: 1.0037\n",
      "Epoch 1/10, Batch 29/111, Training Loss: 1.1042\n",
      "Epoch 1/10, Batch 30/111, Training Loss: 1.0341\n",
      "Epoch 1/10, Batch 31/111, Training Loss: 1.0197\n",
      "Epoch 1/10, Batch 32/111, Training Loss: 0.9518\n",
      "Epoch 1/10, Batch 33/111, Training Loss: 1.0079\n",
      "Epoch 1/10, Batch 34/111, Training Loss: 0.9915\n",
      "Epoch 1/10, Batch 35/111, Training Loss: 0.9733\n",
      "Epoch 1/10, Batch 36/111, Training Loss: 0.9478\n",
      "Epoch 1/10, Batch 37/111, Training Loss: 0.8914\n",
      "Epoch 1/10, Batch 38/111, Training Loss: 0.9767\n",
      "Epoch 1/10, Batch 39/111, Training Loss: 0.9784\n",
      "Epoch 1/10, Batch 40/111, Training Loss: 0.9224\n",
      "Epoch 1/10, Batch 41/111, Training Loss: 0.9525\n",
      "Epoch 1/10, Batch 42/111, Training Loss: 0.9816\n",
      "Epoch 1/10, Batch 43/111, Training Loss: 1.0113\n",
      "Epoch 1/10, Batch 44/111, Training Loss: 0.9795\n",
      "Epoch 1/10, Batch 45/111, Training Loss: 0.9913\n",
      "Epoch 1/10, Batch 46/111, Training Loss: 0.9843\n",
      "Epoch 1/10, Batch 47/111, Training Loss: 0.9357\n",
      "Epoch 1/10, Batch 48/111, Training Loss: 0.9135\n",
      "Epoch 1/10, Batch 49/111, Training Loss: 1.0514\n",
      "Epoch 1/10, Batch 50/111, Training Loss: 1.0352\n",
      "Epoch 1/10, Batch 51/111, Training Loss: 1.1909\n",
      "Epoch 1/10, Batch 52/111, Training Loss: 1.0669\n",
      "Epoch 1/10, Batch 53/111, Training Loss: 0.8837\n",
      "Epoch 1/10, Batch 54/111, Training Loss: 0.9032\n",
      "Epoch 1/10, Batch 55/111, Training Loss: 0.9136\n",
      "Epoch 1/10, Batch 56/111, Training Loss: 0.9593\n",
      "Epoch 1/10, Batch 57/111, Training Loss: 1.1003\n",
      "Epoch 1/10, Batch 58/111, Training Loss: 1.0731\n",
      "Epoch 1/10, Batch 59/111, Training Loss: 0.9179\n",
      "Epoch 1/10, Batch 60/111, Training Loss: 0.9049\n",
      "Epoch 1/10, Batch 61/111, Training Loss: 0.9510\n",
      "Epoch 1/10, Batch 62/111, Training Loss: 0.8988\n",
      "Epoch 1/10, Batch 63/111, Training Loss: 0.8650\n",
      "Epoch 1/10, Batch 64/111, Training Loss: 1.1661\n",
      "Epoch 1/10, Batch 65/111, Training Loss: 0.9735\n",
      "Epoch 1/10, Batch 66/111, Training Loss: 0.8943\n",
      "Epoch 1/10, Batch 67/111, Training Loss: 1.2585\n",
      "Epoch 1/10, Batch 68/111, Training Loss: 0.9010\n",
      "Epoch 1/10, Batch 69/111, Training Loss: 1.0962\n",
      "Epoch 1/10, Batch 70/111, Training Loss: 0.8950\n",
      "Epoch 1/10, Batch 71/111, Training Loss: 0.8051\n",
      "Epoch 1/10, Batch 72/111, Training Loss: 1.0062\n",
      "Epoch 1/10, Batch 73/111, Training Loss: 0.9519\n",
      "Epoch 1/10, Batch 74/111, Training Loss: 1.0965\n",
      "Epoch 1/10, Batch 75/111, Training Loss: 1.0388\n",
      "Epoch 1/10, Batch 76/111, Training Loss: 1.0660\n",
      "Epoch 1/10, Batch 77/111, Training Loss: 0.9638\n",
      "Epoch 1/10, Batch 78/111, Training Loss: 0.9808\n",
      "Epoch 1/10, Batch 79/111, Training Loss: 0.9367\n",
      "Epoch 1/10, Batch 80/111, Training Loss: 0.9252\n",
      "Epoch 1/10, Batch 81/111, Training Loss: 0.8715\n",
      "Epoch 1/10, Batch 82/111, Training Loss: 0.8571\n",
      "Epoch 1/10, Batch 83/111, Training Loss: 0.8621\n",
      "Epoch 1/10, Batch 84/111, Training Loss: 1.2292\n",
      "Epoch 1/10, Batch 85/111, Training Loss: 0.9787\n",
      "Epoch 1/10, Batch 86/111, Training Loss: 1.0021\n",
      "Epoch 1/10, Batch 87/111, Training Loss: 1.1852\n",
      "Epoch 1/10, Batch 88/111, Training Loss: 0.9357\n",
      "Epoch 1/10, Batch 89/111, Training Loss: 1.0238\n",
      "Epoch 1/10, Batch 90/111, Training Loss: 0.9670\n",
      "Epoch 1/10, Batch 91/111, Training Loss: 0.9199\n",
      "Epoch 1/10, Batch 92/111, Training Loss: 0.9380\n",
      "Epoch 1/10, Batch 93/111, Training Loss: 0.9203\n",
      "Epoch 1/10, Batch 94/111, Training Loss: 1.1162\n",
      "Epoch 1/10, Batch 95/111, Training Loss: 1.0481\n",
      "Epoch 1/10, Batch 96/111, Training Loss: 0.8990\n",
      "Epoch 1/10, Batch 97/111, Training Loss: 1.0190\n",
      "Epoch 1/10, Batch 98/111, Training Loss: 0.8386\n",
      "Epoch 1/10, Batch 99/111, Training Loss: 0.9691\n",
      "Epoch 1/10, Batch 100/111, Training Loss: 0.9094\n",
      "Epoch 1/10, Batch 101/111, Training Loss: 0.9172\n",
      "Epoch 1/10, Batch 102/111, Training Loss: 0.8130\n",
      "Epoch 1/10, Batch 103/111, Training Loss: 0.8289\n",
      "Epoch 1/10, Batch 104/111, Training Loss: 1.0134\n",
      "Epoch 1/10, Batch 105/111, Training Loss: 0.9489\n",
      "Epoch 1/10, Batch 106/111, Training Loss: 0.8031\n",
      "Epoch 1/10, Batch 107/111, Training Loss: 1.2458\n",
      "Epoch 1/10, Batch 108/111, Training Loss: 0.8211\n",
      "Epoch 1/10, Batch 109/111, Training Loss: 0.9628\n",
      "Epoch 1/10, Batch 110/111, Training Loss: 0.8940\n",
      "Epoch 1/10, Batch 111/111, Training Loss: 0.9654\n",
      "Epoch 1/10, Training Loss: 1.0557, Validation Loss: 0.9218, Validation Accuracy: 0.5373\n",
      "Epoch 2/10, Batch 1/111, Training Loss: 0.8148\n",
      "Epoch 2/10, Batch 2/111, Training Loss: 0.9183\n",
      "Epoch 2/10, Batch 3/111, Training Loss: 0.8660\n",
      "Epoch 2/10, Batch 4/111, Training Loss: 0.8916\n",
      "Epoch 2/10, Batch 5/111, Training Loss: 0.8746\n",
      "Epoch 2/10, Batch 6/111, Training Loss: 1.0993\n",
      "Epoch 2/10, Batch 7/111, Training Loss: 0.9254\n",
      "Epoch 2/10, Batch 8/111, Training Loss: 0.9596\n",
      "Epoch 2/10, Batch 9/111, Training Loss: 0.8155\n",
      "Epoch 2/10, Batch 10/111, Training Loss: 1.1128\n",
      "Epoch 2/10, Batch 11/111, Training Loss: 0.8278\n",
      "Epoch 2/10, Batch 12/111, Training Loss: 0.8286\n",
      "Epoch 2/10, Batch 13/111, Training Loss: 0.8208\n",
      "Epoch 2/10, Batch 14/111, Training Loss: 0.9167\n",
      "Epoch 2/10, Batch 15/111, Training Loss: 0.8145\n",
      "Epoch 2/10, Batch 16/111, Training Loss: 0.8936\n",
      "Epoch 2/10, Batch 17/111, Training Loss: 0.9176\n",
      "Epoch 2/10, Batch 18/111, Training Loss: 0.8404\n",
      "Epoch 2/10, Batch 19/111, Training Loss: 0.9960\n",
      "Epoch 2/10, Batch 20/111, Training Loss: 0.8111\n",
      "Epoch 2/10, Batch 21/111, Training Loss: 0.7853\n",
      "Epoch 2/10, Batch 22/111, Training Loss: 0.8512\n",
      "Epoch 2/10, Batch 23/111, Training Loss: 0.8852\n",
      "Epoch 2/10, Batch 24/111, Training Loss: 0.9124\n",
      "Epoch 2/10, Batch 25/111, Training Loss: 0.8393\n",
      "Epoch 2/10, Batch 26/111, Training Loss: 0.9404\n",
      "Epoch 2/10, Batch 27/111, Training Loss: 0.8738\n",
      "Epoch 2/10, Batch 28/111, Training Loss: 0.9500\n",
      "Epoch 2/10, Batch 29/111, Training Loss: 0.8301\n",
      "Epoch 2/10, Batch 30/111, Training Loss: 0.8152\n",
      "Epoch 2/10, Batch 31/111, Training Loss: 0.8519\n",
      "Epoch 2/10, Batch 32/111, Training Loss: 0.9320\n",
      "Epoch 2/10, Batch 33/111, Training Loss: 0.8487\n",
      "Epoch 2/10, Batch 34/111, Training Loss: 0.8887\n",
      "Epoch 2/10, Batch 35/111, Training Loss: 0.7919\n",
      "Epoch 2/10, Batch 36/111, Training Loss: 0.8782\n",
      "Epoch 2/10, Batch 37/111, Training Loss: 0.7576\n",
      "Epoch 2/10, Batch 38/111, Training Loss: 0.7774\n",
      "Epoch 2/10, Batch 39/111, Training Loss: 0.8375\n",
      "Epoch 2/10, Batch 40/111, Training Loss: 0.7929\n",
      "Epoch 2/10, Batch 41/111, Training Loss: 0.8304\n",
      "Epoch 2/10, Batch 42/111, Training Loss: 0.8787\n",
      "Epoch 2/10, Batch 43/111, Training Loss: 0.8121\n",
      "Epoch 2/10, Batch 44/111, Training Loss: 0.8160\n",
      "Epoch 2/10, Batch 45/111, Training Loss: 0.7700\n",
      "Epoch 2/10, Batch 46/111, Training Loss: 0.7638\n",
      "Epoch 2/10, Batch 47/111, Training Loss: 0.9294\n",
      "Epoch 2/10, Batch 48/111, Training Loss: 0.9291\n",
      "Epoch 2/10, Batch 49/111, Training Loss: 0.8211\n",
      "Epoch 2/10, Batch 50/111, Training Loss: 0.7722\n",
      "Epoch 2/10, Batch 51/111, Training Loss: 0.8609\n",
      "Epoch 2/10, Batch 52/111, Training Loss: 0.7763\n",
      "Epoch 2/10, Batch 53/111, Training Loss: 0.8216\n",
      "Epoch 2/10, Batch 54/111, Training Loss: 0.8468\n",
      "Epoch 2/10, Batch 55/111, Training Loss: 0.9166\n",
      "Epoch 2/10, Batch 56/111, Training Loss: 0.7465\n",
      "Epoch 2/10, Batch 57/111, Training Loss: 0.7986\n",
      "Epoch 2/10, Batch 58/111, Training Loss: 0.8156\n",
      "Epoch 2/10, Batch 59/111, Training Loss: 0.7974\n",
      "Epoch 2/10, Batch 60/111, Training Loss: 0.8731\n",
      "Epoch 2/10, Batch 61/111, Training Loss: 0.8956\n",
      "Epoch 2/10, Batch 62/111, Training Loss: 0.8076\n",
      "Epoch 2/10, Batch 63/111, Training Loss: 0.8612\n",
      "Epoch 2/10, Batch 64/111, Training Loss: 0.8138\n",
      "Epoch 2/10, Batch 65/111, Training Loss: 0.9044\n",
      "Epoch 2/10, Batch 66/111, Training Loss: 0.8360\n",
      "Epoch 2/10, Batch 67/111, Training Loss: 0.8461\n",
      "Epoch 2/10, Batch 68/111, Training Loss: 0.8776\n",
      "Epoch 2/10, Batch 69/111, Training Loss: 0.8696\n",
      "Epoch 2/10, Batch 70/111, Training Loss: 0.8693\n",
      "Epoch 2/10, Batch 71/111, Training Loss: 0.9308\n",
      "Epoch 2/10, Batch 72/111, Training Loss: 0.7400\n",
      "Epoch 2/10, Batch 73/111, Training Loss: 0.8862\n",
      "Epoch 2/10, Batch 74/111, Training Loss: 0.9431\n",
      "Epoch 2/10, Batch 75/111, Training Loss: 0.8280\n",
      "Epoch 2/10, Batch 76/111, Training Loss: 0.8729\n",
      "Epoch 2/10, Batch 77/111, Training Loss: 0.7905\n",
      "Epoch 2/10, Batch 78/111, Training Loss: 0.8077\n",
      "Epoch 2/10, Batch 79/111, Training Loss: 0.8527\n",
      "Epoch 2/10, Batch 80/111, Training Loss: 0.7073\n",
      "Epoch 2/10, Batch 81/111, Training Loss: 0.8804\n",
      "Epoch 2/10, Batch 82/111, Training Loss: 0.9224\n",
      "Epoch 2/10, Batch 83/111, Training Loss: 0.7645\n",
      "Epoch 2/10, Batch 84/111, Training Loss: 0.7603\n",
      "Epoch 2/10, Batch 85/111, Training Loss: 0.7504\n",
      "Epoch 2/10, Batch 86/111, Training Loss: 0.8643\n",
      "Epoch 2/10, Batch 87/111, Training Loss: 0.8169\n",
      "Epoch 2/10, Batch 88/111, Training Loss: 0.8426\n",
      "Epoch 2/10, Batch 89/111, Training Loss: 0.7374\n",
      "Epoch 2/10, Batch 90/111, Training Loss: 0.8491\n",
      "Epoch 2/10, Batch 91/111, Training Loss: 0.7564\n",
      "Epoch 2/10, Batch 92/111, Training Loss: 0.8044\n",
      "Epoch 2/10, Batch 93/111, Training Loss: 0.7479\n",
      "Epoch 2/10, Batch 94/111, Training Loss: 0.7684\n",
      "Epoch 2/10, Batch 95/111, Training Loss: 0.8853\n",
      "Epoch 2/10, Batch 96/111, Training Loss: 0.8616\n",
      "Epoch 2/10, Batch 97/111, Training Loss: 0.7448\n",
      "Epoch 2/10, Batch 98/111, Training Loss: 0.8408\n",
      "Epoch 2/10, Batch 99/111, Training Loss: 0.9001\n",
      "Epoch 2/10, Batch 100/111, Training Loss: 0.7760\n",
      "Epoch 2/10, Batch 101/111, Training Loss: 0.9841\n",
      "Epoch 2/10, Batch 102/111, Training Loss: 0.7990\n",
      "Epoch 2/10, Batch 103/111, Training Loss: 0.9558\n",
      "Epoch 2/10, Batch 104/111, Training Loss: 0.8840\n",
      "Epoch 2/10, Batch 105/111, Training Loss: 0.9433\n",
      "Epoch 2/10, Batch 106/111, Training Loss: 0.9131\n",
      "Epoch 2/10, Batch 107/111, Training Loss: 0.8252\n",
      "Epoch 2/10, Batch 108/111, Training Loss: 0.8042\n",
      "Epoch 2/10, Batch 109/111, Training Loss: 0.7742\n",
      "Epoch 2/10, Batch 110/111, Training Loss: 0.8716\n",
      "Epoch 2/10, Batch 111/111, Training Loss: 0.9941\n",
      "Epoch 2/10, Training Loss: 0.8516, Validation Loss: 0.9871, Validation Accuracy: 0.5566\n",
      "Epoch 3/10, Batch 1/111, Training Loss: 0.8337\n",
      "Epoch 3/10, Batch 2/111, Training Loss: 0.8696\n",
      "Epoch 3/10, Batch 3/111, Training Loss: 0.8176\n",
      "Epoch 3/10, Batch 4/111, Training Loss: 0.8412\n",
      "Epoch 3/10, Batch 5/111, Training Loss: 0.7929\n",
      "Epoch 3/10, Batch 6/111, Training Loss: 0.8665\n",
      "Epoch 3/10, Batch 7/111, Training Loss: 0.8010\n",
      "Epoch 3/10, Batch 8/111, Training Loss: 0.7179\n",
      "Epoch 3/10, Batch 9/111, Training Loss: 0.9144\n",
      "Epoch 3/10, Batch 10/111, Training Loss: 0.9105\n",
      "Epoch 3/10, Batch 11/111, Training Loss: 0.8910\n",
      "Epoch 3/10, Batch 12/111, Training Loss: 0.8027\n",
      "Epoch 3/10, Batch 13/111, Training Loss: 0.7880\n",
      "Epoch 3/10, Batch 14/111, Training Loss: 0.8206\n",
      "Epoch 3/10, Batch 15/111, Training Loss: 0.7976\n",
      "Epoch 3/10, Batch 16/111, Training Loss: 0.7654\n",
      "Epoch 3/10, Batch 17/111, Training Loss: 0.9290\n",
      "Epoch 3/10, Batch 18/111, Training Loss: 0.8446\n",
      "Epoch 3/10, Batch 19/111, Training Loss: 0.7590\n",
      "Epoch 3/10, Batch 20/111, Training Loss: 0.7817\n",
      "Epoch 3/10, Batch 21/111, Training Loss: 0.7741\n",
      "Epoch 3/10, Batch 22/111, Training Loss: 0.9115\n",
      "Epoch 3/10, Batch 23/111, Training Loss: 0.7210\n",
      "Epoch 3/10, Batch 24/111, Training Loss: 0.8259\n",
      "Epoch 3/10, Batch 25/111, Training Loss: 0.8323\n",
      "Epoch 3/10, Batch 26/111, Training Loss: 0.8473\n",
      "Epoch 3/10, Batch 27/111, Training Loss: 0.8152\n",
      "Epoch 3/10, Batch 28/111, Training Loss: 0.9901\n",
      "Epoch 3/10, Batch 29/111, Training Loss: 0.7838\n",
      "Epoch 3/10, Batch 30/111, Training Loss: 0.8342\n",
      "Epoch 3/10, Batch 31/111, Training Loss: 0.7685\n",
      "Epoch 3/10, Batch 32/111, Training Loss: 0.8076\n",
      "Epoch 3/10, Batch 33/111, Training Loss: 0.7933\n",
      "Epoch 3/10, Batch 34/111, Training Loss: 0.7218\n",
      "Epoch 3/10, Batch 35/111, Training Loss: 0.9295\n",
      "Epoch 3/10, Batch 36/111, Training Loss: 0.9377\n",
      "Epoch 3/10, Batch 37/111, Training Loss: 0.7991\n",
      "Epoch 3/10, Batch 38/111, Training Loss: 0.9018\n",
      "Epoch 3/10, Batch 39/111, Training Loss: 0.8359\n",
      "Epoch 3/10, Batch 40/111, Training Loss: 0.8586\n",
      "Epoch 3/10, Batch 41/111, Training Loss: 0.8889\n",
      "Epoch 3/10, Batch 42/111, Training Loss: 0.7480\n",
      "Epoch 3/10, Batch 43/111, Training Loss: 0.7582\n",
      "Epoch 3/10, Batch 44/111, Training Loss: 0.8324\n",
      "Epoch 3/10, Batch 45/111, Training Loss: 0.7191\n",
      "Epoch 3/10, Batch 46/111, Training Loss: 0.7108\n",
      "Epoch 3/10, Batch 47/111, Training Loss: 0.8074\n",
      "Epoch 3/10, Batch 48/111, Training Loss: 0.9034\n",
      "Epoch 3/10, Batch 49/111, Training Loss: 0.6747\n",
      "Epoch 3/10, Batch 50/111, Training Loss: 0.7976\n",
      "Epoch 3/10, Batch 51/111, Training Loss: 0.8700\n",
      "Epoch 3/10, Batch 52/111, Training Loss: 0.7773\n",
      "Epoch 3/10, Batch 53/111, Training Loss: 0.8502\n",
      "Epoch 3/10, Batch 54/111, Training Loss: 0.7687\n",
      "Epoch 3/10, Batch 55/111, Training Loss: 0.8032\n",
      "Epoch 3/10, Batch 56/111, Training Loss: 0.7806\n",
      "Epoch 3/10, Batch 57/111, Training Loss: 0.7674\n",
      "Epoch 3/10, Batch 58/111, Training Loss: 0.7974\n",
      "Epoch 3/10, Batch 59/111, Training Loss: 0.8490\n",
      "Epoch 3/10, Batch 60/111, Training Loss: 0.7833\n",
      "Epoch 3/10, Batch 61/111, Training Loss: 0.8208\n",
      "Epoch 3/10, Batch 62/111, Training Loss: 0.7336\n",
      "Epoch 3/10, Batch 63/111, Training Loss: 0.7475\n",
      "Epoch 3/10, Batch 64/111, Training Loss: 0.7525\n",
      "Epoch 3/10, Batch 65/111, Training Loss: 0.8727\n",
      "Epoch 3/10, Batch 66/111, Training Loss: 0.7778\n",
      "Epoch 3/10, Batch 67/111, Training Loss: 0.7293\n",
      "Epoch 3/10, Batch 68/111, Training Loss: 0.8834\n",
      "Epoch 3/10, Batch 69/111, Training Loss: 0.7402\n",
      "Epoch 3/10, Batch 70/111, Training Loss: 0.8306\n",
      "Epoch 3/10, Batch 71/111, Training Loss: 0.8061\n",
      "Epoch 3/10, Batch 72/111, Training Loss: 0.7443\n",
      "Epoch 3/10, Batch 73/111, Training Loss: 0.8438\n",
      "Epoch 3/10, Batch 74/111, Training Loss: 0.7970\n",
      "Epoch 3/10, Batch 75/111, Training Loss: 0.8315\n",
      "Epoch 3/10, Batch 76/111, Training Loss: 0.7967\n",
      "Epoch 3/10, Batch 77/111, Training Loss: 0.7798\n",
      "Epoch 3/10, Batch 78/111, Training Loss: 0.7841\n",
      "Epoch 3/10, Batch 79/111, Training Loss: 0.7339\n",
      "Epoch 3/10, Batch 80/111, Training Loss: 0.8263\n",
      "Epoch 3/10, Batch 81/111, Training Loss: 0.8011\n",
      "Epoch 3/10, Batch 82/111, Training Loss: 0.8706\n",
      "Epoch 3/10, Batch 83/111, Training Loss: 0.7157\n",
      "Epoch 3/10, Batch 84/111, Training Loss: 0.7878\n",
      "Epoch 3/10, Batch 85/111, Training Loss: 0.7760\n",
      "Epoch 3/10, Batch 86/111, Training Loss: 0.7333\n",
      "Epoch 3/10, Batch 87/111, Training Loss: 0.7460\n",
      "Epoch 3/10, Batch 88/111, Training Loss: 0.8128\n",
      "Epoch 3/10, Batch 89/111, Training Loss: 0.6727\n",
      "Epoch 3/10, Batch 90/111, Training Loss: 0.7350\n",
      "Epoch 3/10, Batch 91/111, Training Loss: 0.6401\n",
      "Epoch 3/10, Batch 92/111, Training Loss: 0.7893\n",
      "Epoch 3/10, Batch 93/111, Training Loss: 0.7387\n",
      "Epoch 3/10, Batch 94/111, Training Loss: 0.8747\n",
      "Epoch 3/10, Batch 95/111, Training Loss: 0.7704\n",
      "Epoch 3/10, Batch 96/111, Training Loss: 0.7889\n",
      "Epoch 3/10, Batch 97/111, Training Loss: 0.8380\n",
      "Epoch 3/10, Batch 98/111, Training Loss: 0.7917\n",
      "Epoch 3/10, Batch 99/111, Training Loss: 0.7689\n",
      "Epoch 3/10, Batch 100/111, Training Loss: 0.8064\n",
      "Epoch 3/10, Batch 101/111, Training Loss: 0.8017\n",
      "Epoch 3/10, Batch 102/111, Training Loss: 0.9031\n",
      "Epoch 3/10, Batch 103/111, Training Loss: 0.7895\n",
      "Epoch 3/10, Batch 104/111, Training Loss: 0.7868\n",
      "Epoch 3/10, Batch 105/111, Training Loss: 0.8045\n",
      "Epoch 3/10, Batch 106/111, Training Loss: 0.7901\n",
      "Epoch 3/10, Batch 107/111, Training Loss: 0.8816\n",
      "Epoch 3/10, Batch 108/111, Training Loss: 0.7937\n",
      "Epoch 3/10, Batch 109/111, Training Loss: 0.7611\n",
      "Epoch 3/10, Batch 110/111, Training Loss: 0.8322\n",
      "Epoch 3/10, Batch 111/111, Training Loss: 0.5525\n",
      "Epoch 3/10, Training Loss: 0.8028, Validation Loss: 0.9048, Validation Accuracy: 0.5679\n",
      "Epoch 4/10, Batch 1/111, Training Loss: 0.7650\n",
      "Epoch 4/10, Batch 2/111, Training Loss: 0.6807\n",
      "Epoch 4/10, Batch 3/111, Training Loss: 0.7285\n",
      "Epoch 4/10, Batch 4/111, Training Loss: 0.8721\n",
      "Epoch 4/10, Batch 5/111, Training Loss: 0.7608\n",
      "Epoch 4/10, Batch 6/111, Training Loss: 0.7926\n",
      "Epoch 4/10, Batch 7/111, Training Loss: 0.7504\n",
      "Epoch 4/10, Batch 8/111, Training Loss: 0.8877\n",
      "Epoch 4/10, Batch 9/111, Training Loss: 0.8449\n",
      "Epoch 4/10, Batch 10/111, Training Loss: 0.8801\n",
      "Epoch 4/10, Batch 11/111, Training Loss: 0.7704\n",
      "Epoch 4/10, Batch 12/111, Training Loss: 0.7134\n",
      "Epoch 4/10, Batch 13/111, Training Loss: 0.7329\n",
      "Epoch 4/10, Batch 14/111, Training Loss: 0.7452\n",
      "Epoch 4/10, Batch 15/111, Training Loss: 0.8920\n",
      "Epoch 4/10, Batch 16/111, Training Loss: 0.7742\n",
      "Epoch 4/10, Batch 17/111, Training Loss: 0.8166\n",
      "Epoch 4/10, Batch 18/111, Training Loss: 0.7254\n",
      "Epoch 4/10, Batch 19/111, Training Loss: 0.8440\n",
      "Epoch 4/10, Batch 20/111, Training Loss: 0.8200\n",
      "Epoch 4/10, Batch 21/111, Training Loss: 0.7007\n",
      "Epoch 4/10, Batch 22/111, Training Loss: 0.8493\n",
      "Epoch 4/10, Batch 23/111, Training Loss: 0.7420\n",
      "Epoch 4/10, Batch 24/111, Training Loss: 0.7052\n",
      "Epoch 4/10, Batch 25/111, Training Loss: 0.7306\n",
      "Epoch 4/10, Batch 26/111, Training Loss: 0.7739\n",
      "Epoch 4/10, Batch 27/111, Training Loss: 0.8224\n",
      "Epoch 4/10, Batch 28/111, Training Loss: 0.7586\n",
      "Epoch 4/10, Batch 29/111, Training Loss: 0.7645\n",
      "Epoch 4/10, Batch 30/111, Training Loss: 0.7398\n",
      "Epoch 4/10, Batch 31/111, Training Loss: 0.7349\n",
      "Epoch 4/10, Batch 32/111, Training Loss: 0.8022\n",
      "Epoch 4/10, Batch 33/111, Training Loss: 0.7952\n",
      "Epoch 4/10, Batch 34/111, Training Loss: 0.7016\n",
      "Epoch 4/10, Batch 35/111, Training Loss: 0.6824\n",
      "Epoch 4/10, Batch 36/111, Training Loss: 0.6537\n",
      "Epoch 4/10, Batch 37/111, Training Loss: 0.6984\n",
      "Epoch 4/10, Batch 38/111, Training Loss: 0.8130\n",
      "Epoch 4/10, Batch 39/111, Training Loss: 0.7937\n",
      "Epoch 4/10, Batch 40/111, Training Loss: 0.6885\n",
      "Epoch 4/10, Batch 41/111, Training Loss: 0.6938\n",
      "Epoch 4/10, Batch 42/111, Training Loss: 0.7678\n",
      "Epoch 4/10, Batch 43/111, Training Loss: 0.7113\n",
      "Epoch 4/10, Batch 44/111, Training Loss: 0.7239\n",
      "Epoch 4/10, Batch 45/111, Training Loss: 0.7140\n",
      "Epoch 4/10, Batch 46/111, Training Loss: 0.6662\n",
      "Epoch 4/10, Batch 47/111, Training Loss: 0.7390\n",
      "Epoch 4/10, Batch 48/111, Training Loss: 0.8280\n",
      "Epoch 4/10, Batch 49/111, Training Loss: 0.8413\n",
      "Epoch 4/10, Batch 50/111, Training Loss: 0.7411\n",
      "Epoch 4/10, Batch 51/111, Training Loss: 0.7570\n",
      "Epoch 4/10, Batch 52/111, Training Loss: 0.7651\n",
      "Epoch 4/10, Batch 53/111, Training Loss: 0.7292\n",
      "Epoch 4/10, Batch 54/111, Training Loss: 0.7643\n",
      "Epoch 4/10, Batch 55/111, Training Loss: 0.7012\n",
      "Epoch 4/10, Batch 56/111, Training Loss: 0.9205\n",
      "Epoch 4/10, Batch 57/111, Training Loss: 0.7475\n",
      "Epoch 4/10, Batch 58/111, Training Loss: 0.8304\n",
      "Epoch 4/10, Batch 59/111, Training Loss: 0.7552\n",
      "Epoch 4/10, Batch 60/111, Training Loss: 0.8018\n",
      "Epoch 4/10, Batch 61/111, Training Loss: 0.7519\n",
      "Epoch 4/10, Batch 62/111, Training Loss: 0.7979\n",
      "Epoch 4/10, Batch 63/111, Training Loss: 0.7898\n",
      "Epoch 4/10, Batch 64/111, Training Loss: 0.7430\n",
      "Epoch 4/10, Batch 65/111, Training Loss: 0.7095\n",
      "Epoch 4/10, Batch 66/111, Training Loss: 0.7405\n",
      "Epoch 4/10, Batch 67/111, Training Loss: 0.6854\n",
      "Epoch 4/10, Batch 68/111, Training Loss: 0.6524\n",
      "Epoch 4/10, Batch 69/111, Training Loss: 0.7186\n",
      "Epoch 4/10, Batch 70/111, Training Loss: 0.6407\n",
      "Epoch 4/10, Batch 71/111, Training Loss: 0.7165\n",
      "Epoch 4/10, Batch 72/111, Training Loss: 0.7869\n",
      "Epoch 4/10, Batch 73/111, Training Loss: 0.7176\n",
      "Epoch 4/10, Batch 74/111, Training Loss: 0.7060\n",
      "Epoch 4/10, Batch 75/111, Training Loss: 0.7990\n",
      "Epoch 4/10, Batch 76/111, Training Loss: 0.7262\n",
      "Epoch 4/10, Batch 77/111, Training Loss: 0.7376\n",
      "Epoch 4/10, Batch 78/111, Training Loss: 0.6830\n",
      "Epoch 4/10, Batch 79/111, Training Loss: 0.6569\n",
      "Epoch 4/10, Batch 80/111, Training Loss: 0.6433\n",
      "Epoch 4/10, Batch 81/111, Training Loss: 0.6986\n",
      "Epoch 4/10, Batch 82/111, Training Loss: 0.6778\n",
      "Epoch 4/10, Batch 83/111, Training Loss: 0.9006\n",
      "Epoch 4/10, Batch 84/111, Training Loss: 0.7843\n",
      "Epoch 4/10, Batch 85/111, Training Loss: 0.7238\n",
      "Epoch 4/10, Batch 86/111, Training Loss: 0.7009\n",
      "Epoch 4/10, Batch 87/111, Training Loss: 0.7822\n",
      "Epoch 4/10, Batch 88/111, Training Loss: 0.8021\n",
      "Epoch 4/10, Batch 89/111, Training Loss: 0.7942\n",
      "Epoch 4/10, Batch 90/111, Training Loss: 0.6693\n",
      "Epoch 4/10, Batch 91/111, Training Loss: 0.7093\n",
      "Epoch 4/10, Batch 92/111, Training Loss: 0.6642\n",
      "Epoch 4/10, Batch 93/111, Training Loss: 0.8384\n",
      "Epoch 4/10, Batch 94/111, Training Loss: 0.7924\n",
      "Epoch 4/10, Batch 95/111, Training Loss: 0.7441\n",
      "Epoch 4/10, Batch 96/111, Training Loss: 0.6261\n",
      "Epoch 4/10, Batch 97/111, Training Loss: 0.7622\n",
      "Epoch 4/10, Batch 98/111, Training Loss: 0.8311\n",
      "Epoch 4/10, Batch 99/111, Training Loss: 0.6831\n",
      "Epoch 4/10, Batch 100/111, Training Loss: 0.7020\n",
      "Epoch 4/10, Batch 101/111, Training Loss: 0.7556\n",
      "Epoch 4/10, Batch 102/111, Training Loss: 0.6528\n",
      "Epoch 4/10, Batch 103/111, Training Loss: 0.6525\n",
      "Epoch 4/10, Batch 104/111, Training Loss: 0.7446\n",
      "Epoch 4/10, Batch 105/111, Training Loss: 0.6435\n",
      "Epoch 4/10, Batch 106/111, Training Loss: 0.7381\n",
      "Epoch 4/10, Batch 107/111, Training Loss: 0.7126\n",
      "Epoch 4/10, Batch 108/111, Training Loss: 0.7998\n",
      "Epoch 4/10, Batch 109/111, Training Loss: 0.7913\n",
      "Epoch 4/10, Batch 110/111, Training Loss: 0.6624\n",
      "Epoch 4/10, Batch 111/111, Training Loss: 0.9809\n",
      "Epoch 4/10, Training Loss: 0.7511, Validation Loss: 1.0812, Validation Accuracy: 0.5218\n",
      "Epoch 5/10, Batch 1/111, Training Loss: 0.7317\n",
      "Epoch 5/10, Batch 2/111, Training Loss: 0.7261\n",
      "Epoch 5/10, Batch 3/111, Training Loss: 0.7401\n",
      "Epoch 5/10, Batch 4/111, Training Loss: 0.7844\n",
      "Epoch 5/10, Batch 5/111, Training Loss: 0.7690\n",
      "Epoch 5/10, Batch 6/111, Training Loss: 0.6872\n",
      "Epoch 5/10, Batch 7/111, Training Loss: 0.6402\n",
      "Epoch 5/10, Batch 8/111, Training Loss: 0.7479\n",
      "Epoch 5/10, Batch 9/111, Training Loss: 0.7217\n",
      "Epoch 5/10, Batch 10/111, Training Loss: 0.7627\n",
      "Epoch 5/10, Batch 11/111, Training Loss: 0.7066\n",
      "Epoch 5/10, Batch 12/111, Training Loss: 0.6727\n",
      "Epoch 5/10, Batch 13/111, Training Loss: 0.7970\n",
      "Epoch 5/10, Batch 14/111, Training Loss: 0.6755\n",
      "Epoch 5/10, Batch 15/111, Training Loss: 0.7049\n",
      "Epoch 5/10, Batch 16/111, Training Loss: 0.7824\n",
      "Epoch 5/10, Batch 17/111, Training Loss: 0.8671\n",
      "Epoch 5/10, Batch 18/111, Training Loss: 0.6328\n",
      "Epoch 5/10, Batch 19/111, Training Loss: 0.7640\n",
      "Epoch 5/10, Batch 20/111, Training Loss: 0.6538\n",
      "Epoch 5/10, Batch 21/111, Training Loss: 0.6671\n",
      "Epoch 5/10, Batch 22/111, Training Loss: 0.7940\n",
      "Epoch 5/10, Batch 23/111, Training Loss: 0.6714\n",
      "Epoch 5/10, Batch 24/111, Training Loss: 0.8170\n",
      "Epoch 5/10, Batch 25/111, Training Loss: 0.7866\n",
      "Epoch 5/10, Batch 26/111, Training Loss: 0.7024\n",
      "Epoch 5/10, Batch 27/111, Training Loss: 0.6721\n",
      "Epoch 5/10, Batch 28/111, Training Loss: 0.7202\n",
      "Epoch 5/10, Batch 29/111, Training Loss: 0.7072\n",
      "Epoch 5/10, Batch 30/111, Training Loss: 0.7017\n",
      "Epoch 5/10, Batch 31/111, Training Loss: 0.7598\n",
      "Epoch 5/10, Batch 32/111, Training Loss: 0.7605\n",
      "Epoch 5/10, Batch 33/111, Training Loss: 0.8010\n",
      "Epoch 5/10, Batch 34/111, Training Loss: 0.7676\n",
      "Epoch 5/10, Batch 35/111, Training Loss: 0.6776\n",
      "Epoch 5/10, Batch 36/111, Training Loss: 0.6210\n",
      "Epoch 5/10, Batch 37/111, Training Loss: 0.7566\n",
      "Epoch 5/10, Batch 38/111, Training Loss: 0.6699\n",
      "Epoch 5/10, Batch 39/111, Training Loss: 0.6871\n",
      "Epoch 5/10, Batch 40/111, Training Loss: 0.7064\n",
      "Epoch 5/10, Batch 41/111, Training Loss: 0.6675\n",
      "Epoch 5/10, Batch 42/111, Training Loss: 0.7374\n",
      "Epoch 5/10, Batch 43/111, Training Loss: 0.6613\n",
      "Epoch 5/10, Batch 44/111, Training Loss: 0.6346\n",
      "Epoch 5/10, Batch 45/111, Training Loss: 0.6714\n",
      "Epoch 5/10, Batch 46/111, Training Loss: 0.7379\n",
      "Epoch 5/10, Batch 47/111, Training Loss: 0.7822\n",
      "Epoch 5/10, Batch 48/111, Training Loss: 0.7285\n",
      "Epoch 5/10, Batch 49/111, Training Loss: 0.6050\n",
      "Epoch 5/10, Batch 50/111, Training Loss: 0.6227\n",
      "Epoch 5/10, Batch 51/111, Training Loss: 0.7659\n",
      "Epoch 5/10, Batch 52/111, Training Loss: 0.5745\n",
      "Epoch 5/10, Batch 53/111, Training Loss: 0.7203\n",
      "Epoch 5/10, Batch 54/111, Training Loss: 0.7080\n",
      "Epoch 5/10, Batch 55/111, Training Loss: 0.7178\n",
      "Epoch 5/10, Batch 56/111, Training Loss: 0.6629\n",
      "Epoch 5/10, Batch 57/111, Training Loss: 0.7104\n",
      "Epoch 5/10, Batch 58/111, Training Loss: 0.7052\n",
      "Epoch 5/10, Batch 59/111, Training Loss: 0.7431\n",
      "Epoch 5/10, Batch 60/111, Training Loss: 0.7311\n",
      "Epoch 5/10, Batch 61/111, Training Loss: 0.7487\n",
      "Epoch 5/10, Batch 62/111, Training Loss: 0.6678\n",
      "Epoch 5/10, Batch 63/111, Training Loss: 0.7377\n",
      "Epoch 5/10, Batch 64/111, Training Loss: 0.8444\n",
      "Epoch 5/10, Batch 65/111, Training Loss: 0.7934\n",
      "Epoch 5/10, Batch 66/111, Training Loss: 0.7855\n",
      "Epoch 5/10, Batch 67/111, Training Loss: 0.7638\n",
      "Epoch 5/10, Batch 68/111, Training Loss: 0.7978\n",
      "Epoch 5/10, Batch 69/111, Training Loss: 0.8593\n",
      "Epoch 5/10, Batch 70/111, Training Loss: 0.8053\n",
      "Epoch 5/10, Batch 71/111, Training Loss: 0.7301\n",
      "Epoch 5/10, Batch 72/111, Training Loss: 0.7546\n",
      "Epoch 5/10, Batch 73/111, Training Loss: 0.6524\n",
      "Epoch 5/10, Batch 74/111, Training Loss: 0.7601\n",
      "Epoch 5/10, Batch 75/111, Training Loss: 0.7240\n",
      "Epoch 5/10, Batch 76/111, Training Loss: 0.7815\n",
      "Epoch 5/10, Batch 77/111, Training Loss: 0.7671\n",
      "Epoch 5/10, Batch 78/111, Training Loss: 0.6346\n",
      "Epoch 5/10, Batch 79/111, Training Loss: 0.6539\n",
      "Epoch 5/10, Batch 80/111, Training Loss: 0.8194\n",
      "Epoch 5/10, Batch 81/111, Training Loss: 0.7153\n",
      "Epoch 5/10, Batch 82/111, Training Loss: 0.6398\n",
      "Epoch 5/10, Batch 83/111, Training Loss: 0.8826\n",
      "Epoch 5/10, Batch 84/111, Training Loss: 0.7432\n",
      "Epoch 5/10, Batch 85/111, Training Loss: 0.7639\n",
      "Epoch 5/10, Batch 86/111, Training Loss: 0.6370\n",
      "Epoch 5/10, Batch 87/111, Training Loss: 0.6948\n",
      "Epoch 5/10, Batch 88/111, Training Loss: 0.6386\n",
      "Epoch 5/10, Batch 89/111, Training Loss: 0.6963\n",
      "Epoch 5/10, Batch 90/111, Training Loss: 0.7495\n",
      "Epoch 5/10, Batch 91/111, Training Loss: 0.7048\n",
      "Epoch 5/10, Batch 92/111, Training Loss: 0.5617\n",
      "Epoch 5/10, Batch 93/111, Training Loss: 0.6789\n",
      "Epoch 5/10, Batch 94/111, Training Loss: 0.8275\n",
      "Epoch 5/10, Batch 95/111, Training Loss: 0.6576\n",
      "Epoch 5/10, Batch 96/111, Training Loss: 0.6782\n",
      "Epoch 5/10, Batch 97/111, Training Loss: 0.6885\n",
      "Epoch 5/10, Batch 98/111, Training Loss: 0.6116\n",
      "Epoch 5/10, Batch 99/111, Training Loss: 0.7351\n",
      "Epoch 5/10, Batch 100/111, Training Loss: 0.6818\n",
      "Epoch 5/10, Batch 101/111, Training Loss: 0.6340\n",
      "Epoch 5/10, Batch 102/111, Training Loss: 0.6745\n",
      "Epoch 5/10, Batch 103/111, Training Loss: 0.7862\n",
      "Epoch 5/10, Batch 104/111, Training Loss: 0.6652\n",
      "Epoch 5/10, Batch 105/111, Training Loss: 0.6607\n",
      "Epoch 5/10, Batch 106/111, Training Loss: 0.6667\n",
      "Epoch 5/10, Batch 107/111, Training Loss: 0.7497\n",
      "Epoch 5/10, Batch 108/111, Training Loss: 0.7052\n",
      "Epoch 5/10, Batch 109/111, Training Loss: 0.7317\n",
      "Epoch 5/10, Batch 110/111, Training Loss: 0.6455\n",
      "Epoch 5/10, Batch 111/111, Training Loss: 0.6429\n",
      "Epoch 5/10, Training Loss: 0.7165, Validation Loss: 1.1240, Validation Accuracy: 0.5521\n",
      "Epoch 6/10, Batch 1/111, Training Loss: 0.7350\n",
      "Epoch 6/10, Batch 2/111, Training Loss: 0.7184\n",
      "Epoch 6/10, Batch 3/111, Training Loss: 0.6929\n",
      "Epoch 6/10, Batch 4/111, Training Loss: 0.7205\n",
      "Epoch 6/10, Batch 5/111, Training Loss: 0.6902\n",
      "Epoch 6/10, Batch 6/111, Training Loss: 0.7276\n",
      "Epoch 6/10, Batch 7/111, Training Loss: 0.7509\n",
      "Epoch 6/10, Batch 8/111, Training Loss: 0.6105\n",
      "Epoch 6/10, Batch 9/111, Training Loss: 0.6635\n",
      "Epoch 6/10, Batch 10/111, Training Loss: 0.6426\n",
      "Epoch 6/10, Batch 11/111, Training Loss: 0.6711\n",
      "Epoch 6/10, Batch 12/111, Training Loss: 0.6463\n",
      "Epoch 6/10, Batch 13/111, Training Loss: 0.5968\n",
      "Epoch 6/10, Batch 14/111, Training Loss: 0.5699\n",
      "Epoch 6/10, Batch 15/111, Training Loss: 0.6158\n",
      "Epoch 6/10, Batch 16/111, Training Loss: 0.6867\n",
      "Epoch 6/10, Batch 17/111, Training Loss: 0.6932\n",
      "Epoch 6/10, Batch 18/111, Training Loss: 0.7739\n",
      "Epoch 6/10, Batch 19/111, Training Loss: 0.6167\n",
      "Epoch 6/10, Batch 20/111, Training Loss: 0.6582\n",
      "Epoch 6/10, Batch 21/111, Training Loss: 0.5958\n",
      "Epoch 6/10, Batch 22/111, Training Loss: 0.5946\n",
      "Epoch 6/10, Batch 23/111, Training Loss: 0.6035\n",
      "Epoch 6/10, Batch 24/111, Training Loss: 0.7145\n",
      "Epoch 6/10, Batch 25/111, Training Loss: 0.6081\n",
      "Epoch 6/10, Batch 26/111, Training Loss: 0.6212\n",
      "Epoch 6/10, Batch 27/111, Training Loss: 0.6040\n",
      "Epoch 6/10, Batch 28/111, Training Loss: 0.6270\n",
      "Epoch 6/10, Batch 29/111, Training Loss: 0.7674\n",
      "Epoch 6/10, Batch 30/111, Training Loss: 0.6327\n",
      "Epoch 6/10, Batch 31/111, Training Loss: 0.6729\n",
      "Epoch 6/10, Batch 32/111, Training Loss: 0.6611\n",
      "Epoch 6/10, Batch 33/111, Training Loss: 0.5566\n",
      "Epoch 6/10, Batch 34/111, Training Loss: 0.5789\n",
      "Epoch 6/10, Batch 35/111, Training Loss: 0.5798\n",
      "Epoch 6/10, Batch 36/111, Training Loss: 0.5937\n",
      "Epoch 6/10, Batch 37/111, Training Loss: 0.6059\n",
      "Epoch 6/10, Batch 38/111, Training Loss: 0.6582\n",
      "Epoch 6/10, Batch 39/111, Training Loss: 0.5921\n",
      "Epoch 6/10, Batch 40/111, Training Loss: 0.7393\n",
      "Epoch 6/10, Batch 41/111, Training Loss: 0.6052\n",
      "Epoch 6/10, Batch 42/111, Training Loss: 0.5073\n",
      "Epoch 6/10, Batch 43/111, Training Loss: 0.5813\n",
      "Epoch 6/10, Batch 44/111, Training Loss: 0.6111\n",
      "Epoch 6/10, Batch 45/111, Training Loss: 0.7658\n",
      "Epoch 6/10, Batch 46/111, Training Loss: 0.6003\n",
      "Epoch 6/10, Batch 47/111, Training Loss: 0.6206\n",
      "Epoch 6/10, Batch 48/111, Training Loss: 0.6520\n",
      "Epoch 6/10, Batch 49/111, Training Loss: 0.6495\n",
      "Epoch 6/10, Batch 50/111, Training Loss: 0.6221\n",
      "Epoch 6/10, Batch 51/111, Training Loss: 0.6498\n",
      "Epoch 6/10, Batch 52/111, Training Loss: 0.6702\n",
      "Epoch 6/10, Batch 53/111, Training Loss: 0.7137\n",
      "Epoch 6/10, Batch 54/111, Training Loss: 0.6701\n",
      "Epoch 6/10, Batch 55/111, Training Loss: 0.6190\n",
      "Epoch 6/10, Batch 56/111, Training Loss: 0.5362\n",
      "Epoch 6/10, Batch 57/111, Training Loss: 0.5825\n",
      "Epoch 6/10, Batch 58/111, Training Loss: 0.5042\n",
      "Epoch 6/10, Batch 59/111, Training Loss: 0.6778\n",
      "Epoch 6/10, Batch 60/111, Training Loss: 0.5966\n",
      "Epoch 6/10, Batch 61/111, Training Loss: 0.6497\n",
      "Epoch 6/10, Batch 62/111, Training Loss: 0.5926\n",
      "Epoch 6/10, Batch 63/111, Training Loss: 0.6591\n",
      "Epoch 6/10, Batch 64/111, Training Loss: 0.6190\n",
      "Epoch 6/10, Batch 65/111, Training Loss: 0.5952\n",
      "Epoch 6/10, Batch 66/111, Training Loss: 0.5966\n",
      "Epoch 6/10, Batch 67/111, Training Loss: 0.5744\n",
      "Epoch 6/10, Batch 68/111, Training Loss: 0.6450\n",
      "Epoch 6/10, Batch 69/111, Training Loss: 0.6608\n",
      "Epoch 6/10, Batch 70/111, Training Loss: 0.6428\n",
      "Epoch 6/10, Batch 71/111, Training Loss: 0.6269\n",
      "Epoch 6/10, Batch 72/111, Training Loss: 0.7190\n",
      "Epoch 6/10, Batch 73/111, Training Loss: 0.5849\n",
      "Epoch 6/10, Batch 74/111, Training Loss: 0.6787\n",
      "Epoch 6/10, Batch 75/111, Training Loss: 0.6203\n",
      "Epoch 6/10, Batch 76/111, Training Loss: 0.6379\n",
      "Epoch 6/10, Batch 77/111, Training Loss: 0.5250\n",
      "Epoch 6/10, Batch 78/111, Training Loss: 0.6588\n",
      "Epoch 6/10, Batch 79/111, Training Loss: 0.5650\n",
      "Epoch 6/10, Batch 80/111, Training Loss: 0.6361\n",
      "Epoch 6/10, Batch 81/111, Training Loss: 0.6520\n",
      "Epoch 6/10, Batch 82/111, Training Loss: 0.5617\n",
      "Epoch 6/10, Batch 83/111, Training Loss: 0.6107\n",
      "Epoch 6/10, Batch 84/111, Training Loss: 0.6447\n",
      "Epoch 6/10, Batch 85/111, Training Loss: 0.5944\n",
      "Epoch 6/10, Batch 86/111, Training Loss: 0.5723\n",
      "Epoch 6/10, Batch 87/111, Training Loss: 0.6482\n",
      "Epoch 6/10, Batch 88/111, Training Loss: 0.5862\n",
      "Epoch 6/10, Batch 89/111, Training Loss: 0.6503\n",
      "Epoch 6/10, Batch 90/111, Training Loss: 0.5131\n",
      "Epoch 6/10, Batch 91/111, Training Loss: 0.6234\n",
      "Epoch 6/10, Batch 92/111, Training Loss: 0.5938\n",
      "Epoch 6/10, Batch 93/111, Training Loss: 0.6935\n",
      "Epoch 6/10, Batch 94/111, Training Loss: 0.5943\n",
      "Epoch 6/10, Batch 95/111, Training Loss: 0.6818\n",
      "Epoch 6/10, Batch 96/111, Training Loss: 0.5438\n",
      "Epoch 6/10, Batch 97/111, Training Loss: 0.5959\n",
      "Epoch 6/10, Batch 98/111, Training Loss: 0.5772\n",
      "Epoch 6/10, Batch 99/111, Training Loss: 0.5145\n",
      "Epoch 6/10, Batch 100/111, Training Loss: 0.6296\n",
      "Epoch 6/10, Batch 101/111, Training Loss: 0.7015\n",
      "Epoch 6/10, Batch 102/111, Training Loss: 0.6799\n",
      "Epoch 6/10, Batch 103/111, Training Loss: 0.6807\n",
      "Epoch 6/10, Batch 104/111, Training Loss: 0.5288\n",
      "Epoch 6/10, Batch 105/111, Training Loss: 0.6978\n",
      "Epoch 6/10, Batch 106/111, Training Loss: 0.6948\n",
      "Epoch 6/10, Batch 107/111, Training Loss: 0.5976\n",
      "Epoch 6/10, Batch 108/111, Training Loss: 0.5951\n",
      "Epoch 6/10, Batch 109/111, Training Loss: 0.5368\n",
      "Epoch 6/10, Batch 110/111, Training Loss: 0.5270\n",
      "Epoch 6/10, Batch 111/111, Training Loss: 0.5788\n",
      "Epoch 6/10, Training Loss: 0.6298, Validation Loss: 0.6271, Validation Accuracy: 0.7125\n",
      "Epoch 7/10, Batch 1/111, Training Loss: 0.7829\n",
      "Epoch 7/10, Batch 2/111, Training Loss: 0.5792\n",
      "Epoch 7/10, Batch 3/111, Training Loss: 0.5982\n",
      "Epoch 7/10, Batch 4/111, Training Loss: 0.5064\n",
      "Epoch 7/10, Batch 5/111, Training Loss: 0.6559\n",
      "Epoch 7/10, Batch 6/111, Training Loss: 0.6081\n",
      "Epoch 7/10, Batch 7/111, Training Loss: 0.5436\n",
      "Epoch 7/10, Batch 8/111, Training Loss: 0.4958\n",
      "Epoch 7/10, Batch 9/111, Training Loss: 0.6318\n",
      "Epoch 7/10, Batch 10/111, Training Loss: 0.5730\n",
      "Epoch 7/10, Batch 11/111, Training Loss: 0.6036\n",
      "Epoch 7/10, Batch 12/111, Training Loss: 0.6094\n",
      "Epoch 7/10, Batch 13/111, Training Loss: 0.6061\n",
      "Epoch 7/10, Batch 14/111, Training Loss: 0.5646\n",
      "Epoch 7/10, Batch 15/111, Training Loss: 0.6111\n",
      "Epoch 7/10, Batch 16/111, Training Loss: 0.4736\n",
      "Epoch 7/10, Batch 17/111, Training Loss: 0.5725\n",
      "Epoch 7/10, Batch 18/111, Training Loss: 0.6353\n",
      "Epoch 7/10, Batch 19/111, Training Loss: 0.5811\n",
      "Epoch 7/10, Batch 20/111, Training Loss: 0.5394\n",
      "Epoch 7/10, Batch 21/111, Training Loss: 0.5346\n",
      "Epoch 7/10, Batch 22/111, Training Loss: 0.6231\n",
      "Epoch 7/10, Batch 23/111, Training Loss: 0.6146\n",
      "Epoch 7/10, Batch 24/111, Training Loss: 0.6260\n",
      "Epoch 7/10, Batch 25/111, Training Loss: 0.6410\n",
      "Epoch 7/10, Batch 26/111, Training Loss: 0.6701\n",
      "Epoch 7/10, Batch 27/111, Training Loss: 0.5296\n",
      "Epoch 7/10, Batch 28/111, Training Loss: 0.5956\n",
      "Epoch 7/10, Batch 29/111, Training Loss: 0.5115\n",
      "Epoch 7/10, Batch 30/111, Training Loss: 0.7288\n",
      "Epoch 7/10, Batch 31/111, Training Loss: 0.5114\n",
      "Epoch 7/10, Batch 32/111, Training Loss: 0.5722\n",
      "Epoch 7/10, Batch 33/111, Training Loss: 0.7829\n",
      "Epoch 7/10, Batch 34/111, Training Loss: 0.5561\n",
      "Epoch 7/10, Batch 35/111, Training Loss: 0.7121\n",
      "Epoch 7/10, Batch 36/111, Training Loss: 0.5177\n",
      "Epoch 7/10, Batch 37/111, Training Loss: 0.5354\n",
      "Epoch 7/10, Batch 38/111, Training Loss: 0.4955\n",
      "Epoch 7/10, Batch 39/111, Training Loss: 0.6118\n",
      "Epoch 7/10, Batch 40/111, Training Loss: 0.6293\n",
      "Epoch 7/10, Batch 41/111, Training Loss: 0.6614\n",
      "Epoch 7/10, Batch 42/111, Training Loss: 0.5196\n",
      "Epoch 7/10, Batch 43/111, Training Loss: 0.5891\n",
      "Epoch 7/10, Batch 44/111, Training Loss: 0.5675\n",
      "Epoch 7/10, Batch 45/111, Training Loss: 0.5011\n",
      "Epoch 7/10, Batch 46/111, Training Loss: 0.6238\n",
      "Epoch 7/10, Batch 47/111, Training Loss: 0.6065\n",
      "Epoch 7/10, Batch 48/111, Training Loss: 0.6876\n",
      "Epoch 7/10, Batch 49/111, Training Loss: 0.4943\n",
      "Epoch 7/10, Batch 50/111, Training Loss: 0.5150\n",
      "Epoch 7/10, Batch 51/111, Training Loss: 0.6151\n",
      "Epoch 7/10, Batch 52/111, Training Loss: 0.6117\n",
      "Epoch 7/10, Batch 53/111, Training Loss: 0.5286\n",
      "Epoch 7/10, Batch 54/111, Training Loss: 0.5147\n",
      "Epoch 7/10, Batch 55/111, Training Loss: 0.6180\n",
      "Epoch 7/10, Batch 56/111, Training Loss: 0.5687\n",
      "Epoch 7/10, Batch 57/111, Training Loss: 0.6090\n",
      "Epoch 7/10, Batch 58/111, Training Loss: 0.6453\n",
      "Epoch 7/10, Batch 59/111, Training Loss: 0.5842\n",
      "Epoch 7/10, Batch 60/111, Training Loss: 0.5665\n",
      "Epoch 7/10, Batch 61/111, Training Loss: 0.5719\n",
      "Epoch 7/10, Batch 62/111, Training Loss: 0.5925\n",
      "Epoch 7/10, Batch 63/111, Training Loss: 0.6078\n",
      "Epoch 7/10, Batch 64/111, Training Loss: 0.5579\n",
      "Epoch 7/10, Batch 65/111, Training Loss: 0.5893\n",
      "Epoch 7/10, Batch 66/111, Training Loss: 0.4935\n",
      "Epoch 7/10, Batch 67/111, Training Loss: 0.5452\n",
      "Epoch 7/10, Batch 68/111, Training Loss: 0.6023\n",
      "Epoch 7/10, Batch 69/111, Training Loss: 0.7443\n",
      "Epoch 7/10, Batch 70/111, Training Loss: 0.5434\n",
      "Epoch 7/10, Batch 71/111, Training Loss: 0.4782\n",
      "Epoch 7/10, Batch 72/111, Training Loss: 0.6197\n",
      "Epoch 7/10, Batch 73/111, Training Loss: 0.4842\n",
      "Epoch 7/10, Batch 74/111, Training Loss: 0.6012\n",
      "Epoch 7/10, Batch 75/111, Training Loss: 0.6175\n",
      "Epoch 7/10, Batch 76/111, Training Loss: 0.5663\n",
      "Epoch 7/10, Batch 77/111, Training Loss: 0.5268\n",
      "Epoch 7/10, Batch 78/111, Training Loss: 0.5693\n",
      "Epoch 7/10, Batch 79/111, Training Loss: 0.5747\n",
      "Epoch 7/10, Batch 80/111, Training Loss: 0.5770\n",
      "Epoch 7/10, Batch 81/111, Training Loss: 0.5291\n",
      "Epoch 7/10, Batch 82/111, Training Loss: 0.5624\n",
      "Epoch 7/10, Batch 83/111, Training Loss: 0.5109\n",
      "Epoch 7/10, Batch 84/111, Training Loss: 0.5474\n",
      "Epoch 7/10, Batch 85/111, Training Loss: 0.6893\n",
      "Epoch 7/10, Batch 86/111, Training Loss: 0.5463\n",
      "Epoch 7/10, Batch 87/111, Training Loss: 0.5451\n",
      "Epoch 7/10, Batch 88/111, Training Loss: 0.5992\n",
      "Epoch 7/10, Batch 89/111, Training Loss: 0.5840\n",
      "Epoch 7/10, Batch 90/111, Training Loss: 0.7361\n",
      "Epoch 7/10, Batch 91/111, Training Loss: 0.5596\n",
      "Epoch 7/10, Batch 92/111, Training Loss: 0.5768\n",
      "Epoch 7/10, Batch 93/111, Training Loss: 0.6553\n",
      "Epoch 7/10, Batch 94/111, Training Loss: 0.5996\n",
      "Epoch 7/10, Batch 95/111, Training Loss: 0.6709\n",
      "Epoch 7/10, Batch 96/111, Training Loss: 0.6109\n",
      "Epoch 7/10, Batch 97/111, Training Loss: 0.4578\n",
      "Epoch 7/10, Batch 98/111, Training Loss: 0.6301\n",
      "Epoch 7/10, Batch 99/111, Training Loss: 0.5289\n",
      "Epoch 7/10, Batch 100/111, Training Loss: 0.5835\n",
      "Epoch 7/10, Batch 101/111, Training Loss: 0.5638\n",
      "Epoch 7/10, Batch 102/111, Training Loss: 0.5935\n",
      "Epoch 7/10, Batch 103/111, Training Loss: 0.4431\n",
      "Epoch 7/10, Batch 104/111, Training Loss: 0.5797\n",
      "Epoch 7/10, Batch 105/111, Training Loss: 0.5996\n",
      "Epoch 7/10, Batch 106/111, Training Loss: 0.5702\n",
      "Epoch 7/10, Batch 107/111, Training Loss: 0.5433\n",
      "Epoch 7/10, Batch 108/111, Training Loss: 0.6330\n",
      "Epoch 7/10, Batch 109/111, Training Loss: 0.5076\n",
      "Epoch 7/10, Batch 110/111, Training Loss: 0.5255\n",
      "Epoch 7/10, Batch 111/111, Training Loss: 0.6145\n",
      "Epoch 7/10, Training Loss: 0.5834, Validation Loss: 0.5985, Validation Accuracy: 0.7351\n",
      "Epoch 8/10, Batch 1/111, Training Loss: 0.5137\n",
      "Epoch 8/10, Batch 2/111, Training Loss: 0.5267\n",
      "Epoch 8/10, Batch 3/111, Training Loss: 0.4727\n",
      "Epoch 8/10, Batch 4/111, Training Loss: 0.5008\n",
      "Epoch 8/10, Batch 5/111, Training Loss: 0.5098\n",
      "Epoch 8/10, Batch 6/111, Training Loss: 0.5336\n",
      "Epoch 8/10, Batch 7/111, Training Loss: 0.5662\n",
      "Epoch 8/10, Batch 8/111, Training Loss: 0.4843\n",
      "Epoch 8/10, Batch 9/111, Training Loss: 0.5892\n",
      "Epoch 8/10, Batch 10/111, Training Loss: 0.4634\n",
      "Epoch 8/10, Batch 11/111, Training Loss: 0.5048\n",
      "Epoch 8/10, Batch 12/111, Training Loss: 0.5640\n",
      "Epoch 8/10, Batch 13/111, Training Loss: 0.6607\n",
      "Epoch 8/10, Batch 14/111, Training Loss: 0.6435\n",
      "Epoch 8/10, Batch 15/111, Training Loss: 0.5782\n",
      "Epoch 8/10, Batch 16/111, Training Loss: 0.6266\n",
      "Epoch 8/10, Batch 17/111, Training Loss: 0.5480\n",
      "Epoch 8/10, Batch 18/111, Training Loss: 0.5352\n",
      "Epoch 8/10, Batch 19/111, Training Loss: 0.6043\n",
      "Epoch 8/10, Batch 20/111, Training Loss: 0.5101\n",
      "Epoch 8/10, Batch 21/111, Training Loss: 0.5830\n",
      "Epoch 8/10, Batch 22/111, Training Loss: 0.5916\n",
      "Epoch 8/10, Batch 23/111, Training Loss: 0.6598\n",
      "Epoch 8/10, Batch 24/111, Training Loss: 0.6025\n",
      "Epoch 8/10, Batch 25/111, Training Loss: 0.5458\n",
      "Epoch 8/10, Batch 26/111, Training Loss: 0.5595\n",
      "Epoch 8/10, Batch 27/111, Training Loss: 0.5456\n",
      "Epoch 8/10, Batch 28/111, Training Loss: 0.4805\n",
      "Epoch 8/10, Batch 29/111, Training Loss: 0.7247\n",
      "Epoch 8/10, Batch 30/111, Training Loss: 0.6314\n",
      "Epoch 8/10, Batch 31/111, Training Loss: 0.6357\n",
      "Epoch 8/10, Batch 32/111, Training Loss: 0.6646\n",
      "Epoch 8/10, Batch 33/111, Training Loss: 0.5217\n",
      "Epoch 8/10, Batch 34/111, Training Loss: 0.5974\n",
      "Epoch 8/10, Batch 35/111, Training Loss: 0.5836\n",
      "Epoch 8/10, Batch 36/111, Training Loss: 0.5341\n",
      "Epoch 8/10, Batch 37/111, Training Loss: 0.5533\n",
      "Epoch 8/10, Batch 38/111, Training Loss: 0.6043\n",
      "Epoch 8/10, Batch 39/111, Training Loss: 0.5972\n",
      "Epoch 8/10, Batch 40/111, Training Loss: 0.5602\n",
      "Epoch 8/10, Batch 41/111, Training Loss: 0.5859\n",
      "Epoch 8/10, Batch 42/111, Training Loss: 0.6229\n",
      "Epoch 8/10, Batch 43/111, Training Loss: 0.5575\n",
      "Epoch 8/10, Batch 44/111, Training Loss: 0.5921\n",
      "Epoch 8/10, Batch 45/111, Training Loss: 0.6104\n",
      "Epoch 8/10, Batch 46/111, Training Loss: 0.6873\n",
      "Epoch 8/10, Batch 47/111, Training Loss: 0.4850\n",
      "Epoch 8/10, Batch 48/111, Training Loss: 0.5451\n",
      "Epoch 8/10, Batch 49/111, Training Loss: 0.6234\n",
      "Epoch 8/10, Batch 50/111, Training Loss: 0.5142\n",
      "Epoch 8/10, Batch 51/111, Training Loss: 0.5671\n",
      "Epoch 8/10, Batch 52/111, Training Loss: 0.4933\n",
      "Epoch 8/10, Batch 53/111, Training Loss: 0.4845\n",
      "Epoch 8/10, Batch 54/111, Training Loss: 0.5513\n",
      "Epoch 8/10, Batch 55/111, Training Loss: 0.5959\n",
      "Epoch 8/10, Batch 56/111, Training Loss: 0.5574\n",
      "Epoch 8/10, Batch 57/111, Training Loss: 0.4495\n",
      "Epoch 8/10, Batch 58/111, Training Loss: 0.5148\n",
      "Epoch 8/10, Batch 59/111, Training Loss: 0.5542\n",
      "Epoch 8/10, Batch 60/111, Training Loss: 0.4264\n",
      "Epoch 8/10, Batch 61/111, Training Loss: 0.6598\n",
      "Epoch 8/10, Batch 62/111, Training Loss: 0.5510\n",
      "Epoch 8/10, Batch 63/111, Training Loss: 0.5185\n",
      "Epoch 8/10, Batch 64/111, Training Loss: 0.4453\n",
      "Epoch 8/10, Batch 65/111, Training Loss: 0.5696\n",
      "Epoch 8/10, Batch 66/111, Training Loss: 0.4825\n",
      "Epoch 8/10, Batch 67/111, Training Loss: 0.5290\n",
      "Epoch 8/10, Batch 68/111, Training Loss: 0.5136\n",
      "Epoch 8/10, Batch 69/111, Training Loss: 0.6707\n",
      "Epoch 8/10, Batch 70/111, Training Loss: 0.5499\n",
      "Epoch 8/10, Batch 71/111, Training Loss: 0.4892\n",
      "Epoch 8/10, Batch 72/111, Training Loss: 0.4774\n",
      "Epoch 8/10, Batch 73/111, Training Loss: 0.5236\n",
      "Epoch 8/10, Batch 74/111, Training Loss: 0.4763\n",
      "Epoch 8/10, Batch 75/111, Training Loss: 0.5102\n",
      "Epoch 8/10, Batch 76/111, Training Loss: 0.4803\n",
      "Epoch 8/10, Batch 77/111, Training Loss: 0.5219\n",
      "Epoch 8/10, Batch 78/111, Training Loss: 0.5127\n",
      "Epoch 8/10, Batch 79/111, Training Loss: 0.6435\n",
      "Epoch 8/10, Batch 80/111, Training Loss: 0.5972\n",
      "Epoch 8/10, Batch 81/111, Training Loss: 0.6231\n",
      "Epoch 8/10, Batch 82/111, Training Loss: 0.5493\n",
      "Epoch 8/10, Batch 83/111, Training Loss: 0.6200\n",
      "Epoch 8/10, Batch 84/111, Training Loss: 0.6152\n",
      "Epoch 8/10, Batch 85/111, Training Loss: 0.5831\n",
      "Epoch 8/10, Batch 86/111, Training Loss: 0.5041\n",
      "Epoch 8/10, Batch 87/111, Training Loss: 0.5669\n",
      "Epoch 8/10, Batch 88/111, Training Loss: 0.4809\n",
      "Epoch 8/10, Batch 89/111, Training Loss: 0.5481\n",
      "Epoch 8/10, Batch 90/111, Training Loss: 0.5457\n",
      "Epoch 8/10, Batch 91/111, Training Loss: 0.6565\n",
      "Epoch 8/10, Batch 92/111, Training Loss: 0.5066\n",
      "Epoch 8/10, Batch 93/111, Training Loss: 0.5128\n",
      "Epoch 8/10, Batch 94/111, Training Loss: 0.4628\n",
      "Epoch 8/10, Batch 95/111, Training Loss: 0.5488\n",
      "Epoch 8/10, Batch 96/111, Training Loss: 0.6168\n",
      "Epoch 8/10, Batch 97/111, Training Loss: 0.5546\n",
      "Epoch 8/10, Batch 98/111, Training Loss: 0.5720\n",
      "Epoch 8/10, Batch 99/111, Training Loss: 0.5960\n",
      "Epoch 8/10, Batch 100/111, Training Loss: 0.5461\n",
      "Epoch 8/10, Batch 101/111, Training Loss: 0.5238\n",
      "Epoch 8/10, Batch 102/111, Training Loss: 0.4691\n",
      "Epoch 8/10, Batch 103/111, Training Loss: 0.5708\n",
      "Epoch 8/10, Batch 104/111, Training Loss: 0.5414\n",
      "Epoch 8/10, Batch 105/111, Training Loss: 0.5793\n",
      "Epoch 8/10, Batch 106/111, Training Loss: 0.5235\n",
      "Epoch 8/10, Batch 107/111, Training Loss: 0.5570\n",
      "Epoch 8/10, Batch 108/111, Training Loss: 0.5036\n",
      "Epoch 8/10, Batch 109/111, Training Loss: 0.6468\n",
      "Epoch 8/10, Batch 110/111, Training Loss: 0.5074\n",
      "Epoch 8/10, Batch 111/111, Training Loss: 0.7074\n",
      "Epoch 8/10, Training Loss: 0.5567, Validation Loss: 0.5784, Validation Accuracy: 0.7486\n",
      "Epoch 9/10, Batch 1/111, Training Loss: 0.5634\n",
      "Epoch 9/10, Batch 2/111, Training Loss: 0.5236\n",
      "Epoch 9/10, Batch 3/111, Training Loss: 0.5616\n",
      "Epoch 9/10, Batch 4/111, Training Loss: 0.6063\n",
      "Epoch 9/10, Batch 5/111, Training Loss: 0.4329\n",
      "Epoch 9/10, Batch 6/111, Training Loss: 0.5333\n",
      "Epoch 9/10, Batch 7/111, Training Loss: 0.4992\n",
      "Epoch 9/10, Batch 8/111, Training Loss: 0.5806\n",
      "Epoch 9/10, Batch 9/111, Training Loss: 0.5575\n",
      "Epoch 9/10, Batch 10/111, Training Loss: 0.4443\n",
      "Epoch 9/10, Batch 11/111, Training Loss: 0.5137\n",
      "Epoch 9/10, Batch 12/111, Training Loss: 0.5167\n",
      "Epoch 9/10, Batch 13/111, Training Loss: 0.5113\n",
      "Epoch 9/10, Batch 14/111, Training Loss: 0.4998\n",
      "Epoch 9/10, Batch 15/111, Training Loss: 0.5123\n",
      "Epoch 9/10, Batch 16/111, Training Loss: 0.4892\n",
      "Epoch 9/10, Batch 17/111, Training Loss: 0.5312\n",
      "Epoch 9/10, Batch 18/111, Training Loss: 0.4932\n",
      "Epoch 9/10, Batch 19/111, Training Loss: 0.5373\n",
      "Epoch 9/10, Batch 20/111, Training Loss: 0.4601\n",
      "Epoch 9/10, Batch 21/111, Training Loss: 0.3980\n",
      "Epoch 9/10, Batch 22/111, Training Loss: 0.5164\n",
      "Epoch 9/10, Batch 23/111, Training Loss: 0.6405\n",
      "Epoch 9/10, Batch 24/111, Training Loss: 0.5375\n",
      "Epoch 9/10, Batch 25/111, Training Loss: 0.4496\n",
      "Epoch 9/10, Batch 26/111, Training Loss: 0.5361\n",
      "Epoch 9/10, Batch 27/111, Training Loss: 0.4796\n",
      "Epoch 9/10, Batch 28/111, Training Loss: 0.4271\n",
      "Epoch 9/10, Batch 29/111, Training Loss: 0.4660\n",
      "Epoch 9/10, Batch 30/111, Training Loss: 0.4259\n",
      "Epoch 9/10, Batch 31/111, Training Loss: 0.4872\n",
      "Epoch 9/10, Batch 32/111, Training Loss: 0.4333\n",
      "Epoch 9/10, Batch 33/111, Training Loss: 0.6786\n",
      "Epoch 9/10, Batch 34/111, Training Loss: 0.5850\n",
      "Epoch 9/10, Batch 35/111, Training Loss: 0.5899\n",
      "Epoch 9/10, Batch 36/111, Training Loss: 0.5587\n",
      "Epoch 9/10, Batch 37/111, Training Loss: 0.5837\n",
      "Epoch 9/10, Batch 38/111, Training Loss: 0.5201\n",
      "Epoch 9/10, Batch 39/111, Training Loss: 0.5433\n",
      "Epoch 9/10, Batch 40/111, Training Loss: 0.6176\n",
      "Epoch 9/10, Batch 41/111, Training Loss: 0.6465\n",
      "Epoch 9/10, Batch 42/111, Training Loss: 0.5061\n",
      "Epoch 9/10, Batch 43/111, Training Loss: 0.5067\n",
      "Epoch 9/10, Batch 44/111, Training Loss: 0.5442\n",
      "Epoch 9/10, Batch 45/111, Training Loss: 0.5223\n",
      "Epoch 9/10, Batch 46/111, Training Loss: 0.5894\n",
      "Epoch 9/10, Batch 47/111, Training Loss: 0.6622\n",
      "Epoch 9/10, Batch 48/111, Training Loss: 0.6077\n",
      "Epoch 9/10, Batch 49/111, Training Loss: 0.5240\n",
      "Epoch 9/10, Batch 50/111, Training Loss: 0.4417\n",
      "Epoch 9/10, Batch 51/111, Training Loss: 0.5881\n",
      "Epoch 9/10, Batch 52/111, Training Loss: 0.5815\n",
      "Epoch 9/10, Batch 53/111, Training Loss: 0.5947\n",
      "Epoch 9/10, Batch 54/111, Training Loss: 0.5489\n",
      "Epoch 9/10, Batch 55/111, Training Loss: 0.5555\n",
      "Epoch 9/10, Batch 56/111, Training Loss: 0.4493\n",
      "Epoch 9/10, Batch 57/111, Training Loss: 0.5341\n",
      "Epoch 9/10, Batch 58/111, Training Loss: 0.5321\n",
      "Epoch 9/10, Batch 59/111, Training Loss: 0.6624\n",
      "Epoch 9/10, Batch 60/111, Training Loss: 0.4626\n",
      "Epoch 9/10, Batch 61/111, Training Loss: 0.4645\n",
      "Epoch 9/10, Batch 62/111, Training Loss: 0.5773\n",
      "Epoch 9/10, Batch 63/111, Training Loss: 0.5581\n",
      "Epoch 9/10, Batch 64/111, Training Loss: 0.4491\n",
      "Epoch 9/10, Batch 65/111, Training Loss: 0.4808\n",
      "Epoch 9/10, Batch 66/111, Training Loss: 0.6263\n",
      "Epoch 9/10, Batch 67/111, Training Loss: 0.4597\n",
      "Epoch 9/10, Batch 68/111, Training Loss: 0.4570\n",
      "Epoch 9/10, Batch 69/111, Training Loss: 0.5014\n",
      "Epoch 9/10, Batch 70/111, Training Loss: 0.5268\n",
      "Epoch 9/10, Batch 71/111, Training Loss: 0.4914\n",
      "Epoch 9/10, Batch 72/111, Training Loss: 0.4327\n",
      "Epoch 9/10, Batch 73/111, Training Loss: 0.4571\n",
      "Epoch 9/10, Batch 74/111, Training Loss: 0.4834\n",
      "Epoch 9/10, Batch 75/111, Training Loss: 0.4986\n",
      "Epoch 9/10, Batch 76/111, Training Loss: 0.4954\n",
      "Epoch 9/10, Batch 77/111, Training Loss: 0.4286\n",
      "Epoch 9/10, Batch 78/111, Training Loss: 0.6375\n",
      "Epoch 9/10, Batch 79/111, Training Loss: 0.5642\n",
      "Epoch 9/10, Batch 80/111, Training Loss: 0.4806\n",
      "Epoch 9/10, Batch 81/111, Training Loss: 0.4849\n",
      "Epoch 9/10, Batch 82/111, Training Loss: 0.5794\n",
      "Epoch 9/10, Batch 83/111, Training Loss: 0.4239\n",
      "Epoch 9/10, Batch 84/111, Training Loss: 0.6137\n",
      "Epoch 9/10, Batch 85/111, Training Loss: 0.5387\n",
      "Epoch 9/10, Batch 86/111, Training Loss: 0.5394\n",
      "Epoch 9/10, Batch 87/111, Training Loss: 0.4022\n",
      "Epoch 9/10, Batch 88/111, Training Loss: 0.5304\n",
      "Epoch 9/10, Batch 89/111, Training Loss: 0.5766\n",
      "Epoch 9/10, Batch 90/111, Training Loss: 0.4484\n",
      "Epoch 9/10, Batch 91/111, Training Loss: 0.4370\n",
      "Epoch 9/10, Batch 92/111, Training Loss: 0.4682\n",
      "Epoch 9/10, Batch 93/111, Training Loss: 0.4538\n",
      "Epoch 9/10, Batch 94/111, Training Loss: 0.4446\n",
      "Epoch 9/10, Batch 95/111, Training Loss: 0.4587\n",
      "Epoch 9/10, Batch 96/111, Training Loss: 0.5015\n",
      "Epoch 9/10, Batch 97/111, Training Loss: 0.5956\n",
      "Epoch 9/10, Batch 98/111, Training Loss: 0.4809\n",
      "Epoch 9/10, Batch 99/111, Training Loss: 0.4605\n",
      "Epoch 9/10, Batch 100/111, Training Loss: 0.4313\n",
      "Epoch 9/10, Batch 101/111, Training Loss: 0.5333\n",
      "Epoch 9/10, Batch 102/111, Training Loss: 0.5652\n",
      "Epoch 9/10, Batch 103/111, Training Loss: 0.5348\n",
      "Epoch 9/10, Batch 104/111, Training Loss: 0.4495\n",
      "Epoch 9/10, Batch 105/111, Training Loss: 0.4944\n",
      "Epoch 9/10, Batch 106/111, Training Loss: 0.5299\n",
      "Epoch 9/10, Batch 107/111, Training Loss: 0.5556\n",
      "Epoch 9/10, Batch 108/111, Training Loss: 0.3928\n",
      "Epoch 9/10, Batch 109/111, Training Loss: 0.3776\n",
      "Epoch 9/10, Batch 110/111, Training Loss: 0.4321\n",
      "Epoch 9/10, Batch 111/111, Training Loss: 0.5517\n",
      "Epoch 9/10, Training Loss: 0.5158, Validation Loss: 0.5688, Validation Accuracy: 0.7648\n",
      "Epoch 10/10, Batch 1/111, Training Loss: 0.5497\n",
      "Epoch 10/10, Batch 2/111, Training Loss: 0.4870\n",
      "Epoch 10/10, Batch 3/111, Training Loss: 0.4569\n",
      "Epoch 10/10, Batch 4/111, Training Loss: 0.3657\n",
      "Epoch 10/10, Batch 5/111, Training Loss: 0.3995\n",
      "Epoch 10/10, Batch 6/111, Training Loss: 0.5642\n",
      "Epoch 10/10, Batch 7/111, Training Loss: 0.4927\n",
      "Epoch 10/10, Batch 8/111, Training Loss: 0.5563\n",
      "Epoch 10/10, Batch 9/111, Training Loss: 0.4968\n",
      "Epoch 10/10, Batch 10/111, Training Loss: 0.4634\n",
      "Epoch 10/10, Batch 11/111, Training Loss: 0.5340\n",
      "Epoch 10/10, Batch 12/111, Training Loss: 0.5181\n",
      "Epoch 10/10, Batch 13/111, Training Loss: 0.4888\n",
      "Epoch 10/10, Batch 14/111, Training Loss: 0.4598\n",
      "Epoch 10/10, Batch 15/111, Training Loss: 0.4407\n",
      "Epoch 10/10, Batch 16/111, Training Loss: 0.4563\n",
      "Epoch 10/10, Batch 17/111, Training Loss: 0.4004\n",
      "Epoch 10/10, Batch 18/111, Training Loss: 0.4586\n",
      "Epoch 10/10, Batch 19/111, Training Loss: 0.4545\n",
      "Epoch 10/10, Batch 20/111, Training Loss: 0.4997\n",
      "Epoch 10/10, Batch 21/111, Training Loss: 0.4802\n",
      "Epoch 10/10, Batch 22/111, Training Loss: 0.5157\n",
      "Epoch 10/10, Batch 23/111, Training Loss: 0.4885\n",
      "Epoch 10/10, Batch 24/111, Training Loss: 0.7627\n",
      "Epoch 10/10, Batch 25/111, Training Loss: 0.5210\n",
      "Epoch 10/10, Batch 26/111, Training Loss: 0.4303\n",
      "Epoch 10/10, Batch 27/111, Training Loss: 0.4597\n",
      "Epoch 10/10, Batch 28/111, Training Loss: 0.6066\n",
      "Epoch 10/10, Batch 29/111, Training Loss: 0.4209\n",
      "Epoch 10/10, Batch 30/111, Training Loss: 0.5035\n",
      "Epoch 10/10, Batch 31/111, Training Loss: 0.5284\n",
      "Epoch 10/10, Batch 32/111, Training Loss: 0.4176\n",
      "Epoch 10/10, Batch 33/111, Training Loss: 0.4438\n",
      "Epoch 10/10, Batch 34/111, Training Loss: 0.4514\n",
      "Epoch 10/10, Batch 35/111, Training Loss: 0.5370\n",
      "Epoch 10/10, Batch 36/111, Training Loss: 0.5742\n",
      "Epoch 10/10, Batch 37/111, Training Loss: 0.5352\n",
      "Epoch 10/10, Batch 38/111, Training Loss: 0.5225\n",
      "Epoch 10/10, Batch 39/111, Training Loss: 0.5206\n",
      "Epoch 10/10, Batch 40/111, Training Loss: 0.5006\n",
      "Epoch 10/10, Batch 41/111, Training Loss: 0.5097\n",
      "Epoch 10/10, Batch 42/111, Training Loss: 0.4256\n",
      "Epoch 10/10, Batch 43/111, Training Loss: 0.4724\n",
      "Epoch 10/10, Batch 44/111, Training Loss: 0.5052\n",
      "Epoch 10/10, Batch 45/111, Training Loss: 0.5333\n",
      "Epoch 10/10, Batch 46/111, Training Loss: 0.4040\n",
      "Epoch 10/10, Batch 47/111, Training Loss: 0.4926\n",
      "Epoch 10/10, Batch 48/111, Training Loss: 0.4458\n",
      "Epoch 10/10, Batch 49/111, Training Loss: 0.5220\n",
      "Epoch 10/10, Batch 50/111, Training Loss: 0.4424\n",
      "Epoch 10/10, Batch 51/111, Training Loss: 0.4911\n",
      "Epoch 10/10, Batch 52/111, Training Loss: 0.5253\n",
      "Epoch 10/10, Batch 53/111, Training Loss: 0.4601\n",
      "Epoch 10/10, Batch 54/111, Training Loss: 0.5515\n",
      "Epoch 10/10, Batch 55/111, Training Loss: 0.5394\n",
      "Epoch 10/10, Batch 56/111, Training Loss: 0.5424\n",
      "Epoch 10/10, Batch 57/111, Training Loss: 0.4091\n",
      "Epoch 10/10, Batch 58/111, Training Loss: 0.3774\n",
      "Epoch 10/10, Batch 59/111, Training Loss: 0.5205\n",
      "Epoch 10/10, Batch 60/111, Training Loss: 0.3979\n",
      "Epoch 10/10, Batch 61/111, Training Loss: 0.4158\n",
      "Epoch 10/10, Batch 62/111, Training Loss: 0.5290\n",
      "Epoch 10/10, Batch 63/111, Training Loss: 0.4104\n",
      "Epoch 10/10, Batch 64/111, Training Loss: 0.3692\n",
      "Epoch 10/10, Batch 65/111, Training Loss: 0.5282\n",
      "Epoch 10/10, Batch 66/111, Training Loss: 0.4848\n",
      "Epoch 10/10, Batch 67/111, Training Loss: 0.5311\n",
      "Epoch 10/10, Batch 68/111, Training Loss: 0.5061\n",
      "Epoch 10/10, Batch 69/111, Training Loss: 0.3496\n",
      "Epoch 10/10, Batch 70/111, Training Loss: 0.5168\n",
      "Epoch 10/10, Batch 71/111, Training Loss: 0.4581\n",
      "Epoch 10/10, Batch 72/111, Training Loss: 0.4857\n",
      "Epoch 10/10, Batch 73/111, Training Loss: 0.4839\n",
      "Epoch 10/10, Batch 74/111, Training Loss: 0.4122\n",
      "Epoch 10/10, Batch 75/111, Training Loss: 0.4853\n",
      "Epoch 10/10, Batch 76/111, Training Loss: 0.5135\n",
      "Epoch 10/10, Batch 77/111, Training Loss: 0.5454\n",
      "Epoch 10/10, Batch 78/111, Training Loss: 0.4316\n",
      "Epoch 10/10, Batch 79/111, Training Loss: 0.3862\n",
      "Epoch 10/10, Batch 80/111, Training Loss: 0.5260\n",
      "Epoch 10/10, Batch 81/111, Training Loss: 0.4756\n",
      "Epoch 10/10, Batch 82/111, Training Loss: 0.5904\n",
      "Epoch 10/10, Batch 83/111, Training Loss: 0.5659\n",
      "Epoch 10/10, Batch 84/111, Training Loss: 0.5525\n",
      "Epoch 10/10, Batch 85/111, Training Loss: 0.5032\n",
      "Epoch 10/10, Batch 86/111, Training Loss: 0.4209\n",
      "Epoch 10/10, Batch 87/111, Training Loss: 0.4095\n",
      "Epoch 10/10, Batch 88/111, Training Loss: 0.5741\n",
      "Epoch 10/10, Batch 89/111, Training Loss: 0.5651\n",
      "Epoch 10/10, Batch 90/111, Training Loss: 0.5176\n",
      "Epoch 10/10, Batch 91/111, Training Loss: 0.4925\n",
      "Epoch 10/10, Batch 92/111, Training Loss: 0.5080\n",
      "Epoch 10/10, Batch 93/111, Training Loss: 0.4554\n",
      "Epoch 10/10, Batch 94/111, Training Loss: 0.5682\n",
      "Epoch 10/10, Batch 95/111, Training Loss: 0.4487\n",
      "Epoch 10/10, Batch 96/111, Training Loss: 0.4231\n",
      "Epoch 10/10, Batch 97/111, Training Loss: 0.4950\n",
      "Epoch 10/10, Batch 98/111, Training Loss: 0.4526\n",
      "Epoch 10/10, Batch 99/111, Training Loss: 0.5349\n",
      "Epoch 10/10, Batch 100/111, Training Loss: 0.4788\n",
      "Epoch 10/10, Batch 101/111, Training Loss: 0.4924\n",
      "Epoch 10/10, Batch 102/111, Training Loss: 0.4732\n",
      "Epoch 10/10, Batch 103/111, Training Loss: 0.4143\n",
      "Epoch 10/10, Batch 104/111, Training Loss: 0.4446\n",
      "Epoch 10/10, Batch 105/111, Training Loss: 0.5093\n",
      "Epoch 10/10, Batch 106/111, Training Loss: 0.4583\n",
      "Epoch 10/10, Batch 107/111, Training Loss: 0.6429\n",
      "Epoch 10/10, Batch 108/111, Training Loss: 0.4592\n",
      "Epoch 10/10, Batch 109/111, Training Loss: 0.5036\n",
      "Epoch 10/10, Batch 110/111, Training Loss: 0.4255\n",
      "Epoch 10/10, Batch 111/111, Training Loss: 0.3224\n",
      "Epoch 10/10, Training Loss: 0.4854, Validation Loss: 0.5119, Validation Accuracy: 0.7861\n",
      "Test Loss: 0.5027, Test Accuracy: 0.7884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-27 21:06:00,586] Trial 4 finished with value: 0.511853081882 and parameters: {'batch_size': 128, 'learning_rate': 0.007663800832372427, 'weight_decay': 9.880842173137917e-05}. Best is trial 0 with value: 0.2196991708036512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value (best validation loss): 0.2196991708036512\n",
      "  Params: \n",
      "    batch_size: 128\n",
      "    learning_rate: 0.0005523846634426587\n",
      "    weight_decay: 7.827086112748106e-05\n"
     ]
    }
   ],
   "source": [
    "start_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
