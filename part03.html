<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>part03</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KIDxnIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzQxNDE0MSI+CiAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiA8L2c+CiA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSguNDMgLjA0MDEpIiBmaWxsPSIjZmZmIj4KICA8cGF0aCBkPSJtNC4xNCA4Ljc2cTAuMDY4Mi0xLjg5IDIuNDItMS44OSAxLjE2IDAgMS42OCAwLjQyIDAuNTY3IDAuNDEgMC41NjcgMS4xNnYzLjQ3cTAgMC40NjIgMC41MTQgMC40NjIgMC4xMDMgMCAwLjItMC4wMjMxdjAuNzE0cS0wLjM5OSAwLjEwMy0wLjY1MSAwLjEwMy0wLjQ1MiAwLTAuNjkzLTAuMjItMC4yMzEtMC4yLTAuMjg0LTAuNjYyLTAuOTU2IDAuODcyLTIgMC44NzItMC45MDMgMC0xLjQ3LTAuNDcyLTAuNTI1LTAuNDcyLTAuNTI1LTEuMjYgMC0wLjI2MiAwLjA0NTItMC40NzIgMC4wNTY3LTAuMjIgMC4xMTYtMC4zNzggMC4wNjgyLTAuMTY4IDAuMjMxLTAuMzA0IDAuMTU4LTAuMTQ3IDAuMjYyLTAuMjQyIDAuMTE2LTAuMDkxNCAwLjM2OC0wLjE2OCAwLjI2Mi0wLjA5MTQgMC4zOTktMC4xMjYgMC4xMzYtMC4wNDUyIDAuNDcyLTAuMTAzIDAuMzM2LTAuMDU3OCAwLjUwNC0wLjA3OTggMC4xNTgtMC4wMjMxIDAuNTY3LTAuMDc5OCAwLjU1Ni0wLjA2ODIgMC43NzctMC4yMjEgMC4yMi0wLjE1MiAwLjIyLTAuNDQxdi0wLjI1MnEwLTAuNDMtMC4zNTctMC42NjItMC4zMzYtMC4yMzEtMC45NzYtMC4yMzEtMC42NjIgMC0wLjk5OCAwLjI2Mi0wLjMzNiAwLjI1Mi0wLjM5OSAwLjc5OHptMS44OSAzLjY4cTAuNzg4IDAgMS4yNi0wLjQxIDAuNTA0LTAuNDIgMC41MDQtMC45MDN2LTEuMDVxLTAuMjg0IDAuMTM2LTAuODYxIDAuMjMxLTAuNTY3IDAuMDkxNC0wLjk4NyAwLjE1OC0wLjQyIDAuMDY4Mi0wLjc2NiAwLjMyNi0wLjMzNiAwLjI1Mi0wLjMzNiAwLjcwNHQwLjMwNCAwLjcwNCAwLjg2MSAwLjI1MnoiIHN0cm9rZS13aWR0aD0iMS4wNSIvPgogIDxwYXRoIGQ9Im0xMCA0LjU2aDAuOTQ1djMuMTVxMC42NTEtMC45NzYgMS44OS0wLjk3NiAxLjE2IDAgMS44OSAwLjg0IDAuNjgyIDAuODQgMC42ODIgMi4zMSAwIDEuNDctMC43MDQgMi40Mi0wLjcwNCAwLjg4Mi0xLjg5IDAuODgyLTEuMjYgMC0xLjg5LTEuMDJ2MC43NjZoLTAuODV6bTIuNjIgMy4wNHEtMC43NDYgMC0xLjE2IDAuNjQtMC40NTIgMC42My0wLjQ1MiAxLjY4IDAgMS4wNSAwLjQ1MiAxLjY4dDEuMTYgMC42M3EwLjc3NyAwIDEuMjYtMC42MyAwLjQ5NC0wLjY0IDAuNDk0LTEuNjggMC0xLjA1LTAuNDcyLTEuNjgtMC40NjItMC42NC0xLjI2LTAuNjR6IiBzdHJva2Utd2lkdGg9IjEuMDUiLz4KICA8cGF0aCBkPSJtMi43MyAxNS44IDEzLjYgMC4wMDgxYzAuMDA2OSAwIDAtMi42IDAtMi42IDAtMC4wMDc4LTEuMTUgMC0xLjE1IDAtMC4wMDY5IDAtMC4wMDgzIDEuNS0wLjAwODMgMS41LTJlLTMgLTAuMDAxNC0xMS4zLTAuMDAxNC0xMS4zLTAuMDAxNGwtMC4wMDU5Mi0xLjVjMC0wLjAwNzgtMS4xNyAwLjAwMTMtMS4xNyAwLjAwMTN6IiBzdHJva2Utd2lkdGg9Ii45NzUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                messageStyle: 'none',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Imports">Imports<a class="anchor-link" href="#Imports"></a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">platform</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">kagglehub</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mlflow</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mlflow.pytorch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">notebook_tqdm</span> <span class="c1"># Needed for tqdm in Jupyter Notebook (Certain cell outputs will complain if this is not included)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">ResNet18_Weights</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Hyperparameters-Options">Hyperparameters Options<a class="anchor-link" href="#Hyperparameters-Options"></a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">EXPERIMENT_NAME</span> <span class="o">=</span> <span class="s2">"fire-smoke-detection-resnet-tuning"</span>
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">NUM_TRIALS</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">BATCH_SIZE_OPTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
<span class="n">LEARNING_RATE_OPTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]</span>
<span class="n">WEIGHT_DECAY_OPTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">]</span>
<span class="n">EARLY_STOP_PATIENCE</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Download-Dataset">Download Dataset<a class="anchor-link" href="#Download-Dataset"></a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">DATASET_PATH</span> <span class="o">=</span> <span class="n">kagglehub</span><span class="o">.</span><span class="n">dataset_download</span><span class="p">(</span><span class="s2">"sayedgamal99/smoke-fire-detection-yolo"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Data-Augmentation-Options">Data Augmentation Options<a class="anchor-link" href="#Data-Augmentation-Options"></a></h2><p>Input images are expected to be 224x224</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">TRAIN_TRANSFORM</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
<span class="p">])</span>

<span class="n">EVAL_TRANSFORM</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Construct-Custom-Dataset">Construct Custom Dataset<a class="anchor-link" href="#Construct-Custom-Dataset"></a></h2><p>The original dataset is structure as such:</p>
<ul>
<li>[]  = 'No Smoke and No Fire'</li>
<li>0 = 'Smoke Only'</li>
<li>1 = 'Fire and Smoke'</li>
</ul>
<p>The custom dataset modifies this as such:</p>
<ul>
<li>0 = 'No Smoke and No Fire'</li>
<li>1 = 'Smoke Only'</li>
<li>2 = 'Fire and Smoke'</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images_dir</span><span class="p">,</span> <span class="n">labels_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">images_dir</span> <span class="o">=</span> <span class="n">images_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels_dir</span> <span class="o">=</span> <span class="n">labels_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">image_files</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">images_dir</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">image_files</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">img_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_files</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">images_dir</span><span class="p">,</span> <span class="n">img_name</span><span class="p">)</span>
        <span class="n">label_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_dir</span><span class="p">,</span> <span class="n">img_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">".jpg"</span><span class="p">,</span> <span class="s2">".txt"</span><span class="p">))</span>

        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">label_path</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">label_content</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

        <span class="c1"># 0: none, 1: smoke, 2: fire</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">label_content</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">first_number</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">label_content</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">label</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">first_number</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Training-Code">Training Code<a class="anchor-link" href="#Training-Code"></a></h2><p>The training parameters are provided by the Optuna Trails</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_with_params</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Train the model using the provided parameters and datasets.</span>
<span class="sd">    Returns the best validation loss.</span>
<span class="sd">    """</span>
    
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">]</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">"learning_rate"</span><span class="p">]</span>
    <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">"weight_decay"</span><span class="p">]</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">"num_epochs"</span><span class="p">]</span>
    <span class="n">early_stop_patience</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">"early_stop_patience"</span><span class="p">]</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">DEVICE</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet18_Weights</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">)</span>
    <span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">run_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"bs</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">_lr</span><span class="si">{</span><span class="n">learning_rate</span><span class="si">:</span><span class="s2">.0e</span><span class="si">}</span><span class="s2">_wd</span><span class="si">{</span><span class="n">weight_decay</span><span class="si">:</span><span class="s2">.0e</span><span class="si">}</span><span class="s2">"</span>
    <span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">(</span><span class="n">nested</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">run_name</span><span class="o">=</span><span class="n">run_name</span><span class="p">):</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"learning_rate"</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"weight_decay"</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"batch_size"</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"num_epochs"</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"early_stop_patience"</span><span class="p">,</span> <span class="n">early_stop_patience</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"optimizer"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"scheduler"</span><span class="p">,</span> <span class="n">scheduler</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"platform"</span><span class="p">,</span> <span class="n">platform</span><span class="o">.</span><span class="n">platform</span><span class="p">())</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">"python_version"</span><span class="p">,</span> <span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">())</span>

        <span class="n">best_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">'inf'</span><span class="p">)</span>
        <span class="n">best_model_state</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">best_epoch</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">epochs_no_improve</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"batch_training_loss"</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">+</span> <span class="n">batch_idx</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s2">, Batch </span><span class="si">{</span><span class="n">batch_idx</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">, Training Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">val_correct</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">val_total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">val_inputs</span><span class="p">,</span> <span class="n">val_labels</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
                    <span class="n">val_inputs</span><span class="p">,</span> <span class="n">val_labels</span> <span class="o">=</span> <span class="n">val_inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">val_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">val_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">val_inputs</span><span class="p">)</span>
                    <span class="n">v_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">val_outputs</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">)</span>
                    <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">v_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">_</span><span class="p">,</span> <span class="n">val_predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">val_outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">val_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">val_predicted</span> <span class="o">==</span> <span class="n">val_labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">val_total</span> <span class="o">+=</span> <span class="n">val_labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="n">avg_train_loss</span> <span class="o">=</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
            <span class="n">avg_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span>
            <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_val_loss</span><span class="p">)</span>
            <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">val_correct</span> <span class="o">/</span> <span class="n">val_total</span> <span class="k">if</span> <span class="n">val_total</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"training_loss"</span><span class="p">,</span> <span class="n">avg_train_loss</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"validation_loss"</span><span class="p">,</span> <span class="n">avg_val_loss</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"validation_accuracy"</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s2">, Training Loss: </span><span class="si">{</span><span class="n">avg_train_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Validation Loss: </span><span class="si">{</span><span class="n">avg_val_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Validation Accuracy: </span><span class="si">{</span><span class="n">val_accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">"epoch"</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
                <span class="s2">"model_state_dict"</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s2">"optimizer_state_dict"</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s2">"scheduler_state_dict"</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s2">"best_val_loss"</span><span class="p">:</span> <span class="n">best_val_loss</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"checkpoint_epoch_</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">.pth"</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">avg_val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
                <span class="n">epochs_no_improve</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">best_model_state</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">avg_val_loss</span>
                <span class="n">best_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epochs_no_improve</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">epochs_no_improve</span> <span class="o">&gt;=</span> <span class="n">early_stop_patience</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">avg_val_loss</span><span class="p">)</span>
            <span class="n">best_val_accuracy</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">val_losses</span><span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"best_val_accuracy"</span><span class="p">,</span> <span class="n">best_val_accuracy</span><span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"learning_rate"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'lr'</span><span class="p">],</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">best_model_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_model_state</span><span class="p">)</span>
            <span class="n">best_checkpoint</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">"epoch"</span><span class="p">:</span> <span class="n">best_epoch</span><span class="p">,</span>
                <span class="s2">"model_state_dict"</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s2">"optimizer_state_dict"</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s2">"scheduler_state_dict"</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s2">"best_val_loss"</span><span class="p">:</span> <span class="n">best_val_loss</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">best_checkpoint</span><span class="p">,</span> <span class="s2">"best_model.pth"</span><span class="p">)</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="s2">"best_model.pth"</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">"best_model.pth"</span><span class="p">)</span>

        <span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">test_correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">test_total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">test_inputs</span><span class="p">,</span> <span class="n">test_labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
                <span class="n">test_inputs</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">test_inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">test_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_inputs</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
                <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">test_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">test_labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">test_total</span> <span class="o">+=</span> <span class="n">test_labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">avg_test_loss</span> <span class="o">=</span> <span class="n">test_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
        <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test_correct</span> <span class="o">/</span> <span class="n">test_total</span> <span class="k">if</span> <span class="n">test_total</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"test_loss"</span><span class="p">,</span> <span class="n">avg_test_loss</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"test_accuracy"</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test Loss: </span><span class="si">{</span><span class="n">avg_test_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="c1"># Plot the confusion matrix</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">test_inputs</span><span class="p">,</span> <span class="n">test_labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
                <span class="n">test_inputs</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">test_inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">test_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_inputs</span><span class="p">)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">y_true</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">test_labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
                <span class="n">y_pred</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
        <span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">)</span>
        <span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Test Confusion Matrix"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">"test_confusion_matrix.png"</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="s2">"test_confusion_matrix.png"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">"test_confusion_matrix.png"</span><span class="p">)</span>

        <span class="c1"># Calculate average training loss per epoch</span>
        <span class="n">num_batches_per_epoch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
        <span class="n">train_loss_per_epoch</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_losses</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_batches_per_epoch</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_batches_per_epoch</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">val_losses</span><span class="p">))</span>
        <span class="p">]</span>

        <span class="c1"># Plot training loss per epoch</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loss_per_epoch</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">train_loss_per_epoch</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">"o"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training Loss"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Loss"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Training Loss per Epoch"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">"training_loss_per_epoch.png"</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="s2">"training_loss_per_epoch.png"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">"training_loss_per_epoch.png"</span><span class="p">)</span>

        <span class="c1"># Plot validation loss per epoch</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_losses</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">"o"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"orange"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Validation Loss"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Loss"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Validation Loss per Epoch"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">"validation_loss_per_epoch.png"</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="s2">"validation_loss_per_epoch.png"</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">"validation_loss_per_epoch.png"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">best_val_loss</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Experiment-and-Trails-Set-Up">Experiment and Trails Set-Up<a class="anchor-link" href="#Experiment-and-Trails-Set-Up"></a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"batch_size"</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">"batch_size"</span><span class="p">,</span> <span class="n">BATCH_SIZE_OPTIONS</span><span class="p">),</span>
        <span class="s2">"learning_rate"</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">"learning_rate"</span><span class="p">,</span> <span class="n">LEARNING_RATE_OPTIONS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">LEARNING_RATE_OPTIONS</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
        <span class="s2">"weight_decay"</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">"weight_decay"</span><span class="p">,</span> <span class="n">WEIGHT_DECAY_OPTIONS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">WEIGHT_DECAY_OPTIONS</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s2">"num_epochs"</span><span class="p">:</span> <span class="n">NUM_EPOCHS</span><span class="p">,</span>
        <span class="s2">"early_stop_patience"</span><span class="p">:</span> <span class="n">EARLY_STOP_PATIENCE</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">train_with_params</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">start_experiment</span><span class="p">():</span>
    <span class="c1"># Set seed for reproducibility</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

    <span class="c1"># Load training dataset</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span>
        <span class="n">images_dir</span><span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="s2">"data/train/images"</span><span class="p">),</span>
        <span class="n">labels_dir</span><span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="s2">"data/train/labels"</span><span class="p">),</span>
        <span class="n">transform</span><span class="o">=</span><span class="n">TRAIN_TRANSFORM</span>
    <span class="p">)</span>

    <span class="c1"># Load validation dataset</span>
    <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span>
        <span class="n">images_dir</span><span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="s2">"data/val/images"</span><span class="p">),</span>
        <span class="n">labels_dir</span><span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="s2">"data/val/labels"</span><span class="p">),</span>
        <span class="n">transform</span><span class="o">=</span><span class="n">EVAL_TRANSFORM</span>
    <span class="p">)</span>

    <span class="c1"># Load test dataset</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span>
        <span class="n">images_dir</span><span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="s2">"data/test/images"</span><span class="p">),</span>
        <span class="n">labels_dir</span><span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="s2">"data/test/labels"</span><span class="p">),</span>
        <span class="n">transform</span><span class="o">=</span><span class="n">EVAL_TRANSFORM</span>
    <span class="p">)</span>

    <span class="c1"># Create study</span>
    <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">"minimize"</span><span class="p">,</span> <span class="n">study_name</span><span class="o">=</span><span class="n">EXPERIMENT_NAME</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span><span class="n">EXPERIMENT_NAME</span><span class="p">)</span>
    <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">trial</span><span class="p">:</span> <span class="n">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">),</span>
        <span class="n">n_trials</span><span class="o">=</span><span class="n">NUM_TRIALS</span>
    <span class="p">)</span>

    <span class="c1"># Print best trial</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Best trial:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"  Value (best validation loss): </span><span class="si">{</span><span class="n">study</span><span class="o">.</span><span class="n">best_trial</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"  Params: "</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"    </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="c1"># Log best trial info with MLflow</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">"best_val_loss"</span><span class="p">,</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="sa">f</span><span class="s2">"best_</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Start-the-Experiment">Start the Experiment<a class="anchor-link" href="#Start-the-Experiment"></a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">start_experiment</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-04-27 19:21:15,696] A new study created in memory with name: fire-smoke-detection-resnet-tuning
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/10, Batch 1/111, Training Loss: 1.2017
Epoch 1/10, Batch 2/111, Training Loss: 0.8627
Epoch 1/10, Batch 3/111, Training Loss: 0.8118
Epoch 1/10, Batch 4/111, Training Loss: 0.8121
Epoch 1/10, Batch 5/111, Training Loss: 0.7322
Epoch 1/10, Batch 6/111, Training Loss: 0.5535
Epoch 1/10, Batch 7/111, Training Loss: 0.6809
Epoch 1/10, Batch 8/111, Training Loss: 0.5731
Epoch 1/10, Batch 9/111, Training Loss: 0.5743
Epoch 1/10, Batch 10/111, Training Loss: 0.5422
Epoch 1/10, Batch 11/111, Training Loss: 0.6057
Epoch 1/10, Batch 12/111, Training Loss: 0.6286
Epoch 1/10, Batch 13/111, Training Loss: 0.6150
Epoch 1/10, Batch 14/111, Training Loss: 0.6555
Epoch 1/10, Batch 15/111, Training Loss: 0.6015
Epoch 1/10, Batch 16/111, Training Loss: 0.4681
Epoch 1/10, Batch 17/111, Training Loss: 0.6879
Epoch 1/10, Batch 18/111, Training Loss: 0.5191
Epoch 1/10, Batch 19/111, Training Loss: 0.5891
Epoch 1/10, Batch 20/111, Training Loss: 0.3996
Epoch 1/10, Batch 21/111, Training Loss: 0.6512
Epoch 1/10, Batch 22/111, Training Loss: 0.3615
Epoch 1/10, Batch 23/111, Training Loss: 0.4965
Epoch 1/10, Batch 24/111, Training Loss: 0.4663
Epoch 1/10, Batch 25/111, Training Loss: 0.4839
Epoch 1/10, Batch 26/111, Training Loss: 0.5440
Epoch 1/10, Batch 27/111, Training Loss: 0.3634
Epoch 1/10, Batch 28/111, Training Loss: 0.5123
Epoch 1/10, Batch 29/111, Training Loss: 0.3869
Epoch 1/10, Batch 30/111, Training Loss: 0.5010
Epoch 1/10, Batch 31/111, Training Loss: 0.3897
Epoch 1/10, Batch 32/111, Training Loss: 0.3755
Epoch 1/10, Batch 33/111, Training Loss: 0.5564
Epoch 1/10, Batch 34/111, Training Loss: 0.3908
Epoch 1/10, Batch 35/111, Training Loss: 0.6738
Epoch 1/10, Batch 36/111, Training Loss: 0.3861
Epoch 1/10, Batch 37/111, Training Loss: 0.6428
Epoch 1/10, Batch 38/111, Training Loss: 0.4343
Epoch 1/10, Batch 39/111, Training Loss: 0.4197
Epoch 1/10, Batch 40/111, Training Loss: 0.6556
Epoch 1/10, Batch 41/111, Training Loss: 0.4404
Epoch 1/10, Batch 42/111, Training Loss: 0.3500
Epoch 1/10, Batch 43/111, Training Loss: 0.3940
Epoch 1/10, Batch 44/111, Training Loss: 0.4672
Epoch 1/10, Batch 45/111, Training Loss: 0.4839
Epoch 1/10, Batch 46/111, Training Loss: 0.4095
Epoch 1/10, Batch 47/111, Training Loss: 0.4524
Epoch 1/10, Batch 48/111, Training Loss: 0.3956
Epoch 1/10, Batch 49/111, Training Loss: 0.3423
Epoch 1/10, Batch 50/111, Training Loss: 0.4005
Epoch 1/10, Batch 51/111, Training Loss: 0.3692
Epoch 1/10, Batch 52/111, Training Loss: 0.3914
Epoch 1/10, Batch 53/111, Training Loss: 0.3523
Epoch 1/10, Batch 54/111, Training Loss: 0.4379
Epoch 1/10, Batch 55/111, Training Loss: 0.4090
Epoch 1/10, Batch 56/111, Training Loss: 0.5111
Epoch 1/10, Batch 57/111, Training Loss: 0.4100
Epoch 1/10, Batch 58/111, Training Loss: 0.3614
Epoch 1/10, Batch 59/111, Training Loss: 0.4457
Epoch 1/10, Batch 60/111, Training Loss: 0.4005
Epoch 1/10, Batch 61/111, Training Loss: 0.3183
Epoch 1/10, Batch 62/111, Training Loss: 0.4674
Epoch 1/10, Batch 63/111, Training Loss: 0.4323
Epoch 1/10, Batch 64/111, Training Loss: 0.5229
Epoch 1/10, Batch 65/111, Training Loss: 0.3897
Epoch 1/10, Batch 66/111, Training Loss: 0.2452
Epoch 1/10, Batch 67/111, Training Loss: 0.3825
Epoch 1/10, Batch 68/111, Training Loss: 0.3799
Epoch 1/10, Batch 69/111, Training Loss: 0.2588
Epoch 1/10, Batch 70/111, Training Loss: 0.3773
Epoch 1/10, Batch 71/111, Training Loss: 0.3180
Epoch 1/10, Batch 72/111, Training Loss: 0.3289
Epoch 1/10, Batch 73/111, Training Loss: 0.3643
Epoch 1/10, Batch 74/111, Training Loss: 0.4292
Epoch 1/10, Batch 75/111, Training Loss: 0.4024
Epoch 1/10, Batch 76/111, Training Loss: 0.4330
Epoch 1/10, Batch 77/111, Training Loss: 0.3224
Epoch 1/10, Batch 78/111, Training Loss: 0.3958
Epoch 1/10, Batch 79/111, Training Loss: 0.3099
Epoch 1/10, Batch 80/111, Training Loss: 0.3321
Epoch 1/10, Batch 81/111, Training Loss: 0.4588
Epoch 1/10, Batch 82/111, Training Loss: 0.4108
Epoch 1/10, Batch 83/111, Training Loss: 0.3044
Epoch 1/10, Batch 84/111, Training Loss: 0.3698
Epoch 1/10, Batch 85/111, Training Loss: 0.2832
Epoch 1/10, Batch 86/111, Training Loss: 0.3930
Epoch 1/10, Batch 87/111, Training Loss: 0.4793
Epoch 1/10, Batch 88/111, Training Loss: 0.3421
Epoch 1/10, Batch 89/111, Training Loss: 0.5755
Epoch 1/10, Batch 90/111, Training Loss: 0.3339
Epoch 1/10, Batch 91/111, Training Loss: 0.3920
Epoch 1/10, Batch 92/111, Training Loss: 0.3350
Epoch 1/10, Batch 93/111, Training Loss: 0.3148
Epoch 1/10, Batch 94/111, Training Loss: 0.3350
Epoch 1/10, Batch 95/111, Training Loss: 0.3001
Epoch 1/10, Batch 96/111, Training Loss: 0.3046
Epoch 1/10, Batch 97/111, Training Loss: 0.3340
Epoch 1/10, Batch 98/111, Training Loss: 0.3630
Epoch 1/10, Batch 99/111, Training Loss: 0.3847
Epoch 1/10, Batch 100/111, Training Loss: 0.2819
Epoch 1/10, Batch 101/111, Training Loss: 0.2317
Epoch 1/10, Batch 102/111, Training Loss: 0.4920
Epoch 1/10, Batch 103/111, Training Loss: 0.4503
Epoch 1/10, Batch 104/111, Training Loss: 0.2937
Epoch 1/10, Batch 105/111, Training Loss: 0.2815
Epoch 1/10, Batch 106/111, Training Loss: 0.3691
Epoch 1/10, Batch 107/111, Training Loss: 0.3592
Epoch 1/10, Batch 108/111, Training Loss: 0.3324
Epoch 1/10, Batch 109/111, Training Loss: 0.2996
Epoch 1/10, Batch 110/111, Training Loss: 0.2822
Epoch 1/10, Batch 111/111, Training Loss: 0.3221
Epoch 1/10, Training Loss: 0.4455, Validation Loss: 0.4898, Validation Accuracy: 0.8145
Epoch 2/10, Batch 1/111, Training Loss: 0.2949
Epoch 2/10, Batch 2/111, Training Loss: 0.2470
Epoch 2/10, Batch 3/111, Training Loss: 0.2596
Epoch 2/10, Batch 4/111, Training Loss: 0.2496
Epoch 2/10, Batch 5/111, Training Loss: 0.2504
Epoch 2/10, Batch 6/111, Training Loss: 0.2674
Epoch 2/10, Batch 7/111, Training Loss: 0.3215
Epoch 2/10, Batch 8/111, Training Loss: 0.2741
Epoch 2/10, Batch 9/111, Training Loss: 0.2502
Epoch 2/10, Batch 10/111, Training Loss: 0.2327
Epoch 2/10, Batch 11/111, Training Loss: 0.2075
Epoch 2/10, Batch 12/111, Training Loss: 0.3382
Epoch 2/10, Batch 13/111, Training Loss: 0.2148
Epoch 2/10, Batch 14/111, Training Loss: 0.3156
Epoch 2/10, Batch 15/111, Training Loss: 0.3372
Epoch 2/10, Batch 16/111, Training Loss: 0.3367
Epoch 2/10, Batch 17/111, Training Loss: 0.2861
Epoch 2/10, Batch 18/111, Training Loss: 0.2285
Epoch 2/10, Batch 19/111, Training Loss: 0.2114
Epoch 2/10, Batch 20/111, Training Loss: 0.2879
Epoch 2/10, Batch 21/111, Training Loss: 0.3184
Epoch 2/10, Batch 22/111, Training Loss: 0.3237
Epoch 2/10, Batch 23/111, Training Loss: 0.3074
Epoch 2/10, Batch 24/111, Training Loss: 0.2350
Epoch 2/10, Batch 25/111, Training Loss: 0.3397
Epoch 2/10, Batch 26/111, Training Loss: 0.3064
Epoch 2/10, Batch 27/111, Training Loss: 0.2917
Epoch 2/10, Batch 28/111, Training Loss: 0.2539
Epoch 2/10, Batch 29/111, Training Loss: 0.2139
Epoch 2/10, Batch 30/111, Training Loss: 0.2389
Epoch 2/10, Batch 31/111, Training Loss: 0.1948
Epoch 2/10, Batch 32/111, Training Loss: 0.2443
Epoch 2/10, Batch 33/111, Training Loss: 0.2410
Epoch 2/10, Batch 34/111, Training Loss: 0.1842
Epoch 2/10, Batch 35/111, Training Loss: 0.2243
Epoch 2/10, Batch 36/111, Training Loss: 0.4080
Epoch 2/10, Batch 37/111, Training Loss: 0.2789
Epoch 2/10, Batch 38/111, Training Loss: 0.3611
Epoch 2/10, Batch 39/111, Training Loss: 0.4648
Epoch 2/10, Batch 40/111, Training Loss: 0.3512
Epoch 2/10, Batch 41/111, Training Loss: 0.1392
Epoch 2/10, Batch 42/111, Training Loss: 0.2546
Epoch 2/10, Batch 43/111, Training Loss: 0.2976
Epoch 2/10, Batch 44/111, Training Loss: 0.2591
Epoch 2/10, Batch 45/111, Training Loss: 0.3100
Epoch 2/10, Batch 46/111, Training Loss: 0.2024
Epoch 2/10, Batch 47/111, Training Loss: 0.3066
Epoch 2/10, Batch 48/111, Training Loss: 0.2302
Epoch 2/10, Batch 49/111, Training Loss: 0.1688
Epoch 2/10, Batch 50/111, Training Loss: 0.3079
Epoch 2/10, Batch 51/111, Training Loss: 0.2567
Epoch 2/10, Batch 52/111, Training Loss: 0.2982
Epoch 2/10, Batch 53/111, Training Loss: 0.2705
Epoch 2/10, Batch 54/111, Training Loss: 0.3215
Epoch 2/10, Batch 55/111, Training Loss: 0.2054
Epoch 2/10, Batch 56/111, Training Loss: 0.3395
Epoch 2/10, Batch 57/111, Training Loss: 0.2307
Epoch 2/10, Batch 58/111, Training Loss: 0.2584
Epoch 2/10, Batch 59/111, Training Loss: 0.2895
Epoch 2/10, Batch 60/111, Training Loss: 0.2742
Epoch 2/10, Batch 61/111, Training Loss: 0.2649
Epoch 2/10, Batch 62/111, Training Loss: 0.3732
Epoch 2/10, Batch 63/111, Training Loss: 0.3456
Epoch 2/10, Batch 64/111, Training Loss: 0.2607
Epoch 2/10, Batch 65/111, Training Loss: 0.3312
Epoch 2/10, Batch 66/111, Training Loss: 0.2947
Epoch 2/10, Batch 67/111, Training Loss: 0.2000
Epoch 2/10, Batch 68/111, Training Loss: 0.3913
Epoch 2/10, Batch 69/111, Training Loss: 0.2846
Epoch 2/10, Batch 70/111, Training Loss: 0.3729
Epoch 2/10, Batch 71/111, Training Loss: 0.2952
Epoch 2/10, Batch 72/111, Training Loss: 0.3195
Epoch 2/10, Batch 73/111, Training Loss: 0.2633
Epoch 2/10, Batch 74/111, Training Loss: 0.3480
Epoch 2/10, Batch 75/111, Training Loss: 0.3586
Epoch 2/10, Batch 76/111, Training Loss: 0.2177
Epoch 2/10, Batch 77/111, Training Loss: 0.2985
Epoch 2/10, Batch 78/111, Training Loss: 0.3969
Epoch 2/10, Batch 79/111, Training Loss: 0.1781
Epoch 2/10, Batch 80/111, Training Loss: 0.4170
Epoch 2/10, Batch 81/111, Training Loss: 0.4080
Epoch 2/10, Batch 82/111, Training Loss: 0.1772
Epoch 2/10, Batch 83/111, Training Loss: 0.2119
Epoch 2/10, Batch 84/111, Training Loss: 0.2885
Epoch 2/10, Batch 85/111, Training Loss: 0.2024
Epoch 2/10, Batch 86/111, Training Loss: 0.2495
Epoch 2/10, Batch 87/111, Training Loss: 0.3316
Epoch 2/10, Batch 88/111, Training Loss: 0.2844
Epoch 2/10, Batch 89/111, Training Loss: 0.2194
Epoch 2/10, Batch 90/111, Training Loss: 0.1991
Epoch 2/10, Batch 91/111, Training Loss: 0.2369
Epoch 2/10, Batch 92/111, Training Loss: 0.2458
Epoch 2/10, Batch 93/111, Training Loss: 0.2943
Epoch 2/10, Batch 94/111, Training Loss: 0.2803
Epoch 2/10, Batch 95/111, Training Loss: 0.3162
Epoch 2/10, Batch 96/111, Training Loss: 0.3394
Epoch 2/10, Batch 97/111, Training Loss: 0.3220
Epoch 2/10, Batch 98/111, Training Loss: 0.2694
Epoch 2/10, Batch 99/111, Training Loss: 0.2341
Epoch 2/10, Batch 100/111, Training Loss: 0.2523
Epoch 2/10, Batch 101/111, Training Loss: 0.3265
Epoch 2/10, Batch 102/111, Training Loss: 0.3149
Epoch 2/10, Batch 103/111, Training Loss: 0.2486
Epoch 2/10, Batch 104/111, Training Loss: 0.2894
Epoch 2/10, Batch 105/111, Training Loss: 0.1835
Epoch 2/10, Batch 106/111, Training Loss: 0.3024
Epoch 2/10, Batch 107/111, Training Loss: 0.1930
Epoch 2/10, Batch 108/111, Training Loss: 0.2805
Epoch 2/10, Batch 109/111, Training Loss: 0.3260
Epoch 2/10, Batch 110/111, Training Loss: 0.2266
Epoch 2/10, Batch 111/111, Training Loss: 0.1805
Epoch 2/10, Training Loss: 0.2780, Validation Loss: 0.5498, Validation Accuracy: 0.7980
Epoch 3/10, Batch 1/111, Training Loss: 0.1115
Epoch 3/10, Batch 2/111, Training Loss: 0.2208
Epoch 3/10, Batch 3/111, Training Loss: 0.2354
Epoch 3/10, Batch 4/111, Training Loss: 0.3274
Epoch 3/10, Batch 5/111, Training Loss: 0.1873
Epoch 3/10, Batch 6/111, Training Loss: 0.2527
Epoch 3/10, Batch 7/111, Training Loss: 0.1371
Epoch 3/10, Batch 8/111, Training Loss: 0.1474
Epoch 3/10, Batch 9/111, Training Loss: 0.1498
Epoch 3/10, Batch 10/111, Training Loss: 0.2116
Epoch 3/10, Batch 11/111, Training Loss: 0.3081
Epoch 3/10, Batch 12/111, Training Loss: 0.2037
Epoch 3/10, Batch 13/111, Training Loss: 0.2246
Epoch 3/10, Batch 14/111, Training Loss: 0.2822
Epoch 3/10, Batch 15/111, Training Loss: 0.1688
Epoch 3/10, Batch 16/111, Training Loss: 0.1172
Epoch 3/10, Batch 17/111, Training Loss: 0.2116
Epoch 3/10, Batch 18/111, Training Loss: 0.1977
Epoch 3/10, Batch 19/111, Training Loss: 0.1476
Epoch 3/10, Batch 20/111, Training Loss: 0.2815
Epoch 3/10, Batch 21/111, Training Loss: 0.1294
Epoch 3/10, Batch 22/111, Training Loss: 0.1606
Epoch 3/10, Batch 23/111, Training Loss: 0.3213
Epoch 3/10, Batch 24/111, Training Loss: 0.1331
Epoch 3/10, Batch 25/111, Training Loss: 0.2195
Epoch 3/10, Batch 26/111, Training Loss: 0.2462
Epoch 3/10, Batch 27/111, Training Loss: 0.2955
Epoch 3/10, Batch 28/111, Training Loss: 0.2063
Epoch 3/10, Batch 29/111, Training Loss: 0.2222
Epoch 3/10, Batch 30/111, Training Loss: 0.1699
Epoch 3/10, Batch 31/111, Training Loss: 0.1891
Epoch 3/10, Batch 32/111, Training Loss: 0.2109
Epoch 3/10, Batch 33/111, Training Loss: 0.1729
Epoch 3/10, Batch 34/111, Training Loss: 0.2209
Epoch 3/10, Batch 35/111, Training Loss: 0.1566
Epoch 3/10, Batch 36/111, Training Loss: 0.2373
Epoch 3/10, Batch 37/111, Training Loss: 0.2100
Epoch 3/10, Batch 38/111, Training Loss: 0.2012
Epoch 3/10, Batch 39/111, Training Loss: 0.1657
Epoch 3/10, Batch 40/111, Training Loss: 0.1817
Epoch 3/10, Batch 41/111, Training Loss: 0.2761
Epoch 3/10, Batch 42/111, Training Loss: 0.3069
Epoch 3/10, Batch 43/111, Training Loss: 0.2188
Epoch 3/10, Batch 44/111, Training Loss: 0.1782
Epoch 3/10, Batch 45/111, Training Loss: 0.1346
Epoch 3/10, Batch 46/111, Training Loss: 0.1896
Epoch 3/10, Batch 47/111, Training Loss: 0.2221
Epoch 3/10, Batch 48/111, Training Loss: 0.1995
Epoch 3/10, Batch 49/111, Training Loss: 0.1879
Epoch 3/10, Batch 50/111, Training Loss: 0.2093
Epoch 3/10, Batch 51/111, Training Loss: 0.2547
Epoch 3/10, Batch 52/111, Training Loss: 0.1424
Epoch 3/10, Batch 53/111, Training Loss: 0.2147
Epoch 3/10, Batch 54/111, Training Loss: 0.2916
Epoch 3/10, Batch 55/111, Training Loss: 0.2219
Epoch 3/10, Batch 56/111, Training Loss: 0.1698
Epoch 3/10, Batch 57/111, Training Loss: 0.3068
Epoch 3/10, Batch 58/111, Training Loss: 0.1838
Epoch 3/10, Batch 59/111, Training Loss: 0.1937
Epoch 3/10, Batch 60/111, Training Loss: 0.1994
Epoch 3/10, Batch 61/111, Training Loss: 0.2814
Epoch 3/10, Batch 62/111, Training Loss: 0.1703
Epoch 3/10, Batch 63/111, Training Loss: 0.2258
Epoch 3/10, Batch 64/111, Training Loss: 0.1952
Epoch 3/10, Batch 65/111, Training Loss: 0.2021
Epoch 3/10, Batch 66/111, Training Loss: 0.1580
Epoch 3/10, Batch 67/111, Training Loss: 0.2588
Epoch 3/10, Batch 68/111, Training Loss: 0.1913
Epoch 3/10, Batch 69/111, Training Loss: 0.2352
Epoch 3/10, Batch 70/111, Training Loss: 0.2008
Epoch 3/10, Batch 71/111, Training Loss: 0.2366
Epoch 3/10, Batch 72/111, Training Loss: 0.2043
Epoch 3/10, Batch 73/111, Training Loss: 0.2086
Epoch 3/10, Batch 74/111, Training Loss: 0.1395
Epoch 3/10, Batch 75/111, Training Loss: 0.2094
Epoch 3/10, Batch 76/111, Training Loss: 0.2374
Epoch 3/10, Batch 77/111, Training Loss: 0.1640
Epoch 3/10, Batch 78/111, Training Loss: 0.1240
Epoch 3/10, Batch 79/111, Training Loss: 0.1978
Epoch 3/10, Batch 80/111, Training Loss: 0.2286
Epoch 3/10, Batch 81/111, Training Loss: 0.1939
Epoch 3/10, Batch 82/111, Training Loss: 0.3275
Epoch 3/10, Batch 83/111, Training Loss: 0.2361
Epoch 3/10, Batch 84/111, Training Loss: 0.2511
Epoch 3/10, Batch 85/111, Training Loss: 0.2348
Epoch 3/10, Batch 86/111, Training Loss: 0.3136
Epoch 3/10, Batch 87/111, Training Loss: 0.2228
Epoch 3/10, Batch 88/111, Training Loss: 0.2456
Epoch 3/10, Batch 89/111, Training Loss: 0.1220
Epoch 3/10, Batch 90/111, Training Loss: 0.3383
Epoch 3/10, Batch 91/111, Training Loss: 0.1711
Epoch 3/10, Batch 92/111, Training Loss: 0.1765
Epoch 3/10, Batch 93/111, Training Loss: 0.3056
Epoch 3/10, Batch 94/111, Training Loss: 0.1473
Epoch 3/10, Batch 95/111, Training Loss: 0.3232
Epoch 3/10, Batch 96/111, Training Loss: 0.3230
Epoch 3/10, Batch 97/111, Training Loss: 0.1952
Epoch 3/10, Batch 98/111, Training Loss: 0.2772
Epoch 3/10, Batch 99/111, Training Loss: 0.1815
Epoch 3/10, Batch 100/111, Training Loss: 0.3003
Epoch 3/10, Batch 101/111, Training Loss: 0.2113
Epoch 3/10, Batch 102/111, Training Loss: 0.2271
Epoch 3/10, Batch 103/111, Training Loss: 0.2095
Epoch 3/10, Batch 104/111, Training Loss: 0.2585
Epoch 3/10, Batch 105/111, Training Loss: 0.2265
Epoch 3/10, Batch 106/111, Training Loss: 0.1850
Epoch 3/10, Batch 107/111, Training Loss: 0.2997
Epoch 3/10, Batch 108/111, Training Loss: 0.2052
Epoch 3/10, Batch 109/111, Training Loss: 0.2867
Epoch 3/10, Batch 110/111, Training Loss: 0.3108
Epoch 3/10, Batch 111/111, Training Loss: 0.2819
Epoch 3/10, Training Loss: 0.2174, Validation Loss: 0.3628, Validation Accuracy: 0.8548
Epoch 4/10, Batch 1/111, Training Loss: 0.2772
Epoch 4/10, Batch 2/111, Training Loss: 0.2196
Epoch 4/10, Batch 3/111, Training Loss: 0.1717
Epoch 4/10, Batch 4/111, Training Loss: 0.2448
Epoch 4/10, Batch 5/111, Training Loss: 0.3001
Epoch 4/10, Batch 6/111, Training Loss: 0.1547
Epoch 4/10, Batch 7/111, Training Loss: 0.3030
Epoch 4/10, Batch 8/111, Training Loss: 0.1839
Epoch 4/10, Batch 9/111, Training Loss: 0.2502
Epoch 4/10, Batch 10/111, Training Loss: 0.2099
Epoch 4/10, Batch 11/111, Training Loss: 0.1893
Epoch 4/10, Batch 12/111, Training Loss: 0.2112
Epoch 4/10, Batch 13/111, Training Loss: 0.2068
Epoch 4/10, Batch 14/111, Training Loss: 0.1519
Epoch 4/10, Batch 15/111, Training Loss: 0.1926
Epoch 4/10, Batch 16/111, Training Loss: 0.1975
Epoch 4/10, Batch 17/111, Training Loss: 0.2152
Epoch 4/10, Batch 18/111, Training Loss: 0.2483
Epoch 4/10, Batch 19/111, Training Loss: 0.1825
Epoch 4/10, Batch 20/111, Training Loss: 0.1581
Epoch 4/10, Batch 21/111, Training Loss: 0.1881
Epoch 4/10, Batch 22/111, Training Loss: 0.1554
Epoch 4/10, Batch 23/111, Training Loss: 0.1797
Epoch 4/10, Batch 24/111, Training Loss: 0.2431
Epoch 4/10, Batch 25/111, Training Loss: 0.1850
Epoch 4/10, Batch 26/111, Training Loss: 0.2188
Epoch 4/10, Batch 27/111, Training Loss: 0.2356
Epoch 4/10, Batch 28/111, Training Loss: 0.1437
Epoch 4/10, Batch 29/111, Training Loss: 0.2565
Epoch 4/10, Batch 30/111, Training Loss: 0.1280
Epoch 4/10, Batch 31/111, Training Loss: 0.1475
Epoch 4/10, Batch 32/111, Training Loss: 0.1371
Epoch 4/10, Batch 33/111, Training Loss: 0.2457
Epoch 4/10, Batch 34/111, Training Loss: 0.1415
Epoch 4/10, Batch 35/111, Training Loss: 0.0879
Epoch 4/10, Batch 36/111, Training Loss: 0.1947
Epoch 4/10, Batch 37/111, Training Loss: 0.2409
Epoch 4/10, Batch 38/111, Training Loss: 0.1305
Epoch 4/10, Batch 39/111, Training Loss: 0.1879
Epoch 4/10, Batch 40/111, Training Loss: 0.1827
Epoch 4/10, Batch 41/111, Training Loss: 0.1525
Epoch 4/10, Batch 42/111, Training Loss: 0.1635
Epoch 4/10, Batch 43/111, Training Loss: 0.1827
Epoch 4/10, Batch 44/111, Training Loss: 0.2632
Epoch 4/10, Batch 45/111, Training Loss: 0.1912
Epoch 4/10, Batch 46/111, Training Loss: 0.3197
Epoch 4/10, Batch 47/111, Training Loss: 0.2065
Epoch 4/10, Batch 48/111, Training Loss: 0.2077
Epoch 4/10, Batch 49/111, Training Loss: 0.2289
Epoch 4/10, Batch 50/111, Training Loss: 0.1687
Epoch 4/10, Batch 51/111, Training Loss: 0.1559
Epoch 4/10, Batch 52/111, Training Loss: 0.2065
Epoch 4/10, Batch 53/111, Training Loss: 0.1874
Epoch 4/10, Batch 54/111, Training Loss: 0.2708
Epoch 4/10, Batch 55/111, Training Loss: 0.0742
Epoch 4/10, Batch 56/111, Training Loss: 0.1188
Epoch 4/10, Batch 57/111, Training Loss: 0.1506
Epoch 4/10, Batch 58/111, Training Loss: 0.1715
Epoch 4/10, Batch 59/111, Training Loss: 0.3521
Epoch 4/10, Batch 60/111, Training Loss: 0.1709
Epoch 4/10, Batch 61/111, Training Loss: 0.1805
Epoch 4/10, Batch 62/111, Training Loss: 0.1945
Epoch 4/10, Batch 63/111, Training Loss: 0.2070
Epoch 4/10, Batch 64/111, Training Loss: 0.2067
Epoch 4/10, Batch 65/111, Training Loss: 0.2053
Epoch 4/10, Batch 66/111, Training Loss: 0.1857
Epoch 4/10, Batch 67/111, Training Loss: 0.2499
Epoch 4/10, Batch 68/111, Training Loss: 0.1784
Epoch 4/10, Batch 69/111, Training Loss: 0.2136
Epoch 4/10, Batch 70/111, Training Loss: 0.1214
Epoch 4/10, Batch 71/111, Training Loss: 0.2032
Epoch 4/10, Batch 72/111, Training Loss: 0.2778
Epoch 4/10, Batch 73/111, Training Loss: 0.2276
Epoch 4/10, Batch 74/111, Training Loss: 0.2290
Epoch 4/10, Batch 75/111, Training Loss: 0.1916
Epoch 4/10, Batch 76/111, Training Loss: 0.2151
Epoch 4/10, Batch 77/111, Training Loss: 0.1073
Epoch 4/10, Batch 78/111, Training Loss: 0.1743
Epoch 4/10, Batch 79/111, Training Loss: 0.1268
Epoch 4/10, Batch 80/111, Training Loss: 0.2046
Epoch 4/10, Batch 81/111, Training Loss: 0.3224
Epoch 4/10, Batch 82/111, Training Loss: 0.1950
Epoch 4/10, Batch 83/111, Training Loss: 0.1644
Epoch 4/10, Batch 84/111, Training Loss: 0.1625
Epoch 4/10, Batch 85/111, Training Loss: 0.2423
Epoch 4/10, Batch 86/111, Training Loss: 0.1929
Epoch 4/10, Batch 87/111, Training Loss: 0.2059
Epoch 4/10, Batch 88/111, Training Loss: 0.1754
Epoch 4/10, Batch 89/111, Training Loss: 0.2221
Epoch 4/10, Batch 90/111, Training Loss: 0.1396
Epoch 4/10, Batch 91/111, Training Loss: 0.1423
Epoch 4/10, Batch 92/111, Training Loss: 0.1832
Epoch 4/10, Batch 93/111, Training Loss: 0.2981
Epoch 4/10, Batch 94/111, Training Loss: 0.2746
Epoch 4/10, Batch 95/111, Training Loss: 0.1831
Epoch 4/10, Batch 96/111, Training Loss: 0.2211
Epoch 4/10, Batch 97/111, Training Loss: 0.2119
Epoch 4/10, Batch 98/111, Training Loss: 0.1865
Epoch 4/10, Batch 99/111, Training Loss: 0.1492
Epoch 4/10, Batch 100/111, Training Loss: 0.1297
Epoch 4/10, Batch 101/111, Training Loss: 0.2430
Epoch 4/10, Batch 102/111, Training Loss: 0.1685
Epoch 4/10, Batch 103/111, Training Loss: 0.1441
Epoch 4/10, Batch 104/111, Training Loss: 0.1657
Epoch 4/10, Batch 105/111, Training Loss: 0.2143
Epoch 4/10, Batch 106/111, Training Loss: 0.1489
Epoch 4/10, Batch 107/111, Training Loss: 0.1943
Epoch 4/10, Batch 108/111, Training Loss: 0.1009
Epoch 4/10, Batch 109/111, Training Loss: 0.2770
Epoch 4/10, Batch 110/111, Training Loss: 0.2356
Epoch 4/10, Batch 111/111, Training Loss: 0.1236
Epoch 4/10, Training Loss: 0.1960, Validation Loss: 0.3247, Validation Accuracy: 0.8861
Epoch 5/10, Batch 1/111, Training Loss: 0.1354
Epoch 5/10, Batch 2/111, Training Loss: 0.1713
Epoch 5/10, Batch 3/111, Training Loss: 0.1438
Epoch 5/10, Batch 4/111, Training Loss: 0.1771
Epoch 5/10, Batch 5/111, Training Loss: 0.1104
Epoch 5/10, Batch 6/111, Training Loss: 0.1568
Epoch 5/10, Batch 7/111, Training Loss: 0.1413
Epoch 5/10, Batch 8/111, Training Loss: 0.1815
Epoch 5/10, Batch 9/111, Training Loss: 0.1261
Epoch 5/10, Batch 10/111, Training Loss: 0.0961
Epoch 5/10, Batch 11/111, Training Loss: 0.1635
Epoch 5/10, Batch 12/111, Training Loss: 0.1340
Epoch 5/10, Batch 13/111, Training Loss: 0.1200
Epoch 5/10, Batch 14/111, Training Loss: 0.0896
Epoch 5/10, Batch 15/111, Training Loss: 0.1976
Epoch 5/10, Batch 16/111, Training Loss: 0.2378
Epoch 5/10, Batch 17/111, Training Loss: 0.1759
Epoch 5/10, Batch 18/111, Training Loss: 0.1390
Epoch 5/10, Batch 19/111, Training Loss: 0.1338
Epoch 5/10, Batch 20/111, Training Loss: 0.1677
Epoch 5/10, Batch 21/111, Training Loss: 0.2190
Epoch 5/10, Batch 22/111, Training Loss: 0.1829
Epoch 5/10, Batch 23/111, Training Loss: 0.1117
Epoch 5/10, Batch 24/111, Training Loss: 0.1818
Epoch 5/10, Batch 25/111, Training Loss: 0.2235
Epoch 5/10, Batch 26/111, Training Loss: 0.1340
Epoch 5/10, Batch 27/111, Training Loss: 0.2358
Epoch 5/10, Batch 28/111, Training Loss: 0.1862
Epoch 5/10, Batch 29/111, Training Loss: 0.1966
Epoch 5/10, Batch 30/111, Training Loss: 0.1871
Epoch 5/10, Batch 31/111, Training Loss: 0.1861
Epoch 5/10, Batch 32/111, Training Loss: 0.1499
Epoch 5/10, Batch 33/111, Training Loss: 0.1167
Epoch 5/10, Batch 34/111, Training Loss: 0.1419
Epoch 5/10, Batch 35/111, Training Loss: 0.0843
Epoch 5/10, Batch 36/111, Training Loss: 0.1649
Epoch 5/10, Batch 37/111, Training Loss: 0.1208
Epoch 5/10, Batch 38/111, Training Loss: 0.2138
Epoch 5/10, Batch 39/111, Training Loss: 0.0805
Epoch 5/10, Batch 40/111, Training Loss: 0.2577
Epoch 5/10, Batch 41/111, Training Loss: 0.2390
Epoch 5/10, Batch 42/111, Training Loss: 0.0803
Epoch 5/10, Batch 43/111, Training Loss: 0.1223
Epoch 5/10, Batch 44/111, Training Loss: 0.1459
Epoch 5/10, Batch 45/111, Training Loss: 0.1535
Epoch 5/10, Batch 46/111, Training Loss: 0.1818
Epoch 5/10, Batch 47/111, Training Loss: 0.1781
Epoch 5/10, Batch 48/111, Training Loss: 0.1179
Epoch 5/10, Batch 49/111, Training Loss: 0.1710
Epoch 5/10, Batch 50/111, Training Loss: 0.1590
Epoch 5/10, Batch 51/111, Training Loss: 0.1280
Epoch 5/10, Batch 52/111, Training Loss: 0.1381
Epoch 5/10, Batch 53/111, Training Loss: 0.1951
Epoch 5/10, Batch 54/111, Training Loss: 0.1731
Epoch 5/10, Batch 55/111, Training Loss: 0.1319
Epoch 5/10, Batch 56/111, Training Loss: 0.3622
Epoch 5/10, Batch 57/111, Training Loss: 0.1496
Epoch 5/10, Batch 58/111, Training Loss: 0.1306
Epoch 5/10, Batch 59/111, Training Loss: 0.1797
Epoch 5/10, Batch 60/111, Training Loss: 0.1795
Epoch 5/10, Batch 61/111, Training Loss: 0.2395
Epoch 5/10, Batch 62/111, Training Loss: 0.3324
Epoch 5/10, Batch 63/111, Training Loss: 0.0883
Epoch 5/10, Batch 64/111, Training Loss: 0.1073
Epoch 5/10, Batch 65/111, Training Loss: 0.1818
Epoch 5/10, Batch 66/111, Training Loss: 0.1239
Epoch 5/10, Batch 67/111, Training Loss: 0.1062
Epoch 5/10, Batch 68/111, Training Loss: 0.2704
Epoch 5/10, Batch 69/111, Training Loss: 0.1489
Epoch 5/10, Batch 70/111, Training Loss: 0.1537
Epoch 5/10, Batch 71/111, Training Loss: 0.1502
Epoch 5/10, Batch 72/111, Training Loss: 0.1947
Epoch 5/10, Batch 73/111, Training Loss: 0.2393
Epoch 5/10, Batch 74/111, Training Loss: 0.1783
Epoch 5/10, Batch 75/111, Training Loss: 0.1857
Epoch 5/10, Batch 76/111, Training Loss: 0.1639
Epoch 5/10, Batch 77/111, Training Loss: 0.1526
Epoch 5/10, Batch 78/111, Training Loss: 0.1839
Epoch 5/10, Batch 79/111, Training Loss: 0.1087
Epoch 5/10, Batch 80/111, Training Loss: 0.1974
Epoch 5/10, Batch 81/111, Training Loss: 0.2261
Epoch 5/10, Batch 82/111, Training Loss: 0.1608
Epoch 5/10, Batch 83/111, Training Loss: 0.1169
Epoch 5/10, Batch 84/111, Training Loss: 0.0829
Epoch 5/10, Batch 85/111, Training Loss: 0.1889
Epoch 5/10, Batch 86/111, Training Loss: 0.1339
Epoch 5/10, Batch 87/111, Training Loss: 0.1673
Epoch 5/10, Batch 88/111, Training Loss: 0.1374
Epoch 5/10, Batch 89/111, Training Loss: 0.1112
Epoch 5/10, Batch 90/111, Training Loss: 0.1720
Epoch 5/10, Batch 91/111, Training Loss: 0.1571
Epoch 5/10, Batch 92/111, Training Loss: 0.1849
Epoch 5/10, Batch 93/111, Training Loss: 0.2235
Epoch 5/10, Batch 94/111, Training Loss: 0.2258
Epoch 5/10, Batch 95/111, Training Loss: 0.2058
Epoch 5/10, Batch 96/111, Training Loss: 0.1775
Epoch 5/10, Batch 97/111, Training Loss: 0.2159
Epoch 5/10, Batch 98/111, Training Loss: 0.1136
Epoch 5/10, Batch 99/111, Training Loss: 0.1717
Epoch 5/10, Batch 100/111, Training Loss: 0.2197
Epoch 5/10, Batch 101/111, Training Loss: 0.1187
Epoch 5/10, Batch 102/111, Training Loss: 0.1342
Epoch 5/10, Batch 103/111, Training Loss: 0.1646
Epoch 5/10, Batch 104/111, Training Loss: 0.1875
Epoch 5/10, Batch 105/111, Training Loss: 0.1824
Epoch 5/10, Batch 106/111, Training Loss: 0.2419
Epoch 5/10, Batch 107/111, Training Loss: 0.2538
Epoch 5/10, Batch 108/111, Training Loss: 0.2630
Epoch 5/10, Batch 109/111, Training Loss: 0.1729
Epoch 5/10, Batch 110/111, Training Loss: 0.1813
Epoch 5/10, Batch 111/111, Training Loss: 0.2491
Epoch 5/10, Training Loss: 0.1688, Validation Loss: 0.3499, Validation Accuracy: 0.8706
Epoch 6/10, Batch 1/111, Training Loss: 0.1340
Epoch 6/10, Batch 2/111, Training Loss: 0.1736
Epoch 6/10, Batch 3/111, Training Loss: 0.1136
Epoch 6/10, Batch 4/111, Training Loss: 0.1569
Epoch 6/10, Batch 5/111, Training Loss: 0.0874
Epoch 6/10, Batch 6/111, Training Loss: 0.2171
Epoch 6/10, Batch 7/111, Training Loss: 0.1379
Epoch 6/10, Batch 8/111, Training Loss: 0.1497
Epoch 6/10, Batch 9/111, Training Loss: 0.1289
Epoch 6/10, Batch 10/111, Training Loss: 0.1072
Epoch 6/10, Batch 11/111, Training Loss: 0.0941
Epoch 6/10, Batch 12/111, Training Loss: 0.0926
Epoch 6/10, Batch 13/111, Training Loss: 0.1566
Epoch 6/10, Batch 14/111, Training Loss: 0.1262
Epoch 6/10, Batch 15/111, Training Loss: 0.0911
Epoch 6/10, Batch 16/111, Training Loss: 0.1370
Epoch 6/10, Batch 17/111, Training Loss: 0.2119
Epoch 6/10, Batch 18/111, Training Loss: 0.1938
Epoch 6/10, Batch 19/111, Training Loss: 0.1868
Epoch 6/10, Batch 20/111, Training Loss: 0.1466
Epoch 6/10, Batch 21/111, Training Loss: 0.1411
Epoch 6/10, Batch 22/111, Training Loss: 0.1912
Epoch 6/10, Batch 23/111, Training Loss: 0.1151
Epoch 6/10, Batch 24/111, Training Loss: 0.1167
Epoch 6/10, Batch 25/111, Training Loss: 0.1601
Epoch 6/10, Batch 26/111, Training Loss: 0.1550
Epoch 6/10, Batch 27/111, Training Loss: 0.1139
Epoch 6/10, Batch 28/111, Training Loss: 0.1695
Epoch 6/10, Batch 29/111, Training Loss: 0.1046
Epoch 6/10, Batch 30/111, Training Loss: 0.0990
Epoch 6/10, Batch 31/111, Training Loss: 0.1478
Epoch 6/10, Batch 32/111, Training Loss: 0.1504
Epoch 6/10, Batch 33/111, Training Loss: 0.1175
Epoch 6/10, Batch 34/111, Training Loss: 0.0896
Epoch 6/10, Batch 35/111, Training Loss: 0.0772
Epoch 6/10, Batch 36/111, Training Loss: 0.1265
Epoch 6/10, Batch 37/111, Training Loss: 0.2545
Epoch 6/10, Batch 38/111, Training Loss: 0.1295
Epoch 6/10, Batch 39/111, Training Loss: 0.1789
Epoch 6/10, Batch 40/111, Training Loss: 0.1197
Epoch 6/10, Batch 41/111, Training Loss: 0.1016
Epoch 6/10, Batch 42/111, Training Loss: 0.1220
Epoch 6/10, Batch 43/111, Training Loss: 0.1939
Epoch 6/10, Batch 44/111, Training Loss: 0.1904
Epoch 6/10, Batch 45/111, Training Loss: 0.1566
Epoch 6/10, Batch 46/111, Training Loss: 0.1637
Epoch 6/10, Batch 47/111, Training Loss: 0.1858
Epoch 6/10, Batch 48/111, Training Loss: 0.1721
Epoch 6/10, Batch 49/111, Training Loss: 0.1478
Epoch 6/10, Batch 50/111, Training Loss: 0.0937
Epoch 6/10, Batch 51/111, Training Loss: 0.1627
Epoch 6/10, Batch 52/111, Training Loss: 0.1093
Epoch 6/10, Batch 53/111, Training Loss: 0.1431
Epoch 6/10, Batch 54/111, Training Loss: 0.1363
Epoch 6/10, Batch 55/111, Training Loss: 0.1349
Epoch 6/10, Batch 56/111, Training Loss: 0.1248
Epoch 6/10, Batch 57/111, Training Loss: 0.1837
Epoch 6/10, Batch 58/111, Training Loss: 0.2511
Epoch 6/10, Batch 59/111, Training Loss: 0.0592
Epoch 6/10, Batch 60/111, Training Loss: 0.1503
Epoch 6/10, Batch 61/111, Training Loss: 0.1456
Epoch 6/10, Batch 62/111, Training Loss: 0.1271
Epoch 6/10, Batch 63/111, Training Loss: 0.0906
Epoch 6/10, Batch 64/111, Training Loss: 0.2128
Epoch 6/10, Batch 65/111, Training Loss: 0.2016
Epoch 6/10, Batch 66/111, Training Loss: 0.1737
Epoch 6/10, Batch 67/111, Training Loss: 0.1454
Epoch 6/10, Batch 68/111, Training Loss: 0.1397
Epoch 6/10, Batch 69/111, Training Loss: 0.1305
Epoch 6/10, Batch 70/111, Training Loss: 0.1747
Epoch 6/10, Batch 71/111, Training Loss: 0.1880
Epoch 6/10, Batch 72/111, Training Loss: 0.1241
Epoch 6/10, Batch 73/111, Training Loss: 0.1807
Epoch 6/10, Batch 74/111, Training Loss: 0.1317
Epoch 6/10, Batch 75/111, Training Loss: 0.1871
Epoch 6/10, Batch 76/111, Training Loss: 0.1758
Epoch 6/10, Batch 77/111, Training Loss: 0.1538
Epoch 6/10, Batch 78/111, Training Loss: 0.2994
Epoch 6/10, Batch 79/111, Training Loss: 0.0773
Epoch 6/10, Batch 80/111, Training Loss: 0.1275
Epoch 6/10, Batch 81/111, Training Loss: 0.1008
Epoch 6/10, Batch 82/111, Training Loss: 0.1544
Epoch 6/10, Batch 83/111, Training Loss: 0.3111
Epoch 6/10, Batch 84/111, Training Loss: 0.1488
Epoch 6/10, Batch 85/111, Training Loss: 0.1772
Epoch 6/10, Batch 86/111, Training Loss: 0.2705
Epoch 6/10, Batch 87/111, Training Loss: 0.0776
Epoch 6/10, Batch 88/111, Training Loss: 0.1542
Epoch 6/10, Batch 89/111, Training Loss: 0.1510
Epoch 6/10, Batch 90/111, Training Loss: 0.0800
Epoch 6/10, Batch 91/111, Training Loss: 0.1142
Epoch 6/10, Batch 92/111, Training Loss: 0.2377
Epoch 6/10, Batch 93/111, Training Loss: 0.2184
Epoch 6/10, Batch 94/111, Training Loss: 0.1569
Epoch 6/10, Batch 95/111, Training Loss: 0.0865
Epoch 6/10, Batch 96/111, Training Loss: 0.2340
Epoch 6/10, Batch 97/111, Training Loss: 0.2061
Epoch 6/10, Batch 98/111, Training Loss: 0.1720
Epoch 6/10, Batch 99/111, Training Loss: 0.1793
Epoch 6/10, Batch 100/111, Training Loss: 0.1826
Epoch 6/10, Batch 101/111, Training Loss: 0.2345
Epoch 6/10, Batch 102/111, Training Loss: 0.1274
Epoch 6/10, Batch 103/111, Training Loss: 0.1407
Epoch 6/10, Batch 104/111, Training Loss: 0.1089
Epoch 6/10, Batch 105/111, Training Loss: 0.1516
Epoch 6/10, Batch 106/111, Training Loss: 0.1973
Epoch 6/10, Batch 107/111, Training Loss: 0.1350
Epoch 6/10, Batch 108/111, Training Loss: 0.1293
Epoch 6/10, Batch 109/111, Training Loss: 0.1354
Epoch 6/10, Batch 110/111, Training Loss: 0.1475
Epoch 6/10, Batch 111/111, Training Loss: 0.1773
Epoch 6/10, Training Loss: 0.1520, Validation Loss: 0.4720, Validation Accuracy: 0.8293
Epoch 7/10, Batch 1/111, Training Loss: 0.0710
Epoch 7/10, Batch 2/111, Training Loss: 0.0760
Epoch 7/10, Batch 3/111, Training Loss: 0.1256
Epoch 7/10, Batch 4/111, Training Loss: 0.1533
Epoch 7/10, Batch 5/111, Training Loss: 0.1143
Epoch 7/10, Batch 6/111, Training Loss: 0.1185
Epoch 7/10, Batch 7/111, Training Loss: 0.1247
Epoch 7/10, Batch 8/111, Training Loss: 0.1718
Epoch 7/10, Batch 9/111, Training Loss: 0.1298
Epoch 7/10, Batch 10/111, Training Loss: 0.1736
Epoch 7/10, Batch 11/111, Training Loss: 0.0941
Epoch 7/10, Batch 12/111, Training Loss: 0.0802
Epoch 7/10, Batch 13/111, Training Loss: 0.1050
Epoch 7/10, Batch 14/111, Training Loss: 0.1256
Epoch 7/10, Batch 15/111, Training Loss: 0.0906
Epoch 7/10, Batch 16/111, Training Loss: 0.1496
Epoch 7/10, Batch 17/111, Training Loss: 0.1184
Epoch 7/10, Batch 18/111, Training Loss: 0.0786
Epoch 7/10, Batch 19/111, Training Loss: 0.1276
Epoch 7/10, Batch 20/111, Training Loss: 0.0707
Epoch 7/10, Batch 21/111, Training Loss: 0.1187
Epoch 7/10, Batch 22/111, Training Loss: 0.1404
Epoch 7/10, Batch 23/111, Training Loss: 0.0798
Epoch 7/10, Batch 24/111, Training Loss: 0.1123
Epoch 7/10, Batch 25/111, Training Loss: 0.0982
Epoch 7/10, Batch 26/111, Training Loss: 0.1233
Epoch 7/10, Batch 27/111, Training Loss: 0.0932
Epoch 7/10, Batch 28/111, Training Loss: 0.1022
Epoch 7/10, Batch 29/111, Training Loss: 0.1191
Epoch 7/10, Batch 30/111, Training Loss: 0.0650
Epoch 7/10, Batch 31/111, Training Loss: 0.1274
Epoch 7/10, Batch 32/111, Training Loss: 0.1198
Epoch 7/10, Batch 33/111, Training Loss: 0.1141
Epoch 7/10, Batch 34/111, Training Loss: 0.1134
Epoch 7/10, Batch 35/111, Training Loss: 0.0971
Epoch 7/10, Batch 36/111, Training Loss: 0.1056
Epoch 7/10, Batch 37/111, Training Loss: 0.0907
Epoch 7/10, Batch 38/111, Training Loss: 0.1033
Epoch 7/10, Batch 39/111, Training Loss: 0.1398
Epoch 7/10, Batch 40/111, Training Loss: 0.0764
Epoch 7/10, Batch 41/111, Training Loss: 0.0825
Epoch 7/10, Batch 42/111, Training Loss: 0.0749
Epoch 7/10, Batch 43/111, Training Loss: 0.1304
Epoch 7/10, Batch 44/111, Training Loss: 0.0794
Epoch 7/10, Batch 45/111, Training Loss: 0.0486
Epoch 7/10, Batch 46/111, Training Loss: 0.1026
Epoch 7/10, Batch 47/111, Training Loss: 0.0890
Epoch 7/10, Batch 48/111, Training Loss: 0.0957
Epoch 7/10, Batch 49/111, Training Loss: 0.1308
Epoch 7/10, Batch 50/111, Training Loss: 0.1089
Epoch 7/10, Batch 51/111, Training Loss: 0.0734
Epoch 7/10, Batch 52/111, Training Loss: 0.0401
Epoch 7/10, Batch 53/111, Training Loss: 0.0809
Epoch 7/10, Batch 54/111, Training Loss: 0.1549
Epoch 7/10, Batch 55/111, Training Loss: 0.1197
Epoch 7/10, Batch 56/111, Training Loss: 0.0881
Epoch 7/10, Batch 57/111, Training Loss: 0.0819
Epoch 7/10, Batch 58/111, Training Loss: 0.0747
Epoch 7/10, Batch 59/111, Training Loss: 0.1156
Epoch 7/10, Batch 60/111, Training Loss: 0.0878
Epoch 7/10, Batch 61/111, Training Loss: 0.1111
Epoch 7/10, Batch 62/111, Training Loss: 0.1079
Epoch 7/10, Batch 63/111, Training Loss: 0.0668
Epoch 7/10, Batch 64/111, Training Loss: 0.0578
Epoch 7/10, Batch 65/111, Training Loss: 0.0677
Epoch 7/10, Batch 66/111, Training Loss: 0.0803
Epoch 7/10, Batch 67/111, Training Loss: 0.0670
Epoch 7/10, Batch 68/111, Training Loss: 0.0586
Epoch 7/10, Batch 69/111, Training Loss: 0.0901
Epoch 7/10, Batch 70/111, Training Loss: 0.0772
Epoch 7/10, Batch 71/111, Training Loss: 0.0688
Epoch 7/10, Batch 72/111, Training Loss: 0.0971
Epoch 7/10, Batch 73/111, Training Loss: 0.0758
Epoch 7/10, Batch 74/111, Training Loss: 0.1285
Epoch 7/10, Batch 75/111, Training Loss: 0.1033
Epoch 7/10, Batch 76/111, Training Loss: 0.1043
Epoch 7/10, Batch 77/111, Training Loss: 0.0747
Epoch 7/10, Batch 78/111, Training Loss: 0.0905
Epoch 7/10, Batch 79/111, Training Loss: 0.2111
Epoch 7/10, Batch 80/111, Training Loss: 0.0798
Epoch 7/10, Batch 81/111, Training Loss: 0.0775
Epoch 7/10, Batch 82/111, Training Loss: 0.0763
Epoch 7/10, Batch 83/111, Training Loss: 0.0727
Epoch 7/10, Batch 84/111, Training Loss: 0.0641
Epoch 7/10, Batch 85/111, Training Loss: 0.0773
Epoch 7/10, Batch 86/111, Training Loss: 0.0797
Epoch 7/10, Batch 87/111, Training Loss: 0.0929
Epoch 7/10, Batch 88/111, Training Loss: 0.1360
Epoch 7/10, Batch 89/111, Training Loss: 0.0495
Epoch 7/10, Batch 90/111, Training Loss: 0.1064
Epoch 7/10, Batch 91/111, Training Loss: 0.1179
Epoch 7/10, Batch 92/111, Training Loss: 0.0203
Epoch 7/10, Batch 93/111, Training Loss: 0.0505
Epoch 7/10, Batch 94/111, Training Loss: 0.0921
Epoch 7/10, Batch 95/111, Training Loss: 0.0906
Epoch 7/10, Batch 96/111, Training Loss: 0.1047
Epoch 7/10, Batch 97/111, Training Loss: 0.0710
Epoch 7/10, Batch 98/111, Training Loss: 0.0731
Epoch 7/10, Batch 99/111, Training Loss: 0.0926
Epoch 7/10, Batch 100/111, Training Loss: 0.0503
Epoch 7/10, Batch 101/111, Training Loss: 0.0978
Epoch 7/10, Batch 102/111, Training Loss: 0.0819
Epoch 7/10, Batch 103/111, Training Loss: 0.0570
Epoch 7/10, Batch 104/111, Training Loss: 0.0455
Epoch 7/10, Batch 105/111, Training Loss: 0.0652
Epoch 7/10, Batch 106/111, Training Loss: 0.0504
Epoch 7/10, Batch 107/111, Training Loss: 0.0756
Epoch 7/10, Batch 108/111, Training Loss: 0.1028
Epoch 7/10, Batch 109/111, Training Loss: 0.0596
Epoch 7/10, Batch 110/111, Training Loss: 0.0857
Epoch 7/10, Batch 111/111, Training Loss: 0.0633
Epoch 7/10, Training Loss: 0.0946, Validation Loss: 0.2197, Validation Accuracy: 0.9135
Epoch 8/10, Batch 1/111, Training Loss: 0.0701
Epoch 8/10, Batch 2/111, Training Loss: 0.0615
Epoch 8/10, Batch 3/111, Training Loss: 0.0446
Epoch 8/10, Batch 4/111, Training Loss: 0.0656
Epoch 8/10, Batch 5/111, Training Loss: 0.0535
Epoch 8/10, Batch 6/111, Training Loss: 0.0846
Epoch 8/10, Batch 7/111, Training Loss: 0.0611
Epoch 8/10, Batch 8/111, Training Loss: 0.0875
Epoch 8/10, Batch 9/111, Training Loss: 0.0404
Epoch 8/10, Batch 10/111, Training Loss: 0.0826
Epoch 8/10, Batch 11/111, Training Loss: 0.0512
Epoch 8/10, Batch 12/111, Training Loss: 0.0699
Epoch 8/10, Batch 13/111, Training Loss: 0.0380
Epoch 8/10, Batch 14/111, Training Loss: 0.0750
Epoch 8/10, Batch 15/111, Training Loss: 0.0621
Epoch 8/10, Batch 16/111, Training Loss: 0.0693
Epoch 8/10, Batch 17/111, Training Loss: 0.0595
Epoch 8/10, Batch 18/111, Training Loss: 0.0709
Epoch 8/10, Batch 19/111, Training Loss: 0.0539
Epoch 8/10, Batch 20/111, Training Loss: 0.0585
Epoch 8/10, Batch 21/111, Training Loss: 0.0406
Epoch 8/10, Batch 22/111, Training Loss: 0.0585
Epoch 8/10, Batch 23/111, Training Loss: 0.0849
Epoch 8/10, Batch 24/111, Training Loss: 0.1144
Epoch 8/10, Batch 25/111, Training Loss: 0.0783
Epoch 8/10, Batch 26/111, Training Loss: 0.0506
Epoch 8/10, Batch 27/111, Training Loss: 0.0701
Epoch 8/10, Batch 28/111, Training Loss: 0.0417
Epoch 8/10, Batch 29/111, Training Loss: 0.0831
Epoch 8/10, Batch 30/111, Training Loss: 0.0812
Epoch 8/10, Batch 31/111, Training Loss: 0.0398
Epoch 8/10, Batch 32/111, Training Loss: 0.1081
Epoch 8/10, Batch 33/111, Training Loss: 0.0410
Epoch 8/10, Batch 34/111, Training Loss: 0.1611
Epoch 8/10, Batch 35/111, Training Loss: 0.0521
Epoch 8/10, Batch 36/111, Training Loss: 0.0833
Epoch 8/10, Batch 37/111, Training Loss: 0.0613
Epoch 8/10, Batch 38/111, Training Loss: 0.0459
Epoch 8/10, Batch 39/111, Training Loss: 0.0731
Epoch 8/10, Batch 40/111, Training Loss: 0.0382
Epoch 8/10, Batch 41/111, Training Loss: 0.0497
Epoch 8/10, Batch 42/111, Training Loss: 0.0332
Epoch 8/10, Batch 43/111, Training Loss: 0.0689
Epoch 8/10, Batch 44/111, Training Loss: 0.0415
Epoch 8/10, Batch 45/111, Training Loss: 0.0495
Epoch 8/10, Batch 46/111, Training Loss: 0.0401
Epoch 8/10, Batch 47/111, Training Loss: 0.0451
Epoch 8/10, Batch 48/111, Training Loss: 0.0530
Epoch 8/10, Batch 49/111, Training Loss: 0.0645
Epoch 8/10, Batch 50/111, Training Loss: 0.0786
Epoch 8/10, Batch 51/111, Training Loss: 0.1112
Epoch 8/10, Batch 52/111, Training Loss: 0.0810
Epoch 8/10, Batch 53/111, Training Loss: 0.0367
Epoch 8/10, Batch 54/111, Training Loss: 0.0682
Epoch 8/10, Batch 55/111, Training Loss: 0.0603
Epoch 8/10, Batch 56/111, Training Loss: 0.0493
Epoch 8/10, Batch 57/111, Training Loss: 0.0668
Epoch 8/10, Batch 58/111, Training Loss: 0.1140
Epoch 8/10, Batch 59/111, Training Loss: 0.0433
Epoch 8/10, Batch 60/111, Training Loss: 0.0475
Epoch 8/10, Batch 61/111, Training Loss: 0.0484
Epoch 8/10, Batch 62/111, Training Loss: 0.0678
Epoch 8/10, Batch 63/111, Training Loss: 0.0770
Epoch 8/10, Batch 64/111, Training Loss: 0.0769
Epoch 8/10, Batch 65/111, Training Loss: 0.0401
Epoch 8/10, Batch 66/111, Training Loss: 0.0742
Epoch 8/10, Batch 67/111, Training Loss: 0.0298
Epoch 8/10, Batch 68/111, Training Loss: 0.0566
Epoch 8/10, Batch 69/111, Training Loss: 0.1266
Epoch 8/10, Batch 70/111, Training Loss: 0.0533
Epoch 8/10, Batch 71/111, Training Loss: 0.0392
Epoch 8/10, Batch 72/111, Training Loss: 0.1235
Epoch 8/10, Batch 73/111, Training Loss: 0.0637
Epoch 8/10, Batch 74/111, Training Loss: 0.0619
Epoch 8/10, Batch 75/111, Training Loss: 0.0595
Epoch 8/10, Batch 76/111, Training Loss: 0.1082
Epoch 8/10, Batch 77/111, Training Loss: 0.0603
Epoch 8/10, Batch 78/111, Training Loss: 0.0804
Epoch 8/10, Batch 79/111, Training Loss: 0.0626
Epoch 8/10, Batch 80/111, Training Loss: 0.0678
Epoch 8/10, Batch 81/111, Training Loss: 0.0793
Epoch 8/10, Batch 82/111, Training Loss: 0.0632
Epoch 8/10, Batch 83/111, Training Loss: 0.0855
Epoch 8/10, Batch 84/111, Training Loss: 0.0687
Epoch 8/10, Batch 85/111, Training Loss: 0.0349
Epoch 8/10, Batch 86/111, Training Loss: 0.0348
Epoch 8/10, Batch 87/111, Training Loss: 0.0870
Epoch 8/10, Batch 88/111, Training Loss: 0.0354
Epoch 8/10, Batch 89/111, Training Loss: 0.0857
Epoch 8/10, Batch 90/111, Training Loss: 0.0609
Epoch 8/10, Batch 91/111, Training Loss: 0.0466
Epoch 8/10, Batch 92/111, Training Loss: 0.0691
Epoch 8/10, Batch 93/111, Training Loss: 0.0898
Epoch 8/10, Batch 94/111, Training Loss: 0.0563
Epoch 8/10, Batch 95/111, Training Loss: 0.0861
Epoch 8/10, Batch 96/111, Training Loss: 0.0982
Epoch 8/10, Batch 97/111, Training Loss: 0.0660
Epoch 8/10, Batch 98/111, Training Loss: 0.0428
Epoch 8/10, Batch 99/111, Training Loss: 0.0291
Epoch 8/10, Batch 100/111, Training Loss: 0.0512
Epoch 8/10, Batch 101/111, Training Loss: 0.0573
Epoch 8/10, Batch 102/111, Training Loss: 0.0391
Epoch 8/10, Batch 103/111, Training Loss: 0.0671
Epoch 8/10, Batch 104/111, Training Loss: 0.0637
Epoch 8/10, Batch 105/111, Training Loss: 0.0424
Epoch 8/10, Batch 106/111, Training Loss: 0.0404
Epoch 8/10, Batch 107/111, Training Loss: 0.0394
Epoch 8/10, Batch 108/111, Training Loss: 0.0732
Epoch 8/10, Batch 109/111, Training Loss: 0.0696
Epoch 8/10, Batch 110/111, Training Loss: 0.0735
Epoch 8/10, Batch 111/111, Training Loss: 0.0494
Epoch 8/10, Training Loss: 0.0643, Validation Loss: 0.2411, Validation Accuracy: 0.9132
Epoch 9/10, Batch 1/111, Training Loss: 0.0386
Epoch 9/10, Batch 2/111, Training Loss: 0.0378
Epoch 9/10, Batch 3/111, Training Loss: 0.0784
Epoch 9/10, Batch 4/111, Training Loss: 0.0273
Epoch 9/10, Batch 5/111, Training Loss: 0.0434
Epoch 9/10, Batch 6/111, Training Loss: 0.0334
Epoch 9/10, Batch 7/111, Training Loss: 0.0603
Epoch 9/10, Batch 8/111, Training Loss: 0.0392
Epoch 9/10, Batch 9/111, Training Loss: 0.0672
Epoch 9/10, Batch 10/111, Training Loss: 0.0649
Epoch 9/10, Batch 11/111, Training Loss: 0.0384
Epoch 9/10, Batch 12/111, Training Loss: 0.0629
Epoch 9/10, Batch 13/111, Training Loss: 0.0479
Epoch 9/10, Batch 14/111, Training Loss: 0.0261
Epoch 9/10, Batch 15/111, Training Loss: 0.0432
Epoch 9/10, Batch 16/111, Training Loss: 0.0543
Epoch 9/10, Batch 17/111, Training Loss: 0.0236
Epoch 9/10, Batch 18/111, Training Loss: 0.0513
Epoch 9/10, Batch 19/111, Training Loss: 0.0599
Epoch 9/10, Batch 20/111, Training Loss: 0.0614
Epoch 9/10, Batch 21/111, Training Loss: 0.0604
Epoch 9/10, Batch 22/111, Training Loss: 0.0262
Epoch 9/10, Batch 23/111, Training Loss: 0.0512
Epoch 9/10, Batch 24/111, Training Loss: 0.0274
Epoch 9/10, Batch 25/111, Training Loss: 0.0599
Epoch 9/10, Batch 26/111, Training Loss: 0.0878
Epoch 9/10, Batch 27/111, Training Loss: 0.0337
Epoch 9/10, Batch 28/111, Training Loss: 0.0732
Epoch 9/10, Batch 29/111, Training Loss: 0.1064
Epoch 9/10, Batch 30/111, Training Loss: 0.0583
Epoch 9/10, Batch 31/111, Training Loss: 0.0765
Epoch 9/10, Batch 32/111, Training Loss: 0.1093
Epoch 9/10, Batch 33/111, Training Loss: 0.0427
Epoch 9/10, Batch 34/111, Training Loss: 0.0975
Epoch 9/10, Batch 35/111, Training Loss: 0.0698
Epoch 9/10, Batch 36/111, Training Loss: 0.0824
Epoch 9/10, Batch 37/111, Training Loss: 0.0897
Epoch 9/10, Batch 38/111, Training Loss: 0.0451
Epoch 9/10, Batch 39/111, Training Loss: 0.0495
Epoch 9/10, Batch 40/111, Training Loss: 0.0645
Epoch 9/10, Batch 41/111, Training Loss: 0.0423
Epoch 9/10, Batch 42/111, Training Loss: 0.1392
Epoch 9/10, Batch 43/111, Training Loss: 0.0813
Epoch 9/10, Batch 44/111, Training Loss: 0.0431
Epoch 9/10, Batch 45/111, Training Loss: 0.0467
Epoch 9/10, Batch 46/111, Training Loss: 0.0499
Epoch 9/10, Batch 47/111, Training Loss: 0.0781
Epoch 9/10, Batch 48/111, Training Loss: 0.0320
Epoch 9/10, Batch 49/111, Training Loss: 0.0578
Epoch 9/10, Batch 50/111, Training Loss: 0.0707
Epoch 9/10, Batch 51/111, Training Loss: 0.0284
Epoch 9/10, Batch 52/111, Training Loss: 0.0480
Epoch 9/10, Batch 53/111, Training Loss: 0.0282
Epoch 9/10, Batch 54/111, Training Loss: 0.0467
Epoch 9/10, Batch 55/111, Training Loss: 0.0402
Epoch 9/10, Batch 56/111, Training Loss: 0.0634
Epoch 9/10, Batch 57/111, Training Loss: 0.0577
Epoch 9/10, Batch 58/111, Training Loss: 0.0369
Epoch 9/10, Batch 59/111, Training Loss: 0.0429
Epoch 9/10, Batch 60/111, Training Loss: 0.0382
Epoch 9/10, Batch 61/111, Training Loss: 0.0495
Epoch 9/10, Batch 62/111, Training Loss: 0.0552
Epoch 9/10, Batch 63/111, Training Loss: 0.0236
Epoch 9/10, Batch 64/111, Training Loss: 0.0385
Epoch 9/10, Batch 65/111, Training Loss: 0.0387
Epoch 9/10, Batch 66/111, Training Loss: 0.0599
Epoch 9/10, Batch 67/111, Training Loss: 0.0218
Epoch 9/10, Batch 68/111, Training Loss: 0.1506
Epoch 9/10, Batch 69/111, Training Loss: 0.0254
Epoch 9/10, Batch 70/111, Training Loss: 0.0739
Epoch 9/10, Batch 71/111, Training Loss: 0.1095
Epoch 9/10, Batch 72/111, Training Loss: 0.0331
Epoch 9/10, Batch 73/111, Training Loss: 0.0541
Epoch 9/10, Batch 74/111, Training Loss: 0.0484
Epoch 9/10, Batch 75/111, Training Loss: 0.0519
Epoch 9/10, Batch 76/111, Training Loss: 0.0343
Epoch 9/10, Batch 77/111, Training Loss: 0.0212
Epoch 9/10, Batch 78/111, Training Loss: 0.0335
Epoch 9/10, Batch 79/111, Training Loss: 0.0567
Epoch 9/10, Batch 80/111, Training Loss: 0.0202
Epoch 9/10, Batch 81/111, Training Loss: 0.0713
Epoch 9/10, Batch 82/111, Training Loss: 0.0480
Epoch 9/10, Batch 83/111, Training Loss: 0.0634
Epoch 9/10, Batch 84/111, Training Loss: 0.0433
Epoch 9/10, Batch 85/111, Training Loss: 0.0598
Epoch 9/10, Batch 86/111, Training Loss: 0.0140
Epoch 9/10, Batch 87/111, Training Loss: 0.0422
Epoch 9/10, Batch 88/111, Training Loss: 0.0546
Epoch 9/10, Batch 89/111, Training Loss: 0.0475
Epoch 9/10, Batch 90/111, Training Loss: 0.0238
Epoch 9/10, Batch 91/111, Training Loss: 0.0900
Epoch 9/10, Batch 92/111, Training Loss: 0.0552
Epoch 9/10, Batch 93/111, Training Loss: 0.0361
Epoch 9/10, Batch 94/111, Training Loss: 0.0230
Epoch 9/10, Batch 95/111, Training Loss: 0.0459
Epoch 9/10, Batch 96/111, Training Loss: 0.0378
Epoch 9/10, Batch 97/111, Training Loss: 0.0674
Epoch 9/10, Batch 98/111, Training Loss: 0.0251
Epoch 9/10, Batch 99/111, Training Loss: 0.0334
Epoch 9/10, Batch 100/111, Training Loss: 0.0583
Epoch 9/10, Batch 101/111, Training Loss: 0.0607
Epoch 9/10, Batch 102/111, Training Loss: 0.0392
Epoch 9/10, Batch 103/111, Training Loss: 0.0639
Epoch 9/10, Batch 104/111, Training Loss: 0.0419
Epoch 9/10, Batch 105/111, Training Loss: 0.0309
Epoch 9/10, Batch 106/111, Training Loss: 0.0756
Epoch 9/10, Batch 107/111, Training Loss: 0.0505
Epoch 9/10, Batch 108/111, Training Loss: 0.0685
Epoch 9/10, Batch 109/111, Training Loss: 0.0592
Epoch 9/10, Batch 110/111, Training Loss: 0.0317
Epoch 9/10, Batch 111/111, Training Loss: 0.1068
Epoch 9/10, Training Loss: 0.0533, Validation Loss: 0.2603, Validation Accuracy: 0.9100
Epoch 10/10, Batch 1/111, Training Loss: 0.0660
Epoch 10/10, Batch 2/111, Training Loss: 0.0207
Epoch 10/10, Batch 3/111, Training Loss: 0.0353
Epoch 10/10, Batch 4/111, Training Loss: 0.0387
Epoch 10/10, Batch 5/111, Training Loss: 0.0175
Epoch 10/10, Batch 6/111, Training Loss: 0.0212
Epoch 10/10, Batch 7/111, Training Loss: 0.0247
Epoch 10/10, Batch 8/111, Training Loss: 0.0251
Epoch 10/10, Batch 9/111, Training Loss: 0.0265
Epoch 10/10, Batch 10/111, Training Loss: 0.0311
Epoch 10/10, Batch 11/111, Training Loss: 0.0298
Epoch 10/10, Batch 12/111, Training Loss: 0.0388
Epoch 10/10, Batch 13/111, Training Loss: 0.0842
Epoch 10/10, Batch 14/111, Training Loss: 0.0740
Epoch 10/10, Batch 15/111, Training Loss: 0.0509
Epoch 10/10, Batch 16/111, Training Loss: 0.0201
Epoch 10/10, Batch 17/111, Training Loss: 0.0392
Epoch 10/10, Batch 18/111, Training Loss: 0.0432
Epoch 10/10, Batch 19/111, Training Loss: 0.0321
Epoch 10/10, Batch 20/111, Training Loss: 0.0356
Epoch 10/10, Batch 21/111, Training Loss: 0.0437
Epoch 10/10, Batch 22/111, Training Loss: 0.0331
Epoch 10/10, Batch 23/111, Training Loss: 0.0285
Epoch 10/10, Batch 24/111, Training Loss: 0.0175
Epoch 10/10, Batch 25/111, Training Loss: 0.0535
Epoch 10/10, Batch 26/111, Training Loss: 0.0792
Epoch 10/10, Batch 27/111, Training Loss: 0.0503
Epoch 10/10, Batch 28/111, Training Loss: 0.0658
Epoch 10/10, Batch 29/111, Training Loss: 0.0277
Epoch 10/10, Batch 30/111, Training Loss: 0.0856
Epoch 10/10, Batch 31/111, Training Loss: 0.0266
Epoch 10/10, Batch 32/111, Training Loss: 0.0411
Epoch 10/10, Batch 33/111, Training Loss: 0.0399
Epoch 10/10, Batch 34/111, Training Loss: 0.0192
Epoch 10/10, Batch 35/111, Training Loss: 0.0455
Epoch 10/10, Batch 36/111, Training Loss: 0.0577
Epoch 10/10, Batch 37/111, Training Loss: 0.0629
Epoch 10/10, Batch 38/111, Training Loss: 0.0235
Epoch 10/10, Batch 39/111, Training Loss: 0.0152
Epoch 10/10, Batch 40/111, Training Loss: 0.0471
Epoch 10/10, Batch 41/111, Training Loss: 0.0372
Epoch 10/10, Batch 42/111, Training Loss: 0.0271
Epoch 10/10, Batch 43/111, Training Loss: 0.0158
Epoch 10/10, Batch 44/111, Training Loss: 0.0550
Epoch 10/10, Batch 45/111, Training Loss: 0.0139
Epoch 10/10, Batch 46/111, Training Loss: 0.0251
Epoch 10/10, Batch 47/111, Training Loss: 0.0500
Epoch 10/10, Batch 48/111, Training Loss: 0.0628
Epoch 10/10, Batch 49/111, Training Loss: 0.0515
Epoch 10/10, Batch 50/111, Training Loss: 0.0462
Epoch 10/10, Batch 51/111, Training Loss: 0.0406
Epoch 10/10, Batch 52/111, Training Loss: 0.0720
Epoch 10/10, Batch 53/111, Training Loss: 0.0143
Epoch 10/10, Batch 54/111, Training Loss: 0.0261
Epoch 10/10, Batch 55/111, Training Loss: 0.0740
Epoch 10/10, Batch 56/111, Training Loss: 0.0204
Epoch 10/10, Batch 57/111, Training Loss: 0.0290
Epoch 10/10, Batch 58/111, Training Loss: 0.0398
Epoch 10/10, Batch 59/111, Training Loss: 0.0437
Epoch 10/10, Batch 60/111, Training Loss: 0.0473
Epoch 10/10, Batch 61/111, Training Loss: 0.0234
Epoch 10/10, Batch 62/111, Training Loss: 0.0428
Epoch 10/10, Batch 63/111, Training Loss: 0.0375
Epoch 10/10, Batch 64/111, Training Loss: 0.0353
Epoch 10/10, Batch 65/111, Training Loss: 0.0450
Epoch 10/10, Batch 66/111, Training Loss: 0.0136
Epoch 10/10, Batch 67/111, Training Loss: 0.0387
Epoch 10/10, Batch 68/111, Training Loss: 0.0251
Epoch 10/10, Batch 69/111, Training Loss: 0.0616
Epoch 10/10, Batch 70/111, Training Loss: 0.0240
Epoch 10/10, Batch 71/111, Training Loss: 0.0377
Epoch 10/10, Batch 72/111, Training Loss: 0.0305
Epoch 10/10, Batch 73/111, Training Loss: 0.0362
Epoch 10/10, Batch 74/111, Training Loss: 0.0391
Epoch 10/10, Batch 75/111, Training Loss: 0.0656
Epoch 10/10, Batch 76/111, Training Loss: 0.0345
Epoch 10/10, Batch 77/111, Training Loss: 0.0646
Epoch 10/10, Batch 78/111, Training Loss: 0.0275
Epoch 10/10, Batch 79/111, Training Loss: 0.0368
Epoch 10/10, Batch 80/111, Training Loss: 0.0437
Epoch 10/10, Batch 81/111, Training Loss: 0.0711
Epoch 10/10, Batch 82/111, Training Loss: 0.0233
Epoch 10/10, Batch 83/111, Training Loss: 0.0421
Epoch 10/10, Batch 84/111, Training Loss: 0.0400
Epoch 10/10, Batch 85/111, Training Loss: 0.0236
Epoch 10/10, Batch 86/111, Training Loss: 0.0439
Epoch 10/10, Batch 87/111, Training Loss: 0.0182
Epoch 10/10, Batch 88/111, Training Loss: 0.0214
Epoch 10/10, Batch 89/111, Training Loss: 0.0348
Epoch 10/10, Batch 90/111, Training Loss: 0.0532
Epoch 10/10, Batch 91/111, Training Loss: 0.0513
Epoch 10/10, Batch 92/111, Training Loss: 0.0282
Epoch 10/10, Batch 93/111, Training Loss: 0.0278
Epoch 10/10, Batch 94/111, Training Loss: 0.0349
Epoch 10/10, Batch 95/111, Training Loss: 0.0278
Epoch 10/10, Batch 96/111, Training Loss: 0.0223
Epoch 10/10, Batch 97/111, Training Loss: 0.0310
Epoch 10/10, Batch 98/111, Training Loss: 0.0561
Epoch 10/10, Batch 99/111, Training Loss: 0.0359
Epoch 10/10, Batch 100/111, Training Loss: 0.0321
Epoch 10/10, Batch 101/111, Training Loss: 0.0358
Epoch 10/10, Batch 102/111, Training Loss: 0.0926
Epoch 10/10, Batch 103/111, Training Loss: 0.0403
Epoch 10/10, Batch 104/111, Training Loss: 0.0525
Epoch 10/10, Batch 105/111, Training Loss: 0.0595
Epoch 10/10, Batch 106/111, Training Loss: 0.0177
Epoch 10/10, Batch 107/111, Training Loss: 0.0720
Epoch 10/10, Batch 108/111, Training Loss: 0.0332
Epoch 10/10, Batch 109/111, Training Loss: 0.0233
Epoch 10/10, Batch 110/111, Training Loss: 0.0224
Epoch 10/10, Batch 111/111, Training Loss: 0.0136
Epoch 10/10, Training Loss: 0.0392, Validation Loss: 0.2635, Validation Accuracy: 0.9106
Test Loss: 0.2514, Test Accuracy: 0.9222
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-04-27 19:42:16,688] Trial 0 finished with value: 0.2196991708036512 and parameters: {'batch_size': 128, 'learning_rate': 0.0005523846634426587, 'weight_decay': 7.827086112748106e-05}. Best is trial 0 with value: 0.2196991708036512.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/10, Batch 1/883, Training Loss: 1.4911
Epoch 1/10, Batch 2/883, Training Loss: 1.9218
Epoch 1/10, Batch 3/883, Training Loss: 2.5364
Epoch 1/10, Batch 4/883, Training Loss: 4.6537
Epoch 1/10, Batch 5/883, Training Loss: 4.2209
Epoch 1/10, Batch 6/883, Training Loss: 1.5251
Epoch 1/10, Batch 7/883, Training Loss: 1.8556
Epoch 1/10, Batch 8/883, Training Loss: 1.3658
Epoch 1/10, Batch 9/883, Training Loss: 1.2086
Epoch 1/10, Batch 10/883, Training Loss: 1.0472
Epoch 1/10, Batch 11/883, Training Loss: 1.4641
Epoch 1/10, Batch 12/883, Training Loss: 2.1578
Epoch 1/10, Batch 13/883, Training Loss: 0.8838
Epoch 1/10, Batch 14/883, Training Loss: 0.8370
Epoch 1/10, Batch 15/883, Training Loss: 1.1974
Epoch 1/10, Batch 16/883, Training Loss: 1.2286
Epoch 1/10, Batch 17/883, Training Loss: 0.9854
Epoch 1/10, Batch 18/883, Training Loss: 0.9684
Epoch 1/10, Batch 19/883, Training Loss: 1.0462
Epoch 1/10, Batch 20/883, Training Loss: 1.2593
Epoch 1/10, Batch 21/883, Training Loss: 0.9387
Epoch 1/10, Batch 22/883, Training Loss: 0.9811
Epoch 1/10, Batch 23/883, Training Loss: 1.0795
Epoch 1/10, Batch 24/883, Training Loss: 0.9933
Epoch 1/10, Batch 25/883, Training Loss: 0.9121
Epoch 1/10, Batch 26/883, Training Loss: 1.0691
Epoch 1/10, Batch 27/883, Training Loss: 0.8153
Epoch 1/10, Batch 28/883, Training Loss: 0.8892
Epoch 1/10, Batch 29/883, Training Loss: 0.7912
Epoch 1/10, Batch 30/883, Training Loss: 1.1994
Epoch 1/10, Batch 31/883, Training Loss: 1.0549
Epoch 1/10, Batch 32/883, Training Loss: 0.8003
Epoch 1/10, Batch 33/883, Training Loss: 1.4279
Epoch 1/10, Batch 34/883, Training Loss: 0.9606
Epoch 1/10, Batch 35/883, Training Loss: 0.7750
Epoch 1/10, Batch 36/883, Training Loss: 1.0603
Epoch 1/10, Batch 37/883, Training Loss: 1.3187
Epoch 1/10, Batch 38/883, Training Loss: 0.9677
Epoch 1/10, Batch 39/883, Training Loss: 1.0418
Epoch 1/10, Batch 40/883, Training Loss: 1.1745
Epoch 1/10, Batch 41/883, Training Loss: 1.0267
Epoch 1/10, Batch 42/883, Training Loss: 1.3509
Epoch 1/10, Batch 43/883, Training Loss: 1.0122
Epoch 1/10, Batch 44/883, Training Loss: 1.1615
Epoch 1/10, Batch 45/883, Training Loss: 0.8560
Epoch 1/10, Batch 46/883, Training Loss: 1.0018
Epoch 1/10, Batch 47/883, Training Loss: 0.8052
Epoch 1/10, Batch 48/883, Training Loss: 0.9586
Epoch 1/10, Batch 49/883, Training Loss: 1.0508
Epoch 1/10, Batch 50/883, Training Loss: 1.0163
Epoch 1/10, Batch 51/883, Training Loss: 0.8085
Epoch 1/10, Batch 52/883, Training Loss: 0.9489
Epoch 1/10, Batch 53/883, Training Loss: 1.1504
Epoch 1/10, Batch 54/883, Training Loss: 0.9264
Epoch 1/10, Batch 55/883, Training Loss: 0.8304
Epoch 1/10, Batch 56/883, Training Loss: 0.9944
Epoch 1/10, Batch 57/883, Training Loss: 1.0597
Epoch 1/10, Batch 58/883, Training Loss: 0.9949
Epoch 1/10, Batch 59/883, Training Loss: 0.9248
Epoch 1/10, Batch 60/883, Training Loss: 1.0622
Epoch 1/10, Batch 61/883, Training Loss: 0.9715
Epoch 1/10, Batch 62/883, Training Loss: 0.9764
Epoch 1/10, Batch 63/883, Training Loss: 0.8865
Epoch 1/10, Batch 64/883, Training Loss: 0.9988
Epoch 1/10, Batch 65/883, Training Loss: 1.6719
Epoch 1/10, Batch 66/883, Training Loss: 0.9966
Epoch 1/10, Batch 67/883, Training Loss: 0.9038
Epoch 1/10, Batch 68/883, Training Loss: 0.9422
Epoch 1/10, Batch 69/883, Training Loss: 0.9291
Epoch 1/10, Batch 70/883, Training Loss: 1.0069
Epoch 1/10, Batch 71/883, Training Loss: 1.0526
Epoch 1/10, Batch 72/883, Training Loss: 1.1735
Epoch 1/10, Batch 73/883, Training Loss: 1.2479
Epoch 1/10, Batch 74/883, Training Loss: 0.8301
Epoch 1/10, Batch 75/883, Training Loss: 0.8657
Epoch 1/10, Batch 76/883, Training Loss: 0.8584
Epoch 1/10, Batch 77/883, Training Loss: 1.0460
Epoch 1/10, Batch 78/883, Training Loss: 0.9064
Epoch 1/10, Batch 79/883, Training Loss: 0.9522
Epoch 1/10, Batch 80/883, Training Loss: 0.8102
Epoch 1/10, Batch 81/883, Training Loss: 0.8354
Epoch 1/10, Batch 82/883, Training Loss: 1.0225
Epoch 1/10, Batch 83/883, Training Loss: 0.9582
Epoch 1/10, Batch 84/883, Training Loss: 1.3401
Epoch 1/10, Batch 85/883, Training Loss: 1.3261
Epoch 1/10, Batch 86/883, Training Loss: 0.9105
Epoch 1/10, Batch 87/883, Training Loss: 0.8224
Epoch 1/10, Batch 88/883, Training Loss: 1.0484
Epoch 1/10, Batch 89/883, Training Loss: 1.3304
Epoch 1/10, Batch 90/883, Training Loss: 0.9130
Epoch 1/10, Batch 91/883, Training Loss: 1.1680
Epoch 1/10, Batch 92/883, Training Loss: 1.1109
Epoch 1/10, Batch 93/883, Training Loss: 1.0745
Epoch 1/10, Batch 94/883, Training Loss: 1.0735
Epoch 1/10, Batch 95/883, Training Loss: 1.4046
Epoch 1/10, Batch 96/883, Training Loss: 1.4212
Epoch 1/10, Batch 97/883, Training Loss: 0.8880
Epoch 1/10, Batch 98/883, Training Loss: 1.2779
Epoch 1/10, Batch 99/883, Training Loss: 0.6840
Epoch 1/10, Batch 100/883, Training Loss: 0.8741
Epoch 1/10, Batch 101/883, Training Loss: 1.1780
Epoch 1/10, Batch 102/883, Training Loss: 1.0231
Epoch 1/10, Batch 103/883, Training Loss: 1.4717
Epoch 1/10, Batch 104/883, Training Loss: 0.7707
Epoch 1/10, Batch 105/883, Training Loss: 1.3129
Epoch 1/10, Batch 106/883, Training Loss: 0.8469
Epoch 1/10, Batch 107/883, Training Loss: 1.0261
Epoch 1/10, Batch 108/883, Training Loss: 0.7777
Epoch 1/10, Batch 109/883, Training Loss: 0.8512
Epoch 1/10, Batch 110/883, Training Loss: 0.6437
Epoch 1/10, Batch 111/883, Training Loss: 1.0002
Epoch 1/10, Batch 112/883, Training Loss: 0.6073
Epoch 1/10, Batch 113/883, Training Loss: 1.1362
Epoch 1/10, Batch 114/883, Training Loss: 1.0531
Epoch 1/10, Batch 115/883, Training Loss: 1.5993
Epoch 1/10, Batch 116/883, Training Loss: 0.9214
Epoch 1/10, Batch 117/883, Training Loss: 0.7600
Epoch 1/10, Batch 118/883, Training Loss: 0.7817
Epoch 1/10, Batch 119/883, Training Loss: 1.1158
Epoch 1/10, Batch 120/883, Training Loss: 0.8880
Epoch 1/10, Batch 121/883, Training Loss: 0.8643
Epoch 1/10, Batch 122/883, Training Loss: 0.8939
Epoch 1/10, Batch 123/883, Training Loss: 1.2174
Epoch 1/10, Batch 124/883, Training Loss: 1.2478
Epoch 1/10, Batch 125/883, Training Loss: 0.9578
Epoch 1/10, Batch 126/883, Training Loss: 1.1312
Epoch 1/10, Batch 127/883, Training Loss: 1.0162
Epoch 1/10, Batch 128/883, Training Loss: 1.3768
Epoch 1/10, Batch 129/883, Training Loss: 1.7682
Epoch 1/10, Batch 130/883, Training Loss: 0.8861
Epoch 1/10, Batch 131/883, Training Loss: 1.0571
Epoch 1/10, Batch 132/883, Training Loss: 0.7689
Epoch 1/10, Batch 133/883, Training Loss: 1.1295
Epoch 1/10, Batch 134/883, Training Loss: 0.9077
Epoch 1/10, Batch 135/883, Training Loss: 1.2462
Epoch 1/10, Batch 136/883, Training Loss: 1.2888
Epoch 1/10, Batch 137/883, Training Loss: 1.2379
Epoch 1/10, Batch 138/883, Training Loss: 1.2872
Epoch 1/10, Batch 139/883, Training Loss: 1.2780
Epoch 1/10, Batch 140/883, Training Loss: 0.7308
Epoch 1/10, Batch 141/883, Training Loss: 1.0374
Epoch 1/10, Batch 142/883, Training Loss: 1.0228
Epoch 1/10, Batch 143/883, Training Loss: 0.9845
Epoch 1/10, Batch 144/883, Training Loss: 1.0620
Epoch 1/10, Batch 145/883, Training Loss: 0.9394
Epoch 1/10, Batch 146/883, Training Loss: 0.8505
Epoch 1/10, Batch 147/883, Training Loss: 0.8461
Epoch 1/10, Batch 148/883, Training Loss: 1.1259
Epoch 1/10, Batch 149/883, Training Loss: 1.2569
Epoch 1/10, Batch 150/883, Training Loss: 0.9353
Epoch 1/10, Batch 151/883, Training Loss: 1.0665
Epoch 1/10, Batch 152/883, Training Loss: 1.0890
Epoch 1/10, Batch 153/883, Training Loss: 1.0194
Epoch 1/10, Batch 154/883, Training Loss: 1.1068
Epoch 1/10, Batch 155/883, Training Loss: 1.0163
Epoch 1/10, Batch 156/883, Training Loss: 0.9004
Epoch 1/10, Batch 157/883, Training Loss: 0.8607
Epoch 1/10, Batch 158/883, Training Loss: 0.8335
Epoch 1/10, Batch 159/883, Training Loss: 1.0768
Epoch 1/10, Batch 160/883, Training Loss: 0.8101
Epoch 1/10, Batch 161/883, Training Loss: 0.9102
Epoch 1/10, Batch 162/883, Training Loss: 0.8446
Epoch 1/10, Batch 163/883, Training Loss: 1.1543
Epoch 1/10, Batch 164/883, Training Loss: 0.9065
Epoch 1/10, Batch 165/883, Training Loss: 0.9241
Epoch 1/10, Batch 166/883, Training Loss: 0.7157
Epoch 1/10, Batch 167/883, Training Loss: 1.4563
Epoch 1/10, Batch 168/883, Training Loss: 1.0310
Epoch 1/10, Batch 169/883, Training Loss: 0.6666
Epoch 1/10, Batch 170/883, Training Loss: 0.7293
Epoch 1/10, Batch 171/883, Training Loss: 0.7450
Epoch 1/10, Batch 172/883, Training Loss: 0.8615
Epoch 1/10, Batch 173/883, Training Loss: 0.9953
Epoch 1/10, Batch 174/883, Training Loss: 1.0832
Epoch 1/10, Batch 175/883, Training Loss: 1.0849
Epoch 1/10, Batch 176/883, Training Loss: 0.8293
Epoch 1/10, Batch 177/883, Training Loss: 0.8547
Epoch 1/10, Batch 178/883, Training Loss: 0.8797
Epoch 1/10, Batch 179/883, Training Loss: 0.8334
Epoch 1/10, Batch 180/883, Training Loss: 1.0851
Epoch 1/10, Batch 181/883, Training Loss: 0.9177
Epoch 1/10, Batch 182/883, Training Loss: 1.0690
Epoch 1/10, Batch 183/883, Training Loss: 1.1613
Epoch 1/10, Batch 184/883, Training Loss: 1.0088
Epoch 1/10, Batch 185/883, Training Loss: 0.7139
Epoch 1/10, Batch 186/883, Training Loss: 1.2024
Epoch 1/10, Batch 187/883, Training Loss: 0.9700
Epoch 1/10, Batch 188/883, Training Loss: 0.9059
Epoch 1/10, Batch 189/883, Training Loss: 1.3335
Epoch 1/10, Batch 190/883, Training Loss: 1.0308
Epoch 1/10, Batch 191/883, Training Loss: 0.8307
Epoch 1/10, Batch 192/883, Training Loss: 0.7579
Epoch 1/10, Batch 193/883, Training Loss: 0.9812
Epoch 1/10, Batch 194/883, Training Loss: 0.9052
Epoch 1/10, Batch 195/883, Training Loss: 0.9280
Epoch 1/10, Batch 196/883, Training Loss: 1.0330
Epoch 1/10, Batch 197/883, Training Loss: 0.8574
Epoch 1/10, Batch 198/883, Training Loss: 0.7078
Epoch 1/10, Batch 199/883, Training Loss: 0.7870
Epoch 1/10, Batch 200/883, Training Loss: 0.9151
Epoch 1/10, Batch 201/883, Training Loss: 0.9422
Epoch 1/10, Batch 202/883, Training Loss: 1.1083
Epoch 1/10, Batch 203/883, Training Loss: 1.2371
Epoch 1/10, Batch 204/883, Training Loss: 0.9539
Epoch 1/10, Batch 205/883, Training Loss: 0.6852
Epoch 1/10, Batch 206/883, Training Loss: 0.9511
Epoch 1/10, Batch 207/883, Training Loss: 0.7150
Epoch 1/10, Batch 208/883, Training Loss: 0.9927
Epoch 1/10, Batch 209/883, Training Loss: 1.0748
Epoch 1/10, Batch 210/883, Training Loss: 0.7480
Epoch 1/10, Batch 211/883, Training Loss: 0.8044
Epoch 1/10, Batch 212/883, Training Loss: 0.8488
Epoch 1/10, Batch 213/883, Training Loss: 1.1940
Epoch 1/10, Batch 214/883, Training Loss: 1.0434
Epoch 1/10, Batch 215/883, Training Loss: 0.8394
Epoch 1/10, Batch 216/883, Training Loss: 1.2802
Epoch 1/10, Batch 217/883, Training Loss: 0.8526
Epoch 1/10, Batch 218/883, Training Loss: 0.7861
Epoch 1/10, Batch 219/883, Training Loss: 1.1677
Epoch 1/10, Batch 220/883, Training Loss: 0.8261
Epoch 1/10, Batch 221/883, Training Loss: 0.5922
Epoch 1/10, Batch 222/883, Training Loss: 0.8288
Epoch 1/10, Batch 223/883, Training Loss: 0.8032
Epoch 1/10, Batch 224/883, Training Loss: 0.7211
Epoch 1/10, Batch 225/883, Training Loss: 1.1874
Epoch 1/10, Batch 226/883, Training Loss: 0.6587
Epoch 1/10, Batch 227/883, Training Loss: 1.0820
Epoch 1/10, Batch 228/883, Training Loss: 0.6423
Epoch 1/10, Batch 229/883, Training Loss: 1.0158
Epoch 1/10, Batch 230/883, Training Loss: 0.9179
Epoch 1/10, Batch 231/883, Training Loss: 0.7260
Epoch 1/10, Batch 232/883, Training Loss: 1.0806
Epoch 1/10, Batch 233/883, Training Loss: 1.1695
Epoch 1/10, Batch 234/883, Training Loss: 1.1365
Epoch 1/10, Batch 235/883, Training Loss: 1.0810
Epoch 1/10, Batch 236/883, Training Loss: 0.8432
Epoch 1/10, Batch 237/883, Training Loss: 0.8753
Epoch 1/10, Batch 238/883, Training Loss: 0.8265
Epoch 1/10, Batch 239/883, Training Loss: 1.4434
Epoch 1/10, Batch 240/883, Training Loss: 1.0503
Epoch 1/10, Batch 241/883, Training Loss: 1.1960
Epoch 1/10, Batch 242/883, Training Loss: 1.0313
Epoch 1/10, Batch 243/883, Training Loss: 1.0682
Epoch 1/10, Batch 244/883, Training Loss: 0.9550
Epoch 1/10, Batch 245/883, Training Loss: 0.9566
Epoch 1/10, Batch 246/883, Training Loss: 0.7315
Epoch 1/10, Batch 247/883, Training Loss: 0.7971
Epoch 1/10, Batch 248/883, Training Loss: 0.7552
Epoch 1/10, Batch 249/883, Training Loss: 0.8826
Epoch 1/10, Batch 250/883, Training Loss: 0.8725
Epoch 1/10, Batch 251/883, Training Loss: 0.7708
Epoch 1/10, Batch 252/883, Training Loss: 0.8319
Epoch 1/10, Batch 253/883, Training Loss: 0.8860
Epoch 1/10, Batch 254/883, Training Loss: 0.7721
Epoch 1/10, Batch 255/883, Training Loss: 0.8800
Epoch 1/10, Batch 256/883, Training Loss: 0.8905
Epoch 1/10, Batch 257/883, Training Loss: 1.2776
Epoch 1/10, Batch 258/883, Training Loss: 1.1060
Epoch 1/10, Batch 259/883, Training Loss: 0.7989
Epoch 1/10, Batch 260/883, Training Loss: 0.9079
Epoch 1/10, Batch 261/883, Training Loss: 0.7475
Epoch 1/10, Batch 262/883, Training Loss: 1.1594
Epoch 1/10, Batch 263/883, Training Loss: 0.9415
Epoch 1/10, Batch 264/883, Training Loss: 0.8766
Epoch 1/10, Batch 265/883, Training Loss: 0.7986
Epoch 1/10, Batch 266/883, Training Loss: 0.9032
Epoch 1/10, Batch 267/883, Training Loss: 0.9320
Epoch 1/10, Batch 268/883, Training Loss: 1.0569
Epoch 1/10, Batch 269/883, Training Loss: 0.8247
Epoch 1/10, Batch 270/883, Training Loss: 1.1823
Epoch 1/10, Batch 271/883, Training Loss: 0.9605
Epoch 1/10, Batch 272/883, Training Loss: 0.8243
Epoch 1/10, Batch 273/883, Training Loss: 0.8668
Epoch 1/10, Batch 274/883, Training Loss: 0.7618
Epoch 1/10, Batch 275/883, Training Loss: 0.9031
Epoch 1/10, Batch 276/883, Training Loss: 1.0908
Epoch 1/10, Batch 277/883, Training Loss: 0.8605
Epoch 1/10, Batch 278/883, Training Loss: 0.8031
Epoch 1/10, Batch 279/883, Training Loss: 1.1018
Epoch 1/10, Batch 280/883, Training Loss: 0.8190
Epoch 1/10, Batch 281/883, Training Loss: 1.0682
Epoch 1/10, Batch 282/883, Training Loss: 1.1493
Epoch 1/10, Batch 283/883, Training Loss: 0.7886
Epoch 1/10, Batch 284/883, Training Loss: 0.9957
Epoch 1/10, Batch 285/883, Training Loss: 0.9663
Epoch 1/10, Batch 286/883, Training Loss: 1.0785
Epoch 1/10, Batch 287/883, Training Loss: 0.8830
Epoch 1/10, Batch 288/883, Training Loss: 0.9635
Epoch 1/10, Batch 289/883, Training Loss: 1.0319
Epoch 1/10, Batch 290/883, Training Loss: 1.1062
Epoch 1/10, Batch 291/883, Training Loss: 0.8648
Epoch 1/10, Batch 292/883, Training Loss: 1.0942
Epoch 1/10, Batch 293/883, Training Loss: 1.0498
Epoch 1/10, Batch 294/883, Training Loss: 0.7984
Epoch 1/10, Batch 295/883, Training Loss: 0.7678
Epoch 1/10, Batch 296/883, Training Loss: 0.8245
Epoch 1/10, Batch 297/883, Training Loss: 0.8271
Epoch 1/10, Batch 298/883, Training Loss: 0.9115
Epoch 1/10, Batch 299/883, Training Loss: 0.8991
Epoch 1/10, Batch 300/883, Training Loss: 0.9697
Epoch 1/10, Batch 301/883, Training Loss: 1.2204
Epoch 1/10, Batch 302/883, Training Loss: 0.9142
Epoch 1/10, Batch 303/883, Training Loss: 1.1656
Epoch 1/10, Batch 304/883, Training Loss: 0.6623
Epoch 1/10, Batch 305/883, Training Loss: 0.7292
Epoch 1/10, Batch 306/883, Training Loss: 1.2368
Epoch 1/10, Batch 307/883, Training Loss: 1.1265
Epoch 1/10, Batch 308/883, Training Loss: 0.7071
Epoch 1/10, Batch 309/883, Training Loss: 0.8615
Epoch 1/10, Batch 310/883, Training Loss: 1.0083
Epoch 1/10, Batch 311/883, Training Loss: 0.9584
Epoch 1/10, Batch 312/883, Training Loss: 1.1341
Epoch 1/10, Batch 313/883, Training Loss: 1.0073
Epoch 1/10, Batch 314/883, Training Loss: 0.8886
Epoch 1/10, Batch 315/883, Training Loss: 0.9039
Epoch 1/10, Batch 316/883, Training Loss: 1.0160
Epoch 1/10, Batch 317/883, Training Loss: 0.9525
Epoch 1/10, Batch 318/883, Training Loss: 0.7719
Epoch 1/10, Batch 319/883, Training Loss: 0.9099
Epoch 1/10, Batch 320/883, Training Loss: 0.8650
Epoch 1/10, Batch 321/883, Training Loss: 1.0081
Epoch 1/10, Batch 322/883, Training Loss: 1.1034
Epoch 1/10, Batch 323/883, Training Loss: 0.6848
Epoch 1/10, Batch 324/883, Training Loss: 0.8626
Epoch 1/10, Batch 325/883, Training Loss: 0.8456
Epoch 1/10, Batch 326/883, Training Loss: 0.9698
Epoch 1/10, Batch 327/883, Training Loss: 0.7804
Epoch 1/10, Batch 328/883, Training Loss: 1.0176
Epoch 1/10, Batch 329/883, Training Loss: 0.7475
Epoch 1/10, Batch 330/883, Training Loss: 1.3917
Epoch 1/10, Batch 331/883, Training Loss: 0.8571
Epoch 1/10, Batch 332/883, Training Loss: 0.7435
Epoch 1/10, Batch 333/883, Training Loss: 0.9491
Epoch 1/10, Batch 334/883, Training Loss: 0.7327
Epoch 1/10, Batch 335/883, Training Loss: 0.8725
Epoch 1/10, Batch 336/883, Training Loss: 1.0990
Epoch 1/10, Batch 337/883, Training Loss: 1.1128
Epoch 1/10, Batch 338/883, Training Loss: 1.3142
Epoch 1/10, Batch 339/883, Training Loss: 0.9225
Epoch 1/10, Batch 340/883, Training Loss: 0.8265
Epoch 1/10, Batch 341/883, Training Loss: 0.9020
Epoch 1/10, Batch 342/883, Training Loss: 1.1480
Epoch 1/10, Batch 343/883, Training Loss: 0.7061
Epoch 1/10, Batch 344/883, Training Loss: 1.0516
Epoch 1/10, Batch 345/883, Training Loss: 1.2486
Epoch 1/10, Batch 346/883, Training Loss: 1.1404
Epoch 1/10, Batch 347/883, Training Loss: 0.8229
Epoch 1/10, Batch 348/883, Training Loss: 1.4779
Epoch 1/10, Batch 349/883, Training Loss: 1.4876
Epoch 1/10, Batch 350/883, Training Loss: 1.1879
Epoch 1/10, Batch 351/883, Training Loss: 1.0209
Epoch 1/10, Batch 352/883, Training Loss: 0.9807
Epoch 1/10, Batch 353/883, Training Loss: 0.8733
Epoch 1/10, Batch 354/883, Training Loss: 0.9617
Epoch 1/10, Batch 355/883, Training Loss: 1.0383
Epoch 1/10, Batch 356/883, Training Loss: 1.0439
Epoch 1/10, Batch 357/883, Training Loss: 1.2813
Epoch 1/10, Batch 358/883, Training Loss: 1.1100
Epoch 1/10, Batch 359/883, Training Loss: 0.9782
Epoch 1/10, Batch 360/883, Training Loss: 1.0092
Epoch 1/10, Batch 361/883, Training Loss: 0.8512
Epoch 1/10, Batch 362/883, Training Loss: 0.8625
Epoch 1/10, Batch 363/883, Training Loss: 0.9175
Epoch 1/10, Batch 364/883, Training Loss: 0.8316
Epoch 1/10, Batch 365/883, Training Loss: 1.2456
Epoch 1/10, Batch 366/883, Training Loss: 1.2276
Epoch 1/10, Batch 367/883, Training Loss: 0.8037
Epoch 1/10, Batch 368/883, Training Loss: 0.8152
Epoch 1/10, Batch 369/883, Training Loss: 0.8563
Epoch 1/10, Batch 370/883, Training Loss: 0.8303
Epoch 1/10, Batch 371/883, Training Loss: 0.7845
Epoch 1/10, Batch 372/883, Training Loss: 0.8005
Epoch 1/10, Batch 373/883, Training Loss: 1.1291
Epoch 1/10, Batch 374/883, Training Loss: 0.9671
Epoch 1/10, Batch 375/883, Training Loss: 0.6733
Epoch 1/10, Batch 376/883, Training Loss: 0.7759
Epoch 1/10, Batch 377/883, Training Loss: 1.0625
Epoch 1/10, Batch 378/883, Training Loss: 0.7550
Epoch 1/10, Batch 379/883, Training Loss: 0.6554
Epoch 1/10, Batch 380/883, Training Loss: 1.0844
Epoch 1/10, Batch 381/883, Training Loss: 0.8025
Epoch 1/10, Batch 382/883, Training Loss: 0.9011
Epoch 1/10, Batch 383/883, Training Loss: 0.9112
Epoch 1/10, Batch 384/883, Training Loss: 1.0304
Epoch 1/10, Batch 385/883, Training Loss: 0.6287
Epoch 1/10, Batch 386/883, Training Loss: 1.1149
Epoch 1/10, Batch 387/883, Training Loss: 0.9755
Epoch 1/10, Batch 388/883, Training Loss: 0.6184
Epoch 1/10, Batch 389/883, Training Loss: 1.1763
Epoch 1/10, Batch 390/883, Training Loss: 0.7294
Epoch 1/10, Batch 391/883, Training Loss: 0.6300
Epoch 1/10, Batch 392/883, Training Loss: 0.7400
Epoch 1/10, Batch 393/883, Training Loss: 1.0829
Epoch 1/10, Batch 394/883, Training Loss: 0.9470
Epoch 1/10, Batch 395/883, Training Loss: 1.0191
Epoch 1/10, Batch 396/883, Training Loss: 0.9155
Epoch 1/10, Batch 397/883, Training Loss: 0.9585
Epoch 1/10, Batch 398/883, Training Loss: 0.8554
Epoch 1/10, Batch 399/883, Training Loss: 0.8427
Epoch 1/10, Batch 400/883, Training Loss: 0.8381
Epoch 1/10, Batch 401/883, Training Loss: 1.0530
Epoch 1/10, Batch 402/883, Training Loss: 0.9892
Epoch 1/10, Batch 403/883, Training Loss: 0.6305
Epoch 1/10, Batch 404/883, Training Loss: 1.0172
Epoch 1/10, Batch 405/883, Training Loss: 0.8480
Epoch 1/10, Batch 406/883, Training Loss: 0.8029
Epoch 1/10, Batch 407/883, Training Loss: 1.0914
Epoch 1/10, Batch 408/883, Training Loss: 0.7970
Epoch 1/10, Batch 409/883, Training Loss: 1.1856
Epoch 1/10, Batch 410/883, Training Loss: 0.8575
Epoch 1/10, Batch 411/883, Training Loss: 1.0611
Epoch 1/10, Batch 412/883, Training Loss: 1.2010
Epoch 1/10, Batch 413/883, Training Loss: 0.9962
Epoch 1/10, Batch 414/883, Training Loss: 0.9811
Epoch 1/10, Batch 415/883, Training Loss: 0.9819
Epoch 1/10, Batch 416/883, Training Loss: 0.8098
Epoch 1/10, Batch 417/883, Training Loss: 0.9745
Epoch 1/10, Batch 418/883, Training Loss: 0.8295
Epoch 1/10, Batch 419/883, Training Loss: 1.1205
Epoch 1/10, Batch 420/883, Training Loss: 0.9536
Epoch 1/10, Batch 421/883, Training Loss: 1.2010
Epoch 1/10, Batch 422/883, Training Loss: 0.8053
Epoch 1/10, Batch 423/883, Training Loss: 0.8416
Epoch 1/10, Batch 424/883, Training Loss: 0.8962
Epoch 1/10, Batch 425/883, Training Loss: 1.0206
Epoch 1/10, Batch 426/883, Training Loss: 0.7781
Epoch 1/10, Batch 427/883, Training Loss: 0.9424
Epoch 1/10, Batch 428/883, Training Loss: 0.8009
Epoch 1/10, Batch 429/883, Training Loss: 0.8128
Epoch 1/10, Batch 430/883, Training Loss: 0.9002
Epoch 1/10, Batch 431/883, Training Loss: 0.8543
Epoch 1/10, Batch 432/883, Training Loss: 1.0387
Epoch 1/10, Batch 433/883, Training Loss: 0.9528
Epoch 1/10, Batch 434/883, Training Loss: 0.7435
Epoch 1/10, Batch 435/883, Training Loss: 1.0879
Epoch 1/10, Batch 436/883, Training Loss: 0.8278
Epoch 1/10, Batch 437/883, Training Loss: 0.8362
Epoch 1/10, Batch 438/883, Training Loss: 0.7025
Epoch 1/10, Batch 439/883, Training Loss: 0.8530
Epoch 1/10, Batch 440/883, Training Loss: 0.8031
Epoch 1/10, Batch 441/883, Training Loss: 0.6795
Epoch 1/10, Batch 442/883, Training Loss: 0.9323
Epoch 1/10, Batch 443/883, Training Loss: 0.7693
Epoch 1/10, Batch 444/883, Training Loss: 0.8685
Epoch 1/10, Batch 445/883, Training Loss: 0.8682
Epoch 1/10, Batch 446/883, Training Loss: 0.8719
Epoch 1/10, Batch 447/883, Training Loss: 0.9903
Epoch 1/10, Batch 448/883, Training Loss: 0.7089
Epoch 1/10, Batch 449/883, Training Loss: 1.1408
Epoch 1/10, Batch 450/883, Training Loss: 0.8819
Epoch 1/10, Batch 451/883, Training Loss: 0.7346
Epoch 1/10, Batch 452/883, Training Loss: 1.4193
Epoch 1/10, Batch 453/883, Training Loss: 1.0419
Epoch 1/10, Batch 454/883, Training Loss: 0.8556
Epoch 1/10, Batch 455/883, Training Loss: 0.9514
Epoch 1/10, Batch 456/883, Training Loss: 0.9165
Epoch 1/10, Batch 457/883, Training Loss: 0.8100
Epoch 1/10, Batch 458/883, Training Loss: 1.0568
Epoch 1/10, Batch 459/883, Training Loss: 0.8177
Epoch 1/10, Batch 460/883, Training Loss: 0.8507
Epoch 1/10, Batch 461/883, Training Loss: 0.7774
Epoch 1/10, Batch 462/883, Training Loss: 0.7939
Epoch 1/10, Batch 463/883, Training Loss: 0.7837
Epoch 1/10, Batch 464/883, Training Loss: 0.7543
Epoch 1/10, Batch 465/883, Training Loss: 0.7877
Epoch 1/10, Batch 466/883, Training Loss: 0.8737
Epoch 1/10, Batch 467/883, Training Loss: 0.6533
Epoch 1/10, Batch 468/883, Training Loss: 0.6686
Epoch 1/10, Batch 469/883, Training Loss: 0.9751
Epoch 1/10, Batch 470/883, Training Loss: 0.8877
Epoch 1/10, Batch 471/883, Training Loss: 0.8024
Epoch 1/10, Batch 472/883, Training Loss: 1.1434
Epoch 1/10, Batch 473/883, Training Loss: 0.7697
Epoch 1/10, Batch 474/883, Training Loss: 0.9849
Epoch 1/10, Batch 475/883, Training Loss: 0.7877
Epoch 1/10, Batch 476/883, Training Loss: 0.7717
Epoch 1/10, Batch 477/883, Training Loss: 1.0720
Epoch 1/10, Batch 478/883, Training Loss: 0.8363
Epoch 1/10, Batch 479/883, Training Loss: 0.9225
Epoch 1/10, Batch 480/883, Training Loss: 0.8751
Epoch 1/10, Batch 481/883, Training Loss: 0.9826
Epoch 1/10, Batch 482/883, Training Loss: 0.9443
Epoch 1/10, Batch 483/883, Training Loss: 0.9361
Epoch 1/10, Batch 484/883, Training Loss: 0.7186
Epoch 1/10, Batch 485/883, Training Loss: 1.4734
Epoch 1/10, Batch 486/883, Training Loss: 1.1612
Epoch 1/10, Batch 487/883, Training Loss: 0.9148
Epoch 1/10, Batch 488/883, Training Loss: 0.9175
Epoch 1/10, Batch 489/883, Training Loss: 0.8023
Epoch 1/10, Batch 490/883, Training Loss: 0.8972
Epoch 1/10, Batch 491/883, Training Loss: 0.8486
Epoch 1/10, Batch 492/883, Training Loss: 0.8490
Epoch 1/10, Batch 493/883, Training Loss: 0.6696
Epoch 1/10, Batch 494/883, Training Loss: 0.7889
Epoch 1/10, Batch 495/883, Training Loss: 0.8506
Epoch 1/10, Batch 496/883, Training Loss: 0.7221
Epoch 1/10, Batch 497/883, Training Loss: 0.7449
Epoch 1/10, Batch 498/883, Training Loss: 0.8085
Epoch 1/10, Batch 499/883, Training Loss: 0.6918
Epoch 1/10, Batch 500/883, Training Loss: 1.2142
Epoch 1/10, Batch 501/883, Training Loss: 0.7754
Epoch 1/10, Batch 502/883, Training Loss: 0.8334
Epoch 1/10, Batch 503/883, Training Loss: 0.6893
Epoch 1/10, Batch 504/883, Training Loss: 1.0093
Epoch 1/10, Batch 505/883, Training Loss: 0.7450
Epoch 1/10, Batch 506/883, Training Loss: 0.6801
Epoch 1/10, Batch 507/883, Training Loss: 0.6833
Epoch 1/10, Batch 508/883, Training Loss: 0.9163
Epoch 1/10, Batch 509/883, Training Loss: 1.0452
Epoch 1/10, Batch 510/883, Training Loss: 0.6910
Epoch 1/10, Batch 511/883, Training Loss: 0.9065
Epoch 1/10, Batch 512/883, Training Loss: 1.0047
Epoch 1/10, Batch 513/883, Training Loss: 0.8247
Epoch 1/10, Batch 514/883, Training Loss: 1.1834
Epoch 1/10, Batch 515/883, Training Loss: 0.7128
Epoch 1/10, Batch 516/883, Training Loss: 0.7585
Epoch 1/10, Batch 517/883, Training Loss: 0.9733
Epoch 1/10, Batch 518/883, Training Loss: 0.7503
Epoch 1/10, Batch 519/883, Training Loss: 0.6102
Epoch 1/10, Batch 520/883, Training Loss: 1.4744
Epoch 1/10, Batch 521/883, Training Loss: 0.9290
Epoch 1/10, Batch 522/883, Training Loss: 1.0313
Epoch 1/10, Batch 523/883, Training Loss: 1.0212
Epoch 1/10, Batch 524/883, Training Loss: 0.8038
Epoch 1/10, Batch 525/883, Training Loss: 0.6404
Epoch 1/10, Batch 526/883, Training Loss: 0.6186
Epoch 1/10, Batch 527/883, Training Loss: 0.5778
Epoch 1/10, Batch 528/883, Training Loss: 0.9407
Epoch 1/10, Batch 529/883, Training Loss: 0.7371
Epoch 1/10, Batch 530/883, Training Loss: 0.9552
Epoch 1/10, Batch 531/883, Training Loss: 1.5110
Epoch 1/10, Batch 532/883, Training Loss: 1.0930
Epoch 1/10, Batch 533/883, Training Loss: 1.0887
Epoch 1/10, Batch 534/883, Training Loss: 0.8069
Epoch 1/10, Batch 535/883, Training Loss: 0.7079
Epoch 1/10, Batch 536/883, Training Loss: 0.8274
Epoch 1/10, Batch 537/883, Training Loss: 0.9861
Epoch 1/10, Batch 538/883, Training Loss: 0.6851
Epoch 1/10, Batch 539/883, Training Loss: 1.0274
Epoch 1/10, Batch 540/883, Training Loss: 0.8024
Epoch 1/10, Batch 541/883, Training Loss: 0.9772
Epoch 1/10, Batch 542/883, Training Loss: 0.9563
Epoch 1/10, Batch 543/883, Training Loss: 0.7519
Epoch 1/10, Batch 544/883, Training Loss: 0.8643
Epoch 1/10, Batch 545/883, Training Loss: 0.8666
Epoch 1/10, Batch 546/883, Training Loss: 0.9173
Epoch 1/10, Batch 547/883, Training Loss: 1.0108
Epoch 1/10, Batch 548/883, Training Loss: 0.7738
Epoch 1/10, Batch 549/883, Training Loss: 0.7721
Epoch 1/10, Batch 550/883, Training Loss: 0.8095
Epoch 1/10, Batch 551/883, Training Loss: 0.8383
Epoch 1/10, Batch 552/883, Training Loss: 0.9451
Epoch 1/10, Batch 553/883, Training Loss: 0.8876
Epoch 1/10, Batch 554/883, Training Loss: 0.8248
Epoch 1/10, Batch 555/883, Training Loss: 0.7264
Epoch 1/10, Batch 556/883, Training Loss: 0.7864
Epoch 1/10, Batch 557/883, Training Loss: 0.7779
Epoch 1/10, Batch 558/883, Training Loss: 1.2178
Epoch 1/10, Batch 559/883, Training Loss: 0.9630
Epoch 1/10, Batch 560/883, Training Loss: 1.0060
Epoch 1/10, Batch 561/883, Training Loss: 0.9119
Epoch 1/10, Batch 562/883, Training Loss: 0.8841
Epoch 1/10, Batch 563/883, Training Loss: 1.1207
Epoch 1/10, Batch 564/883, Training Loss: 0.7878
Epoch 1/10, Batch 565/883, Training Loss: 0.9081
Epoch 1/10, Batch 566/883, Training Loss: 1.0581
Epoch 1/10, Batch 567/883, Training Loss: 1.1717
Epoch 1/10, Batch 568/883, Training Loss: 1.0560
Epoch 1/10, Batch 569/883, Training Loss: 1.0040
Epoch 1/10, Batch 570/883, Training Loss: 0.7504
Epoch 1/10, Batch 571/883, Training Loss: 0.7575
Epoch 1/10, Batch 572/883, Training Loss: 0.7534
Epoch 1/10, Batch 573/883, Training Loss: 0.8516
Epoch 1/10, Batch 574/883, Training Loss: 1.2205
Epoch 1/10, Batch 575/883, Training Loss: 1.0711
Epoch 1/10, Batch 576/883, Training Loss: 0.6991
Epoch 1/10, Batch 577/883, Training Loss: 0.7982
Epoch 1/10, Batch 578/883, Training Loss: 0.7949
Epoch 1/10, Batch 579/883, Training Loss: 0.8871
Epoch 1/10, Batch 580/883, Training Loss: 0.8392
Epoch 1/10, Batch 581/883, Training Loss: 0.9670
Epoch 1/10, Batch 582/883, Training Loss: 0.7472
Epoch 1/10, Batch 583/883, Training Loss: 1.5008
Epoch 1/10, Batch 584/883, Training Loss: 0.9915
Epoch 1/10, Batch 585/883, Training Loss: 1.1280
Epoch 1/10, Batch 586/883, Training Loss: 0.9813
Epoch 1/10, Batch 587/883, Training Loss: 0.8758
Epoch 1/10, Batch 588/883, Training Loss: 0.9830
Epoch 1/10, Batch 589/883, Training Loss: 0.8729
Epoch 1/10, Batch 590/883, Training Loss: 1.0458
Epoch 1/10, Batch 591/883, Training Loss: 1.0648
Epoch 1/10, Batch 592/883, Training Loss: 0.9709
Epoch 1/10, Batch 593/883, Training Loss: 0.8476
Epoch 1/10, Batch 594/883, Training Loss: 0.9285
Epoch 1/10, Batch 595/883, Training Loss: 0.8524
Epoch 1/10, Batch 596/883, Training Loss: 0.9895
Epoch 1/10, Batch 597/883, Training Loss: 0.8884
Epoch 1/10, Batch 598/883, Training Loss: 0.9629
Epoch 1/10, Batch 599/883, Training Loss: 0.7721
Epoch 1/10, Batch 600/883, Training Loss: 0.7610
Epoch 1/10, Batch 601/883, Training Loss: 0.8774
Epoch 1/10, Batch 602/883, Training Loss: 0.8974
Epoch 1/10, Batch 603/883, Training Loss: 0.7898
Epoch 1/10, Batch 604/883, Training Loss: 0.9455
Epoch 1/10, Batch 605/883, Training Loss: 1.0998
Epoch 1/10, Batch 606/883, Training Loss: 1.1058
Epoch 1/10, Batch 607/883, Training Loss: 0.8642
Epoch 1/10, Batch 608/883, Training Loss: 0.8171
Epoch 1/10, Batch 609/883, Training Loss: 1.0668
Epoch 1/10, Batch 610/883, Training Loss: 1.2227
Epoch 1/10, Batch 611/883, Training Loss: 0.9986
Epoch 1/10, Batch 612/883, Training Loss: 0.9413
Epoch 1/10, Batch 613/883, Training Loss: 0.8511
Epoch 1/10, Batch 614/883, Training Loss: 1.0307
Epoch 1/10, Batch 615/883, Training Loss: 0.8495
Epoch 1/10, Batch 616/883, Training Loss: 0.8221
Epoch 1/10, Batch 617/883, Training Loss: 1.2421
Epoch 1/10, Batch 618/883, Training Loss: 1.1344
Epoch 1/10, Batch 619/883, Training Loss: 0.7323
Epoch 1/10, Batch 620/883, Training Loss: 0.9510
Epoch 1/10, Batch 621/883, Training Loss: 0.8265
Epoch 1/10, Batch 622/883, Training Loss: 0.7695
Epoch 1/10, Batch 623/883, Training Loss: 0.6389
Epoch 1/10, Batch 624/883, Training Loss: 0.9832
Epoch 1/10, Batch 625/883, Training Loss: 1.2863
Epoch 1/10, Batch 626/883, Training Loss: 0.9281
Epoch 1/10, Batch 627/883, Training Loss: 1.2131
Epoch 1/10, Batch 628/883, Training Loss: 0.8155
Epoch 1/10, Batch 629/883, Training Loss: 1.1368
Epoch 1/10, Batch 630/883, Training Loss: 1.0116
Epoch 1/10, Batch 631/883, Training Loss: 0.8680
Epoch 1/10, Batch 632/883, Training Loss: 0.8977
Epoch 1/10, Batch 633/883, Training Loss: 0.9346
Epoch 1/10, Batch 634/883, Training Loss: 0.8279
Epoch 1/10, Batch 635/883, Training Loss: 0.8746
Epoch 1/10, Batch 636/883, Training Loss: 0.9414
Epoch 1/10, Batch 637/883, Training Loss: 0.9127
Epoch 1/10, Batch 638/883, Training Loss: 0.7999
Epoch 1/10, Batch 639/883, Training Loss: 0.9925
Epoch 1/10, Batch 640/883, Training Loss: 0.7787
Epoch 1/10, Batch 641/883, Training Loss: 0.6963
Epoch 1/10, Batch 642/883, Training Loss: 0.9084
Epoch 1/10, Batch 643/883, Training Loss: 0.7663
Epoch 1/10, Batch 644/883, Training Loss: 0.6931
Epoch 1/10, Batch 645/883, Training Loss: 0.9206
Epoch 1/10, Batch 646/883, Training Loss: 0.8896
Epoch 1/10, Batch 647/883, Training Loss: 0.9095
Epoch 1/10, Batch 648/883, Training Loss: 0.9274
Epoch 1/10, Batch 649/883, Training Loss: 0.9009
Epoch 1/10, Batch 650/883, Training Loss: 1.0959
Epoch 1/10, Batch 651/883, Training Loss: 0.8793
Epoch 1/10, Batch 652/883, Training Loss: 1.1841
Epoch 1/10, Batch 653/883, Training Loss: 0.8588
Epoch 1/10, Batch 654/883, Training Loss: 0.9088
Epoch 1/10, Batch 655/883, Training Loss: 0.7819
Epoch 1/10, Batch 656/883, Training Loss: 0.7862
Epoch 1/10, Batch 657/883, Training Loss: 0.7807
Epoch 1/10, Batch 658/883, Training Loss: 0.8085
Epoch 1/10, Batch 659/883, Training Loss: 0.6935
Epoch 1/10, Batch 660/883, Training Loss: 0.9935
Epoch 1/10, Batch 661/883, Training Loss: 1.1385
Epoch 1/10, Batch 662/883, Training Loss: 0.9253
Epoch 1/10, Batch 663/883, Training Loss: 0.7577
Epoch 1/10, Batch 664/883, Training Loss: 0.6930
Epoch 1/10, Batch 665/883, Training Loss: 0.8756
Epoch 1/10, Batch 666/883, Training Loss: 1.1061
Epoch 1/10, Batch 667/883, Training Loss: 0.8162
Epoch 1/10, Batch 668/883, Training Loss: 0.6474
Epoch 1/10, Batch 669/883, Training Loss: 0.7192
Epoch 1/10, Batch 670/883, Training Loss: 0.9832
Epoch 1/10, Batch 671/883, Training Loss: 0.6634
Epoch 1/10, Batch 672/883, Training Loss: 0.7616
Epoch 1/10, Batch 673/883, Training Loss: 1.0163
Epoch 1/10, Batch 674/883, Training Loss: 1.0156
Epoch 1/10, Batch 675/883, Training Loss: 0.9566
Epoch 1/10, Batch 676/883, Training Loss: 0.7417
Epoch 1/10, Batch 677/883, Training Loss: 0.7322
Epoch 1/10, Batch 678/883, Training Loss: 0.9151
Epoch 1/10, Batch 679/883, Training Loss: 0.6991
Epoch 1/10, Batch 680/883, Training Loss: 0.8079
Epoch 1/10, Batch 681/883, Training Loss: 0.8500
Epoch 1/10, Batch 682/883, Training Loss: 0.7854
Epoch 1/10, Batch 683/883, Training Loss: 0.9298
Epoch 1/10, Batch 684/883, Training Loss: 0.7601
Epoch 1/10, Batch 685/883, Training Loss: 1.3324
Epoch 1/10, Batch 686/883, Training Loss: 0.9599
Epoch 1/10, Batch 687/883, Training Loss: 1.0506
Epoch 1/10, Batch 688/883, Training Loss: 0.9715
Epoch 1/10, Batch 689/883, Training Loss: 0.9359
Epoch 1/10, Batch 690/883, Training Loss: 0.9169
Epoch 1/10, Batch 691/883, Training Loss: 0.8126
Epoch 1/10, Batch 692/883, Training Loss: 0.7747
Epoch 1/10, Batch 693/883, Training Loss: 0.8219
Epoch 1/10, Batch 694/883, Training Loss: 1.0556
Epoch 1/10, Batch 695/883, Training Loss: 1.1272
Epoch 1/10, Batch 696/883, Training Loss: 0.8484
Epoch 1/10, Batch 697/883, Training Loss: 0.9932
Epoch 1/10, Batch 698/883, Training Loss: 0.6640
Epoch 1/10, Batch 699/883, Training Loss: 0.9980
Epoch 1/10, Batch 700/883, Training Loss: 0.7147
Epoch 1/10, Batch 701/883, Training Loss: 1.0330
Epoch 1/10, Batch 702/883, Training Loss: 1.2114
Epoch 1/10, Batch 703/883, Training Loss: 0.7631
Epoch 1/10, Batch 704/883, Training Loss: 0.7280
Epoch 1/10, Batch 705/883, Training Loss: 0.8487
Epoch 1/10, Batch 706/883, Training Loss: 0.9553
Epoch 1/10, Batch 707/883, Training Loss: 0.8974
Epoch 1/10, Batch 708/883, Training Loss: 0.9045
Epoch 1/10, Batch 709/883, Training Loss: 0.8207
Epoch 1/10, Batch 710/883, Training Loss: 0.8018
Epoch 1/10, Batch 711/883, Training Loss: 0.8067
Epoch 1/10, Batch 712/883, Training Loss: 1.0201
Epoch 1/10, Batch 713/883, Training Loss: 0.9528
Epoch 1/10, Batch 714/883, Training Loss: 0.8812
Epoch 1/10, Batch 715/883, Training Loss: 0.8073
Epoch 1/10, Batch 716/883, Training Loss: 0.7886
Epoch 1/10, Batch 717/883, Training Loss: 0.8859
Epoch 1/10, Batch 718/883, Training Loss: 1.2225
Epoch 1/10, Batch 719/883, Training Loss: 1.0962
Epoch 1/10, Batch 720/883, Training Loss: 0.7909
Epoch 1/10, Batch 721/883, Training Loss: 1.0825
Epoch 1/10, Batch 722/883, Training Loss: 1.1264
Epoch 1/10, Batch 723/883, Training Loss: 0.7531
Epoch 1/10, Batch 724/883, Training Loss: 1.1025
Epoch 1/10, Batch 725/883, Training Loss: 0.8059
Epoch 1/10, Batch 726/883, Training Loss: 1.1639
Epoch 1/10, Batch 727/883, Training Loss: 1.0385
Epoch 1/10, Batch 728/883, Training Loss: 1.1976
Epoch 1/10, Batch 729/883, Training Loss: 0.9020
Epoch 1/10, Batch 730/883, Training Loss: 0.9001
Epoch 1/10, Batch 731/883, Training Loss: 0.9797
Epoch 1/10, Batch 732/883, Training Loss: 0.8135
Epoch 1/10, Batch 733/883, Training Loss: 0.9319
Epoch 1/10, Batch 734/883, Training Loss: 0.9206
Epoch 1/10, Batch 735/883, Training Loss: 0.9823
Epoch 1/10, Batch 736/883, Training Loss: 0.8129
Epoch 1/10, Batch 737/883, Training Loss: 0.7743
Epoch 1/10, Batch 738/883, Training Loss: 1.1950
Epoch 1/10, Batch 739/883, Training Loss: 0.7928
Epoch 1/10, Batch 740/883, Training Loss: 0.9419
Epoch 1/10, Batch 741/883, Training Loss: 0.7352
Epoch 1/10, Batch 742/883, Training Loss: 0.8419
Epoch 1/10, Batch 743/883, Training Loss: 0.7536
Epoch 1/10, Batch 744/883, Training Loss: 0.8055
Epoch 1/10, Batch 745/883, Training Loss: 0.9462
Epoch 1/10, Batch 746/883, Training Loss: 0.6561
Epoch 1/10, Batch 747/883, Training Loss: 0.9005
Epoch 1/10, Batch 748/883, Training Loss: 0.6389
Epoch 1/10, Batch 749/883, Training Loss: 0.8509
Epoch 1/10, Batch 750/883, Training Loss: 0.8272
Epoch 1/10, Batch 751/883, Training Loss: 0.7967
Epoch 1/10, Batch 752/883, Training Loss: 1.0229
Epoch 1/10, Batch 753/883, Training Loss: 0.7689
Epoch 1/10, Batch 754/883, Training Loss: 0.7393
Epoch 1/10, Batch 755/883, Training Loss: 0.9627
Epoch 1/10, Batch 756/883, Training Loss: 0.8902
Epoch 1/10, Batch 757/883, Training Loss: 0.9252
Epoch 1/10, Batch 758/883, Training Loss: 1.2061
Epoch 1/10, Batch 759/883, Training Loss: 0.7367
Epoch 1/10, Batch 760/883, Training Loss: 0.8563
Epoch 1/10, Batch 761/883, Training Loss: 0.6760
Epoch 1/10, Batch 762/883, Training Loss: 0.9184
Epoch 1/10, Batch 763/883, Training Loss: 0.9555
Epoch 1/10, Batch 764/883, Training Loss: 0.9029
Epoch 1/10, Batch 765/883, Training Loss: 1.2289
Epoch 1/10, Batch 766/883, Training Loss: 0.6598
Epoch 1/10, Batch 767/883, Training Loss: 0.7829
Epoch 1/10, Batch 768/883, Training Loss: 1.0474
Epoch 1/10, Batch 769/883, Training Loss: 0.9588
Epoch 1/10, Batch 770/883, Training Loss: 0.7426
Epoch 1/10, Batch 771/883, Training Loss: 0.9640
Epoch 1/10, Batch 772/883, Training Loss: 0.9081
Epoch 1/10, Batch 773/883, Training Loss: 0.7403
Epoch 1/10, Batch 774/883, Training Loss: 0.7536
Epoch 1/10, Batch 775/883, Training Loss: 1.2123
Epoch 1/10, Batch 776/883, Training Loss: 0.7412
Epoch 1/10, Batch 777/883, Training Loss: 0.9266
Epoch 1/10, Batch 778/883, Training Loss: 0.9690
Epoch 1/10, Batch 779/883, Training Loss: 1.5494
Epoch 1/10, Batch 780/883, Training Loss: 0.8904
Epoch 1/10, Batch 781/883, Training Loss: 0.8018
Epoch 1/10, Batch 782/883, Training Loss: 0.6002
Epoch 1/10, Batch 783/883, Training Loss: 0.8722
Epoch 1/10, Batch 784/883, Training Loss: 0.9890
Epoch 1/10, Batch 785/883, Training Loss: 0.7694
Epoch 1/10, Batch 786/883, Training Loss: 0.8674
Epoch 1/10, Batch 787/883, Training Loss: 0.9715
Epoch 1/10, Batch 788/883, Training Loss: 1.1811
Epoch 1/10, Batch 789/883, Training Loss: 0.9470
Epoch 1/10, Batch 790/883, Training Loss: 1.0147
Epoch 1/10, Batch 791/883, Training Loss: 0.9833
Epoch 1/10, Batch 792/883, Training Loss: 0.7329
Epoch 1/10, Batch 793/883, Training Loss: 1.0472
Epoch 1/10, Batch 794/883, Training Loss: 0.9082
Epoch 1/10, Batch 795/883, Training Loss: 1.0253
Epoch 1/10, Batch 796/883, Training Loss: 0.8268
Epoch 1/10, Batch 797/883, Training Loss: 0.9734
Epoch 1/10, Batch 798/883, Training Loss: 0.8024
Epoch 1/10, Batch 799/883, Training Loss: 0.8460
Epoch 1/10, Batch 800/883, Training Loss: 0.9065
Epoch 1/10, Batch 801/883, Training Loss: 0.7376
Epoch 1/10, Batch 802/883, Training Loss: 0.9518
Epoch 1/10, Batch 803/883, Training Loss: 0.9548
Epoch 1/10, Batch 804/883, Training Loss: 0.7135
Epoch 1/10, Batch 805/883, Training Loss: 0.8340
Epoch 1/10, Batch 806/883, Training Loss: 0.8917
Epoch 1/10, Batch 807/883, Training Loss: 0.7867
Epoch 1/10, Batch 808/883, Training Loss: 0.9297
Epoch 1/10, Batch 809/883, Training Loss: 0.6615
Epoch 1/10, Batch 810/883, Training Loss: 0.7042
Epoch 1/10, Batch 811/883, Training Loss: 0.7798
Epoch 1/10, Batch 812/883, Training Loss: 1.1615
Epoch 1/10, Batch 813/883, Training Loss: 0.5500
Epoch 1/10, Batch 814/883, Training Loss: 0.8594
Epoch 1/10, Batch 815/883, Training Loss: 0.7843
Epoch 1/10, Batch 816/883, Training Loss: 0.7963
Epoch 1/10, Batch 817/883, Training Loss: 0.5889
Epoch 1/10, Batch 818/883, Training Loss: 0.6683
Epoch 1/10, Batch 819/883, Training Loss: 0.9262
Epoch 1/10, Batch 820/883, Training Loss: 0.7229
Epoch 1/10, Batch 821/883, Training Loss: 1.1290
Epoch 1/10, Batch 822/883, Training Loss: 0.8996
Epoch 1/10, Batch 823/883, Training Loss: 0.7064
Epoch 1/10, Batch 824/883, Training Loss: 1.0338
Epoch 1/10, Batch 825/883, Training Loss: 0.7351
Epoch 1/10, Batch 826/883, Training Loss: 0.9632
Epoch 1/10, Batch 827/883, Training Loss: 0.8568
Epoch 1/10, Batch 828/883, Training Loss: 0.9230
Epoch 1/10, Batch 829/883, Training Loss: 0.9049
Epoch 1/10, Batch 830/883, Training Loss: 0.7083
Epoch 1/10, Batch 831/883, Training Loss: 0.8419
Epoch 1/10, Batch 832/883, Training Loss: 0.8713
Epoch 1/10, Batch 833/883, Training Loss: 1.2013
Epoch 1/10, Batch 834/883, Training Loss: 0.5909
Epoch 1/10, Batch 835/883, Training Loss: 0.8131
Epoch 1/10, Batch 836/883, Training Loss: 0.8979
Epoch 1/10, Batch 837/883, Training Loss: 1.1228
Epoch 1/10, Batch 838/883, Training Loss: 0.9237
Epoch 1/10, Batch 839/883, Training Loss: 0.8997
Epoch 1/10, Batch 840/883, Training Loss: 1.1565
Epoch 1/10, Batch 841/883, Training Loss: 0.8920
Epoch 1/10, Batch 842/883, Training Loss: 0.9012
Epoch 1/10, Batch 843/883, Training Loss: 0.9666
Epoch 1/10, Batch 844/883, Training Loss: 0.8625
Epoch 1/10, Batch 845/883, Training Loss: 0.9822
Epoch 1/10, Batch 846/883, Training Loss: 0.9539
Epoch 1/10, Batch 847/883, Training Loss: 1.1246
Epoch 1/10, Batch 848/883, Training Loss: 0.8948
Epoch 1/10, Batch 849/883, Training Loss: 0.9493
Epoch 1/10, Batch 850/883, Training Loss: 0.8363
Epoch 1/10, Batch 851/883, Training Loss: 0.9397
Epoch 1/10, Batch 852/883, Training Loss: 0.8783
Epoch 1/10, Batch 853/883, Training Loss: 0.8581
Epoch 1/10, Batch 854/883, Training Loss: 0.7386
Epoch 1/10, Batch 855/883, Training Loss: 0.8680
Epoch 1/10, Batch 856/883, Training Loss: 0.9891
Epoch 1/10, Batch 857/883, Training Loss: 0.8706
Epoch 1/10, Batch 858/883, Training Loss: 0.8591
Epoch 1/10, Batch 859/883, Training Loss: 0.6952
Epoch 1/10, Batch 860/883, Training Loss: 0.6745
Epoch 1/10, Batch 861/883, Training Loss: 0.6765
Epoch 1/10, Batch 862/883, Training Loss: 0.6931
Epoch 1/10, Batch 863/883, Training Loss: 0.6775
Epoch 1/10, Batch 864/883, Training Loss: 0.8571
Epoch 1/10, Batch 865/883, Training Loss: 0.9661
Epoch 1/10, Batch 866/883, Training Loss: 0.7117
Epoch 1/10, Batch 867/883, Training Loss: 0.9105
Epoch 1/10, Batch 868/883, Training Loss: 1.0683
Epoch 1/10, Batch 869/883, Training Loss: 0.8258
Epoch 1/10, Batch 870/883, Training Loss: 0.9643
Epoch 1/10, Batch 871/883, Training Loss: 0.8782
Epoch 1/10, Batch 872/883, Training Loss: 0.8077
Epoch 1/10, Batch 873/883, Training Loss: 0.9264
Epoch 1/10, Batch 874/883, Training Loss: 0.8104
Epoch 1/10, Batch 875/883, Training Loss: 1.1200
Epoch 1/10, Batch 876/883, Training Loss: 1.1373
Epoch 1/10, Batch 877/883, Training Loss: 0.9868
Epoch 1/10, Batch 878/883, Training Loss: 0.9717
Epoch 1/10, Batch 879/883, Training Loss: 0.9303
Epoch 1/10, Batch 880/883, Training Loss: 0.6256
Epoch 1/10, Batch 881/883, Training Loss: 0.6330
Epoch 1/10, Batch 882/883, Training Loss: 0.7908
Epoch 1/10, Batch 883/883, Training Loss: 0.6081
Epoch 1/10, Training Loss: 0.9472, Validation Loss: 0.8312, Validation Accuracy: 0.5976
Epoch 2/10, Batch 1/883, Training Loss: 0.8899
Epoch 2/10, Batch 2/883, Training Loss: 0.9091
Epoch 2/10, Batch 3/883, Training Loss: 1.0008
Epoch 2/10, Batch 4/883, Training Loss: 0.6707
Epoch 2/10, Batch 5/883, Training Loss: 0.9081
Epoch 2/10, Batch 6/883, Training Loss: 0.9750
Epoch 2/10, Batch 7/883, Training Loss: 1.3256
Epoch 2/10, Batch 8/883, Training Loss: 1.1821
Epoch 2/10, Batch 9/883, Training Loss: 1.1256
Epoch 2/10, Batch 10/883, Training Loss: 0.8438
Epoch 2/10, Batch 11/883, Training Loss: 0.8383
Epoch 2/10, Batch 12/883, Training Loss: 1.0314
Epoch 2/10, Batch 13/883, Training Loss: 0.7474
Epoch 2/10, Batch 14/883, Training Loss: 0.9533
Epoch 2/10, Batch 15/883, Training Loss: 0.9812
Epoch 2/10, Batch 16/883, Training Loss: 0.7530
Epoch 2/10, Batch 17/883, Training Loss: 0.9027
Epoch 2/10, Batch 18/883, Training Loss: 1.1882
Epoch 2/10, Batch 19/883, Training Loss: 0.9049
Epoch 2/10, Batch 20/883, Training Loss: 0.9175
Epoch 2/10, Batch 21/883, Training Loss: 0.8449
Epoch 2/10, Batch 22/883, Training Loss: 0.9760
Epoch 2/10, Batch 23/883, Training Loss: 1.0296
Epoch 2/10, Batch 24/883, Training Loss: 0.8821
Epoch 2/10, Batch 25/883, Training Loss: 0.9239
Epoch 2/10, Batch 26/883, Training Loss: 0.9710
Epoch 2/10, Batch 27/883, Training Loss: 0.7531
Epoch 2/10, Batch 28/883, Training Loss: 0.8134
Epoch 2/10, Batch 29/883, Training Loss: 0.8632
Epoch 2/10, Batch 30/883, Training Loss: 0.8771
Epoch 2/10, Batch 31/883, Training Loss: 0.9068
Epoch 2/10, Batch 32/883, Training Loss: 1.1086
Epoch 2/10, Batch 33/883, Training Loss: 1.0224
Epoch 2/10, Batch 34/883, Training Loss: 0.7790
Epoch 2/10, Batch 35/883, Training Loss: 0.8002
Epoch 2/10, Batch 36/883, Training Loss: 0.8987
Epoch 2/10, Batch 37/883, Training Loss: 0.7287
Epoch 2/10, Batch 38/883, Training Loss: 0.8004
Epoch 2/10, Batch 39/883, Training Loss: 0.8812
Epoch 2/10, Batch 40/883, Training Loss: 1.0043
Epoch 2/10, Batch 41/883, Training Loss: 0.9366
Epoch 2/10, Batch 42/883, Training Loss: 0.7437
Epoch 2/10, Batch 43/883, Training Loss: 0.8330
Epoch 2/10, Batch 44/883, Training Loss: 1.0928
Epoch 2/10, Batch 45/883, Training Loss: 0.9652
Epoch 2/10, Batch 46/883, Training Loss: 0.9741
Epoch 2/10, Batch 47/883, Training Loss: 0.8781
Epoch 2/10, Batch 48/883, Training Loss: 0.9006
Epoch 2/10, Batch 49/883, Training Loss: 0.8727
Epoch 2/10, Batch 50/883, Training Loss: 1.1386
Epoch 2/10, Batch 51/883, Training Loss: 0.9474
Epoch 2/10, Batch 52/883, Training Loss: 1.0155
Epoch 2/10, Batch 53/883, Training Loss: 1.0674
Epoch 2/10, Batch 54/883, Training Loss: 1.0149
Epoch 2/10, Batch 55/883, Training Loss: 0.8699
Epoch 2/10, Batch 56/883, Training Loss: 0.8532
Epoch 2/10, Batch 57/883, Training Loss: 0.7542
Epoch 2/10, Batch 58/883, Training Loss: 0.9236
Epoch 2/10, Batch 59/883, Training Loss: 0.8677
Epoch 2/10, Batch 60/883, Training Loss: 0.8571
Epoch 2/10, Batch 61/883, Training Loss: 0.7966
Epoch 2/10, Batch 62/883, Training Loss: 0.8967
Epoch 2/10, Batch 63/883, Training Loss: 0.7988
Epoch 2/10, Batch 64/883, Training Loss: 1.1460
Epoch 2/10, Batch 65/883, Training Loss: 0.8201
Epoch 2/10, Batch 66/883, Training Loss: 1.0158
Epoch 2/10, Batch 67/883, Training Loss: 1.0378
Epoch 2/10, Batch 68/883, Training Loss: 0.8617
Epoch 2/10, Batch 69/883, Training Loss: 0.7743
Epoch 2/10, Batch 70/883, Training Loss: 0.8345
Epoch 2/10, Batch 71/883, Training Loss: 0.7759
Epoch 2/10, Batch 72/883, Training Loss: 0.8831
Epoch 2/10, Batch 73/883, Training Loss: 0.9513
Epoch 2/10, Batch 74/883, Training Loss: 0.8457
Epoch 2/10, Batch 75/883, Training Loss: 0.8864
Epoch 2/10, Batch 76/883, Training Loss: 0.8134
Epoch 2/10, Batch 77/883, Training Loss: 0.7119
Epoch 2/10, Batch 78/883, Training Loss: 0.9562
Epoch 2/10, Batch 79/883, Training Loss: 0.7892
Epoch 2/10, Batch 80/883, Training Loss: 0.9195
Epoch 2/10, Batch 81/883, Training Loss: 0.7785
Epoch 2/10, Batch 82/883, Training Loss: 0.8954
Epoch 2/10, Batch 83/883, Training Loss: 0.6715
Epoch 2/10, Batch 84/883, Training Loss: 0.8202
Epoch 2/10, Batch 85/883, Training Loss: 0.8413
Epoch 2/10, Batch 86/883, Training Loss: 0.6641
Epoch 2/10, Batch 87/883, Training Loss: 0.6793
Epoch 2/10, Batch 88/883, Training Loss: 0.6945
Epoch 2/10, Batch 89/883, Training Loss: 1.4163
Epoch 2/10, Batch 90/883, Training Loss: 0.7876
Epoch 2/10, Batch 91/883, Training Loss: 0.9114
Epoch 2/10, Batch 92/883, Training Loss: 0.9325
Epoch 2/10, Batch 93/883, Training Loss: 1.1570
Epoch 2/10, Batch 94/883, Training Loss: 0.9644
Epoch 2/10, Batch 95/883, Training Loss: 0.7790
Epoch 2/10, Batch 96/883, Training Loss: 0.8545
Epoch 2/10, Batch 97/883, Training Loss: 0.8728
Epoch 2/10, Batch 98/883, Training Loss: 0.7922
Epoch 2/10, Batch 99/883, Training Loss: 0.7315
Epoch 2/10, Batch 100/883, Training Loss: 0.7538
Epoch 2/10, Batch 101/883, Training Loss: 0.8588
Epoch 2/10, Batch 102/883, Training Loss: 0.8636
Epoch 2/10, Batch 103/883, Training Loss: 0.8597
Epoch 2/10, Batch 104/883, Training Loss: 0.8145
Epoch 2/10, Batch 105/883, Training Loss: 0.9449
Epoch 2/10, Batch 106/883, Training Loss: 0.6941
Epoch 2/10, Batch 107/883, Training Loss: 0.6277
Epoch 2/10, Batch 108/883, Training Loss: 1.0604
Epoch 2/10, Batch 109/883, Training Loss: 0.7220
Epoch 2/10, Batch 110/883, Training Loss: 0.7765
Epoch 2/10, Batch 111/883, Training Loss: 0.5628
Epoch 2/10, Batch 112/883, Training Loss: 0.9922
Epoch 2/10, Batch 113/883, Training Loss: 0.8172
Epoch 2/10, Batch 114/883, Training Loss: 0.6508
Epoch 2/10, Batch 115/883, Training Loss: 0.7717
Epoch 2/10, Batch 116/883, Training Loss: 0.7110
Epoch 2/10, Batch 117/883, Training Loss: 0.7957
Epoch 2/10, Batch 118/883, Training Loss: 0.8440
Epoch 2/10, Batch 119/883, Training Loss: 0.8303
Epoch 2/10, Batch 120/883, Training Loss: 0.9406
Epoch 2/10, Batch 121/883, Training Loss: 0.8077
Epoch 2/10, Batch 122/883, Training Loss: 0.6890
Epoch 2/10, Batch 123/883, Training Loss: 0.7511
Epoch 2/10, Batch 124/883, Training Loss: 0.8313
Epoch 2/10, Batch 125/883, Training Loss: 0.9669
Epoch 2/10, Batch 126/883, Training Loss: 0.8011
Epoch 2/10, Batch 127/883, Training Loss: 0.6693
Epoch 2/10, Batch 128/883, Training Loss: 0.8164
Epoch 2/10, Batch 129/883, Training Loss: 1.0053
Epoch 2/10, Batch 130/883, Training Loss: 0.8566
Epoch 2/10, Batch 131/883, Training Loss: 0.8501
Epoch 2/10, Batch 132/883, Training Loss: 1.0300
Epoch 2/10, Batch 133/883, Training Loss: 0.7709
Epoch 2/10, Batch 134/883, Training Loss: 0.6661
Epoch 2/10, Batch 135/883, Training Loss: 0.9178
Epoch 2/10, Batch 136/883, Training Loss: 0.8454
Epoch 2/10, Batch 137/883, Training Loss: 1.1179
Epoch 2/10, Batch 138/883, Training Loss: 1.0954
Epoch 2/10, Batch 139/883, Training Loss: 0.6178
Epoch 2/10, Batch 140/883, Training Loss: 0.9722
Epoch 2/10, Batch 141/883, Training Loss: 0.9302
Epoch 2/10, Batch 142/883, Training Loss: 0.6566
Epoch 2/10, Batch 143/883, Training Loss: 0.6719
Epoch 2/10, Batch 144/883, Training Loss: 0.7984
Epoch 2/10, Batch 145/883, Training Loss: 0.8238
Epoch 2/10, Batch 146/883, Training Loss: 1.1443
Epoch 2/10, Batch 147/883, Training Loss: 1.0014
Epoch 2/10, Batch 148/883, Training Loss: 0.8292
Epoch 2/10, Batch 149/883, Training Loss: 0.8738
Epoch 2/10, Batch 150/883, Training Loss: 0.7991
Epoch 2/10, Batch 151/883, Training Loss: 0.5422
Epoch 2/10, Batch 152/883, Training Loss: 0.6191
Epoch 2/10, Batch 153/883, Training Loss: 0.9585
Epoch 2/10, Batch 154/883, Training Loss: 1.1000
Epoch 2/10, Batch 155/883, Training Loss: 0.7333
Epoch 2/10, Batch 156/883, Training Loss: 0.7641
Epoch 2/10, Batch 157/883, Training Loss: 0.5880
Epoch 2/10, Batch 158/883, Training Loss: 0.7053
Epoch 2/10, Batch 159/883, Training Loss: 1.0583
Epoch 2/10, Batch 160/883, Training Loss: 0.9032
Epoch 2/10, Batch 161/883, Training Loss: 0.6636
Epoch 2/10, Batch 162/883, Training Loss: 0.9227
Epoch 2/10, Batch 163/883, Training Loss: 0.8451
Epoch 2/10, Batch 164/883, Training Loss: 0.5847
Epoch 2/10, Batch 165/883, Training Loss: 0.8722
Epoch 2/10, Batch 166/883, Training Loss: 0.6715
Epoch 2/10, Batch 167/883, Training Loss: 0.9045
Epoch 2/10, Batch 168/883, Training Loss: 0.9914
Epoch 2/10, Batch 169/883, Training Loss: 0.9264
Epoch 2/10, Batch 170/883, Training Loss: 0.8094
Epoch 2/10, Batch 171/883, Training Loss: 0.7549
Epoch 2/10, Batch 172/883, Training Loss: 0.9303
Epoch 2/10, Batch 173/883, Training Loss: 0.6508
Epoch 2/10, Batch 174/883, Training Loss: 0.7232
Epoch 2/10, Batch 175/883, Training Loss: 0.9599
Epoch 2/10, Batch 176/883, Training Loss: 0.7025
Epoch 2/10, Batch 177/883, Training Loss: 0.8112
Epoch 2/10, Batch 178/883, Training Loss: 0.9033
Epoch 2/10, Batch 179/883, Training Loss: 0.5732
Epoch 2/10, Batch 180/883, Training Loss: 0.8564
Epoch 2/10, Batch 181/883, Training Loss: 0.4575
Epoch 2/10, Batch 182/883, Training Loss: 0.8098
Epoch 2/10, Batch 183/883, Training Loss: 0.8988
Epoch 2/10, Batch 184/883, Training Loss: 1.4210
Epoch 2/10, Batch 185/883, Training Loss: 0.8596
Epoch 2/10, Batch 186/883, Training Loss: 0.7510
Epoch 2/10, Batch 187/883, Training Loss: 0.7313
Epoch 2/10, Batch 188/883, Training Loss: 0.8584
Epoch 2/10, Batch 189/883, Training Loss: 0.7866
Epoch 2/10, Batch 190/883, Training Loss: 0.7627
Epoch 2/10, Batch 191/883, Training Loss: 0.6982
Epoch 2/10, Batch 192/883, Training Loss: 0.8488
Epoch 2/10, Batch 193/883, Training Loss: 0.7991
Epoch 2/10, Batch 194/883, Training Loss: 0.9051
Epoch 2/10, Batch 195/883, Training Loss: 0.9417
Epoch 2/10, Batch 196/883, Training Loss: 0.9291
Epoch 2/10, Batch 197/883, Training Loss: 0.9130
Epoch 2/10, Batch 198/883, Training Loss: 0.7731
Epoch 2/10, Batch 199/883, Training Loss: 0.7865
Epoch 2/10, Batch 200/883, Training Loss: 0.9695
Epoch 2/10, Batch 201/883, Training Loss: 0.7496
Epoch 2/10, Batch 202/883, Training Loss: 0.7528
Epoch 2/10, Batch 203/883, Training Loss: 0.8253
Epoch 2/10, Batch 204/883, Training Loss: 0.6205
Epoch 2/10, Batch 205/883, Training Loss: 1.1186
Epoch 2/10, Batch 206/883, Training Loss: 0.9358
Epoch 2/10, Batch 207/883, Training Loss: 0.7285
Epoch 2/10, Batch 208/883, Training Loss: 0.7577
Epoch 2/10, Batch 209/883, Training Loss: 0.7102
Epoch 2/10, Batch 210/883, Training Loss: 0.7732
Epoch 2/10, Batch 211/883, Training Loss: 0.9232
Epoch 2/10, Batch 212/883, Training Loss: 1.3305
Epoch 2/10, Batch 213/883, Training Loss: 0.8871
Epoch 2/10, Batch 214/883, Training Loss: 0.7510
Epoch 2/10, Batch 215/883, Training Loss: 0.8319
Epoch 2/10, Batch 216/883, Training Loss: 0.6438
Epoch 2/10, Batch 217/883, Training Loss: 0.9846
Epoch 2/10, Batch 218/883, Training Loss: 0.8970
Epoch 2/10, Batch 219/883, Training Loss: 0.8858
Epoch 2/10, Batch 220/883, Training Loss: 1.0055
Epoch 2/10, Batch 221/883, Training Loss: 0.7710
Epoch 2/10, Batch 222/883, Training Loss: 1.0702
Epoch 2/10, Batch 223/883, Training Loss: 0.8546
Epoch 2/10, Batch 224/883, Training Loss: 0.7990
Epoch 2/10, Batch 225/883, Training Loss: 0.7798
Epoch 2/10, Batch 226/883, Training Loss: 0.7432
Epoch 2/10, Batch 227/883, Training Loss: 0.7465
Epoch 2/10, Batch 228/883, Training Loss: 0.9861
Epoch 2/10, Batch 229/883, Training Loss: 0.7513
Epoch 2/10, Batch 230/883, Training Loss: 0.7512
Epoch 2/10, Batch 231/883, Training Loss: 0.8759
Epoch 2/10, Batch 232/883, Training Loss: 0.6673
Epoch 2/10, Batch 233/883, Training Loss: 0.7499
Epoch 2/10, Batch 234/883, Training Loss: 1.1509
Epoch 2/10, Batch 235/883, Training Loss: 0.8648
Epoch 2/10, Batch 236/883, Training Loss: 0.6366
Epoch 2/10, Batch 237/883, Training Loss: 0.8768
Epoch 2/10, Batch 238/883, Training Loss: 0.9070
Epoch 2/10, Batch 239/883, Training Loss: 0.7618
Epoch 2/10, Batch 240/883, Training Loss: 0.8073
Epoch 2/10, Batch 241/883, Training Loss: 0.7436
Epoch 2/10, Batch 242/883, Training Loss: 0.7143
Epoch 2/10, Batch 243/883, Training Loss: 0.8739
Epoch 2/10, Batch 244/883, Training Loss: 0.5427
Epoch 2/10, Batch 245/883, Training Loss: 0.7893
Epoch 2/10, Batch 246/883, Training Loss: 1.3692
Epoch 2/10, Batch 247/883, Training Loss: 0.9851
Epoch 2/10, Batch 248/883, Training Loss: 0.8932
Epoch 2/10, Batch 249/883, Training Loss: 0.9834
Epoch 2/10, Batch 250/883, Training Loss: 0.6635
Epoch 2/10, Batch 251/883, Training Loss: 0.7895
Epoch 2/10, Batch 252/883, Training Loss: 0.7763
Epoch 2/10, Batch 253/883, Training Loss: 0.7129
Epoch 2/10, Batch 254/883, Training Loss: 0.7134
Epoch 2/10, Batch 255/883, Training Loss: 1.2012
Epoch 2/10, Batch 256/883, Training Loss: 0.9790
Epoch 2/10, Batch 257/883, Training Loss: 1.0873
Epoch 2/10, Batch 258/883, Training Loss: 0.7669
Epoch 2/10, Batch 259/883, Training Loss: 0.6972
Epoch 2/10, Batch 260/883, Training Loss: 0.9962
Epoch 2/10, Batch 261/883, Training Loss: 0.9825
Epoch 2/10, Batch 262/883, Training Loss: 0.8552
Epoch 2/10, Batch 263/883, Training Loss: 0.8341
Epoch 2/10, Batch 264/883, Training Loss: 0.8502
Epoch 2/10, Batch 265/883, Training Loss: 0.9288
Epoch 2/10, Batch 266/883, Training Loss: 0.9287
Epoch 2/10, Batch 267/883, Training Loss: 0.8661
Epoch 2/10, Batch 268/883, Training Loss: 0.8760
Epoch 2/10, Batch 269/883, Training Loss: 0.7542
Epoch 2/10, Batch 270/883, Training Loss: 0.9084
Epoch 2/10, Batch 271/883, Training Loss: 0.9014
Epoch 2/10, Batch 272/883, Training Loss: 0.8623
Epoch 2/10, Batch 273/883, Training Loss: 0.9214
Epoch 2/10, Batch 274/883, Training Loss: 0.7343
Epoch 2/10, Batch 275/883, Training Loss: 0.7984
Epoch 2/10, Batch 276/883, Training Loss: 0.7394
Epoch 2/10, Batch 277/883, Training Loss: 0.8981
Epoch 2/10, Batch 278/883, Training Loss: 0.9542
Epoch 2/10, Batch 279/883, Training Loss: 0.5675
Epoch 2/10, Batch 280/883, Training Loss: 0.6062
Epoch 2/10, Batch 281/883, Training Loss: 1.0410
Epoch 2/10, Batch 282/883, Training Loss: 0.9154
Epoch 2/10, Batch 283/883, Training Loss: 0.7428
Epoch 2/10, Batch 284/883, Training Loss: 0.7800
Epoch 2/10, Batch 285/883, Training Loss: 0.7849
Epoch 2/10, Batch 286/883, Training Loss: 0.8590
Epoch 2/10, Batch 287/883, Training Loss: 0.8000
Epoch 2/10, Batch 288/883, Training Loss: 0.6274
Epoch 2/10, Batch 289/883, Training Loss: 0.6733
Epoch 2/10, Batch 290/883, Training Loss: 0.9851
Epoch 2/10, Batch 291/883, Training Loss: 1.0783
Epoch 2/10, Batch 292/883, Training Loss: 0.9886
Epoch 2/10, Batch 293/883, Training Loss: 0.9676
Epoch 2/10, Batch 294/883, Training Loss: 1.0545
Epoch 2/10, Batch 295/883, Training Loss: 0.7955
Epoch 2/10, Batch 296/883, Training Loss: 0.7737
Epoch 2/10, Batch 297/883, Training Loss: 0.8849
Epoch 2/10, Batch 298/883, Training Loss: 0.9649
Epoch 2/10, Batch 299/883, Training Loss: 0.8383
Epoch 2/10, Batch 300/883, Training Loss: 0.9012
Epoch 2/10, Batch 301/883, Training Loss: 0.7469
Epoch 2/10, Batch 302/883, Training Loss: 0.8198
Epoch 2/10, Batch 303/883, Training Loss: 0.8893
Epoch 2/10, Batch 304/883, Training Loss: 0.9315
Epoch 2/10, Batch 305/883, Training Loss: 0.7537
Epoch 2/10, Batch 306/883, Training Loss: 1.1024
Epoch 2/10, Batch 307/883, Training Loss: 0.8283
Epoch 2/10, Batch 308/883, Training Loss: 0.8157
Epoch 2/10, Batch 309/883, Training Loss: 0.7947
Epoch 2/10, Batch 310/883, Training Loss: 0.8427
Epoch 2/10, Batch 311/883, Training Loss: 0.7926
Epoch 2/10, Batch 312/883, Training Loss: 0.9066
Epoch 2/10, Batch 313/883, Training Loss: 0.9462
Epoch 2/10, Batch 314/883, Training Loss: 0.9099
Epoch 2/10, Batch 315/883, Training Loss: 0.7686
Epoch 2/10, Batch 316/883, Training Loss: 0.6936
Epoch 2/10, Batch 317/883, Training Loss: 0.8133
Epoch 2/10, Batch 318/883, Training Loss: 0.8694
Epoch 2/10, Batch 319/883, Training Loss: 0.7598
Epoch 2/10, Batch 320/883, Training Loss: 0.7474
Epoch 2/10, Batch 321/883, Training Loss: 0.7815
Epoch 2/10, Batch 322/883, Training Loss: 0.7874
Epoch 2/10, Batch 323/883, Training Loss: 0.9521
Epoch 2/10, Batch 324/883, Training Loss: 0.9126
Epoch 2/10, Batch 325/883, Training Loss: 1.6082
Epoch 2/10, Batch 326/883, Training Loss: 1.1435
Epoch 2/10, Batch 327/883, Training Loss: 0.8394
Epoch 2/10, Batch 328/883, Training Loss: 0.6633
Epoch 2/10, Batch 329/883, Training Loss: 0.7876
Epoch 2/10, Batch 330/883, Training Loss: 0.7372
Epoch 2/10, Batch 331/883, Training Loss: 0.9055
Epoch 2/10, Batch 332/883, Training Loss: 0.6776
Epoch 2/10, Batch 333/883, Training Loss: 0.7915
Epoch 2/10, Batch 334/883, Training Loss: 0.8364
Epoch 2/10, Batch 335/883, Training Loss: 0.7391
Epoch 2/10, Batch 336/883, Training Loss: 0.8544
Epoch 2/10, Batch 337/883, Training Loss: 0.8617
Epoch 2/10, Batch 338/883, Training Loss: 0.6422
Epoch 2/10, Batch 339/883, Training Loss: 0.9036
Epoch 2/10, Batch 340/883, Training Loss: 0.7771
Epoch 2/10, Batch 341/883, Training Loss: 0.8169
Epoch 2/10, Batch 342/883, Training Loss: 0.8724
Epoch 2/10, Batch 343/883, Training Loss: 0.5419
Epoch 2/10, Batch 344/883, Training Loss: 0.8285
Epoch 2/10, Batch 345/883, Training Loss: 1.0493
Epoch 2/10, Batch 346/883, Training Loss: 0.8542
Epoch 2/10, Batch 347/883, Training Loss: 1.1176
Epoch 2/10, Batch 348/883, Training Loss: 0.9170
Epoch 2/10, Batch 349/883, Training Loss: 0.7660
Epoch 2/10, Batch 350/883, Training Loss: 0.7966
Epoch 2/10, Batch 351/883, Training Loss: 0.8072
Epoch 2/10, Batch 352/883, Training Loss: 0.8181
Epoch 2/10, Batch 353/883, Training Loss: 0.9622
Epoch 2/10, Batch 354/883, Training Loss: 1.0796
Epoch 2/10, Batch 355/883, Training Loss: 0.9641
Epoch 2/10, Batch 356/883, Training Loss: 0.8728
Epoch 2/10, Batch 357/883, Training Loss: 0.7167
Epoch 2/10, Batch 358/883, Training Loss: 0.8588
Epoch 2/10, Batch 359/883, Training Loss: 0.8970
Epoch 2/10, Batch 360/883, Training Loss: 0.7888
Epoch 2/10, Batch 361/883, Training Loss: 0.6450
Epoch 2/10, Batch 362/883, Training Loss: 0.9379
Epoch 2/10, Batch 363/883, Training Loss: 0.6696
Epoch 2/10, Batch 364/883, Training Loss: 0.7086
Epoch 2/10, Batch 365/883, Training Loss: 0.6711
Epoch 2/10, Batch 366/883, Training Loss: 0.9893
Epoch 2/10, Batch 367/883, Training Loss: 1.0368
Epoch 2/10, Batch 368/883, Training Loss: 0.6947
Epoch 2/10, Batch 369/883, Training Loss: 1.0959
Epoch 2/10, Batch 370/883, Training Loss: 1.0300
Epoch 2/10, Batch 371/883, Training Loss: 1.0254
Epoch 2/10, Batch 372/883, Training Loss: 0.8576
Epoch 2/10, Batch 373/883, Training Loss: 0.9053
Epoch 2/10, Batch 374/883, Training Loss: 1.0001
Epoch 2/10, Batch 375/883, Training Loss: 0.7243
Epoch 2/10, Batch 376/883, Training Loss: 0.8415
Epoch 2/10, Batch 377/883, Training Loss: 0.9432
Epoch 2/10, Batch 378/883, Training Loss: 1.0327
Epoch 2/10, Batch 379/883, Training Loss: 0.7907
Epoch 2/10, Batch 380/883, Training Loss: 0.8612
Epoch 2/10, Batch 381/883, Training Loss: 0.7745
Epoch 2/10, Batch 382/883, Training Loss: 0.9327
Epoch 2/10, Batch 383/883, Training Loss: 0.8815
Epoch 2/10, Batch 384/883, Training Loss: 1.0651
Epoch 2/10, Batch 385/883, Training Loss: 0.6666
Epoch 2/10, Batch 386/883, Training Loss: 0.8421
Epoch 2/10, Batch 387/883, Training Loss: 0.9306
Epoch 2/10, Batch 388/883, Training Loss: 0.7220
Epoch 2/10, Batch 389/883, Training Loss: 0.7719
Epoch 2/10, Batch 390/883, Training Loss: 0.8822
Epoch 2/10, Batch 391/883, Training Loss: 0.7286
Epoch 2/10, Batch 392/883, Training Loss: 0.7653
Epoch 2/10, Batch 393/883, Training Loss: 0.7652
Epoch 2/10, Batch 394/883, Training Loss: 0.9942
Epoch 2/10, Batch 395/883, Training Loss: 0.8637
Epoch 2/10, Batch 396/883, Training Loss: 0.6555
Epoch 2/10, Batch 397/883, Training Loss: 0.7126
Epoch 2/10, Batch 398/883, Training Loss: 0.7474
Epoch 2/10, Batch 399/883, Training Loss: 0.7128
Epoch 2/10, Batch 400/883, Training Loss: 0.8127
Epoch 2/10, Batch 401/883, Training Loss: 0.7462
Epoch 2/10, Batch 402/883, Training Loss: 1.2243
Epoch 2/10, Batch 403/883, Training Loss: 1.1520
Epoch 2/10, Batch 404/883, Training Loss: 1.0365
Epoch 2/10, Batch 405/883, Training Loss: 0.7290
Epoch 2/10, Batch 406/883, Training Loss: 0.9259
Epoch 2/10, Batch 407/883, Training Loss: 0.7501
Epoch 2/10, Batch 408/883, Training Loss: 0.7352
Epoch 2/10, Batch 409/883, Training Loss: 0.7344
Epoch 2/10, Batch 410/883, Training Loss: 0.8721
Epoch 2/10, Batch 411/883, Training Loss: 0.6805
Epoch 2/10, Batch 412/883, Training Loss: 0.7788
Epoch 2/10, Batch 413/883, Training Loss: 0.9095
Epoch 2/10, Batch 414/883, Training Loss: 0.7099
Epoch 2/10, Batch 415/883, Training Loss: 0.7593
Epoch 2/10, Batch 416/883, Training Loss: 0.8435
Epoch 2/10, Batch 417/883, Training Loss: 0.7963
Epoch 2/10, Batch 418/883, Training Loss: 0.6342
Epoch 2/10, Batch 419/883, Training Loss: 0.6945
Epoch 2/10, Batch 420/883, Training Loss: 0.8650
Epoch 2/10, Batch 421/883, Training Loss: 0.7368
Epoch 2/10, Batch 422/883, Training Loss: 1.0874
Epoch 2/10, Batch 423/883, Training Loss: 0.7979
Epoch 2/10, Batch 424/883, Training Loss: 0.6861
Epoch 2/10, Batch 425/883, Training Loss: 0.8968
Epoch 2/10, Batch 426/883, Training Loss: 0.6547
Epoch 2/10, Batch 427/883, Training Loss: 0.6547
Epoch 2/10, Batch 428/883, Training Loss: 0.8732
Epoch 2/10, Batch 429/883, Training Loss: 0.8568
Epoch 2/10, Batch 430/883, Training Loss: 0.8570
Epoch 2/10, Batch 431/883, Training Loss: 0.8354
Epoch 2/10, Batch 432/883, Training Loss: 1.1310
Epoch 2/10, Batch 433/883, Training Loss: 1.2537
Epoch 2/10, Batch 434/883, Training Loss: 0.6826
Epoch 2/10, Batch 435/883, Training Loss: 0.9584
Epoch 2/10, Batch 436/883, Training Loss: 0.9531
Epoch 2/10, Batch 437/883, Training Loss: 0.8828
Epoch 2/10, Batch 438/883, Training Loss: 0.6529
Epoch 2/10, Batch 439/883, Training Loss: 0.6262
Epoch 2/10, Batch 440/883, Training Loss: 0.8094
Epoch 2/10, Batch 441/883, Training Loss: 0.6626
Epoch 2/10, Batch 442/883, Training Loss: 0.7037
Epoch 2/10, Batch 443/883, Training Loss: 0.6406
Epoch 2/10, Batch 444/883, Training Loss: 0.6394
Epoch 2/10, Batch 445/883, Training Loss: 0.9814
Epoch 2/10, Batch 446/883, Training Loss: 0.6581
Epoch 2/10, Batch 447/883, Training Loss: 0.6770
Epoch 2/10, Batch 448/883, Training Loss: 0.8064
Epoch 2/10, Batch 449/883, Training Loss: 0.8624
Epoch 2/10, Batch 450/883, Training Loss: 0.6789
Epoch 2/10, Batch 451/883, Training Loss: 0.9984
Epoch 2/10, Batch 452/883, Training Loss: 1.0313
Epoch 2/10, Batch 453/883, Training Loss: 0.6392
Epoch 2/10, Batch 454/883, Training Loss: 0.6835
Epoch 2/10, Batch 455/883, Training Loss: 0.9183
Epoch 2/10, Batch 456/883, Training Loss: 0.7933
Epoch 2/10, Batch 457/883, Training Loss: 0.8230
Epoch 2/10, Batch 458/883, Training Loss: 1.1058
Epoch 2/10, Batch 459/883, Training Loss: 0.9411
Epoch 2/10, Batch 460/883, Training Loss: 0.8451
Epoch 2/10, Batch 461/883, Training Loss: 1.0598
Epoch 2/10, Batch 462/883, Training Loss: 0.9590
Epoch 2/10, Batch 463/883, Training Loss: 0.8223
Epoch 2/10, Batch 464/883, Training Loss: 0.9699
Epoch 2/10, Batch 465/883, Training Loss: 0.7963
Epoch 2/10, Batch 466/883, Training Loss: 1.0546
Epoch 2/10, Batch 467/883, Training Loss: 0.8386
Epoch 2/10, Batch 468/883, Training Loss: 0.9743
Epoch 2/10, Batch 469/883, Training Loss: 0.8331
Epoch 2/10, Batch 470/883, Training Loss: 0.8517
Epoch 2/10, Batch 471/883, Training Loss: 1.0370
Epoch 2/10, Batch 472/883, Training Loss: 0.8811
Epoch 2/10, Batch 473/883, Training Loss: 0.9285
Epoch 2/10, Batch 474/883, Training Loss: 0.7920
Epoch 2/10, Batch 475/883, Training Loss: 0.8557
Epoch 2/10, Batch 476/883, Training Loss: 0.8649
Epoch 2/10, Batch 477/883, Training Loss: 0.7962
Epoch 2/10, Batch 478/883, Training Loss: 0.7317
Epoch 2/10, Batch 479/883, Training Loss: 1.0197
Epoch 2/10, Batch 480/883, Training Loss: 0.9617
Epoch 2/10, Batch 481/883, Training Loss: 0.6664
Epoch 2/10, Batch 482/883, Training Loss: 0.7347
Epoch 2/10, Batch 483/883, Training Loss: 0.6111
Epoch 2/10, Batch 484/883, Training Loss: 1.1719
Epoch 2/10, Batch 485/883, Training Loss: 0.6805
Epoch 2/10, Batch 486/883, Training Loss: 0.8721
Epoch 2/10, Batch 487/883, Training Loss: 0.7404
Epoch 2/10, Batch 488/883, Training Loss: 0.7152
Epoch 2/10, Batch 489/883, Training Loss: 0.8410
Epoch 2/10, Batch 490/883, Training Loss: 0.6492
Epoch 2/10, Batch 491/883, Training Loss: 0.9287
Epoch 2/10, Batch 492/883, Training Loss: 1.1807
Epoch 2/10, Batch 493/883, Training Loss: 0.8966
Epoch 2/10, Batch 494/883, Training Loss: 0.7205
Epoch 2/10, Batch 495/883, Training Loss: 0.5552
Epoch 2/10, Batch 496/883, Training Loss: 0.7547
Epoch 2/10, Batch 497/883, Training Loss: 0.7753
Epoch 2/10, Batch 498/883, Training Loss: 0.8668
Epoch 2/10, Batch 499/883, Training Loss: 0.5535
Epoch 2/10, Batch 500/883, Training Loss: 1.0440
Epoch 2/10, Batch 501/883, Training Loss: 0.7225
Epoch 2/10, Batch 502/883, Training Loss: 0.9192
Epoch 2/10, Batch 503/883, Training Loss: 0.8497
Epoch 2/10, Batch 504/883, Training Loss: 0.7121
Epoch 2/10, Batch 505/883, Training Loss: 0.5415
Epoch 2/10, Batch 506/883, Training Loss: 1.0253
Epoch 2/10, Batch 507/883, Training Loss: 1.0467
Epoch 2/10, Batch 508/883, Training Loss: 1.3409
Epoch 2/10, Batch 509/883, Training Loss: 0.7027
Epoch 2/10, Batch 510/883, Training Loss: 0.9839
Epoch 2/10, Batch 511/883, Training Loss: 0.7246
Epoch 2/10, Batch 512/883, Training Loss: 0.8168
Epoch 2/10, Batch 513/883, Training Loss: 0.8157
Epoch 2/10, Batch 514/883, Training Loss: 1.0255
Epoch 2/10, Batch 515/883, Training Loss: 0.9826
Epoch 2/10, Batch 516/883, Training Loss: 0.7119
Epoch 2/10, Batch 517/883, Training Loss: 0.7801
Epoch 2/10, Batch 518/883, Training Loss: 0.5164
Epoch 2/10, Batch 519/883, Training Loss: 0.8813
Epoch 2/10, Batch 520/883, Training Loss: 1.0842
Epoch 2/10, Batch 521/883, Training Loss: 0.7279
Epoch 2/10, Batch 522/883, Training Loss: 0.9899
Epoch 2/10, Batch 523/883, Training Loss: 0.7363
Epoch 2/10, Batch 524/883, Training Loss: 0.6470
Epoch 2/10, Batch 525/883, Training Loss: 1.1189
Epoch 2/10, Batch 526/883, Training Loss: 0.5841
Epoch 2/10, Batch 527/883, Training Loss: 0.7270
Epoch 2/10, Batch 528/883, Training Loss: 1.2961
Epoch 2/10, Batch 529/883, Training Loss: 1.1752
Epoch 2/10, Batch 530/883, Training Loss: 0.7556
Epoch 2/10, Batch 531/883, Training Loss: 0.7892
Epoch 2/10, Batch 532/883, Training Loss: 0.9757
Epoch 2/10, Batch 533/883, Training Loss: 0.7554
Epoch 2/10, Batch 534/883, Training Loss: 0.8263
Epoch 2/10, Batch 535/883, Training Loss: 1.0597
Epoch 2/10, Batch 536/883, Training Loss: 0.9377
Epoch 2/10, Batch 537/883, Training Loss: 0.8090
Epoch 2/10, Batch 538/883, Training Loss: 0.8646
Epoch 2/10, Batch 539/883, Training Loss: 0.6861
Epoch 2/10, Batch 540/883, Training Loss: 1.0245
Epoch 2/10, Batch 541/883, Training Loss: 0.8407
Epoch 2/10, Batch 542/883, Training Loss: 0.6873
Epoch 2/10, Batch 543/883, Training Loss: 0.7494
Epoch 2/10, Batch 544/883, Training Loss: 0.6587
Epoch 2/10, Batch 545/883, Training Loss: 0.8996
Epoch 2/10, Batch 546/883, Training Loss: 1.1130
Epoch 2/10, Batch 547/883, Training Loss: 0.6530
Epoch 2/10, Batch 548/883, Training Loss: 0.7661
Epoch 2/10, Batch 549/883, Training Loss: 0.5868
Epoch 2/10, Batch 550/883, Training Loss: 0.4999
Epoch 2/10, Batch 551/883, Training Loss: 0.7802
Epoch 2/10, Batch 552/883, Training Loss: 0.9803
Epoch 2/10, Batch 553/883, Training Loss: 1.1527
Epoch 2/10, Batch 554/883, Training Loss: 0.7137
Epoch 2/10, Batch 555/883, Training Loss: 0.7635
Epoch 2/10, Batch 556/883, Training Loss: 0.7027
Epoch 2/10, Batch 557/883, Training Loss: 0.8532
Epoch 2/10, Batch 558/883, Training Loss: 0.7181
Epoch 2/10, Batch 559/883, Training Loss: 0.8889
Epoch 2/10, Batch 560/883, Training Loss: 0.7110
Epoch 2/10, Batch 561/883, Training Loss: 0.9345
Epoch 2/10, Batch 562/883, Training Loss: 0.8067
Epoch 2/10, Batch 563/883, Training Loss: 1.1398
Epoch 2/10, Batch 564/883, Training Loss: 0.7229
Epoch 2/10, Batch 565/883, Training Loss: 0.7596
Epoch 2/10, Batch 566/883, Training Loss: 0.8592
Epoch 2/10, Batch 567/883, Training Loss: 0.7461
Epoch 2/10, Batch 568/883, Training Loss: 0.7179
Epoch 2/10, Batch 569/883, Training Loss: 0.8529
Epoch 2/10, Batch 570/883, Training Loss: 0.8652
Epoch 2/10, Batch 571/883, Training Loss: 0.8734
Epoch 2/10, Batch 572/883, Training Loss: 0.6160
Epoch 2/10, Batch 573/883, Training Loss: 0.6353
Epoch 2/10, Batch 574/883, Training Loss: 0.7240
Epoch 2/10, Batch 575/883, Training Loss: 0.6265
Epoch 2/10, Batch 576/883, Training Loss: 0.9833
Epoch 2/10, Batch 577/883, Training Loss: 0.8225
Epoch 2/10, Batch 578/883, Training Loss: 1.1469
Epoch 2/10, Batch 579/883, Training Loss: 0.5056
Epoch 2/10, Batch 580/883, Training Loss: 0.9867
Epoch 2/10, Batch 581/883, Training Loss: 0.8261
Epoch 2/10, Batch 582/883, Training Loss: 0.5699
Epoch 2/10, Batch 583/883, Training Loss: 1.1355
Epoch 2/10, Batch 584/883, Training Loss: 1.2138
Epoch 2/10, Batch 585/883, Training Loss: 0.9828
Epoch 2/10, Batch 586/883, Training Loss: 0.6770
Epoch 2/10, Batch 587/883, Training Loss: 0.8643
Epoch 2/10, Batch 588/883, Training Loss: 0.8268
Epoch 2/10, Batch 589/883, Training Loss: 0.8530
Epoch 2/10, Batch 590/883, Training Loss: 0.7919
Epoch 2/10, Batch 591/883, Training Loss: 1.1556
Epoch 2/10, Batch 592/883, Training Loss: 0.7906
Epoch 2/10, Batch 593/883, Training Loss: 0.8765
Epoch 2/10, Batch 594/883, Training Loss: 0.9463
Epoch 2/10, Batch 595/883, Training Loss: 0.7528
Epoch 2/10, Batch 596/883, Training Loss: 0.8427
Epoch 2/10, Batch 597/883, Training Loss: 0.8833
Epoch 2/10, Batch 598/883, Training Loss: 0.7873
Epoch 2/10, Batch 599/883, Training Loss: 0.8105
Epoch 2/10, Batch 600/883, Training Loss: 0.9310
Epoch 2/10, Batch 601/883, Training Loss: 0.8936
Epoch 2/10, Batch 602/883, Training Loss: 0.8520
Epoch 2/10, Batch 603/883, Training Loss: 0.8621
Epoch 2/10, Batch 604/883, Training Loss: 0.7880
Epoch 2/10, Batch 605/883, Training Loss: 0.8676
Epoch 2/10, Batch 606/883, Training Loss: 0.9796
Epoch 2/10, Batch 607/883, Training Loss: 0.7074
Epoch 2/10, Batch 608/883, Training Loss: 0.5709
Epoch 2/10, Batch 609/883, Training Loss: 0.8850
Epoch 2/10, Batch 610/883, Training Loss: 0.8958
Epoch 2/10, Batch 611/883, Training Loss: 1.0022
Epoch 2/10, Batch 612/883, Training Loss: 0.9190
Epoch 2/10, Batch 613/883, Training Loss: 0.6391
Epoch 2/10, Batch 614/883, Training Loss: 0.7811
Epoch 2/10, Batch 615/883, Training Loss: 0.9643
Epoch 2/10, Batch 616/883, Training Loss: 0.9004
Epoch 2/10, Batch 617/883, Training Loss: 0.8575
Epoch 2/10, Batch 618/883, Training Loss: 1.0387
Epoch 2/10, Batch 619/883, Training Loss: 0.7281
Epoch 2/10, Batch 620/883, Training Loss: 0.8103
Epoch 2/10, Batch 621/883, Training Loss: 0.6898
Epoch 2/10, Batch 622/883, Training Loss: 0.7777
Epoch 2/10, Batch 623/883, Training Loss: 1.1648
Epoch 2/10, Batch 624/883, Training Loss: 0.8116
Epoch 2/10, Batch 625/883, Training Loss: 0.8497
Epoch 2/10, Batch 626/883, Training Loss: 1.0113
Epoch 2/10, Batch 627/883, Training Loss: 0.4255
Epoch 2/10, Batch 628/883, Training Loss: 0.7695
Epoch 2/10, Batch 629/883, Training Loss: 0.5171
Epoch 2/10, Batch 630/883, Training Loss: 0.6202
Epoch 2/10, Batch 631/883, Training Loss: 0.7805
Epoch 2/10, Batch 632/883, Training Loss: 0.8753
Epoch 2/10, Batch 633/883, Training Loss: 0.9672
Epoch 2/10, Batch 634/883, Training Loss: 0.8924
Epoch 2/10, Batch 635/883, Training Loss: 0.6415
Epoch 2/10, Batch 636/883, Training Loss: 0.9242
Epoch 2/10, Batch 637/883, Training Loss: 0.6813
Epoch 2/10, Batch 638/883, Training Loss: 0.7454
Epoch 2/10, Batch 639/883, Training Loss: 0.6096
Epoch 2/10, Batch 640/883, Training Loss: 0.9457
Epoch 2/10, Batch 641/883, Training Loss: 0.7411
Epoch 2/10, Batch 642/883, Training Loss: 0.7509
Epoch 2/10, Batch 643/883, Training Loss: 0.9320
Epoch 2/10, Batch 644/883, Training Loss: 0.8300
Epoch 2/10, Batch 645/883, Training Loss: 0.9930
Epoch 2/10, Batch 646/883, Training Loss: 0.6824
Epoch 2/10, Batch 647/883, Training Loss: 0.9556
Epoch 2/10, Batch 648/883, Training Loss: 1.0583
Epoch 2/10, Batch 649/883, Training Loss: 0.8142
Epoch 2/10, Batch 650/883, Training Loss: 0.7184
Epoch 2/10, Batch 651/883, Training Loss: 0.9981
Epoch 2/10, Batch 652/883, Training Loss: 0.7271
Epoch 2/10, Batch 653/883, Training Loss: 0.6354
Epoch 2/10, Batch 654/883, Training Loss: 1.3615
Epoch 2/10, Batch 655/883, Training Loss: 0.7367
Epoch 2/10, Batch 656/883, Training Loss: 0.8141
Epoch 2/10, Batch 657/883, Training Loss: 1.1086
Epoch 2/10, Batch 658/883, Training Loss: 0.6938
Epoch 2/10, Batch 659/883, Training Loss: 0.6148
Epoch 2/10, Batch 660/883, Training Loss: 0.7425
Epoch 2/10, Batch 661/883, Training Loss: 0.7234
Epoch 2/10, Batch 662/883, Training Loss: 0.6378
Epoch 2/10, Batch 663/883, Training Loss: 0.7985
Epoch 2/10, Batch 664/883, Training Loss: 0.8523
Epoch 2/10, Batch 665/883, Training Loss: 0.8917
Epoch 2/10, Batch 666/883, Training Loss: 0.6959
Epoch 2/10, Batch 667/883, Training Loss: 0.8124
Epoch 2/10, Batch 668/883, Training Loss: 0.7961
Epoch 2/10, Batch 669/883, Training Loss: 0.7008
Epoch 2/10, Batch 670/883, Training Loss: 0.9138
Epoch 2/10, Batch 671/883, Training Loss: 0.7013
Epoch 2/10, Batch 672/883, Training Loss: 0.7425
Epoch 2/10, Batch 673/883, Training Loss: 0.5860
Epoch 2/10, Batch 674/883, Training Loss: 0.7822
Epoch 2/10, Batch 675/883, Training Loss: 0.5573
Epoch 2/10, Batch 676/883, Training Loss: 0.6678
Epoch 2/10, Batch 677/883, Training Loss: 0.9959
Epoch 2/10, Batch 678/883, Training Loss: 1.0552
Epoch 2/10, Batch 679/883, Training Loss: 0.9050
Epoch 2/10, Batch 680/883, Training Loss: 0.6941
Epoch 2/10, Batch 681/883, Training Loss: 0.9725
Epoch 2/10, Batch 682/883, Training Loss: 0.6128
Epoch 2/10, Batch 683/883, Training Loss: 0.7364
Epoch 2/10, Batch 684/883, Training Loss: 0.6324
Epoch 2/10, Batch 685/883, Training Loss: 0.6911
Epoch 2/10, Batch 686/883, Training Loss: 0.6547
Epoch 2/10, Batch 687/883, Training Loss: 0.7267
Epoch 2/10, Batch 688/883, Training Loss: 0.8117
Epoch 2/10, Batch 689/883, Training Loss: 0.6064
Epoch 2/10, Batch 690/883, Training Loss: 0.7355
Epoch 2/10, Batch 691/883, Training Loss: 0.7618
Epoch 2/10, Batch 692/883, Training Loss: 0.7338
Epoch 2/10, Batch 693/883, Training Loss: 0.8943
Epoch 2/10, Batch 694/883, Training Loss: 0.5514
Epoch 2/10, Batch 695/883, Training Loss: 0.7974
Epoch 2/10, Batch 696/883, Training Loss: 0.7066
Epoch 2/10, Batch 697/883, Training Loss: 0.6955
Epoch 2/10, Batch 698/883, Training Loss: 1.4959
Epoch 2/10, Batch 699/883, Training Loss: 0.6718
Epoch 2/10, Batch 700/883, Training Loss: 0.6903
Epoch 2/10, Batch 701/883, Training Loss: 0.9519
Epoch 2/10, Batch 702/883, Training Loss: 0.8090
Epoch 2/10, Batch 703/883, Training Loss: 0.7796
Epoch 2/10, Batch 704/883, Training Loss: 0.6575
Epoch 2/10, Batch 705/883, Training Loss: 0.6508
Epoch 2/10, Batch 706/883, Training Loss: 0.9765
Epoch 2/10, Batch 707/883, Training Loss: 0.8430
Epoch 2/10, Batch 708/883, Training Loss: 0.7534
Epoch 2/10, Batch 709/883, Training Loss: 0.7763
Epoch 2/10, Batch 710/883, Training Loss: 0.7079
Epoch 2/10, Batch 711/883, Training Loss: 0.8386
Epoch 2/10, Batch 712/883, Training Loss: 0.9956
Epoch 2/10, Batch 713/883, Training Loss: 0.6942
Epoch 2/10, Batch 714/883, Training Loss: 0.9134
Epoch 2/10, Batch 715/883, Training Loss: 1.1382
Epoch 2/10, Batch 716/883, Training Loss: 0.9952
Epoch 2/10, Batch 717/883, Training Loss: 0.8353
Epoch 2/10, Batch 718/883, Training Loss: 0.7516
Epoch 2/10, Batch 719/883, Training Loss: 0.9609
Epoch 2/10, Batch 720/883, Training Loss: 0.7983
Epoch 2/10, Batch 721/883, Training Loss: 1.0448
Epoch 2/10, Batch 722/883, Training Loss: 1.3459
Epoch 2/10, Batch 723/883, Training Loss: 0.7765
Epoch 2/10, Batch 724/883, Training Loss: 0.8542
Epoch 2/10, Batch 725/883, Training Loss: 1.0321
Epoch 2/10, Batch 726/883, Training Loss: 0.8810
Epoch 2/10, Batch 727/883, Training Loss: 0.8150
Epoch 2/10, Batch 728/883, Training Loss: 0.9962
Epoch 2/10, Batch 729/883, Training Loss: 0.8991
Epoch 2/10, Batch 730/883, Training Loss: 0.8274
Epoch 2/10, Batch 731/883, Training Loss: 0.8848
Epoch 2/10, Batch 732/883, Training Loss: 0.7198
Epoch 2/10, Batch 733/883, Training Loss: 0.7671
Epoch 2/10, Batch 734/883, Training Loss: 0.8359
Epoch 2/10, Batch 735/883, Training Loss: 0.7545
Epoch 2/10, Batch 736/883, Training Loss: 0.7841
Epoch 2/10, Batch 737/883, Training Loss: 0.9463
Epoch 2/10, Batch 738/883, Training Loss: 0.8199
Epoch 2/10, Batch 739/883, Training Loss: 0.7203
Epoch 2/10, Batch 740/883, Training Loss: 0.7494
Epoch 2/10, Batch 741/883, Training Loss: 0.7389
Epoch 2/10, Batch 742/883, Training Loss: 0.6891
Epoch 2/10, Batch 743/883, Training Loss: 0.6811
Epoch 2/10, Batch 744/883, Training Loss: 0.6972
Epoch 2/10, Batch 745/883, Training Loss: 0.8311
Epoch 2/10, Batch 746/883, Training Loss: 0.7706
Epoch 2/10, Batch 747/883, Training Loss: 0.8062
Epoch 2/10, Batch 748/883, Training Loss: 0.6232
Epoch 2/10, Batch 749/883, Training Loss: 1.1299
Epoch 2/10, Batch 750/883, Training Loss: 0.7689
Epoch 2/10, Batch 751/883, Training Loss: 1.1939
Epoch 2/10, Batch 752/883, Training Loss: 0.6996
Epoch 2/10, Batch 753/883, Training Loss: 0.7961
Epoch 2/10, Batch 754/883, Training Loss: 0.9395
Epoch 2/10, Batch 755/883, Training Loss: 0.7763
Epoch 2/10, Batch 756/883, Training Loss: 0.7823
Epoch 2/10, Batch 757/883, Training Loss: 1.1060
Epoch 2/10, Batch 758/883, Training Loss: 0.6916
Epoch 2/10, Batch 759/883, Training Loss: 1.6227
Epoch 2/10, Batch 760/883, Training Loss: 0.7002
Epoch 2/10, Batch 761/883, Training Loss: 0.9217
Epoch 2/10, Batch 762/883, Training Loss: 1.0404
Epoch 2/10, Batch 763/883, Training Loss: 0.7234
Epoch 2/10, Batch 764/883, Training Loss: 0.9079
Epoch 2/10, Batch 765/883, Training Loss: 0.9325
Epoch 2/10, Batch 766/883, Training Loss: 0.7835
Epoch 2/10, Batch 767/883, Training Loss: 0.7124
Epoch 2/10, Batch 768/883, Training Loss: 0.7549
Epoch 2/10, Batch 769/883, Training Loss: 0.7573
Epoch 2/10, Batch 770/883, Training Loss: 0.8316
Epoch 2/10, Batch 771/883, Training Loss: 0.7518
Epoch 2/10, Batch 772/883, Training Loss: 0.9057
Epoch 2/10, Batch 773/883, Training Loss: 0.8378
Epoch 2/10, Batch 774/883, Training Loss: 0.5861
Epoch 2/10, Batch 775/883, Training Loss: 0.5124
Epoch 2/10, Batch 776/883, Training Loss: 1.0281
Epoch 2/10, Batch 777/883, Training Loss: 0.5466
Epoch 2/10, Batch 778/883, Training Loss: 0.9714
Epoch 2/10, Batch 779/883, Training Loss: 0.7893
Epoch 2/10, Batch 780/883, Training Loss: 0.7505
Epoch 2/10, Batch 781/883, Training Loss: 0.6427
Epoch 2/10, Batch 782/883, Training Loss: 0.8463
Epoch 2/10, Batch 783/883, Training Loss: 0.8825
Epoch 2/10, Batch 784/883, Training Loss: 0.7228
Epoch 2/10, Batch 785/883, Training Loss: 0.9986
Epoch 2/10, Batch 786/883, Training Loss: 0.6942
Epoch 2/10, Batch 787/883, Training Loss: 0.7244
Epoch 2/10, Batch 788/883, Training Loss: 0.7691
Epoch 2/10, Batch 789/883, Training Loss: 0.7320
Epoch 2/10, Batch 790/883, Training Loss: 1.0683
Epoch 2/10, Batch 791/883, Training Loss: 0.7271
Epoch 2/10, Batch 792/883, Training Loss: 0.8903
Epoch 2/10, Batch 793/883, Training Loss: 0.9971
Epoch 2/10, Batch 794/883, Training Loss: 0.8175
Epoch 2/10, Batch 795/883, Training Loss: 0.7360
Epoch 2/10, Batch 796/883, Training Loss: 0.8966
Epoch 2/10, Batch 797/883, Training Loss: 0.8877
Epoch 2/10, Batch 798/883, Training Loss: 0.8746
Epoch 2/10, Batch 799/883, Training Loss: 0.8617
Epoch 2/10, Batch 800/883, Training Loss: 0.8800
Epoch 2/10, Batch 801/883, Training Loss: 0.8036
Epoch 2/10, Batch 802/883, Training Loss: 0.9008
Epoch 2/10, Batch 803/883, Training Loss: 0.7445
Epoch 2/10, Batch 804/883, Training Loss: 1.0326
Epoch 2/10, Batch 805/883, Training Loss: 0.7078
Epoch 2/10, Batch 806/883, Training Loss: 0.7736
Epoch 2/10, Batch 807/883, Training Loss: 0.8160
Epoch 2/10, Batch 808/883, Training Loss: 0.7041
Epoch 2/10, Batch 809/883, Training Loss: 0.6191
Epoch 2/10, Batch 810/883, Training Loss: 0.8062
Epoch 2/10, Batch 811/883, Training Loss: 0.6892
Epoch 2/10, Batch 812/883, Training Loss: 0.8987
Epoch 2/10, Batch 813/883, Training Loss: 0.7005
Epoch 2/10, Batch 814/883, Training Loss: 1.0147
Epoch 2/10, Batch 815/883, Training Loss: 0.8232
Epoch 2/10, Batch 816/883, Training Loss: 0.8578
Epoch 2/10, Batch 817/883, Training Loss: 0.9987
Epoch 2/10, Batch 818/883, Training Loss: 0.6565
Epoch 2/10, Batch 819/883, Training Loss: 0.7599
Epoch 2/10, Batch 820/883, Training Loss: 0.9217
Epoch 2/10, Batch 821/883, Training Loss: 0.9009
Epoch 2/10, Batch 822/883, Training Loss: 0.6332
Epoch 2/10, Batch 823/883, Training Loss: 0.7585
Epoch 2/10, Batch 824/883, Training Loss: 0.8525
Epoch 2/10, Batch 825/883, Training Loss: 0.8069
Epoch 2/10, Batch 826/883, Training Loss: 0.7870
Epoch 2/10, Batch 827/883, Training Loss: 0.8785
Epoch 2/10, Batch 828/883, Training Loss: 0.8906
Epoch 2/10, Batch 829/883, Training Loss: 0.7138
Epoch 2/10, Batch 830/883, Training Loss: 0.6060
Epoch 2/10, Batch 831/883, Training Loss: 0.5600
Epoch 2/10, Batch 832/883, Training Loss: 0.8473
Epoch 2/10, Batch 833/883, Training Loss: 0.7807
Epoch 2/10, Batch 834/883, Training Loss: 0.9387
Epoch 2/10, Batch 835/883, Training Loss: 0.7554
Epoch 2/10, Batch 836/883, Training Loss: 0.7897
Epoch 2/10, Batch 837/883, Training Loss: 0.9708
Epoch 2/10, Batch 838/883, Training Loss: 0.9308
Epoch 2/10, Batch 839/883, Training Loss: 0.8328
Epoch 2/10, Batch 840/883, Training Loss: 0.9819
Epoch 2/10, Batch 841/883, Training Loss: 0.5487
Epoch 2/10, Batch 842/883, Training Loss: 0.7764
Epoch 2/10, Batch 843/883, Training Loss: 0.6213
Epoch 2/10, Batch 844/883, Training Loss: 0.8864
Epoch 2/10, Batch 845/883, Training Loss: 0.8931
Epoch 2/10, Batch 846/883, Training Loss: 0.6199
Epoch 2/10, Batch 847/883, Training Loss: 0.5639
Epoch 2/10, Batch 848/883, Training Loss: 0.7666
Epoch 2/10, Batch 849/883, Training Loss: 0.6367
Epoch 2/10, Batch 850/883, Training Loss: 0.7658
Epoch 2/10, Batch 851/883, Training Loss: 0.5416
Epoch 2/10, Batch 852/883, Training Loss: 0.8882
Epoch 2/10, Batch 853/883, Training Loss: 0.6367
Epoch 2/10, Batch 854/883, Training Loss: 0.7934
Epoch 2/10, Batch 855/883, Training Loss: 0.5011
Epoch 2/10, Batch 856/883, Training Loss: 0.9485
Epoch 2/10, Batch 857/883, Training Loss: 1.0016
Epoch 2/10, Batch 858/883, Training Loss: 0.7241
Epoch 2/10, Batch 859/883, Training Loss: 0.9063
Epoch 2/10, Batch 860/883, Training Loss: 0.7672
Epoch 2/10, Batch 861/883, Training Loss: 0.7985
Epoch 2/10, Batch 862/883, Training Loss: 1.0301
Epoch 2/10, Batch 863/883, Training Loss: 0.9265
Epoch 2/10, Batch 864/883, Training Loss: 0.6829
Epoch 2/10, Batch 865/883, Training Loss: 0.8755
Epoch 2/10, Batch 866/883, Training Loss: 0.7420
Epoch 2/10, Batch 867/883, Training Loss: 0.7801
Epoch 2/10, Batch 868/883, Training Loss: 0.8269
Epoch 2/10, Batch 869/883, Training Loss: 0.7714
Epoch 2/10, Batch 870/883, Training Loss: 0.7592
Epoch 2/10, Batch 871/883, Training Loss: 0.6941
Epoch 2/10, Batch 872/883, Training Loss: 0.6284
Epoch 2/10, Batch 873/883, Training Loss: 0.8239
Epoch 2/10, Batch 874/883, Training Loss: 0.7884
Epoch 2/10, Batch 875/883, Training Loss: 1.0129
Epoch 2/10, Batch 876/883, Training Loss: 0.8723
Epoch 2/10, Batch 877/883, Training Loss: 0.6715
Epoch 2/10, Batch 878/883, Training Loss: 0.8523
Epoch 2/10, Batch 879/883, Training Loss: 0.6493
Epoch 2/10, Batch 880/883, Training Loss: 0.8843
Epoch 2/10, Batch 881/883, Training Loss: 0.8319
Epoch 2/10, Batch 882/883, Training Loss: 0.8460
Epoch 2/10, Batch 883/883, Training Loss: 0.8431
Epoch 2/10, Training Loss: 0.8387, Validation Loss: 0.8155, Validation Accuracy: 0.5905
Epoch 3/10, Batch 1/883, Training Loss: 1.0648
Epoch 3/10, Batch 2/883, Training Loss: 0.6600
Epoch 3/10, Batch 3/883, Training Loss: 0.6179
Epoch 3/10, Batch 4/883, Training Loss: 0.6575
Epoch 3/10, Batch 5/883, Training Loss: 0.8612
Epoch 3/10, Batch 6/883, Training Loss: 0.8370
Epoch 3/10, Batch 7/883, Training Loss: 0.8230
Epoch 3/10, Batch 8/883, Training Loss: 0.5816
Epoch 3/10, Batch 9/883, Training Loss: 0.6097
Epoch 3/10, Batch 10/883, Training Loss: 1.0458
Epoch 3/10, Batch 11/883, Training Loss: 0.8031
Epoch 3/10, Batch 12/883, Training Loss: 0.8462
Epoch 3/10, Batch 13/883, Training Loss: 0.9296
Epoch 3/10, Batch 14/883, Training Loss: 0.7534
Epoch 3/10, Batch 15/883, Training Loss: 0.5330
Epoch 3/10, Batch 16/883, Training Loss: 0.6068
Epoch 3/10, Batch 17/883, Training Loss: 1.0292
Epoch 3/10, Batch 18/883, Training Loss: 0.7826
Epoch 3/10, Batch 19/883, Training Loss: 0.7710
Epoch 3/10, Batch 20/883, Training Loss: 0.7008
Epoch 3/10, Batch 21/883, Training Loss: 0.6924
Epoch 3/10, Batch 22/883, Training Loss: 0.6801
Epoch 3/10, Batch 23/883, Training Loss: 0.7462
Epoch 3/10, Batch 24/883, Training Loss: 1.1444
Epoch 3/10, Batch 25/883, Training Loss: 0.6296
Epoch 3/10, Batch 26/883, Training Loss: 0.7557
Epoch 3/10, Batch 27/883, Training Loss: 0.9639
Epoch 3/10, Batch 28/883, Training Loss: 0.9104
Epoch 3/10, Batch 29/883, Training Loss: 0.6965
Epoch 3/10, Batch 30/883, Training Loss: 0.7512
Epoch 3/10, Batch 31/883, Training Loss: 0.8935
Epoch 3/10, Batch 32/883, Training Loss: 0.6499
Epoch 3/10, Batch 33/883, Training Loss: 0.8657
Epoch 3/10, Batch 34/883, Training Loss: 0.8061
Epoch 3/10, Batch 35/883, Training Loss: 0.7622
Epoch 3/10, Batch 36/883, Training Loss: 0.6135
Epoch 3/10, Batch 37/883, Training Loss: 0.7050
Epoch 3/10, Batch 38/883, Training Loss: 0.6785
Epoch 3/10, Batch 39/883, Training Loss: 0.7005
Epoch 3/10, Batch 40/883, Training Loss: 0.9009
Epoch 3/10, Batch 41/883, Training Loss: 0.4873
Epoch 3/10, Batch 42/883, Training Loss: 0.9844
Epoch 3/10, Batch 43/883, Training Loss: 1.2986
Epoch 3/10, Batch 44/883, Training Loss: 0.8392
Epoch 3/10, Batch 45/883, Training Loss: 0.8588
Epoch 3/10, Batch 46/883, Training Loss: 0.9749
Epoch 3/10, Batch 47/883, Training Loss: 0.7787
Epoch 3/10, Batch 48/883, Training Loss: 0.6041
Epoch 3/10, Batch 49/883, Training Loss: 0.8855
Epoch 3/10, Batch 50/883, Training Loss: 0.8485
Epoch 3/10, Batch 51/883, Training Loss: 0.8083
Epoch 3/10, Batch 52/883, Training Loss: 0.7721
Epoch 3/10, Batch 53/883, Training Loss: 0.6896
Epoch 3/10, Batch 54/883, Training Loss: 0.5530
Epoch 3/10, Batch 55/883, Training Loss: 0.8396
Epoch 3/10, Batch 56/883, Training Loss: 0.7888
Epoch 3/10, Batch 57/883, Training Loss: 0.9795
Epoch 3/10, Batch 58/883, Training Loss: 0.8617
Epoch 3/10, Batch 59/883, Training Loss: 0.6573
Epoch 3/10, Batch 60/883, Training Loss: 0.7784
Epoch 3/10, Batch 61/883, Training Loss: 1.0302
Epoch 3/10, Batch 62/883, Training Loss: 0.7143
Epoch 3/10, Batch 63/883, Training Loss: 0.8494
Epoch 3/10, Batch 64/883, Training Loss: 0.7124
Epoch 3/10, Batch 65/883, Training Loss: 0.9145
Epoch 3/10, Batch 66/883, Training Loss: 0.7292
Epoch 3/10, Batch 67/883, Training Loss: 1.0515
Epoch 3/10, Batch 68/883, Training Loss: 0.7204
Epoch 3/10, Batch 69/883, Training Loss: 0.8124
Epoch 3/10, Batch 70/883, Training Loss: 0.9066
Epoch 3/10, Batch 71/883, Training Loss: 0.8931
Epoch 3/10, Batch 72/883, Training Loss: 0.7374
Epoch 3/10, Batch 73/883, Training Loss: 0.7533
Epoch 3/10, Batch 74/883, Training Loss: 0.6812
Epoch 3/10, Batch 75/883, Training Loss: 0.6782
Epoch 3/10, Batch 76/883, Training Loss: 0.6708
Epoch 3/10, Batch 77/883, Training Loss: 0.7463
Epoch 3/10, Batch 78/883, Training Loss: 0.5179
Epoch 3/10, Batch 79/883, Training Loss: 0.8459
Epoch 3/10, Batch 80/883, Training Loss: 0.6875
Epoch 3/10, Batch 81/883, Training Loss: 0.6390
Epoch 3/10, Batch 82/883, Training Loss: 0.9654
Epoch 3/10, Batch 83/883, Training Loss: 0.5582
Epoch 3/10, Batch 84/883, Training Loss: 0.7583
Epoch 3/10, Batch 85/883, Training Loss: 0.9066
Epoch 3/10, Batch 86/883, Training Loss: 0.5616
Epoch 3/10, Batch 87/883, Training Loss: 0.7360
Epoch 3/10, Batch 88/883, Training Loss: 1.0576
Epoch 3/10, Batch 89/883, Training Loss: 1.0989
Epoch 3/10, Batch 90/883, Training Loss: 0.7610
Epoch 3/10, Batch 91/883, Training Loss: 0.7022
Epoch 3/10, Batch 92/883, Training Loss: 0.8100
Epoch 3/10, Batch 93/883, Training Loss: 0.8420
Epoch 3/10, Batch 94/883, Training Loss: 0.6914
Epoch 3/10, Batch 95/883, Training Loss: 0.7739
Epoch 3/10, Batch 96/883, Training Loss: 0.6316
Epoch 3/10, Batch 97/883, Training Loss: 0.8834
Epoch 3/10, Batch 98/883, Training Loss: 0.6518
Epoch 3/10, Batch 99/883, Training Loss: 0.6726
Epoch 3/10, Batch 100/883, Training Loss: 0.7791
Epoch 3/10, Batch 101/883, Training Loss: 0.7359
Epoch 3/10, Batch 102/883, Training Loss: 0.6213
Epoch 3/10, Batch 103/883, Training Loss: 0.7793
Epoch 3/10, Batch 104/883, Training Loss: 0.6366
Epoch 3/10, Batch 105/883, Training Loss: 0.7385
Epoch 3/10, Batch 106/883, Training Loss: 0.5594
Epoch 3/10, Batch 107/883, Training Loss: 0.6765
Epoch 3/10, Batch 108/883, Training Loss: 0.9396
Epoch 3/10, Batch 109/883, Training Loss: 0.8511
Epoch 3/10, Batch 110/883, Training Loss: 0.8864
Epoch 3/10, Batch 111/883, Training Loss: 1.0217
Epoch 3/10, Batch 112/883, Training Loss: 0.9568
Epoch 3/10, Batch 113/883, Training Loss: 0.7054
Epoch 3/10, Batch 114/883, Training Loss: 1.1641
Epoch 3/10, Batch 115/883, Training Loss: 0.8369
Epoch 3/10, Batch 116/883, Training Loss: 1.0477
Epoch 3/10, Batch 117/883, Training Loss: 0.7069
Epoch 3/10, Batch 118/883, Training Loss: 0.7194
Epoch 3/10, Batch 119/883, Training Loss: 0.8610
Epoch 3/10, Batch 120/883, Training Loss: 0.7876
Epoch 3/10, Batch 121/883, Training Loss: 0.8012
Epoch 3/10, Batch 122/883, Training Loss: 0.9258
Epoch 3/10, Batch 123/883, Training Loss: 0.9711
Epoch 3/10, Batch 124/883, Training Loss: 0.7485
Epoch 3/10, Batch 125/883, Training Loss: 0.5492
Epoch 3/10, Batch 126/883, Training Loss: 0.5899
Epoch 3/10, Batch 127/883, Training Loss: 0.7520
Epoch 3/10, Batch 128/883, Training Loss: 0.7156
Epoch 3/10, Batch 129/883, Training Loss: 0.6979
Epoch 3/10, Batch 130/883, Training Loss: 0.8343
Epoch 3/10, Batch 131/883, Training Loss: 1.0641
Epoch 3/10, Batch 132/883, Training Loss: 1.1138
Epoch 3/10, Batch 133/883, Training Loss: 0.5386
Epoch 3/10, Batch 134/883, Training Loss: 0.5835
Epoch 3/10, Batch 135/883, Training Loss: 0.9058
Epoch 3/10, Batch 136/883, Training Loss: 0.8072
Epoch 3/10, Batch 137/883, Training Loss: 0.7987
Epoch 3/10, Batch 138/883, Training Loss: 0.9984
Epoch 3/10, Batch 139/883, Training Loss: 0.8694
Epoch 3/10, Batch 140/883, Training Loss: 0.9725
Epoch 3/10, Batch 141/883, Training Loss: 0.9368
Epoch 3/10, Batch 142/883, Training Loss: 0.6686
Epoch 3/10, Batch 143/883, Training Loss: 0.6486
Epoch 3/10, Batch 144/883, Training Loss: 1.0155
Epoch 3/10, Batch 145/883, Training Loss: 0.7744
Epoch 3/10, Batch 146/883, Training Loss: 0.8555
Epoch 3/10, Batch 147/883, Training Loss: 0.8468
Epoch 3/10, Batch 148/883, Training Loss: 0.8172
Epoch 3/10, Batch 149/883, Training Loss: 0.6849
Epoch 3/10, Batch 150/883, Training Loss: 0.9764
Epoch 3/10, Batch 151/883, Training Loss: 0.8758
Epoch 3/10, Batch 152/883, Training Loss: 0.7772
Epoch 3/10, Batch 153/883, Training Loss: 0.5973
Epoch 3/10, Batch 154/883, Training Loss: 0.8461
Epoch 3/10, Batch 155/883, Training Loss: 1.0234
Epoch 3/10, Batch 156/883, Training Loss: 0.6791
Epoch 3/10, Batch 157/883, Training Loss: 0.9623
Epoch 3/10, Batch 158/883, Training Loss: 0.6889
Epoch 3/10, Batch 159/883, Training Loss: 0.5017
Epoch 3/10, Batch 160/883, Training Loss: 0.8532
Epoch 3/10, Batch 161/883, Training Loss: 0.6332
Epoch 3/10, Batch 162/883, Training Loss: 0.7121
Epoch 3/10, Batch 163/883, Training Loss: 0.9404
Epoch 3/10, Batch 164/883, Training Loss: 0.7148
Epoch 3/10, Batch 165/883, Training Loss: 0.8381
Epoch 3/10, Batch 166/883, Training Loss: 0.7451
Epoch 3/10, Batch 167/883, Training Loss: 1.1536
Epoch 3/10, Batch 168/883, Training Loss: 0.7118
Epoch 3/10, Batch 169/883, Training Loss: 1.2773
Epoch 3/10, Batch 170/883, Training Loss: 0.8136
Epoch 3/10, Batch 171/883, Training Loss: 1.0289
Epoch 3/10, Batch 172/883, Training Loss: 0.8147
Epoch 3/10, Batch 173/883, Training Loss: 0.6259
Epoch 3/10, Batch 174/883, Training Loss: 0.7060
Epoch 3/10, Batch 175/883, Training Loss: 0.8435
Epoch 3/10, Batch 176/883, Training Loss: 0.8861
Epoch 3/10, Batch 177/883, Training Loss: 0.6771
Epoch 3/10, Batch 178/883, Training Loss: 0.8026
Epoch 3/10, Batch 179/883, Training Loss: 0.8087
Epoch 3/10, Batch 180/883, Training Loss: 0.7752
Epoch 3/10, Batch 181/883, Training Loss: 0.7465
Epoch 3/10, Batch 182/883, Training Loss: 0.8626
Epoch 3/10, Batch 183/883, Training Loss: 0.7689
Epoch 3/10, Batch 184/883, Training Loss: 0.6157
Epoch 3/10, Batch 185/883, Training Loss: 0.6299
Epoch 3/10, Batch 186/883, Training Loss: 0.9366
Epoch 3/10, Batch 187/883, Training Loss: 0.8204
Epoch 3/10, Batch 188/883, Training Loss: 1.0486
Epoch 3/10, Batch 189/883, Training Loss: 0.8422
Epoch 3/10, Batch 190/883, Training Loss: 0.6140
Epoch 3/10, Batch 191/883, Training Loss: 1.0255
Epoch 3/10, Batch 192/883, Training Loss: 0.7988
Epoch 3/10, Batch 193/883, Training Loss: 0.9605
Epoch 3/10, Batch 194/883, Training Loss: 0.9680
Epoch 3/10, Batch 195/883, Training Loss: 1.0660
Epoch 3/10, Batch 196/883, Training Loss: 0.7664
Epoch 3/10, Batch 197/883, Training Loss: 0.9033
Epoch 3/10, Batch 198/883, Training Loss: 0.7284
Epoch 3/10, Batch 199/883, Training Loss: 0.9858
Epoch 3/10, Batch 200/883, Training Loss: 0.7251
Epoch 3/10, Batch 201/883, Training Loss: 0.8835
Epoch 3/10, Batch 202/883, Training Loss: 0.6229
Epoch 3/10, Batch 203/883, Training Loss: 0.7812
Epoch 3/10, Batch 204/883, Training Loss: 0.7609
Epoch 3/10, Batch 205/883, Training Loss: 0.6430
Epoch 3/10, Batch 206/883, Training Loss: 0.6120
Epoch 3/10, Batch 207/883, Training Loss: 0.7169
Epoch 3/10, Batch 208/883, Training Loss: 0.8536
Epoch 3/10, Batch 209/883, Training Loss: 0.8372
Epoch 3/10, Batch 210/883, Training Loss: 0.6229
Epoch 3/10, Batch 211/883, Training Loss: 0.9109
Epoch 3/10, Batch 212/883, Training Loss: 0.6511
Epoch 3/10, Batch 213/883, Training Loss: 1.1578
Epoch 3/10, Batch 214/883, Training Loss: 0.5305
Epoch 3/10, Batch 215/883, Training Loss: 0.7974
Epoch 3/10, Batch 216/883, Training Loss: 0.7672
Epoch 3/10, Batch 217/883, Training Loss: 1.1660
Epoch 3/10, Batch 218/883, Training Loss: 0.8514
Epoch 3/10, Batch 219/883, Training Loss: 0.9382
Epoch 3/10, Batch 220/883, Training Loss: 0.6132
Epoch 3/10, Batch 221/883, Training Loss: 0.7783
Epoch 3/10, Batch 222/883, Training Loss: 0.9126
Epoch 3/10, Batch 223/883, Training Loss: 0.8168
Epoch 3/10, Batch 224/883, Training Loss: 0.8676
Epoch 3/10, Batch 225/883, Training Loss: 0.8333
Epoch 3/10, Batch 226/883, Training Loss: 0.8992
Epoch 3/10, Batch 227/883, Training Loss: 0.8081
Epoch 3/10, Batch 228/883, Training Loss: 0.9390
Epoch 3/10, Batch 229/883, Training Loss: 0.7391
Epoch 3/10, Batch 230/883, Training Loss: 0.9007
Epoch 3/10, Batch 231/883, Training Loss: 0.9680
Epoch 3/10, Batch 232/883, Training Loss: 0.6658
Epoch 3/10, Batch 233/883, Training Loss: 0.9679
Epoch 3/10, Batch 234/883, Training Loss: 0.7579
Epoch 3/10, Batch 235/883, Training Loss: 0.8435
Epoch 3/10, Batch 236/883, Training Loss: 0.7484
Epoch 3/10, Batch 237/883, Training Loss: 0.6233
Epoch 3/10, Batch 238/883, Training Loss: 0.6284
Epoch 3/10, Batch 239/883, Training Loss: 1.1018
Epoch 3/10, Batch 240/883, Training Loss: 0.8336
Epoch 3/10, Batch 241/883, Training Loss: 0.6752
Epoch 3/10, Batch 242/883, Training Loss: 0.8801
Epoch 3/10, Batch 243/883, Training Loss: 0.6746
Epoch 3/10, Batch 244/883, Training Loss: 1.1673
Epoch 3/10, Batch 245/883, Training Loss: 0.9801
Epoch 3/10, Batch 246/883, Training Loss: 0.5522
Epoch 3/10, Batch 247/883, Training Loss: 0.7082
Epoch 3/10, Batch 248/883, Training Loss: 0.6666
Epoch 3/10, Batch 249/883, Training Loss: 0.6927
Epoch 3/10, Batch 250/883, Training Loss: 0.7452
Epoch 3/10, Batch 251/883, Training Loss: 0.7198
Epoch 3/10, Batch 252/883, Training Loss: 0.8468
Epoch 3/10, Batch 253/883, Training Loss: 0.9821
Epoch 3/10, Batch 254/883, Training Loss: 0.8540
Epoch 3/10, Batch 255/883, Training Loss: 0.7737
Epoch 3/10, Batch 256/883, Training Loss: 0.6265
Epoch 3/10, Batch 257/883, Training Loss: 0.6317
Epoch 3/10, Batch 258/883, Training Loss: 1.1300
Epoch 3/10, Batch 259/883, Training Loss: 0.6603
Epoch 3/10, Batch 260/883, Training Loss: 0.9005
Epoch 3/10, Batch 261/883, Training Loss: 0.8322
Epoch 3/10, Batch 262/883, Training Loss: 0.6667
Epoch 3/10, Batch 263/883, Training Loss: 0.6132
Epoch 3/10, Batch 264/883, Training Loss: 0.6294
Epoch 3/10, Batch 265/883, Training Loss: 0.8111
Epoch 3/10, Batch 266/883, Training Loss: 0.9722
Epoch 3/10, Batch 267/883, Training Loss: 1.0388
Epoch 3/10, Batch 268/883, Training Loss: 0.8489
Epoch 3/10, Batch 269/883, Training Loss: 0.8726
Epoch 3/10, Batch 270/883, Training Loss: 0.7146
Epoch 3/10, Batch 271/883, Training Loss: 0.9992
Epoch 3/10, Batch 272/883, Training Loss: 0.6104
Epoch 3/10, Batch 273/883, Training Loss: 0.7413
Epoch 3/10, Batch 274/883, Training Loss: 1.0415
Epoch 3/10, Batch 275/883, Training Loss: 0.6738
Epoch 3/10, Batch 276/883, Training Loss: 0.7238
Epoch 3/10, Batch 277/883, Training Loss: 0.9639
Epoch 3/10, Batch 278/883, Training Loss: 0.7517
Epoch 3/10, Batch 279/883, Training Loss: 0.7318
Epoch 3/10, Batch 280/883, Training Loss: 0.8431
Epoch 3/10, Batch 281/883, Training Loss: 0.6342
Epoch 3/10, Batch 282/883, Training Loss: 0.8981
Epoch 3/10, Batch 283/883, Training Loss: 0.9408
Epoch 3/10, Batch 284/883, Training Loss: 0.9388
Epoch 3/10, Batch 285/883, Training Loss: 0.7134
Epoch 3/10, Batch 286/883, Training Loss: 0.8643
Epoch 3/10, Batch 287/883, Training Loss: 0.6546
Epoch 3/10, Batch 288/883, Training Loss: 1.0318
Epoch 3/10, Batch 289/883, Training Loss: 0.7534
Epoch 3/10, Batch 290/883, Training Loss: 0.8350
Epoch 3/10, Batch 291/883, Training Loss: 0.7223
Epoch 3/10, Batch 292/883, Training Loss: 0.6917
Epoch 3/10, Batch 293/883, Training Loss: 0.7971
Epoch 3/10, Batch 294/883, Training Loss: 0.6911
Epoch 3/10, Batch 295/883, Training Loss: 0.7839
Epoch 3/10, Batch 296/883, Training Loss: 0.7958
Epoch 3/10, Batch 297/883, Training Loss: 0.5996
Epoch 3/10, Batch 298/883, Training Loss: 0.4985
Epoch 3/10, Batch 299/883, Training Loss: 0.6558
Epoch 3/10, Batch 300/883, Training Loss: 0.8406
Epoch 3/10, Batch 301/883, Training Loss: 0.7550
Epoch 3/10, Batch 302/883, Training Loss: 0.5479
Epoch 3/10, Batch 303/883, Training Loss: 0.5786
Epoch 3/10, Batch 304/883, Training Loss: 0.7708
Epoch 3/10, Batch 305/883, Training Loss: 0.7617
Epoch 3/10, Batch 306/883, Training Loss: 0.7174
Epoch 3/10, Batch 307/883, Training Loss: 0.6327
Epoch 3/10, Batch 308/883, Training Loss: 1.0177
Epoch 3/10, Batch 309/883, Training Loss: 0.8387
Epoch 3/10, Batch 310/883, Training Loss: 0.9958
Epoch 3/10, Batch 311/883, Training Loss: 1.0564
Epoch 3/10, Batch 312/883, Training Loss: 0.7065
Epoch 3/10, Batch 313/883, Training Loss: 0.8144
Epoch 3/10, Batch 314/883, Training Loss: 0.7924
Epoch 3/10, Batch 315/883, Training Loss: 0.8673
Epoch 3/10, Batch 316/883, Training Loss: 0.7439
Epoch 3/10, Batch 317/883, Training Loss: 0.8741
Epoch 3/10, Batch 318/883, Training Loss: 0.6488
Epoch 3/10, Batch 319/883, Training Loss: 0.6026
Epoch 3/10, Batch 320/883, Training Loss: 0.8534
Epoch 3/10, Batch 321/883, Training Loss: 1.3388
Epoch 3/10, Batch 322/883, Training Loss: 1.0217
Epoch 3/10, Batch 323/883, Training Loss: 0.6685
Epoch 3/10, Batch 324/883, Training Loss: 0.7420
Epoch 3/10, Batch 325/883, Training Loss: 0.7449
Epoch 3/10, Batch 326/883, Training Loss: 0.7753
Epoch 3/10, Batch 327/883, Training Loss: 1.1058
Epoch 3/10, Batch 328/883, Training Loss: 0.5775
Epoch 3/10, Batch 329/883, Training Loss: 0.7184
Epoch 3/10, Batch 330/883, Training Loss: 0.8596
Epoch 3/10, Batch 331/883, Training Loss: 1.0429
Epoch 3/10, Batch 332/883, Training Loss: 0.8599
Epoch 3/10, Batch 333/883, Training Loss: 0.7998
Epoch 3/10, Batch 334/883, Training Loss: 0.7983
Epoch 3/10, Batch 335/883, Training Loss: 0.7325
Epoch 3/10, Batch 336/883, Training Loss: 0.8537
Epoch 3/10, Batch 337/883, Training Loss: 0.8313
Epoch 3/10, Batch 338/883, Training Loss: 0.6135
Epoch 3/10, Batch 339/883, Training Loss: 1.2016
Epoch 3/10, Batch 340/883, Training Loss: 0.8097
Epoch 3/10, Batch 341/883, Training Loss: 0.8150
Epoch 3/10, Batch 342/883, Training Loss: 0.6462
Epoch 3/10, Batch 343/883, Training Loss: 0.8419
Epoch 3/10, Batch 344/883, Training Loss: 0.8493
Epoch 3/10, Batch 345/883, Training Loss: 0.8490
Epoch 3/10, Batch 346/883, Training Loss: 0.8971
Epoch 3/10, Batch 347/883, Training Loss: 0.6761
Epoch 3/10, Batch 348/883, Training Loss: 0.8640
Epoch 3/10, Batch 349/883, Training Loss: 0.9945
Epoch 3/10, Batch 350/883, Training Loss: 0.6525
Epoch 3/10, Batch 351/883, Training Loss: 0.6101
Epoch 3/10, Batch 352/883, Training Loss: 0.6355
Epoch 3/10, Batch 353/883, Training Loss: 0.5263
Epoch 3/10, Batch 354/883, Training Loss: 0.8394
Epoch 3/10, Batch 355/883, Training Loss: 0.6792
Epoch 3/10, Batch 356/883, Training Loss: 0.9748
Epoch 3/10, Batch 357/883, Training Loss: 0.9221
Epoch 3/10, Batch 358/883, Training Loss: 0.7228
Epoch 3/10, Batch 359/883, Training Loss: 0.9631
Epoch 3/10, Batch 360/883, Training Loss: 0.9002
Epoch 3/10, Batch 361/883, Training Loss: 0.7794
Epoch 3/10, Batch 362/883, Training Loss: 0.8176
Epoch 3/10, Batch 363/883, Training Loss: 0.9656
Epoch 3/10, Batch 364/883, Training Loss: 0.6194
Epoch 3/10, Batch 365/883, Training Loss: 0.6367
Epoch 3/10, Batch 366/883, Training Loss: 0.9083
Epoch 3/10, Batch 367/883, Training Loss: 0.6507
Epoch 3/10, Batch 368/883, Training Loss: 0.7423
Epoch 3/10, Batch 369/883, Training Loss: 0.5756
Epoch 3/10, Batch 370/883, Training Loss: 0.5888
Epoch 3/10, Batch 371/883, Training Loss: 1.1708
Epoch 3/10, Batch 372/883, Training Loss: 1.2738
Epoch 3/10, Batch 373/883, Training Loss: 0.5120
Epoch 3/10, Batch 374/883, Training Loss: 0.6826
Epoch 3/10, Batch 375/883, Training Loss: 1.1229
Epoch 3/10, Batch 376/883, Training Loss: 0.6134
Epoch 3/10, Batch 377/883, Training Loss: 0.8782
Epoch 3/10, Batch 378/883, Training Loss: 0.7075
Epoch 3/10, Batch 379/883, Training Loss: 0.7192
Epoch 3/10, Batch 380/883, Training Loss: 1.0566
Epoch 3/10, Batch 381/883, Training Loss: 0.6290
Epoch 3/10, Batch 382/883, Training Loss: 0.6685
Epoch 3/10, Batch 383/883, Training Loss: 0.6537
Epoch 3/10, Batch 384/883, Training Loss: 0.8058
Epoch 3/10, Batch 385/883, Training Loss: 0.9206
Epoch 3/10, Batch 386/883, Training Loss: 0.7566
Epoch 3/10, Batch 387/883, Training Loss: 0.5406
Epoch 3/10, Batch 388/883, Training Loss: 0.9090
Epoch 3/10, Batch 389/883, Training Loss: 0.6626
Epoch 3/10, Batch 390/883, Training Loss: 0.7684
Epoch 3/10, Batch 391/883, Training Loss: 0.7165
Epoch 3/10, Batch 392/883, Training Loss: 0.7976
Epoch 3/10, Batch 393/883, Training Loss: 0.8752
Epoch 3/10, Batch 394/883, Training Loss: 0.8683
Epoch 3/10, Batch 395/883, Training Loss: 0.7125
Epoch 3/10, Batch 396/883, Training Loss: 0.6638
Epoch 3/10, Batch 397/883, Training Loss: 1.0505
Epoch 3/10, Batch 398/883, Training Loss: 0.9153
Epoch 3/10, Batch 399/883, Training Loss: 0.6846
Epoch 3/10, Batch 400/883, Training Loss: 0.6037
Epoch 3/10, Batch 401/883, Training Loss: 1.0692
Epoch 3/10, Batch 402/883, Training Loss: 0.6342
Epoch 3/10, Batch 403/883, Training Loss: 0.6896
Epoch 3/10, Batch 404/883, Training Loss: 0.7559
Epoch 3/10, Batch 405/883, Training Loss: 0.7022
Epoch 3/10, Batch 406/883, Training Loss: 1.0650
Epoch 3/10, Batch 407/883, Training Loss: 0.8737
Epoch 3/10, Batch 408/883, Training Loss: 0.8888
Epoch 3/10, Batch 409/883, Training Loss: 0.6916
Epoch 3/10, Batch 410/883, Training Loss: 0.7760
Epoch 3/10, Batch 411/883, Training Loss: 1.0101
Epoch 3/10, Batch 412/883, Training Loss: 0.9460
Epoch 3/10, Batch 413/883, Training Loss: 0.6756
Epoch 3/10, Batch 414/883, Training Loss: 0.6808
Epoch 3/10, Batch 415/883, Training Loss: 0.9443
Epoch 3/10, Batch 416/883, Training Loss: 0.9414
Epoch 3/10, Batch 417/883, Training Loss: 0.6691
Epoch 3/10, Batch 418/883, Training Loss: 0.6766
Epoch 3/10, Batch 419/883, Training Loss: 0.7601
Epoch 3/10, Batch 420/883, Training Loss: 0.6792
Epoch 3/10, Batch 421/883, Training Loss: 0.8867
Epoch 3/10, Batch 422/883, Training Loss: 0.9526
Epoch 3/10, Batch 423/883, Training Loss: 0.9165
Epoch 3/10, Batch 424/883, Training Loss: 0.7035
Epoch 3/10, Batch 425/883, Training Loss: 0.5978
Epoch 3/10, Batch 426/883, Training Loss: 0.6678
Epoch 3/10, Batch 427/883, Training Loss: 0.9906
Epoch 3/10, Batch 428/883, Training Loss: 0.7741
Epoch 3/10, Batch 429/883, Training Loss: 1.1942
Epoch 3/10, Batch 430/883, Training Loss: 0.6750
Epoch 3/10, Batch 431/883, Training Loss: 0.9395
Epoch 3/10, Batch 432/883, Training Loss: 0.9086
Epoch 3/10, Batch 433/883, Training Loss: 0.8288
Epoch 3/10, Batch 434/883, Training Loss: 0.7806
Epoch 3/10, Batch 435/883, Training Loss: 0.9436
Epoch 3/10, Batch 436/883, Training Loss: 1.1008
Epoch 3/10, Batch 437/883, Training Loss: 0.9937
Epoch 3/10, Batch 438/883, Training Loss: 1.1924
Epoch 3/10, Batch 439/883, Training Loss: 1.2591
Epoch 3/10, Batch 440/883, Training Loss: 1.0745
Epoch 3/10, Batch 441/883, Training Loss: 0.7573
Epoch 3/10, Batch 442/883, Training Loss: 0.8098
Epoch 3/10, Batch 443/883, Training Loss: 0.8731
Epoch 3/10, Batch 444/883, Training Loss: 0.6356
Epoch 3/10, Batch 445/883, Training Loss: 0.6769
Epoch 3/10, Batch 446/883, Training Loss: 0.7126
Epoch 3/10, Batch 447/883, Training Loss: 0.8913
Epoch 3/10, Batch 448/883, Training Loss: 0.7121
Epoch 3/10, Batch 449/883, Training Loss: 0.6269
Epoch 3/10, Batch 450/883, Training Loss: 0.8160
Epoch 3/10, Batch 451/883, Training Loss: 0.6753
Epoch 3/10, Batch 452/883, Training Loss: 1.0728
Epoch 3/10, Batch 453/883, Training Loss: 0.9043
Epoch 3/10, Batch 454/883, Training Loss: 0.8221
Epoch 3/10, Batch 455/883, Training Loss: 1.0442
Epoch 3/10, Batch 456/883, Training Loss: 0.9582
Epoch 3/10, Batch 457/883, Training Loss: 0.6372
Epoch 3/10, Batch 458/883, Training Loss: 0.7059
Epoch 3/10, Batch 459/883, Training Loss: 0.9104
Epoch 3/10, Batch 460/883, Training Loss: 0.6603
Epoch 3/10, Batch 461/883, Training Loss: 0.7728
Epoch 3/10, Batch 462/883, Training Loss: 1.1446
Epoch 3/10, Batch 463/883, Training Loss: 0.6127
Epoch 3/10, Batch 464/883, Training Loss: 0.8297
Epoch 3/10, Batch 465/883, Training Loss: 0.8226
Epoch 3/10, Batch 466/883, Training Loss: 0.5534
Epoch 3/10, Batch 467/883, Training Loss: 0.6502
Epoch 3/10, Batch 468/883, Training Loss: 0.9244
Epoch 3/10, Batch 469/883, Training Loss: 1.3249
Epoch 3/10, Batch 470/883, Training Loss: 0.7670
Epoch 3/10, Batch 471/883, Training Loss: 1.0246
Epoch 3/10, Batch 472/883, Training Loss: 0.6349
Epoch 3/10, Batch 473/883, Training Loss: 0.7860
Epoch 3/10, Batch 474/883, Training Loss: 0.6891
Epoch 3/10, Batch 475/883, Training Loss: 0.8412
Epoch 3/10, Batch 476/883, Training Loss: 0.6320
Epoch 3/10, Batch 477/883, Training Loss: 0.6031
Epoch 3/10, Batch 478/883, Training Loss: 0.8047
Epoch 3/10, Batch 479/883, Training Loss: 1.0314
Epoch 3/10, Batch 480/883, Training Loss: 0.8097
Epoch 3/10, Batch 481/883, Training Loss: 0.6832
Epoch 3/10, Batch 482/883, Training Loss: 0.9912
Epoch 3/10, Batch 483/883, Training Loss: 0.7637
Epoch 3/10, Batch 484/883, Training Loss: 0.6363
Epoch 3/10, Batch 485/883, Training Loss: 0.8543
Epoch 3/10, Batch 486/883, Training Loss: 0.7682
Epoch 3/10, Batch 487/883, Training Loss: 0.7525
Epoch 3/10, Batch 488/883, Training Loss: 0.6294
Epoch 3/10, Batch 489/883, Training Loss: 0.8399
Epoch 3/10, Batch 490/883, Training Loss: 0.6144
Epoch 3/10, Batch 491/883, Training Loss: 0.9097
Epoch 3/10, Batch 492/883, Training Loss: 0.8203
Epoch 3/10, Batch 493/883, Training Loss: 0.7757
Epoch 3/10, Batch 494/883, Training Loss: 0.8232
Epoch 3/10, Batch 495/883, Training Loss: 0.7493
Epoch 3/10, Batch 496/883, Training Loss: 0.9038
Epoch 3/10, Batch 497/883, Training Loss: 0.9622
Epoch 3/10, Batch 498/883, Training Loss: 0.8937
Epoch 3/10, Batch 499/883, Training Loss: 0.7581
Epoch 3/10, Batch 500/883, Training Loss: 0.6261
Epoch 3/10, Batch 501/883, Training Loss: 0.7032
Epoch 3/10, Batch 502/883, Training Loss: 0.7813
Epoch 3/10, Batch 503/883, Training Loss: 0.7091
Epoch 3/10, Batch 504/883, Training Loss: 0.7962
Epoch 3/10, Batch 505/883, Training Loss: 0.7784
Epoch 3/10, Batch 506/883, Training Loss: 0.7563
Epoch 3/10, Batch 507/883, Training Loss: 0.6480
Epoch 3/10, Batch 508/883, Training Loss: 0.6447
Epoch 3/10, Batch 509/883, Training Loss: 0.9794
Epoch 3/10, Batch 510/883, Training Loss: 0.6608
Epoch 3/10, Batch 511/883, Training Loss: 0.5568
Epoch 3/10, Batch 512/883, Training Loss: 0.9565
Epoch 3/10, Batch 513/883, Training Loss: 1.0796
Epoch 3/10, Batch 514/883, Training Loss: 0.5787
Epoch 3/10, Batch 515/883, Training Loss: 0.9075
Epoch 3/10, Batch 516/883, Training Loss: 0.8382
Epoch 3/10, Batch 517/883, Training Loss: 0.6571
Epoch 3/10, Batch 518/883, Training Loss: 0.7872
Epoch 3/10, Batch 519/883, Training Loss: 0.5670
Epoch 3/10, Batch 520/883, Training Loss: 1.0828
Epoch 3/10, Batch 521/883, Training Loss: 0.6092
Epoch 3/10, Batch 522/883, Training Loss: 0.5495
Epoch 3/10, Batch 523/883, Training Loss: 0.7330
Epoch 3/10, Batch 524/883, Training Loss: 0.8874
Epoch 3/10, Batch 525/883, Training Loss: 0.9056
Epoch 3/10, Batch 526/883, Training Loss: 0.9272
Epoch 3/10, Batch 527/883, Training Loss: 0.6370
Epoch 3/10, Batch 528/883, Training Loss: 0.8727
Epoch 3/10, Batch 529/883, Training Loss: 0.5163
Epoch 3/10, Batch 530/883, Training Loss: 1.1816
Epoch 3/10, Batch 531/883, Training Loss: 0.6370
Epoch 3/10, Batch 532/883, Training Loss: 1.0771
Epoch 3/10, Batch 533/883, Training Loss: 0.6504
Epoch 3/10, Batch 534/883, Training Loss: 0.8589
Epoch 3/10, Batch 535/883, Training Loss: 0.6673
Epoch 3/10, Batch 536/883, Training Loss: 0.7184
Epoch 3/10, Batch 537/883, Training Loss: 0.6693
Epoch 3/10, Batch 538/883, Training Loss: 0.7241
Epoch 3/10, Batch 539/883, Training Loss: 1.1381
Epoch 3/10, Batch 540/883, Training Loss: 0.9581
Epoch 3/10, Batch 541/883, Training Loss: 0.5609
Epoch 3/10, Batch 542/883, Training Loss: 0.7358
Epoch 3/10, Batch 543/883, Training Loss: 0.6855
Epoch 3/10, Batch 544/883, Training Loss: 0.6901
Epoch 3/10, Batch 545/883, Training Loss: 0.8719
Epoch 3/10, Batch 546/883, Training Loss: 0.5542
Epoch 3/10, Batch 547/883, Training Loss: 0.8531
Epoch 3/10, Batch 548/883, Training Loss: 0.5849
Epoch 3/10, Batch 549/883, Training Loss: 0.8050
Epoch 3/10, Batch 550/883, Training Loss: 0.6897
Epoch 3/10, Batch 551/883, Training Loss: 1.2337
Epoch 3/10, Batch 552/883, Training Loss: 1.0413
Epoch 3/10, Batch 553/883, Training Loss: 0.4092
Epoch 3/10, Batch 554/883, Training Loss: 0.8717
Epoch 3/10, Batch 555/883, Training Loss: 0.9267
Epoch 3/10, Batch 556/883, Training Loss: 0.6144
Epoch 3/10, Batch 557/883, Training Loss: 0.7717
Epoch 3/10, Batch 558/883, Training Loss: 1.0487
Epoch 3/10, Batch 559/883, Training Loss: 0.7511
Epoch 3/10, Batch 560/883, Training Loss: 0.8755
Epoch 3/10, Batch 561/883, Training Loss: 0.8485
Epoch 3/10, Batch 562/883, Training Loss: 0.8492
Epoch 3/10, Batch 563/883, Training Loss: 0.7511
Epoch 3/10, Batch 564/883, Training Loss: 0.9137
Epoch 3/10, Batch 565/883, Training Loss: 0.6662
Epoch 3/10, Batch 566/883, Training Loss: 0.6525
Epoch 3/10, Batch 567/883, Training Loss: 0.8647
Epoch 3/10, Batch 568/883, Training Loss: 0.8669
Epoch 3/10, Batch 569/883, Training Loss: 0.9170
Epoch 3/10, Batch 570/883, Training Loss: 1.0424
Epoch 3/10, Batch 571/883, Training Loss: 0.8184
Epoch 3/10, Batch 572/883, Training Loss: 0.8928
Epoch 3/10, Batch 573/883, Training Loss: 0.7938
Epoch 3/10, Batch 574/883, Training Loss: 0.9035
Epoch 3/10, Batch 575/883, Training Loss: 0.9679
Epoch 3/10, Batch 576/883, Training Loss: 0.7446
Epoch 3/10, Batch 577/883, Training Loss: 0.7012
Epoch 3/10, Batch 578/883, Training Loss: 0.7795
Epoch 3/10, Batch 579/883, Training Loss: 0.8045
Epoch 3/10, Batch 580/883, Training Loss: 0.6399
Epoch 3/10, Batch 581/883, Training Loss: 0.7478
Epoch 3/10, Batch 582/883, Training Loss: 0.6370
Epoch 3/10, Batch 583/883, Training Loss: 0.6597
Epoch 3/10, Batch 584/883, Training Loss: 0.7126
Epoch 3/10, Batch 585/883, Training Loss: 0.9918
Epoch 3/10, Batch 586/883, Training Loss: 0.7727
Epoch 3/10, Batch 587/883, Training Loss: 0.6618
Epoch 3/10, Batch 588/883, Training Loss: 0.8347
Epoch 3/10, Batch 589/883, Training Loss: 0.8183
Epoch 3/10, Batch 590/883, Training Loss: 0.8021
Epoch 3/10, Batch 591/883, Training Loss: 0.9568
Epoch 3/10, Batch 592/883, Training Loss: 1.0258
Epoch 3/10, Batch 593/883, Training Loss: 0.8593
Epoch 3/10, Batch 594/883, Training Loss: 0.8238
Epoch 3/10, Batch 595/883, Training Loss: 0.5980
Epoch 3/10, Batch 596/883, Training Loss: 0.7932
Epoch 3/10, Batch 597/883, Training Loss: 0.5851
Epoch 3/10, Batch 598/883, Training Loss: 0.7738
Epoch 3/10, Batch 599/883, Training Loss: 0.9505
Epoch 3/10, Batch 600/883, Training Loss: 0.7293
Epoch 3/10, Batch 601/883, Training Loss: 0.7433
Epoch 3/10, Batch 602/883, Training Loss: 0.8247
Epoch 3/10, Batch 603/883, Training Loss: 0.6027
Epoch 3/10, Batch 604/883, Training Loss: 0.9425
Epoch 3/10, Batch 605/883, Training Loss: 0.8175
Epoch 3/10, Batch 606/883, Training Loss: 0.8853
Epoch 3/10, Batch 607/883, Training Loss: 0.7614
Epoch 3/10, Batch 608/883, Training Loss: 0.9092
Epoch 3/10, Batch 609/883, Training Loss: 0.5671
Epoch 3/10, Batch 610/883, Training Loss: 0.7737
Epoch 3/10, Batch 611/883, Training Loss: 0.6974
Epoch 3/10, Batch 612/883, Training Loss: 0.8191
Epoch 3/10, Batch 613/883, Training Loss: 0.6593
Epoch 3/10, Batch 614/883, Training Loss: 1.0117
Epoch 3/10, Batch 615/883, Training Loss: 0.9820
Epoch 3/10, Batch 616/883, Training Loss: 0.7173
Epoch 3/10, Batch 617/883, Training Loss: 0.6977
Epoch 3/10, Batch 618/883, Training Loss: 0.8021
Epoch 3/10, Batch 619/883, Training Loss: 0.6584
Epoch 3/10, Batch 620/883, Training Loss: 0.8547
Epoch 3/10, Batch 621/883, Training Loss: 0.7388
Epoch 3/10, Batch 622/883, Training Loss: 0.6863
Epoch 3/10, Batch 623/883, Training Loss: 0.7416
Epoch 3/10, Batch 624/883, Training Loss: 0.6979
Epoch 3/10, Batch 625/883, Training Loss: 0.8493
Epoch 3/10, Batch 626/883, Training Loss: 0.8494
Epoch 3/10, Batch 627/883, Training Loss: 0.7579
Epoch 3/10, Batch 628/883, Training Loss: 0.7796
Epoch 3/10, Batch 629/883, Training Loss: 0.8476
Epoch 3/10, Batch 630/883, Training Loss: 0.8472
Epoch 3/10, Batch 631/883, Training Loss: 0.5488
Epoch 3/10, Batch 632/883, Training Loss: 0.8420
Epoch 3/10, Batch 633/883, Training Loss: 0.7145
Epoch 3/10, Batch 634/883, Training Loss: 0.9249
Epoch 3/10, Batch 635/883, Training Loss: 0.9430
Epoch 3/10, Batch 636/883, Training Loss: 0.9363
Epoch 3/10, Batch 637/883, Training Loss: 0.6032
Epoch 3/10, Batch 638/883, Training Loss: 1.0980
Epoch 3/10, Batch 639/883, Training Loss: 0.6454
Epoch 3/10, Batch 640/883, Training Loss: 0.7439
Epoch 3/10, Batch 641/883, Training Loss: 0.7183
Epoch 3/10, Batch 642/883, Training Loss: 0.9003
Epoch 3/10, Batch 643/883, Training Loss: 0.7806
Epoch 3/10, Batch 644/883, Training Loss: 0.8581
Epoch 3/10, Batch 645/883, Training Loss: 0.8578
Epoch 3/10, Batch 646/883, Training Loss: 0.7548
Epoch 3/10, Batch 647/883, Training Loss: 0.8513
Epoch 3/10, Batch 648/883, Training Loss: 0.7043
Epoch 3/10, Batch 649/883, Training Loss: 0.6113
Epoch 3/10, Batch 650/883, Training Loss: 0.6891
Epoch 3/10, Batch 651/883, Training Loss: 0.6471
Epoch 3/10, Batch 652/883, Training Loss: 0.7800
Epoch 3/10, Batch 653/883, Training Loss: 0.7735
Epoch 3/10, Batch 654/883, Training Loss: 0.9564
Epoch 3/10, Batch 655/883, Training Loss: 1.2705
Epoch 3/10, Batch 656/883, Training Loss: 0.6885
Epoch 3/10, Batch 657/883, Training Loss: 0.7972
Epoch 3/10, Batch 658/883, Training Loss: 0.6707
Epoch 3/10, Batch 659/883, Training Loss: 0.9105
Epoch 3/10, Batch 660/883, Training Loss: 0.7273
Epoch 3/10, Batch 661/883, Training Loss: 0.7986
Epoch 3/10, Batch 662/883, Training Loss: 0.6591
Epoch 3/10, Batch 663/883, Training Loss: 0.7860
Epoch 3/10, Batch 664/883, Training Loss: 0.7219
Epoch 3/10, Batch 665/883, Training Loss: 0.6697
Epoch 3/10, Batch 666/883, Training Loss: 0.7145
Epoch 3/10, Batch 667/883, Training Loss: 0.8433
Epoch 3/10, Batch 668/883, Training Loss: 0.8555
Epoch 3/10, Batch 669/883, Training Loss: 0.8809
Epoch 3/10, Batch 670/883, Training Loss: 0.9598
Epoch 3/10, Batch 671/883, Training Loss: 0.8379
Epoch 3/10, Batch 672/883, Training Loss: 0.7149
Epoch 3/10, Batch 673/883, Training Loss: 0.8756
Epoch 3/10, Batch 674/883, Training Loss: 0.6575
Epoch 3/10, Batch 675/883, Training Loss: 0.6665
Epoch 3/10, Batch 676/883, Training Loss: 0.5717
Epoch 3/10, Batch 677/883, Training Loss: 0.7305
Epoch 3/10, Batch 678/883, Training Loss: 0.7644
Epoch 3/10, Batch 679/883, Training Loss: 0.6885
Epoch 3/10, Batch 680/883, Training Loss: 0.6942
Epoch 3/10, Batch 681/883, Training Loss: 1.1805
Epoch 3/10, Batch 682/883, Training Loss: 0.5743
Epoch 3/10, Batch 683/883, Training Loss: 0.5492
Epoch 3/10, Batch 684/883, Training Loss: 0.5682
Epoch 3/10, Batch 685/883, Training Loss: 0.5887
Epoch 3/10, Batch 686/883, Training Loss: 0.7424
Epoch 3/10, Batch 687/883, Training Loss: 0.7744
Epoch 3/10, Batch 688/883, Training Loss: 0.5918
Epoch 3/10, Batch 689/883, Training Loss: 0.7635
Epoch 3/10, Batch 690/883, Training Loss: 0.6868
Epoch 3/10, Batch 691/883, Training Loss: 1.0850
Epoch 3/10, Batch 692/883, Training Loss: 0.5353
Epoch 3/10, Batch 693/883, Training Loss: 0.6690
Epoch 3/10, Batch 694/883, Training Loss: 0.7035
Epoch 3/10, Batch 695/883, Training Loss: 0.6693
Epoch 3/10, Batch 696/883, Training Loss: 0.9410
Epoch 3/10, Batch 697/883, Training Loss: 1.0601
Epoch 3/10, Batch 698/883, Training Loss: 0.5665
Epoch 3/10, Batch 699/883, Training Loss: 1.0084
Epoch 3/10, Batch 700/883, Training Loss: 0.5187
Epoch 3/10, Batch 701/883, Training Loss: 1.0773
Epoch 3/10, Batch 702/883, Training Loss: 0.4654
Epoch 3/10, Batch 703/883, Training Loss: 1.1441
Epoch 3/10, Batch 704/883, Training Loss: 0.7734
Epoch 3/10, Batch 705/883, Training Loss: 0.7916
Epoch 3/10, Batch 706/883, Training Loss: 0.7746
Epoch 3/10, Batch 707/883, Training Loss: 1.0031
Epoch 3/10, Batch 708/883, Training Loss: 0.7308
Epoch 3/10, Batch 709/883, Training Loss: 0.6881
Epoch 3/10, Batch 710/883, Training Loss: 0.5886
Epoch 3/10, Batch 711/883, Training Loss: 0.7274
Epoch 3/10, Batch 712/883, Training Loss: 0.9812
Epoch 3/10, Batch 713/883, Training Loss: 0.6560
Epoch 3/10, Batch 714/883, Training Loss: 0.7475
Epoch 3/10, Batch 715/883, Training Loss: 0.6982
Epoch 3/10, Batch 716/883, Training Loss: 0.8092
Epoch 3/10, Batch 717/883, Training Loss: 0.7760
Epoch 3/10, Batch 718/883, Training Loss: 0.9541
Epoch 3/10, Batch 719/883, Training Loss: 0.7774
Epoch 3/10, Batch 720/883, Training Loss: 0.6482
Epoch 3/10, Batch 721/883, Training Loss: 0.8545
Epoch 3/10, Batch 722/883, Training Loss: 0.8559
Epoch 3/10, Batch 723/883, Training Loss: 0.6649
Epoch 3/10, Batch 724/883, Training Loss: 0.9618
Epoch 3/10, Batch 725/883, Training Loss: 1.1324
Epoch 3/10, Batch 726/883, Training Loss: 0.8356
Epoch 3/10, Batch 727/883, Training Loss: 1.2455
Epoch 3/10, Batch 728/883, Training Loss: 0.8939
Epoch 3/10, Batch 729/883, Training Loss: 0.7233
Epoch 3/10, Batch 730/883, Training Loss: 0.7227
Epoch 3/10, Batch 731/883, Training Loss: 0.8385
Epoch 3/10, Batch 732/883, Training Loss: 0.8083
Epoch 3/10, Batch 733/883, Training Loss: 0.7478
Epoch 3/10, Batch 734/883, Training Loss: 0.6953
Epoch 3/10, Batch 735/883, Training Loss: 0.7544
Epoch 3/10, Batch 736/883, Training Loss: 0.8121
Epoch 3/10, Batch 737/883, Training Loss: 0.8975
Epoch 3/10, Batch 738/883, Training Loss: 0.7325
Epoch 3/10, Batch 739/883, Training Loss: 1.0390
Epoch 3/10, Batch 740/883, Training Loss: 0.5556
Epoch 3/10, Batch 741/883, Training Loss: 0.9469
Epoch 3/10, Batch 742/883, Training Loss: 0.8013
Epoch 3/10, Batch 743/883, Training Loss: 0.8748
Epoch 3/10, Batch 744/883, Training Loss: 0.7800
Epoch 3/10, Batch 745/883, Training Loss: 0.4784
Epoch 3/10, Batch 746/883, Training Loss: 0.6925
Epoch 3/10, Batch 747/883, Training Loss: 0.6412
Epoch 3/10, Batch 748/883, Training Loss: 0.8737
Epoch 3/10, Batch 749/883, Training Loss: 0.6989
Epoch 3/10, Batch 750/883, Training Loss: 0.6050
Epoch 3/10, Batch 751/883, Training Loss: 0.6749
Epoch 3/10, Batch 752/883, Training Loss: 0.7192
Epoch 3/10, Batch 753/883, Training Loss: 0.6668
Epoch 3/10, Batch 754/883, Training Loss: 0.6110
Epoch 3/10, Batch 755/883, Training Loss: 0.8864
Epoch 3/10, Batch 756/883, Training Loss: 0.7235
Epoch 3/10, Batch 757/883, Training Loss: 1.1437
Epoch 3/10, Batch 758/883, Training Loss: 0.8368
Epoch 3/10, Batch 759/883, Training Loss: 0.5664
Epoch 3/10, Batch 760/883, Training Loss: 1.1028
Epoch 3/10, Batch 761/883, Training Loss: 1.4054
Epoch 3/10, Batch 762/883, Training Loss: 0.6830
Epoch 3/10, Batch 763/883, Training Loss: 0.6514
Epoch 3/10, Batch 764/883, Training Loss: 0.7415
Epoch 3/10, Batch 765/883, Training Loss: 0.9042
Epoch 3/10, Batch 766/883, Training Loss: 0.6427
Epoch 3/10, Batch 767/883, Training Loss: 0.5827
Epoch 3/10, Batch 768/883, Training Loss: 0.6852
Epoch 3/10, Batch 769/883, Training Loss: 0.8532
Epoch 3/10, Batch 770/883, Training Loss: 0.8571
Epoch 3/10, Batch 771/883, Training Loss: 0.9086
Epoch 3/10, Batch 772/883, Training Loss: 1.0015
Epoch 3/10, Batch 773/883, Training Loss: 0.9051
Epoch 3/10, Batch 774/883, Training Loss: 0.9016
Epoch 3/10, Batch 775/883, Training Loss: 0.9547
Epoch 3/10, Batch 776/883, Training Loss: 1.2082
Epoch 3/10, Batch 777/883, Training Loss: 0.9104
Epoch 3/10, Batch 778/883, Training Loss: 0.7042
Epoch 3/10, Batch 779/883, Training Loss: 0.9326
Epoch 3/10, Batch 780/883, Training Loss: 0.8294
Epoch 3/10, Batch 781/883, Training Loss: 0.7347
Epoch 3/10, Batch 782/883, Training Loss: 0.8186
Epoch 3/10, Batch 783/883, Training Loss: 0.8632
Epoch 3/10, Batch 784/883, Training Loss: 0.7658
Epoch 3/10, Batch 785/883, Training Loss: 0.7629
Epoch 3/10, Batch 786/883, Training Loss: 0.6445
Epoch 3/10, Batch 787/883, Training Loss: 0.9376
Epoch 3/10, Batch 788/883, Training Loss: 0.8394
Epoch 3/10, Batch 789/883, Training Loss: 0.8133
Epoch 3/10, Batch 790/883, Training Loss: 0.8284
Epoch 3/10, Batch 791/883, Training Loss: 0.6054
Epoch 3/10, Batch 792/883, Training Loss: 0.8938
Epoch 3/10, Batch 793/883, Training Loss: 0.8120
Epoch 3/10, Batch 794/883, Training Loss: 0.5796
Epoch 3/10, Batch 795/883, Training Loss: 0.6446
Epoch 3/10, Batch 796/883, Training Loss: 0.6274
Epoch 3/10, Batch 797/883, Training Loss: 0.6772
Epoch 3/10, Batch 798/883, Training Loss: 0.6197
Epoch 3/10, Batch 799/883, Training Loss: 0.8301
Epoch 3/10, Batch 800/883, Training Loss: 1.0332
Epoch 3/10, Batch 801/883, Training Loss: 0.8302
Epoch 3/10, Batch 802/883, Training Loss: 0.9111
Epoch 3/10, Batch 803/883, Training Loss: 0.6426
Epoch 3/10, Batch 804/883, Training Loss: 0.9678
Epoch 3/10, Batch 805/883, Training Loss: 0.6308
Epoch 3/10, Batch 806/883, Training Loss: 0.6559
Epoch 3/10, Batch 807/883, Training Loss: 0.6343
Epoch 3/10, Batch 808/883, Training Loss: 0.6548
Epoch 3/10, Batch 809/883, Training Loss: 0.6387
Epoch 3/10, Batch 810/883, Training Loss: 0.7736
Epoch 3/10, Batch 811/883, Training Loss: 0.7169
Epoch 3/10, Batch 812/883, Training Loss: 0.7588
Epoch 3/10, Batch 813/883, Training Loss: 0.6330
Epoch 3/10, Batch 814/883, Training Loss: 0.8042
Epoch 3/10, Batch 815/883, Training Loss: 0.9242
Epoch 3/10, Batch 816/883, Training Loss: 0.8904
Epoch 3/10, Batch 817/883, Training Loss: 0.7108
Epoch 3/10, Batch 818/883, Training Loss: 0.9402
Epoch 3/10, Batch 819/883, Training Loss: 0.6630
Epoch 3/10, Batch 820/883, Training Loss: 0.8536
Epoch 3/10, Batch 821/883, Training Loss: 0.9338
Epoch 3/10, Batch 822/883, Training Loss: 0.7427
Epoch 3/10, Batch 823/883, Training Loss: 0.7507
Epoch 3/10, Batch 824/883, Training Loss: 0.7399
Epoch 3/10, Batch 825/883, Training Loss: 0.8434
Epoch 3/10, Batch 826/883, Training Loss: 0.7928
Epoch 3/10, Batch 827/883, Training Loss: 0.8481
Epoch 3/10, Batch 828/883, Training Loss: 0.5760
Epoch 3/10, Batch 829/883, Training Loss: 0.6827
Epoch 3/10, Batch 830/883, Training Loss: 0.6609
Epoch 3/10, Batch 831/883, Training Loss: 0.9971
Epoch 3/10, Batch 832/883, Training Loss: 0.9159
Epoch 3/10, Batch 833/883, Training Loss: 0.9319
Epoch 3/10, Batch 834/883, Training Loss: 0.8731
Epoch 3/10, Batch 835/883, Training Loss: 1.2021
Epoch 3/10, Batch 836/883, Training Loss: 0.9062
Epoch 3/10, Batch 837/883, Training Loss: 0.9999
Epoch 3/10, Batch 838/883, Training Loss: 0.6048
Epoch 3/10, Batch 839/883, Training Loss: 0.6219
Epoch 3/10, Batch 840/883, Training Loss: 0.7907
Epoch 3/10, Batch 841/883, Training Loss: 0.8025
Epoch 3/10, Batch 842/883, Training Loss: 0.8421
Epoch 3/10, Batch 843/883, Training Loss: 0.9420
Epoch 3/10, Batch 844/883, Training Loss: 0.7406
Epoch 3/10, Batch 845/883, Training Loss: 0.9986
Epoch 3/10, Batch 846/883, Training Loss: 0.8012
Epoch 3/10, Batch 847/883, Training Loss: 0.6443
Epoch 3/10, Batch 848/883, Training Loss: 0.6452
Epoch 3/10, Batch 849/883, Training Loss: 0.9940
Epoch 3/10, Batch 850/883, Training Loss: 0.7395
Epoch 3/10, Batch 851/883, Training Loss: 0.7852
Epoch 3/10, Batch 852/883, Training Loss: 0.7434
Epoch 3/10, Batch 853/883, Training Loss: 0.9843
Epoch 3/10, Batch 854/883, Training Loss: 0.6130
Epoch 3/10, Batch 855/883, Training Loss: 1.0980
Epoch 3/10, Batch 856/883, Training Loss: 0.8709
Epoch 3/10, Batch 857/883, Training Loss: 0.5582
Epoch 3/10, Batch 858/883, Training Loss: 0.7753
Epoch 3/10, Batch 859/883, Training Loss: 0.6657
Epoch 3/10, Batch 860/883, Training Loss: 0.9964
Epoch 3/10, Batch 861/883, Training Loss: 0.9100
Epoch 3/10, Batch 862/883, Training Loss: 0.9567
Epoch 3/10, Batch 863/883, Training Loss: 0.7457
Epoch 3/10, Batch 864/883, Training Loss: 0.7012
Epoch 3/10, Batch 865/883, Training Loss: 0.6851
Epoch 3/10, Batch 866/883, Training Loss: 0.9424
Epoch 3/10, Batch 867/883, Training Loss: 0.6342
Epoch 3/10, Batch 868/883, Training Loss: 0.6326
Epoch 3/10, Batch 869/883, Training Loss: 0.8556
Epoch 3/10, Batch 870/883, Training Loss: 0.9937
Epoch 3/10, Batch 871/883, Training Loss: 0.8024
Epoch 3/10, Batch 872/883, Training Loss: 0.8485
Epoch 3/10, Batch 873/883, Training Loss: 0.7762
Epoch 3/10, Batch 874/883, Training Loss: 0.5579
Epoch 3/10, Batch 875/883, Training Loss: 0.8227
Epoch 3/10, Batch 876/883, Training Loss: 0.9099
Epoch 3/10, Batch 877/883, Training Loss: 0.7673
Epoch 3/10, Batch 878/883, Training Loss: 0.7528
Epoch 3/10, Batch 879/883, Training Loss: 0.6706
Epoch 3/10, Batch 880/883, Training Loss: 0.7199
Epoch 3/10, Batch 881/883, Training Loss: 0.9126
Epoch 3/10, Batch 882/883, Training Loss: 1.2960
Epoch 3/10, Batch 883/883, Training Loss: 0.6490
Epoch 3/10, Training Loss: 0.7990, Validation Loss: 2.0011, Validation Accuracy: 0.5573
Epoch 4/10, Batch 1/883, Training Loss: 0.9054
Epoch 4/10, Batch 2/883, Training Loss: 0.8542
Epoch 4/10, Batch 3/883, Training Loss: 0.7504
Epoch 4/10, Batch 4/883, Training Loss: 0.5947
Epoch 4/10, Batch 5/883, Training Loss: 0.9141
Epoch 4/10, Batch 6/883, Training Loss: 0.8388
Epoch 4/10, Batch 7/883, Training Loss: 0.6741
Epoch 4/10, Batch 8/883, Training Loss: 0.7463
Epoch 4/10, Batch 9/883, Training Loss: 1.1162
Epoch 4/10, Batch 10/883, Training Loss: 0.7791
Epoch 4/10, Batch 11/883, Training Loss: 0.7646
Epoch 4/10, Batch 12/883, Training Loss: 0.7076
Epoch 4/10, Batch 13/883, Training Loss: 0.6620
Epoch 4/10, Batch 14/883, Training Loss: 0.5973
Epoch 4/10, Batch 15/883, Training Loss: 0.7949
Epoch 4/10, Batch 16/883, Training Loss: 0.6567
Epoch 4/10, Batch 17/883, Training Loss: 0.8979
Epoch 4/10, Batch 18/883, Training Loss: 0.6958
Epoch 4/10, Batch 19/883, Training Loss: 0.6244
Epoch 4/10, Batch 20/883, Training Loss: 0.7534
Epoch 4/10, Batch 21/883, Training Loss: 0.6985
Epoch 4/10, Batch 22/883, Training Loss: 0.8934
Epoch 4/10, Batch 23/883, Training Loss: 0.7975
Epoch 4/10, Batch 24/883, Training Loss: 1.2639
Epoch 4/10, Batch 25/883, Training Loss: 0.8561
Epoch 4/10, Batch 26/883, Training Loss: 1.0674
Epoch 4/10, Batch 27/883, Training Loss: 0.8528
Epoch 4/10, Batch 28/883, Training Loss: 0.7026
Epoch 4/10, Batch 29/883, Training Loss: 0.8178
Epoch 4/10, Batch 30/883, Training Loss: 0.8379
Epoch 4/10, Batch 31/883, Training Loss: 0.8075
Epoch 4/10, Batch 32/883, Training Loss: 0.6967
Epoch 4/10, Batch 33/883, Training Loss: 0.7666
Epoch 4/10, Batch 34/883, Training Loss: 0.8597
Epoch 4/10, Batch 35/883, Training Loss: 0.7332
Epoch 4/10, Batch 36/883, Training Loss: 0.7146
Epoch 4/10, Batch 37/883, Training Loss: 0.9313
Epoch 4/10, Batch 38/883, Training Loss: 0.9297
Epoch 4/10, Batch 39/883, Training Loss: 1.0094
Epoch 4/10, Batch 40/883, Training Loss: 1.1303
Epoch 4/10, Batch 41/883, Training Loss: 0.8756
Epoch 4/10, Batch 42/883, Training Loss: 0.7099
Epoch 4/10, Batch 43/883, Training Loss: 0.7775
Epoch 4/10, Batch 44/883, Training Loss: 0.7283
Epoch 4/10, Batch 45/883, Training Loss: 0.8437
Epoch 4/10, Batch 46/883, Training Loss: 0.9428
Epoch 4/10, Batch 47/883, Training Loss: 0.7384
Epoch 4/10, Batch 48/883, Training Loss: 0.6556
Epoch 4/10, Batch 49/883, Training Loss: 0.8842
Epoch 4/10, Batch 50/883, Training Loss: 0.7289
Epoch 4/10, Batch 51/883, Training Loss: 0.6661
Epoch 4/10, Batch 52/883, Training Loss: 0.6059
Epoch 4/10, Batch 53/883, Training Loss: 0.8131
Epoch 4/10, Batch 54/883, Training Loss: 0.8101
Epoch 4/10, Batch 55/883, Training Loss: 0.8208
Epoch 4/10, Batch 56/883, Training Loss: 0.8028
Epoch 4/10, Batch 57/883, Training Loss: 0.8489
Epoch 4/10, Batch 58/883, Training Loss: 0.7085
Epoch 4/10, Batch 59/883, Training Loss: 0.5776
Epoch 4/10, Batch 60/883, Training Loss: 0.8359
Epoch 4/10, Batch 61/883, Training Loss: 0.7549
Epoch 4/10, Batch 62/883, Training Loss: 1.3003
Epoch 4/10, Batch 63/883, Training Loss: 1.0394
Epoch 4/10, Batch 64/883, Training Loss: 0.7641
Epoch 4/10, Batch 65/883, Training Loss: 0.7888
Epoch 4/10, Batch 66/883, Training Loss: 0.8908
Epoch 4/10, Batch 67/883, Training Loss: 0.9939
Epoch 4/10, Batch 68/883, Training Loss: 0.7073
Epoch 4/10, Batch 69/883, Training Loss: 0.8913
Epoch 4/10, Batch 70/883, Training Loss: 0.9364
Epoch 4/10, Batch 71/883, Training Loss: 0.7034
Epoch 4/10, Batch 72/883, Training Loss: 0.5901
Epoch 4/10, Batch 73/883, Training Loss: 0.9520
Epoch 4/10, Batch 74/883, Training Loss: 1.0752
Epoch 4/10, Batch 75/883, Training Loss: 0.6688
Epoch 4/10, Batch 76/883, Training Loss: 0.8467
Epoch 4/10, Batch 77/883, Training Loss: 0.6760
Epoch 4/10, Batch 78/883, Training Loss: 0.7357
Epoch 4/10, Batch 79/883, Training Loss: 0.7556
Epoch 4/10, Batch 80/883, Training Loss: 0.7615
Epoch 4/10, Batch 81/883, Training Loss: 0.7618
Epoch 4/10, Batch 82/883, Training Loss: 0.7888
Epoch 4/10, Batch 83/883, Training Loss: 0.7186
Epoch 4/10, Batch 84/883, Training Loss: 0.8096
Epoch 4/10, Batch 85/883, Training Loss: 0.8092
Epoch 4/10, Batch 86/883, Training Loss: 0.8985
Epoch 4/10, Batch 87/883, Training Loss: 0.5882
Epoch 4/10, Batch 88/883, Training Loss: 0.4549
Epoch 4/10, Batch 89/883, Training Loss: 0.9159
Epoch 4/10, Batch 90/883, Training Loss: 0.9161
Epoch 4/10, Batch 91/883, Training Loss: 0.8043
Epoch 4/10, Batch 92/883, Training Loss: 0.8822
Epoch 4/10, Batch 93/883, Training Loss: 0.6875
Epoch 4/10, Batch 94/883, Training Loss: 0.8014
Epoch 4/10, Batch 95/883, Training Loss: 0.7219
Epoch 4/10, Batch 96/883, Training Loss: 0.7410
Epoch 4/10, Batch 97/883, Training Loss: 0.7070
Epoch 4/10, Batch 98/883, Training Loss: 0.6436
Epoch 4/10, Batch 99/883, Training Loss: 0.6088
Epoch 4/10, Batch 100/883, Training Loss: 0.6649
Epoch 4/10, Batch 101/883, Training Loss: 0.8024
Epoch 4/10, Batch 102/883, Training Loss: 0.5744
Epoch 4/10, Batch 103/883, Training Loss: 0.6502
Epoch 4/10, Batch 104/883, Training Loss: 0.9233
Epoch 4/10, Batch 105/883, Training Loss: 0.8805
Epoch 4/10, Batch 106/883, Training Loss: 0.8535
Epoch 4/10, Batch 107/883, Training Loss: 0.7921
Epoch 4/10, Batch 108/883, Training Loss: 0.7267
Epoch 4/10, Batch 109/883, Training Loss: 0.5050
Epoch 4/10, Batch 110/883, Training Loss: 1.1170
Epoch 4/10, Batch 111/883, Training Loss: 0.6116
Epoch 4/10, Batch 112/883, Training Loss: 0.6584
Epoch 4/10, Batch 113/883, Training Loss: 0.7259
Epoch 4/10, Batch 114/883, Training Loss: 0.7345
Epoch 4/10, Batch 115/883, Training Loss: 0.5374
Epoch 4/10, Batch 116/883, Training Loss: 0.8264
Epoch 4/10, Batch 117/883, Training Loss: 0.9283
Epoch 4/10, Batch 118/883, Training Loss: 0.9109
Epoch 4/10, Batch 119/883, Training Loss: 0.7871
Epoch 4/10, Batch 120/883, Training Loss: 0.8197
Epoch 4/10, Batch 121/883, Training Loss: 0.9814
Epoch 4/10, Batch 122/883, Training Loss: 0.8353
Epoch 4/10, Batch 123/883, Training Loss: 0.8565
Epoch 4/10, Batch 124/883, Training Loss: 0.6273
Epoch 4/10, Batch 125/883, Training Loss: 0.6538
Epoch 4/10, Batch 126/883, Training Loss: 1.0162
Epoch 4/10, Batch 127/883, Training Loss: 0.6474
Epoch 4/10, Batch 128/883, Training Loss: 0.8954
Epoch 4/10, Batch 129/883, Training Loss: 0.8475
Epoch 4/10, Batch 130/883, Training Loss: 0.7972
Epoch 4/10, Batch 131/883, Training Loss: 0.7298
Epoch 4/10, Batch 132/883, Training Loss: 0.5030
Epoch 4/10, Batch 133/883, Training Loss: 0.7415
Epoch 4/10, Batch 134/883, Training Loss: 0.4714
Epoch 4/10, Batch 135/883, Training Loss: 0.7402
Epoch 4/10, Batch 136/883, Training Loss: 0.7938
Epoch 4/10, Batch 137/883, Training Loss: 0.7129
Epoch 4/10, Batch 138/883, Training Loss: 0.9758
Epoch 4/10, Batch 139/883, Training Loss: 0.7183
Epoch 4/10, Batch 140/883, Training Loss: 0.8205
Epoch 4/10, Batch 141/883, Training Loss: 0.6236
Epoch 4/10, Batch 142/883, Training Loss: 0.5863
Epoch 4/10, Batch 143/883, Training Loss: 0.9671
Epoch 4/10, Batch 144/883, Training Loss: 0.6204
Epoch 4/10, Batch 145/883, Training Loss: 0.7189
Epoch 4/10, Batch 146/883, Training Loss: 0.5377
Epoch 4/10, Batch 147/883, Training Loss: 0.8564
Epoch 4/10, Batch 148/883, Training Loss: 0.6219
Epoch 4/10, Batch 149/883, Training Loss: 0.7807
Epoch 4/10, Batch 150/883, Training Loss: 0.6317
Epoch 4/10, Batch 151/883, Training Loss: 0.7718
Epoch 4/10, Batch 152/883, Training Loss: 0.8707
Epoch 4/10, Batch 153/883, Training Loss: 0.7279
Epoch 4/10, Batch 154/883, Training Loss: 0.7753
Epoch 4/10, Batch 155/883, Training Loss: 0.6904
Epoch 4/10, Batch 156/883, Training Loss: 0.6903
Epoch 4/10, Batch 157/883, Training Loss: 0.8361
Epoch 4/10, Batch 158/883, Training Loss: 0.9488
Epoch 4/10, Batch 159/883, Training Loss: 0.8202
Epoch 4/10, Batch 160/883, Training Loss: 0.4836
Epoch 4/10, Batch 161/883, Training Loss: 0.7572
Epoch 4/10, Batch 162/883, Training Loss: 0.4701
Epoch 4/10, Batch 163/883, Training Loss: 0.6384
Epoch 4/10, Batch 164/883, Training Loss: 0.7434
Epoch 4/10, Batch 165/883, Training Loss: 0.9810
Epoch 4/10, Batch 166/883, Training Loss: 0.9900
Epoch 4/10, Batch 167/883, Training Loss: 0.7895
Epoch 4/10, Batch 168/883, Training Loss: 0.4256
Epoch 4/10, Batch 169/883, Training Loss: 0.8436
Epoch 4/10, Batch 170/883, Training Loss: 0.8980
Epoch 4/10, Batch 171/883, Training Loss: 0.7016
Epoch 4/10, Batch 172/883, Training Loss: 0.7105
Epoch 4/10, Batch 173/883, Training Loss: 0.7546
Epoch 4/10, Batch 174/883, Training Loss: 0.8462
Epoch 4/10, Batch 175/883, Training Loss: 0.8940
Epoch 4/10, Batch 176/883, Training Loss: 1.2118
Epoch 4/10, Batch 177/883, Training Loss: 0.9226
Epoch 4/10, Batch 178/883, Training Loss: 0.8085
Epoch 4/10, Batch 179/883, Training Loss: 0.7986
Epoch 4/10, Batch 180/883, Training Loss: 0.8123
Epoch 4/10, Batch 181/883, Training Loss: 0.6641
Epoch 4/10, Batch 182/883, Training Loss: 0.9577
Epoch 4/10, Batch 183/883, Training Loss: 0.8201
Epoch 4/10, Batch 184/883, Training Loss: 0.6642
Epoch 4/10, Batch 185/883, Training Loss: 0.8430
Epoch 4/10, Batch 186/883, Training Loss: 0.5445
Epoch 4/10, Batch 187/883, Training Loss: 0.6088
Epoch 4/10, Batch 188/883, Training Loss: 0.8167
Epoch 4/10, Batch 189/883, Training Loss: 0.8363
Epoch 4/10, Batch 190/883, Training Loss: 0.5917
Epoch 4/10, Batch 191/883, Training Loss: 0.9431
Epoch 4/10, Batch 192/883, Training Loss: 0.7480
Epoch 4/10, Batch 193/883, Training Loss: 0.8032
Epoch 4/10, Batch 194/883, Training Loss: 0.7865
Epoch 4/10, Batch 195/883, Training Loss: 0.5988
Epoch 4/10, Batch 196/883, Training Loss: 0.8725
Epoch 4/10, Batch 197/883, Training Loss: 0.9023
Epoch 4/10, Batch 198/883, Training Loss: 0.9644
Epoch 4/10, Batch 199/883, Training Loss: 0.7620
Epoch 4/10, Batch 200/883, Training Loss: 1.0920
Epoch 4/10, Batch 201/883, Training Loss: 0.7036
Epoch 4/10, Batch 202/883, Training Loss: 0.5914
Epoch 4/10, Batch 203/883, Training Loss: 0.7071
Epoch 4/10, Batch 204/883, Training Loss: 0.9515
Epoch 4/10, Batch 205/883, Training Loss: 0.7760
Epoch 4/10, Batch 206/883, Training Loss: 0.9645
Epoch 4/10, Batch 207/883, Training Loss: 0.7558
Epoch 4/10, Batch 208/883, Training Loss: 0.8313
Epoch 4/10, Batch 209/883, Training Loss: 0.6010
Epoch 4/10, Batch 210/883, Training Loss: 0.6261
Epoch 4/10, Batch 211/883, Training Loss: 0.5546
Epoch 4/10, Batch 212/883, Training Loss: 0.4997
Epoch 4/10, Batch 213/883, Training Loss: 0.6259
Epoch 4/10, Batch 214/883, Training Loss: 0.6619
Epoch 4/10, Batch 215/883, Training Loss: 0.5465
Epoch 4/10, Batch 216/883, Training Loss: 0.5547
Epoch 4/10, Batch 217/883, Training Loss: 0.9950
Epoch 4/10, Batch 218/883, Training Loss: 0.7582
Epoch 4/10, Batch 219/883, Training Loss: 0.6987
Epoch 4/10, Batch 220/883, Training Loss: 0.7577
Epoch 4/10, Batch 221/883, Training Loss: 0.5869
Epoch 4/10, Batch 222/883, Training Loss: 0.9714
Epoch 4/10, Batch 223/883, Training Loss: 1.0181
Epoch 4/10, Batch 224/883, Training Loss: 0.7902
Epoch 4/10, Batch 225/883, Training Loss: 0.8799
Epoch 4/10, Batch 226/883, Training Loss: 0.8698
Epoch 4/10, Batch 227/883, Training Loss: 1.1116
Epoch 4/10, Batch 228/883, Training Loss: 0.7925
Epoch 4/10, Batch 229/883, Training Loss: 0.7928
Epoch 4/10, Batch 230/883, Training Loss: 0.9418
Epoch 4/10, Batch 231/883, Training Loss: 0.8943
Epoch 4/10, Batch 232/883, Training Loss: 0.5309
Epoch 4/10, Batch 233/883, Training Loss: 0.8798
Epoch 4/10, Batch 234/883, Training Loss: 0.6373
Epoch 4/10, Batch 235/883, Training Loss: 0.8426
Epoch 4/10, Batch 236/883, Training Loss: 0.6405
Epoch 4/10, Batch 237/883, Training Loss: 0.7251
Epoch 4/10, Batch 238/883, Training Loss: 0.6551
Epoch 4/10, Batch 239/883, Training Loss: 0.8608
Epoch 4/10, Batch 240/883, Training Loss: 0.6921
Epoch 4/10, Batch 241/883, Training Loss: 0.8197
Epoch 4/10, Batch 242/883, Training Loss: 0.8414
Epoch 4/10, Batch 243/883, Training Loss: 0.6612
Epoch 4/10, Batch 244/883, Training Loss: 0.7467
Epoch 4/10, Batch 245/883, Training Loss: 0.5236
Epoch 4/10, Batch 246/883, Training Loss: 0.6394
Epoch 4/10, Batch 247/883, Training Loss: 0.8058
Epoch 4/10, Batch 248/883, Training Loss: 1.1819
Epoch 4/10, Batch 249/883, Training Loss: 0.3935
Epoch 4/10, Batch 250/883, Training Loss: 0.4141
Epoch 4/10, Batch 251/883, Training Loss: 0.8780
Epoch 4/10, Batch 252/883, Training Loss: 1.3836
Epoch 4/10, Batch 253/883, Training Loss: 1.0515
Epoch 4/10, Batch 254/883, Training Loss: 0.7059
Epoch 4/10, Batch 255/883, Training Loss: 1.3113
Epoch 4/10, Batch 256/883, Training Loss: 0.8651
Epoch 4/10, Batch 257/883, Training Loss: 0.8431
Epoch 4/10, Batch 258/883, Training Loss: 0.5298
Epoch 4/10, Batch 259/883, Training Loss: 0.8983
Epoch 4/10, Batch 260/883, Training Loss: 0.6406
Epoch 4/10, Batch 261/883, Training Loss: 0.7492
Epoch 4/10, Batch 262/883, Training Loss: 0.7638
Epoch 4/10, Batch 263/883, Training Loss: 0.7348
Epoch 4/10, Batch 264/883, Training Loss: 0.5689
Epoch 4/10, Batch 265/883, Training Loss: 0.7688
Epoch 4/10, Batch 266/883, Training Loss: 0.8692
Epoch 4/10, Batch 267/883, Training Loss: 0.8199
Epoch 4/10, Batch 268/883, Training Loss: 0.8197
Epoch 4/10, Batch 269/883, Training Loss: 0.7921
Epoch 4/10, Batch 270/883, Training Loss: 0.6851
Epoch 4/10, Batch 271/883, Training Loss: 0.8837
Epoch 4/10, Batch 272/883, Training Loss: 0.5862
Epoch 4/10, Batch 273/883, Training Loss: 0.6737
Epoch 4/10, Batch 274/883, Training Loss: 0.7195
Epoch 4/10, Batch 275/883, Training Loss: 1.0913
Epoch 4/10, Batch 276/883, Training Loss: 0.7285
Epoch 4/10, Batch 277/883, Training Loss: 0.6651
Epoch 4/10, Batch 278/883, Training Loss: 1.2600
Epoch 4/10, Batch 279/883, Training Loss: 0.8676
Epoch 4/10, Batch 280/883, Training Loss: 1.0195
Epoch 4/10, Batch 281/883, Training Loss: 0.6332
Epoch 4/10, Batch 282/883, Training Loss: 0.5725
Epoch 4/10, Batch 283/883, Training Loss: 0.9012
Epoch 4/10, Batch 284/883, Training Loss: 0.6340
Epoch 4/10, Batch 285/883, Training Loss: 0.7967
Epoch 4/10, Batch 286/883, Training Loss: 0.5622
Epoch 4/10, Batch 287/883, Training Loss: 0.8031
Epoch 4/10, Batch 288/883, Training Loss: 1.0621
Epoch 4/10, Batch 289/883, Training Loss: 0.8072
Epoch 4/10, Batch 290/883, Training Loss: 0.7435
Epoch 4/10, Batch 291/883, Training Loss: 0.6448
Epoch 4/10, Batch 292/883, Training Loss: 0.8023
Epoch 4/10, Batch 293/883, Training Loss: 0.6318
Epoch 4/10, Batch 294/883, Training Loss: 0.8953
Epoch 4/10, Batch 295/883, Training Loss: 0.6113
Epoch 4/10, Batch 296/883, Training Loss: 0.7350
Epoch 4/10, Batch 297/883, Training Loss: 0.7733
Epoch 4/10, Batch 298/883, Training Loss: 0.6985
Epoch 4/10, Batch 299/883, Training Loss: 0.6572
Epoch 4/10, Batch 300/883, Training Loss: 0.9263
Epoch 4/10, Batch 301/883, Training Loss: 0.9109
Epoch 4/10, Batch 302/883, Training Loss: 0.5804
Epoch 4/10, Batch 303/883, Training Loss: 0.8448
Epoch 4/10, Batch 304/883, Training Loss: 0.5904
Epoch 4/10, Batch 305/883, Training Loss: 0.9071
Epoch 4/10, Batch 306/883, Training Loss: 0.4667
Epoch 4/10, Batch 307/883, Training Loss: 0.7015
Epoch 4/10, Batch 308/883, Training Loss: 0.6415
Epoch 4/10, Batch 309/883, Training Loss: 1.0311
Epoch 4/10, Batch 310/883, Training Loss: 1.1088
Epoch 4/10, Batch 311/883, Training Loss: 0.8739
Epoch 4/10, Batch 312/883, Training Loss: 0.8904
Epoch 4/10, Batch 313/883, Training Loss: 0.5142
Epoch 4/10, Batch 314/883, Training Loss: 0.8188
Epoch 4/10, Batch 315/883, Training Loss: 0.8066
Epoch 4/10, Batch 316/883, Training Loss: 1.0014
Epoch 4/10, Batch 317/883, Training Loss: 1.0697
Epoch 4/10, Batch 318/883, Training Loss: 0.8590
Epoch 4/10, Batch 319/883, Training Loss: 0.7478
Epoch 4/10, Batch 320/883, Training Loss: 0.7093
Epoch 4/10, Batch 321/883, Training Loss: 0.8414
Epoch 4/10, Batch 322/883, Training Loss: 0.7666
Epoch 4/10, Batch 323/883, Training Loss: 0.6944
Epoch 4/10, Batch 324/883, Training Loss: 0.7654
Epoch 4/10, Batch 325/883, Training Loss: 1.0756
Epoch 4/10, Batch 326/883, Training Loss: 0.6938
Epoch 4/10, Batch 327/883, Training Loss: 0.7267
Epoch 4/10, Batch 328/883, Training Loss: 0.7074
Epoch 4/10, Batch 329/883, Training Loss: 0.8820
Epoch 4/10, Batch 330/883, Training Loss: 0.6909
Epoch 4/10, Batch 331/883, Training Loss: 0.6802
Epoch 4/10, Batch 332/883, Training Loss: 0.8289
Epoch 4/10, Batch 333/883, Training Loss: 0.6260
Epoch 4/10, Batch 334/883, Training Loss: 0.9884
Epoch 4/10, Batch 335/883, Training Loss: 0.6115
Epoch 4/10, Batch 336/883, Training Loss: 1.0220
Epoch 4/10, Batch 337/883, Training Loss: 0.5899
Epoch 4/10, Batch 338/883, Training Loss: 0.8306
Epoch 4/10, Batch 339/883, Training Loss: 0.9385
Epoch 4/10, Batch 340/883, Training Loss: 0.7461
Epoch 4/10, Batch 341/883, Training Loss: 0.7717
Epoch 4/10, Batch 342/883, Training Loss: 0.6584
Epoch 4/10, Batch 343/883, Training Loss: 0.7537
Epoch 4/10, Batch 344/883, Training Loss: 0.6318
Epoch 4/10, Batch 345/883, Training Loss: 0.5980
Epoch 4/10, Batch 346/883, Training Loss: 1.0931
Epoch 4/10, Batch 347/883, Training Loss: 0.5217
Epoch 4/10, Batch 348/883, Training Loss: 0.7733
Epoch 4/10, Batch 349/883, Training Loss: 0.9753
Epoch 4/10, Batch 350/883, Training Loss: 0.5833
Epoch 4/10, Batch 351/883, Training Loss: 1.0055
Epoch 4/10, Batch 352/883, Training Loss: 0.8794
Epoch 4/10, Batch 353/883, Training Loss: 0.7269
Epoch 4/10, Batch 354/883, Training Loss: 1.0686
Epoch 4/10, Batch 355/883, Training Loss: 0.5967
Epoch 4/10, Batch 356/883, Training Loss: 0.5562
Epoch 4/10, Batch 357/883, Training Loss: 0.8073
Epoch 4/10, Batch 358/883, Training Loss: 0.6964
Epoch 4/10, Batch 359/883, Training Loss: 0.9859
Epoch 4/10, Batch 360/883, Training Loss: 0.6138
Epoch 4/10, Batch 361/883, Training Loss: 0.9677
Epoch 4/10, Batch 362/883, Training Loss: 0.8323
Epoch 4/10, Batch 363/883, Training Loss: 0.8272
Epoch 4/10, Batch 364/883, Training Loss: 0.6572
Epoch 4/10, Batch 365/883, Training Loss: 0.5827
Epoch 4/10, Batch 366/883, Training Loss: 0.8154
Epoch 4/10, Batch 367/883, Training Loss: 0.6794
Epoch 4/10, Batch 368/883, Training Loss: 0.7968
Epoch 4/10, Batch 369/883, Training Loss: 0.8643
Epoch 4/10, Batch 370/883, Training Loss: 0.7142
Epoch 4/10, Batch 371/883, Training Loss: 0.8754
Epoch 4/10, Batch 372/883, Training Loss: 0.6135
Epoch 4/10, Batch 373/883, Training Loss: 0.8044
Epoch 4/10, Batch 374/883, Training Loss: 0.7058
Epoch 4/10, Batch 375/883, Training Loss: 0.9143
Epoch 4/10, Batch 376/883, Training Loss: 0.7558
Epoch 4/10, Batch 377/883, Training Loss: 0.8637
Epoch 4/10, Batch 378/883, Training Loss: 0.7640
Epoch 4/10, Batch 379/883, Training Loss: 0.5545
Epoch 4/10, Batch 380/883, Training Loss: 0.5971
Epoch 4/10, Batch 381/883, Training Loss: 0.9224
Epoch 4/10, Batch 382/883, Training Loss: 0.6948
Epoch 4/10, Batch 383/883, Training Loss: 0.6461
Epoch 4/10, Batch 384/883, Training Loss: 0.8969
Epoch 4/10, Batch 385/883, Training Loss: 0.9104
Epoch 4/10, Batch 386/883, Training Loss: 0.7470
Epoch 4/10, Batch 387/883, Training Loss: 0.8303
Epoch 4/10, Batch 388/883, Training Loss: 0.9630
Epoch 4/10, Batch 389/883, Training Loss: 0.8653
Epoch 4/10, Batch 390/883, Training Loss: 0.7039
Epoch 4/10, Batch 391/883, Training Loss: 0.9974
Epoch 4/10, Batch 392/883, Training Loss: 0.6587
Epoch 4/10, Batch 393/883, Training Loss: 0.7574
Epoch 4/10, Batch 394/883, Training Loss: 0.8775
Epoch 4/10, Batch 395/883, Training Loss: 0.8364
Epoch 4/10, Batch 396/883, Training Loss: 0.7441
Epoch 4/10, Batch 397/883, Training Loss: 0.7876
Epoch 4/10, Batch 398/883, Training Loss: 0.6538
Epoch 4/10, Batch 399/883, Training Loss: 0.9564
Epoch 4/10, Batch 400/883, Training Loss: 0.8413
Epoch 4/10, Batch 401/883, Training Loss: 0.7144
Epoch 4/10, Batch 402/883, Training Loss: 0.5810
Epoch 4/10, Batch 403/883, Training Loss: 0.9173
Epoch 4/10, Batch 404/883, Training Loss: 0.7744
Epoch 4/10, Batch 405/883, Training Loss: 0.6004
Epoch 4/10, Batch 406/883, Training Loss: 0.7549
Epoch 4/10, Batch 407/883, Training Loss: 0.6396
Epoch 4/10, Batch 408/883, Training Loss: 0.6552
Epoch 4/10, Batch 409/883, Training Loss: 0.7306
Epoch 4/10, Batch 410/883, Training Loss: 0.8815
Epoch 4/10, Batch 411/883, Training Loss: 0.7164
Epoch 4/10, Batch 412/883, Training Loss: 0.8001
Epoch 4/10, Batch 413/883, Training Loss: 0.7691
Epoch 4/10, Batch 414/883, Training Loss: 0.7390
Epoch 4/10, Batch 415/883, Training Loss: 0.5340
Epoch 4/10, Batch 416/883, Training Loss: 0.7098
Epoch 4/10, Batch 417/883, Training Loss: 0.9243
Epoch 4/10, Batch 418/883, Training Loss: 1.0189
Epoch 4/10, Batch 419/883, Training Loss: 0.7186
Epoch 4/10, Batch 420/883, Training Loss: 0.7010
Epoch 4/10, Batch 421/883, Training Loss: 0.7776
Epoch 4/10, Batch 422/883, Training Loss: 0.6584
Epoch 4/10, Batch 423/883, Training Loss: 0.8816
Epoch 4/10, Batch 424/883, Training Loss: 0.8315
Epoch 4/10, Batch 425/883, Training Loss: 1.0160
Epoch 4/10, Batch 426/883, Training Loss: 0.8988
Epoch 4/10, Batch 427/883, Training Loss: 0.9683
Epoch 4/10, Batch 428/883, Training Loss: 0.7690
Epoch 4/10, Batch 429/883, Training Loss: 0.9353
Epoch 4/10, Batch 430/883, Training Loss: 0.7367
Epoch 4/10, Batch 431/883, Training Loss: 0.8244
Epoch 4/10, Batch 432/883, Training Loss: 0.5181
Epoch 4/10, Batch 433/883, Training Loss: 0.6577
Epoch 4/10, Batch 434/883, Training Loss: 0.8927
Epoch 4/10, Batch 435/883, Training Loss: 0.8779
Epoch 4/10, Batch 436/883, Training Loss: 0.6459
Epoch 4/10, Batch 437/883, Training Loss: 0.8720
Epoch 4/10, Batch 438/883, Training Loss: 0.6042
Epoch 4/10, Batch 439/883, Training Loss: 0.8019
Epoch 4/10, Batch 440/883, Training Loss: 0.6844
Epoch 4/10, Batch 441/883, Training Loss: 0.8409
Epoch 4/10, Batch 442/883, Training Loss: 0.5954
Epoch 4/10, Batch 443/883, Training Loss: 0.8215
Epoch 4/10, Batch 444/883, Training Loss: 0.7504
Epoch 4/10, Batch 445/883, Training Loss: 0.6832
Epoch 4/10, Batch 446/883, Training Loss: 0.7780
Epoch 4/10, Batch 447/883, Training Loss: 0.5977
Epoch 4/10, Batch 448/883, Training Loss: 0.8163
Epoch 4/10, Batch 449/883, Training Loss: 0.5758
Epoch 4/10, Batch 450/883, Training Loss: 0.8721
Epoch 4/10, Batch 451/883, Training Loss: 0.8112
Epoch 4/10, Batch 452/883, Training Loss: 0.8149
Epoch 4/10, Batch 453/883, Training Loss: 0.7972
Epoch 4/10, Batch 454/883, Training Loss: 0.8996
Epoch 4/10, Batch 455/883, Training Loss: 0.6029
Epoch 4/10, Batch 456/883, Training Loss: 0.7350
Epoch 4/10, Batch 457/883, Training Loss: 0.8467
Epoch 4/10, Batch 458/883, Training Loss: 0.7736
Epoch 4/10, Batch 459/883, Training Loss: 1.0225
Epoch 4/10, Batch 460/883, Training Loss: 0.8723
Epoch 4/10, Batch 461/883, Training Loss: 0.8273
Epoch 4/10, Batch 462/883, Training Loss: 0.6770
Epoch 4/10, Batch 463/883, Training Loss: 0.9165
Epoch 4/10, Batch 464/883, Training Loss: 0.8208
Epoch 4/10, Batch 465/883, Training Loss: 0.7880
Epoch 4/10, Batch 466/883, Training Loss: 0.7247
Epoch 4/10, Batch 467/883, Training Loss: 0.9024
Epoch 4/10, Batch 468/883, Training Loss: 0.5077
Epoch 4/10, Batch 469/883, Training Loss: 0.8343
Epoch 4/10, Batch 470/883, Training Loss: 0.9594
Epoch 4/10, Batch 471/883, Training Loss: 0.9225
Epoch 4/10, Batch 472/883, Training Loss: 0.7459
Epoch 4/10, Batch 473/883, Training Loss: 0.6487
Epoch 4/10, Batch 474/883, Training Loss: 0.5573
Epoch 4/10, Batch 475/883, Training Loss: 0.7815
Epoch 4/10, Batch 476/883, Training Loss: 0.8861
Epoch 4/10, Batch 477/883, Training Loss: 0.7326
Epoch 4/10, Batch 478/883, Training Loss: 0.5517
Epoch 4/10, Batch 479/883, Training Loss: 0.8545
Epoch 4/10, Batch 480/883, Training Loss: 0.6183
Epoch 4/10, Batch 481/883, Training Loss: 0.6349
Epoch 4/10, Batch 482/883, Training Loss: 0.8567
Epoch 4/10, Batch 483/883, Training Loss: 1.3048
Epoch 4/10, Batch 484/883, Training Loss: 0.5758
Epoch 4/10, Batch 485/883, Training Loss: 0.8667
Epoch 4/10, Batch 486/883, Training Loss: 0.7902
Epoch 4/10, Batch 487/883, Training Loss: 0.7829
Epoch 4/10, Batch 488/883, Training Loss: 0.4569
Epoch 4/10, Batch 489/883, Training Loss: 0.6323
Epoch 4/10, Batch 490/883, Training Loss: 0.9046
Epoch 4/10, Batch 491/883, Training Loss: 0.9474
Epoch 4/10, Batch 492/883, Training Loss: 0.8777
Epoch 4/10, Batch 493/883, Training Loss: 0.8118
Epoch 4/10, Batch 494/883, Training Loss: 0.5111
Epoch 4/10, Batch 495/883, Training Loss: 0.9014
Epoch 4/10, Batch 496/883, Training Loss: 1.2222
Epoch 4/10, Batch 497/883, Training Loss: 0.8663
Epoch 4/10, Batch 498/883, Training Loss: 0.5906
Epoch 4/10, Batch 499/883, Training Loss: 0.6593
Epoch 4/10, Batch 500/883, Training Loss: 0.7686
Epoch 4/10, Batch 501/883, Training Loss: 0.8043
Epoch 4/10, Batch 502/883, Training Loss: 0.6249
Epoch 4/10, Batch 503/883, Training Loss: 0.7473
Epoch 4/10, Batch 504/883, Training Loss: 0.7540
Epoch 4/10, Batch 505/883, Training Loss: 0.8089
Epoch 4/10, Batch 506/883, Training Loss: 0.8028
Epoch 4/10, Batch 507/883, Training Loss: 0.7897
Epoch 4/10, Batch 508/883, Training Loss: 0.6629
Epoch 4/10, Batch 509/883, Training Loss: 0.5894
Epoch 4/10, Batch 510/883, Training Loss: 0.7758
Epoch 4/10, Batch 511/883, Training Loss: 0.7626
Epoch 4/10, Batch 512/883, Training Loss: 0.9788
Epoch 4/10, Batch 513/883, Training Loss: 1.0173
Epoch 4/10, Batch 514/883, Training Loss: 0.6908
Epoch 4/10, Batch 515/883, Training Loss: 0.5157
Epoch 4/10, Batch 516/883, Training Loss: 0.7628
Epoch 4/10, Batch 517/883, Training Loss: 1.4679
Epoch 4/10, Batch 518/883, Training Loss: 0.8563
Epoch 4/10, Batch 519/883, Training Loss: 0.5765
Epoch 4/10, Batch 520/883, Training Loss: 0.8956
Epoch 4/10, Batch 521/883, Training Loss: 0.7307
Epoch 4/10, Batch 522/883, Training Loss: 0.7020
Epoch 4/10, Batch 523/883, Training Loss: 0.9417
Epoch 4/10, Batch 524/883, Training Loss: 0.7935
Epoch 4/10, Batch 525/883, Training Loss: 0.7854
Epoch 4/10, Batch 526/883, Training Loss: 1.0642
Epoch 4/10, Batch 527/883, Training Loss: 0.9086
Epoch 4/10, Batch 528/883, Training Loss: 0.6405
Epoch 4/10, Batch 529/883, Training Loss: 1.0579
Epoch 4/10, Batch 530/883, Training Loss: 0.5943
Epoch 4/10, Batch 531/883, Training Loss: 0.7261
Epoch 4/10, Batch 532/883, Training Loss: 0.6196
Epoch 4/10, Batch 533/883, Training Loss: 0.6275
Epoch 4/10, Batch 534/883, Training Loss: 0.6614
Epoch 4/10, Batch 535/883, Training Loss: 0.7291
Epoch 4/10, Batch 536/883, Training Loss: 0.6254
Epoch 4/10, Batch 537/883, Training Loss: 0.6637
Epoch 4/10, Batch 538/883, Training Loss: 0.7125
Epoch 4/10, Batch 539/883, Training Loss: 0.6660
Epoch 4/10, Batch 540/883, Training Loss: 0.7090
Epoch 4/10, Batch 541/883, Training Loss: 0.8985
Epoch 4/10, Batch 542/883, Training Loss: 0.7028
Epoch 4/10, Batch 543/883, Training Loss: 0.8421
Epoch 4/10, Batch 544/883, Training Loss: 0.7686
Epoch 4/10, Batch 545/883, Training Loss: 1.0882
Epoch 4/10, Batch 546/883, Training Loss: 0.6766
Epoch 4/10, Batch 547/883, Training Loss: 0.7657
Epoch 4/10, Batch 548/883, Training Loss: 0.9353
Epoch 4/10, Batch 549/883, Training Loss: 0.7790
Epoch 4/10, Batch 550/883, Training Loss: 0.8745
Epoch 4/10, Batch 551/883, Training Loss: 0.8320
Epoch 4/10, Batch 552/883, Training Loss: 1.0576
Epoch 4/10, Batch 553/883, Training Loss: 0.6526
Epoch 4/10, Batch 554/883, Training Loss: 0.6366
Epoch 4/10, Batch 555/883, Training Loss: 0.6921
Epoch 4/10, Batch 556/883, Training Loss: 0.5598
Epoch 4/10, Batch 557/883, Training Loss: 0.6903
Epoch 4/10, Batch 558/883, Training Loss: 0.7748
Epoch 4/10, Batch 559/883, Training Loss: 0.6004
Epoch 4/10, Batch 560/883, Training Loss: 0.7059
Epoch 4/10, Batch 561/883, Training Loss: 0.7778
Epoch 4/10, Batch 562/883, Training Loss: 0.8834
Epoch 4/10, Batch 563/883, Training Loss: 0.7623
Epoch 4/10, Batch 564/883, Training Loss: 0.7949
Epoch 4/10, Batch 565/883, Training Loss: 0.6608
Epoch 4/10, Batch 566/883, Training Loss: 0.7339
Epoch 4/10, Batch 567/883, Training Loss: 0.8998
Epoch 4/10, Batch 568/883, Training Loss: 1.1374
Epoch 4/10, Batch 569/883, Training Loss: 0.9312
Epoch 4/10, Batch 570/883, Training Loss: 0.9359
Epoch 4/10, Batch 571/883, Training Loss: 0.6050
Epoch 4/10, Batch 572/883, Training Loss: 0.9632
Epoch 4/10, Batch 573/883, Training Loss: 0.7635
Epoch 4/10, Batch 574/883, Training Loss: 0.8256
Epoch 4/10, Batch 575/883, Training Loss: 0.8349
Epoch 4/10, Batch 576/883, Training Loss: 0.6421
Epoch 4/10, Batch 577/883, Training Loss: 0.8386
Epoch 4/10, Batch 578/883, Training Loss: 0.6643
Epoch 4/10, Batch 579/883, Training Loss: 1.0379
Epoch 4/10, Batch 580/883, Training Loss: 0.7328
Epoch 4/10, Batch 581/883, Training Loss: 0.5804
Epoch 4/10, Batch 582/883, Training Loss: 0.7924
Epoch 4/10, Batch 583/883, Training Loss: 0.9428
Epoch 4/10, Batch 584/883, Training Loss: 0.7227
Epoch 4/10, Batch 585/883, Training Loss: 0.9069
Epoch 4/10, Batch 586/883, Training Loss: 0.6690
Epoch 4/10, Batch 587/883, Training Loss: 0.7360
Epoch 4/10, Batch 588/883, Training Loss: 0.8938
Epoch 4/10, Batch 589/883, Training Loss: 0.7992
Epoch 4/10, Batch 590/883, Training Loss: 0.9174
Epoch 4/10, Batch 591/883, Training Loss: 0.8249
Epoch 4/10, Batch 592/883, Training Loss: 0.7955
Epoch 4/10, Batch 593/883, Training Loss: 0.6936
Epoch 4/10, Batch 594/883, Training Loss: 0.8675
Epoch 4/10, Batch 595/883, Training Loss: 0.7375
Epoch 4/10, Batch 596/883, Training Loss: 0.6459
Epoch 4/10, Batch 597/883, Training Loss: 0.5669
Epoch 4/10, Batch 598/883, Training Loss: 0.5210
Epoch 4/10, Batch 599/883, Training Loss: 1.0158
Epoch 4/10, Batch 600/883, Training Loss: 0.7263
Epoch 4/10, Batch 601/883, Training Loss: 0.6103
Epoch 4/10, Batch 602/883, Training Loss: 0.7730
Epoch 4/10, Batch 603/883, Training Loss: 1.1755
Epoch 4/10, Batch 604/883, Training Loss: 0.8136
Epoch 4/10, Batch 605/883, Training Loss: 0.4830
Epoch 4/10, Batch 606/883, Training Loss: 0.7962
Epoch 4/10, Batch 607/883, Training Loss: 0.7734
Epoch 4/10, Batch 608/883, Training Loss: 0.7197
Epoch 4/10, Batch 609/883, Training Loss: 0.6448
Epoch 4/10, Batch 610/883, Training Loss: 0.9998
Epoch 4/10, Batch 611/883, Training Loss: 0.8308
Epoch 4/10, Batch 612/883, Training Loss: 1.1770
Epoch 4/10, Batch 613/883, Training Loss: 0.7850
Epoch 4/10, Batch 614/883, Training Loss: 0.7431
Epoch 4/10, Batch 615/883, Training Loss: 0.9605
Epoch 4/10, Batch 616/883, Training Loss: 1.0208
Epoch 4/10, Batch 617/883, Training Loss: 0.7585
Epoch 4/10, Batch 618/883, Training Loss: 0.5905
Epoch 4/10, Batch 619/883, Training Loss: 0.7874
Epoch 4/10, Batch 620/883, Training Loss: 0.5927
Epoch 4/10, Batch 621/883, Training Loss: 0.7172
Epoch 4/10, Batch 622/883, Training Loss: 1.0130
Epoch 4/10, Batch 623/883, Training Loss: 0.8148
Epoch 4/10, Batch 624/883, Training Loss: 0.6224
Epoch 4/10, Batch 625/883, Training Loss: 0.6689
Epoch 4/10, Batch 626/883, Training Loss: 0.7768
Epoch 4/10, Batch 627/883, Training Loss: 0.5829
Epoch 4/10, Batch 628/883, Training Loss: 0.6802
Epoch 4/10, Batch 629/883, Training Loss: 0.7021
Epoch 4/10, Batch 630/883, Training Loss: 0.7539
Epoch 4/10, Batch 631/883, Training Loss: 1.1414
Epoch 4/10, Batch 632/883, Training Loss: 0.6298
Epoch 4/10, Batch 633/883, Training Loss: 0.5912
Epoch 4/10, Batch 634/883, Training Loss: 0.4924
Epoch 4/10, Batch 635/883, Training Loss: 0.5682
Epoch 4/10, Batch 636/883, Training Loss: 0.9189
Epoch 4/10, Batch 637/883, Training Loss: 0.9187
Epoch 4/10, Batch 638/883, Training Loss: 0.9472
Epoch 4/10, Batch 639/883, Training Loss: 0.6180
Epoch 4/10, Batch 640/883, Training Loss: 0.4956
Epoch 4/10, Batch 641/883, Training Loss: 0.6942
Epoch 4/10, Batch 642/883, Training Loss: 1.0362
Epoch 4/10, Batch 643/883, Training Loss: 0.8234
Epoch 4/10, Batch 644/883, Training Loss: 0.5377
Epoch 4/10, Batch 645/883, Training Loss: 0.8744
Epoch 4/10, Batch 646/883, Training Loss: 0.6956
Epoch 4/10, Batch 647/883, Training Loss: 0.5281
Epoch 4/10, Batch 648/883, Training Loss: 0.5692
Epoch 4/10, Batch 649/883, Training Loss: 0.8234
Epoch 4/10, Batch 650/883, Training Loss: 0.7541
Epoch 4/10, Batch 651/883, Training Loss: 1.0646
Epoch 4/10, Batch 652/883, Training Loss: 0.5491
Epoch 4/10, Batch 653/883, Training Loss: 1.2260
Epoch 4/10, Batch 654/883, Training Loss: 0.5928
Epoch 4/10, Batch 655/883, Training Loss: 0.7489
Epoch 4/10, Batch 656/883, Training Loss: 0.5335
Epoch 4/10, Batch 657/883, Training Loss: 0.4877
Epoch 4/10, Batch 658/883, Training Loss: 0.7113
Epoch 4/10, Batch 659/883, Training Loss: 0.7181
Epoch 4/10, Batch 660/883, Training Loss: 0.7458
Epoch 4/10, Batch 661/883, Training Loss: 0.9504
Epoch 4/10, Batch 662/883, Training Loss: 0.8598
Epoch 4/10, Batch 663/883, Training Loss: 0.9011
Epoch 4/10, Batch 664/883, Training Loss: 0.7336
Epoch 4/10, Batch 665/883, Training Loss: 0.6718
Epoch 4/10, Batch 666/883, Training Loss: 0.6719
Epoch 4/10, Batch 667/883, Training Loss: 0.6877
Epoch 4/10, Batch 668/883, Training Loss: 0.7742
Epoch 4/10, Batch 669/883, Training Loss: 0.5791
Epoch 4/10, Batch 670/883, Training Loss: 0.8420
Epoch 4/10, Batch 671/883, Training Loss: 0.9913
Epoch 4/10, Batch 672/883, Training Loss: 0.7697
Epoch 4/10, Batch 673/883, Training Loss: 0.8766
Epoch 4/10, Batch 674/883, Training Loss: 0.9865
Epoch 4/10, Batch 675/883, Training Loss: 1.1347
Epoch 4/10, Batch 676/883, Training Loss: 0.8394
Epoch 4/10, Batch 677/883, Training Loss: 0.9275
Epoch 4/10, Batch 678/883, Training Loss: 0.9051
Epoch 4/10, Batch 679/883, Training Loss: 0.6574
Epoch 4/10, Batch 680/883, Training Loss: 0.7604
Epoch 4/10, Batch 681/883, Training Loss: 1.2355
Epoch 4/10, Batch 682/883, Training Loss: 0.7722
Epoch 4/10, Batch 683/883, Training Loss: 1.1952
Epoch 4/10, Batch 684/883, Training Loss: 0.8350
Epoch 4/10, Batch 685/883, Training Loss: 0.5427
Epoch 4/10, Batch 686/883, Training Loss: 0.7128
Epoch 4/10, Batch 687/883, Training Loss: 0.8032
Epoch 4/10, Batch 688/883, Training Loss: 0.8514
Epoch 4/10, Batch 689/883, Training Loss: 0.6903
Epoch 4/10, Batch 690/883, Training Loss: 0.8384
Epoch 4/10, Batch 691/883, Training Loss: 0.7735
Epoch 4/10, Batch 692/883, Training Loss: 0.9548
Epoch 4/10, Batch 693/883, Training Loss: 0.6213
Epoch 4/10, Batch 694/883, Training Loss: 0.6376
Epoch 4/10, Batch 695/883, Training Loss: 0.9873
Epoch 4/10, Batch 696/883, Training Loss: 0.5899
Epoch 4/10, Batch 697/883, Training Loss: 0.7448
Epoch 4/10, Batch 698/883, Training Loss: 0.6933
Epoch 4/10, Batch 699/883, Training Loss: 0.6052
Epoch 4/10, Batch 700/883, Training Loss: 0.6242
Epoch 4/10, Batch 701/883, Training Loss: 0.8085
Epoch 4/10, Batch 702/883, Training Loss: 0.6695
Epoch 4/10, Batch 703/883, Training Loss: 1.0634
Epoch 4/10, Batch 704/883, Training Loss: 0.8362
Epoch 4/10, Batch 705/883, Training Loss: 0.6622
Epoch 4/10, Batch 706/883, Training Loss: 0.5808
Epoch 4/10, Batch 707/883, Training Loss: 0.8147
Epoch 4/10, Batch 708/883, Training Loss: 0.8978
Epoch 4/10, Batch 709/883, Training Loss: 0.7234
Epoch 4/10, Batch 710/883, Training Loss: 0.6876
Epoch 4/10, Batch 711/883, Training Loss: 0.6234
Epoch 4/10, Batch 712/883, Training Loss: 0.8937
Epoch 4/10, Batch 713/883, Training Loss: 0.5884
Epoch 4/10, Batch 714/883, Training Loss: 0.9291
Epoch 4/10, Batch 715/883, Training Loss: 0.7182
Epoch 4/10, Batch 716/883, Training Loss: 0.6893
Epoch 4/10, Batch 717/883, Training Loss: 0.9379
Epoch 4/10, Batch 718/883, Training Loss: 0.8313
Epoch 4/10, Batch 719/883, Training Loss: 0.7314
Epoch 4/10, Batch 720/883, Training Loss: 0.8035
Epoch 4/10, Batch 721/883, Training Loss: 0.7431
Epoch 4/10, Batch 722/883, Training Loss: 0.8024
Epoch 4/10, Batch 723/883, Training Loss: 0.9475
Epoch 4/10, Batch 724/883, Training Loss: 0.6232
Epoch 4/10, Batch 725/883, Training Loss: 0.5933
Epoch 4/10, Batch 726/883, Training Loss: 0.5585
Epoch 4/10, Batch 727/883, Training Loss: 0.6057
Epoch 4/10, Batch 728/883, Training Loss: 0.6103
Epoch 4/10, Batch 729/883, Training Loss: 0.5728
Epoch 4/10, Batch 730/883, Training Loss: 0.7784
Epoch 4/10, Batch 731/883, Training Loss: 0.5395
Epoch 4/10, Batch 732/883, Training Loss: 0.9173
Epoch 4/10, Batch 733/883, Training Loss: 1.1069
Epoch 4/10, Batch 734/883, Training Loss: 0.7145
Epoch 4/10, Batch 735/883, Training Loss: 1.0696
Epoch 4/10, Batch 736/883, Training Loss: 0.6869
Epoch 4/10, Batch 737/883, Training Loss: 0.8443
Epoch 4/10, Batch 738/883, Training Loss: 1.0668
Epoch 4/10, Batch 739/883, Training Loss: 0.7599
Epoch 4/10, Batch 740/883, Training Loss: 0.7073
Epoch 4/10, Batch 741/883, Training Loss: 1.0202
Epoch 4/10, Batch 742/883, Training Loss: 0.7548
Epoch 4/10, Batch 743/883, Training Loss: 0.9353
Epoch 4/10, Batch 744/883, Training Loss: 0.7010
Epoch 4/10, Batch 745/883, Training Loss: 0.6926
Epoch 4/10, Batch 746/883, Training Loss: 0.6340
Epoch 4/10, Batch 747/883, Training Loss: 0.7593
Epoch 4/10, Batch 748/883, Training Loss: 0.6262
Epoch 4/10, Batch 749/883, Training Loss: 0.7540
Epoch 4/10, Batch 750/883, Training Loss: 0.5416
Epoch 4/10, Batch 751/883, Training Loss: 0.6075
Epoch 4/10, Batch 752/883, Training Loss: 0.7862
Epoch 4/10, Batch 753/883, Training Loss: 0.6485
Epoch 4/10, Batch 754/883, Training Loss: 0.8400
Epoch 4/10, Batch 755/883, Training Loss: 0.9155
Epoch 4/10, Batch 756/883, Training Loss: 0.6831
Epoch 4/10, Batch 757/883, Training Loss: 0.7460
Epoch 4/10, Batch 758/883, Training Loss: 0.4753
Epoch 4/10, Batch 759/883, Training Loss: 0.9789
Epoch 4/10, Batch 760/883, Training Loss: 0.5792
Epoch 4/10, Batch 761/883, Training Loss: 0.5102
Epoch 4/10, Batch 762/883, Training Loss: 0.8104
Epoch 4/10, Batch 763/883, Training Loss: 0.6531
Epoch 4/10, Batch 764/883, Training Loss: 0.6774
Epoch 4/10, Batch 765/883, Training Loss: 0.8463
Epoch 4/10, Batch 766/883, Training Loss: 0.5114
Epoch 4/10, Batch 767/883, Training Loss: 0.7465
Epoch 4/10, Batch 768/883, Training Loss: 0.8723
Epoch 4/10, Batch 769/883, Training Loss: 0.9584
Epoch 4/10, Batch 770/883, Training Loss: 0.7634
Epoch 4/10, Batch 771/883, Training Loss: 0.9187
Epoch 4/10, Batch 772/883, Training Loss: 0.7956
Epoch 4/10, Batch 773/883, Training Loss: 0.6334
Epoch 4/10, Batch 774/883, Training Loss: 1.0311
Epoch 4/10, Batch 775/883, Training Loss: 1.0256
Epoch 4/10, Batch 776/883, Training Loss: 0.8788
Epoch 4/10, Batch 777/883, Training Loss: 0.8448
Epoch 4/10, Batch 778/883, Training Loss: 0.8537
Epoch 4/10, Batch 779/883, Training Loss: 0.7379
Epoch 4/10, Batch 780/883, Training Loss: 0.7959
Epoch 4/10, Batch 781/883, Training Loss: 0.6386
Epoch 4/10, Batch 782/883, Training Loss: 0.7541
Epoch 4/10, Batch 783/883, Training Loss: 0.7220
Epoch 4/10, Batch 784/883, Training Loss: 0.7242
Epoch 4/10, Batch 785/883, Training Loss: 0.7015
Epoch 4/10, Batch 786/883, Training Loss: 0.8230
Epoch 4/10, Batch 787/883, Training Loss: 0.7331
Epoch 4/10, Batch 788/883, Training Loss: 0.8475
Epoch 4/10, Batch 789/883, Training Loss: 0.7224
Epoch 4/10, Batch 790/883, Training Loss: 0.9377
Epoch 4/10, Batch 791/883, Training Loss: 0.8064
Epoch 4/10, Batch 792/883, Training Loss: 0.5827
Epoch 4/10, Batch 793/883, Training Loss: 0.8840
Epoch 4/10, Batch 794/883, Training Loss: 1.0170
Epoch 4/10, Batch 795/883, Training Loss: 0.7399
Epoch 4/10, Batch 796/883, Training Loss: 0.8400
Epoch 4/10, Batch 797/883, Training Loss: 0.7458
Epoch 4/10, Batch 798/883, Training Loss: 0.7815
Epoch 4/10, Batch 799/883, Training Loss: 0.7104
Epoch 4/10, Batch 800/883, Training Loss: 0.6237
Epoch 4/10, Batch 801/883, Training Loss: 0.5250
Epoch 4/10, Batch 802/883, Training Loss: 0.6886
Epoch 4/10, Batch 803/883, Training Loss: 0.5907
Epoch 4/10, Batch 804/883, Training Loss: 0.5895
Epoch 4/10, Batch 805/883, Training Loss: 0.6974
Epoch 4/10, Batch 806/883, Training Loss: 0.8538
Epoch 4/10, Batch 807/883, Training Loss: 1.1197
Epoch 4/10, Batch 808/883, Training Loss: 0.7690
Epoch 4/10, Batch 809/883, Training Loss: 0.9922
Epoch 4/10, Batch 810/883, Training Loss: 0.8542
Epoch 4/10, Batch 811/883, Training Loss: 0.8859
Epoch 4/10, Batch 812/883, Training Loss: 0.7302
Epoch 4/10, Batch 813/883, Training Loss: 0.8515
Epoch 4/10, Batch 814/883, Training Loss: 0.6175
Epoch 4/10, Batch 815/883, Training Loss: 0.8147
Epoch 4/10, Batch 816/883, Training Loss: 0.5919
Epoch 4/10, Batch 817/883, Training Loss: 0.5998
Epoch 4/10, Batch 818/883, Training Loss: 0.8311
Epoch 4/10, Batch 819/883, Training Loss: 1.0956
Epoch 4/10, Batch 820/883, Training Loss: 0.6379
Epoch 4/10, Batch 821/883, Training Loss: 0.9181
Epoch 4/10, Batch 822/883, Training Loss: 0.6883
Epoch 4/10, Batch 823/883, Training Loss: 0.5826
Epoch 4/10, Batch 824/883, Training Loss: 0.6661
Epoch 4/10, Batch 825/883, Training Loss: 0.6681
Epoch 4/10, Batch 826/883, Training Loss: 0.8102
Epoch 4/10, Batch 827/883, Training Loss: 0.8675
Epoch 4/10, Batch 828/883, Training Loss: 0.7614
Epoch 4/10, Batch 829/883, Training Loss: 0.6744
Epoch 4/10, Batch 830/883, Training Loss: 0.6757
Epoch 4/10, Batch 831/883, Training Loss: 0.8535
Epoch 4/10, Batch 832/883, Training Loss: 0.8570
Epoch 4/10, Batch 833/883, Training Loss: 0.5646
Epoch 4/10, Batch 834/883, Training Loss: 1.0557
Epoch 4/10, Batch 835/883, Training Loss: 0.9458
Epoch 4/10, Batch 836/883, Training Loss: 0.7217
Epoch 4/10, Batch 837/883, Training Loss: 0.6380
Epoch 4/10, Batch 838/883, Training Loss: 0.8031
Epoch 4/10, Batch 839/883, Training Loss: 0.9062
Epoch 4/10, Batch 840/883, Training Loss: 0.6296
Epoch 4/10, Batch 841/883, Training Loss: 0.8361
Epoch 4/10, Batch 842/883, Training Loss: 0.8599
Epoch 4/10, Batch 843/883, Training Loss: 0.6229
Epoch 4/10, Batch 844/883, Training Loss: 0.6248
Epoch 4/10, Batch 845/883, Training Loss: 0.7939
Epoch 4/10, Batch 846/883, Training Loss: 0.8475
Epoch 4/10, Batch 847/883, Training Loss: 1.1928
Epoch 4/10, Batch 848/883, Training Loss: 0.6838
Epoch 4/10, Batch 849/883, Training Loss: 0.7964
Epoch 4/10, Batch 850/883, Training Loss: 0.6318
Epoch 4/10, Batch 851/883, Training Loss: 0.9360
Epoch 4/10, Batch 852/883, Training Loss: 0.8009
Epoch 4/10, Batch 853/883, Training Loss: 0.6351
Epoch 4/10, Batch 854/883, Training Loss: 0.8747
Epoch 4/10, Batch 855/883, Training Loss: 0.7873
Epoch 4/10, Batch 856/883, Training Loss: 0.7155
Epoch 4/10, Batch 857/883, Training Loss: 0.9085
Epoch 4/10, Batch 858/883, Training Loss: 0.7871
Epoch 4/10, Batch 859/883, Training Loss: 0.8316
Epoch 4/10, Batch 860/883, Training Loss: 0.5861
Epoch 4/10, Batch 861/883, Training Loss: 0.5764
Epoch 4/10, Batch 862/883, Training Loss: 0.7233
Epoch 4/10, Batch 863/883, Training Loss: 0.7347
Epoch 4/10, Batch 864/883, Training Loss: 0.6868
Epoch 4/10, Batch 865/883, Training Loss: 0.6839
Epoch 4/10, Batch 866/883, Training Loss: 0.5636
Epoch 4/10, Batch 867/883, Training Loss: 0.7008
Epoch 4/10, Batch 868/883, Training Loss: 0.6268
Epoch 4/10, Batch 869/883, Training Loss: 0.6622
Epoch 4/10, Batch 870/883, Training Loss: 0.4918
Epoch 4/10, Batch 871/883, Training Loss: 1.1040
Epoch 4/10, Batch 872/883, Training Loss: 0.6142
Epoch 4/10, Batch 873/883, Training Loss: 0.9861
Epoch 4/10, Batch 874/883, Training Loss: 0.9572
Epoch 4/10, Batch 875/883, Training Loss: 0.6702
Epoch 4/10, Batch 876/883, Training Loss: 0.4733
Epoch 4/10, Batch 877/883, Training Loss: 0.5324
Epoch 4/10, Batch 878/883, Training Loss: 0.5106
Epoch 4/10, Batch 879/883, Training Loss: 0.8537
Epoch 4/10, Batch 880/883, Training Loss: 0.6167
Epoch 4/10, Batch 881/883, Training Loss: 0.6926
Epoch 4/10, Batch 882/883, Training Loss: 0.9191
Epoch 4/10, Batch 883/883, Training Loss: 1.4602
Epoch 4/10, Training Loss: 0.7768, Validation Loss: 0.7569, Validation Accuracy: 0.6205
Epoch 5/10, Batch 1/883, Training Loss: 0.6765
Epoch 5/10, Batch 2/883, Training Loss: 0.5183
Epoch 5/10, Batch 3/883, Training Loss: 0.8079
Epoch 5/10, Batch 4/883, Training Loss: 0.8032
Epoch 5/10, Batch 5/883, Training Loss: 0.8133
Epoch 5/10, Batch 6/883, Training Loss: 0.5354
Epoch 5/10, Batch 7/883, Training Loss: 0.7169
Epoch 5/10, Batch 8/883, Training Loss: 0.5411
Epoch 5/10, Batch 9/883, Training Loss: 0.7732
Epoch 5/10, Batch 10/883, Training Loss: 0.7235
Epoch 5/10, Batch 11/883, Training Loss: 1.0235
Epoch 5/10, Batch 12/883, Training Loss: 1.0000
Epoch 5/10, Batch 13/883, Training Loss: 0.5808
Epoch 5/10, Batch 14/883, Training Loss: 0.5790
Epoch 5/10, Batch 15/883, Training Loss: 0.7467
Epoch 5/10, Batch 16/883, Training Loss: 0.7513
Epoch 5/10, Batch 17/883, Training Loss: 0.9248
Epoch 5/10, Batch 18/883, Training Loss: 0.6153
Epoch 5/10, Batch 19/883, Training Loss: 0.7919
Epoch 5/10, Batch 20/883, Training Loss: 0.8120
Epoch 5/10, Batch 21/883, Training Loss: 0.6956
Epoch 5/10, Batch 22/883, Training Loss: 0.7931
Epoch 5/10, Batch 23/883, Training Loss: 0.8164
Epoch 5/10, Batch 24/883, Training Loss: 0.7398
Epoch 5/10, Batch 25/883, Training Loss: 0.9812
Epoch 5/10, Batch 26/883, Training Loss: 0.6897
Epoch 5/10, Batch 27/883, Training Loss: 0.8275
Epoch 5/10, Batch 28/883, Training Loss: 0.7142
Epoch 5/10, Batch 29/883, Training Loss: 0.6221
Epoch 5/10, Batch 30/883, Training Loss: 0.6127
Epoch 5/10, Batch 31/883, Training Loss: 0.7140
Epoch 5/10, Batch 32/883, Training Loss: 0.9622
Epoch 5/10, Batch 33/883, Training Loss: 1.0873
Epoch 5/10, Batch 34/883, Training Loss: 0.6488
Epoch 5/10, Batch 35/883, Training Loss: 1.0514
Epoch 5/10, Batch 36/883, Training Loss: 0.8321
Epoch 5/10, Batch 37/883, Training Loss: 0.7032
Epoch 5/10, Batch 38/883, Training Loss: 0.6512
Epoch 5/10, Batch 39/883, Training Loss: 0.6585
Epoch 5/10, Batch 40/883, Training Loss: 0.5702
Epoch 5/10, Batch 41/883, Training Loss: 0.7617
Epoch 5/10, Batch 42/883, Training Loss: 0.5608
Epoch 5/10, Batch 43/883, Training Loss: 0.7579
Epoch 5/10, Batch 44/883, Training Loss: 0.8575
Epoch 5/10, Batch 45/883, Training Loss: 0.7179
Epoch 5/10, Batch 46/883, Training Loss: 0.5976
Epoch 5/10, Batch 47/883, Training Loss: 0.7580
Epoch 5/10, Batch 48/883, Training Loss: 0.6237
Epoch 5/10, Batch 49/883, Training Loss: 0.6251
Epoch 5/10, Batch 50/883, Training Loss: 0.6066
Epoch 5/10, Batch 51/883, Training Loss: 0.7430
Epoch 5/10, Batch 52/883, Training Loss: 0.8236
Epoch 5/10, Batch 53/883, Training Loss: 0.7878
Epoch 5/10, Batch 54/883, Training Loss: 0.6279
Epoch 5/10, Batch 55/883, Training Loss: 0.9711
Epoch 5/10, Batch 56/883, Training Loss: 0.7434
Epoch 5/10, Batch 57/883, Training Loss: 0.6798
Epoch 5/10, Batch 58/883, Training Loss: 0.6473
Epoch 5/10, Batch 59/883, Training Loss: 0.6866
Epoch 5/10, Batch 60/883, Training Loss: 0.6207
Epoch 5/10, Batch 61/883, Training Loss: 0.8792
Epoch 5/10, Batch 62/883, Training Loss: 0.9738
Epoch 5/10, Batch 63/883, Training Loss: 0.7533
Epoch 5/10, Batch 64/883, Training Loss: 0.5532
Epoch 5/10, Batch 65/883, Training Loss: 0.7209
Epoch 5/10, Batch 66/883, Training Loss: 0.7378
Epoch 5/10, Batch 67/883, Training Loss: 0.6212
Epoch 5/10, Batch 68/883, Training Loss: 0.6541
Epoch 5/10, Batch 69/883, Training Loss: 0.5987
Epoch 5/10, Batch 70/883, Training Loss: 0.7419
Epoch 5/10, Batch 71/883, Training Loss: 0.6925
Epoch 5/10, Batch 72/883, Training Loss: 0.7133
Epoch 5/10, Batch 73/883, Training Loss: 0.7092
Epoch 5/10, Batch 74/883, Training Loss: 0.6304
Epoch 5/10, Batch 75/883, Training Loss: 0.6610
Epoch 5/10, Batch 76/883, Training Loss: 0.8263
Epoch 5/10, Batch 77/883, Training Loss: 0.8004
Epoch 5/10, Batch 78/883, Training Loss: 0.4912
Epoch 5/10, Batch 79/883, Training Loss: 0.8681
Epoch 5/10, Batch 80/883, Training Loss: 0.6444
Epoch 5/10, Batch 81/883, Training Loss: 0.6465
Epoch 5/10, Batch 82/883, Training Loss: 0.7183
Epoch 5/10, Batch 83/883, Training Loss: 0.7322
Epoch 5/10, Batch 84/883, Training Loss: 0.8812
Epoch 5/10, Batch 85/883, Training Loss: 0.7030
Epoch 5/10, Batch 86/883, Training Loss: 0.7361
Epoch 5/10, Batch 87/883, Training Loss: 0.6524
Epoch 5/10, Batch 88/883, Training Loss: 0.8276
Epoch 5/10, Batch 89/883, Training Loss: 1.2824
Epoch 5/10, Batch 90/883, Training Loss: 0.6067
Epoch 5/10, Batch 91/883, Training Loss: 0.5237
Epoch 5/10, Batch 92/883, Training Loss: 1.0901
Epoch 5/10, Batch 93/883, Training Loss: 0.5633
Epoch 5/10, Batch 94/883, Training Loss: 0.6352
Epoch 5/10, Batch 95/883, Training Loss: 0.5877
Epoch 5/10, Batch 96/883, Training Loss: 0.6401
Epoch 5/10, Batch 97/883, Training Loss: 0.6676
Epoch 5/10, Batch 98/883, Training Loss: 1.0416
Epoch 5/10, Batch 99/883, Training Loss: 0.5423
Epoch 5/10, Batch 100/883, Training Loss: 0.8236
Epoch 5/10, Batch 101/883, Training Loss: 0.6405
Epoch 5/10, Batch 102/883, Training Loss: 0.5123
Epoch 5/10, Batch 103/883, Training Loss: 0.7778
Epoch 5/10, Batch 104/883, Training Loss: 0.6921
Epoch 5/10, Batch 105/883, Training Loss: 0.7909
Epoch 5/10, Batch 106/883, Training Loss: 0.7503
Epoch 5/10, Batch 107/883, Training Loss: 0.5426
Epoch 5/10, Batch 108/883, Training Loss: 0.6024
Epoch 5/10, Batch 109/883, Training Loss: 0.7673
Epoch 5/10, Batch 110/883, Training Loss: 0.7292
Epoch 5/10, Batch 111/883, Training Loss: 0.8553
Epoch 5/10, Batch 112/883, Training Loss: 0.9401
Epoch 5/10, Batch 113/883, Training Loss: 0.8044
Epoch 5/10, Batch 114/883, Training Loss: 0.8481
Epoch 5/10, Batch 115/883, Training Loss: 0.7431
Epoch 5/10, Batch 116/883, Training Loss: 0.6667
Epoch 5/10, Batch 117/883, Training Loss: 0.5866
Epoch 5/10, Batch 118/883, Training Loss: 0.9390
Epoch 5/10, Batch 119/883, Training Loss: 1.0373
Epoch 5/10, Batch 120/883, Training Loss: 0.8128
Epoch 5/10, Batch 121/883, Training Loss: 1.2001
Epoch 5/10, Batch 122/883, Training Loss: 0.7282
Epoch 5/10, Batch 123/883, Training Loss: 0.7268
Epoch 5/10, Batch 124/883, Training Loss: 1.1748
Epoch 5/10, Batch 125/883, Training Loss: 0.7435
Epoch 5/10, Batch 126/883, Training Loss: 1.0748
Epoch 5/10, Batch 127/883, Training Loss: 0.7737
Epoch 5/10, Batch 128/883, Training Loss: 0.6358
Epoch 5/10, Batch 129/883, Training Loss: 0.5510
Epoch 5/10, Batch 130/883, Training Loss: 0.6460
Epoch 5/10, Batch 131/883, Training Loss: 0.5960
Epoch 5/10, Batch 132/883, Training Loss: 0.8742
Epoch 5/10, Batch 133/883, Training Loss: 0.8095
Epoch 5/10, Batch 134/883, Training Loss: 0.8324
Epoch 5/10, Batch 135/883, Training Loss: 0.5296
Epoch 5/10, Batch 136/883, Training Loss: 0.8735
Epoch 5/10, Batch 137/883, Training Loss: 0.9359
Epoch 5/10, Batch 138/883, Training Loss: 0.8871
Epoch 5/10, Batch 139/883, Training Loss: 0.6216
Epoch 5/10, Batch 140/883, Training Loss: 0.7021
Epoch 5/10, Batch 141/883, Training Loss: 0.5267
Epoch 5/10, Batch 142/883, Training Loss: 0.8651
Epoch 5/10, Batch 143/883, Training Loss: 0.7194
Epoch 5/10, Batch 144/883, Training Loss: 0.6245
Epoch 5/10, Batch 145/883, Training Loss: 0.9830
Epoch 5/10, Batch 146/883, Training Loss: 0.7972
Epoch 5/10, Batch 147/883, Training Loss: 1.1816
Epoch 5/10, Batch 148/883, Training Loss: 0.6478
Epoch 5/10, Batch 149/883, Training Loss: 0.5050
Epoch 5/10, Batch 150/883, Training Loss: 0.8430
Epoch 5/10, Batch 151/883, Training Loss: 0.5838
Epoch 5/10, Batch 152/883, Training Loss: 0.6487
Epoch 5/10, Batch 153/883, Training Loss: 0.5483
Epoch 5/10, Batch 154/883, Training Loss: 0.9377
Epoch 5/10, Batch 155/883, Training Loss: 0.9487
Epoch 5/10, Batch 156/883, Training Loss: 0.8573
Epoch 5/10, Batch 157/883, Training Loss: 0.8093
Epoch 5/10, Batch 158/883, Training Loss: 0.7865
Epoch 5/10, Batch 159/883, Training Loss: 0.6823
Epoch 5/10, Batch 160/883, Training Loss: 0.7192
Epoch 5/10, Batch 161/883, Training Loss: 0.6539
Epoch 5/10, Batch 162/883, Training Loss: 0.7935
Epoch 5/10, Batch 163/883, Training Loss: 0.6817
Epoch 5/10, Batch 164/883, Training Loss: 0.8327
Epoch 5/10, Batch 165/883, Training Loss: 0.5243
Epoch 5/10, Batch 166/883, Training Loss: 0.6024
Epoch 5/10, Batch 167/883, Training Loss: 0.6084
Epoch 5/10, Batch 168/883, Training Loss: 0.6531
Epoch 5/10, Batch 169/883, Training Loss: 0.6622
Epoch 5/10, Batch 170/883, Training Loss: 0.7516
Epoch 5/10, Batch 171/883, Training Loss: 0.6629
Epoch 5/10, Batch 172/883, Training Loss: 0.5788
Epoch 5/10, Batch 173/883, Training Loss: 0.7694
Epoch 5/10, Batch 174/883, Training Loss: 0.7555
Epoch 5/10, Batch 175/883, Training Loss: 1.0432
Epoch 5/10, Batch 176/883, Training Loss: 0.6165
Epoch 5/10, Batch 177/883, Training Loss: 0.7082
Epoch 5/10, Batch 178/883, Training Loss: 0.8005
Epoch 5/10, Batch 179/883, Training Loss: 0.7503
Epoch 5/10, Batch 180/883, Training Loss: 0.8852
Epoch 5/10, Batch 181/883, Training Loss: 0.6838
Epoch 5/10, Batch 182/883, Training Loss: 0.6925
Epoch 5/10, Batch 183/883, Training Loss: 0.5219
Epoch 5/10, Batch 184/883, Training Loss: 0.7875
Epoch 5/10, Batch 185/883, Training Loss: 0.6435
Epoch 5/10, Batch 186/883, Training Loss: 0.6146
Epoch 5/10, Batch 187/883, Training Loss: 0.5884
Epoch 5/10, Batch 188/883, Training Loss: 0.7770
Epoch 5/10, Batch 189/883, Training Loss: 1.2591
Epoch 5/10, Batch 190/883, Training Loss: 0.9424
Epoch 5/10, Batch 191/883, Training Loss: 0.6643
Epoch 5/10, Batch 192/883, Training Loss: 0.6257
Epoch 5/10, Batch 193/883, Training Loss: 1.1363
Epoch 5/10, Batch 194/883, Training Loss: 0.5277
Epoch 5/10, Batch 195/883, Training Loss: 0.7191
Epoch 5/10, Batch 196/883, Training Loss: 0.9462
Epoch 5/10, Batch 197/883, Training Loss: 0.7481
Epoch 5/10, Batch 198/883, Training Loss: 0.6933
Epoch 5/10, Batch 199/883, Training Loss: 0.5250
Epoch 5/10, Batch 200/883, Training Loss: 0.7812
Epoch 5/10, Batch 201/883, Training Loss: 0.7641
Epoch 5/10, Batch 202/883, Training Loss: 0.8271
Epoch 5/10, Batch 203/883, Training Loss: 0.5196
Epoch 5/10, Batch 204/883, Training Loss: 0.7154
Epoch 5/10, Batch 205/883, Training Loss: 0.6691
Epoch 5/10, Batch 206/883, Training Loss: 0.4818
Epoch 5/10, Batch 207/883, Training Loss: 1.2681
Epoch 5/10, Batch 208/883, Training Loss: 0.5879
Epoch 5/10, Batch 209/883, Training Loss: 0.6975
Epoch 5/10, Batch 210/883, Training Loss: 0.8017
Epoch 5/10, Batch 211/883, Training Loss: 0.6899
Epoch 5/10, Batch 212/883, Training Loss: 0.6404
Epoch 5/10, Batch 213/883, Training Loss: 0.5861
Epoch 5/10, Batch 214/883, Training Loss: 0.6851
Epoch 5/10, Batch 215/883, Training Loss: 0.7697
Epoch 5/10, Batch 216/883, Training Loss: 0.6669
Epoch 5/10, Batch 217/883, Training Loss: 0.5493
Epoch 5/10, Batch 218/883, Training Loss: 0.8472
Epoch 5/10, Batch 219/883, Training Loss: 0.7405
Epoch 5/10, Batch 220/883, Training Loss: 0.7788
Epoch 5/10, Batch 221/883, Training Loss: 0.7102
Epoch 5/10, Batch 222/883, Training Loss: 0.8612
Epoch 5/10, Batch 223/883, Training Loss: 0.7475
Epoch 5/10, Batch 224/883, Training Loss: 0.3973
Epoch 5/10, Batch 225/883, Training Loss: 0.7417
Epoch 5/10, Batch 226/883, Training Loss: 0.6894
Epoch 5/10, Batch 227/883, Training Loss: 0.5331
Epoch 5/10, Batch 228/883, Training Loss: 0.7736
Epoch 5/10, Batch 229/883, Training Loss: 0.8019
Epoch 5/10, Batch 230/883, Training Loss: 0.6938
Epoch 5/10, Batch 231/883, Training Loss: 0.5142
Epoch 5/10, Batch 232/883, Training Loss: 0.5527
Epoch 5/10, Batch 233/883, Training Loss: 0.4931
Epoch 5/10, Batch 234/883, Training Loss: 0.5203
Epoch 5/10, Batch 235/883, Training Loss: 0.7596
Epoch 5/10, Batch 236/883, Training Loss: 0.8783
Epoch 5/10, Batch 237/883, Training Loss: 0.9351
Epoch 5/10, Batch 238/883, Training Loss: 0.7821
Epoch 5/10, Batch 239/883, Training Loss: 0.6182
Epoch 5/10, Batch 240/883, Training Loss: 0.7650
Epoch 5/10, Batch 241/883, Training Loss: 0.6444
Epoch 5/10, Batch 242/883, Training Loss: 0.6785
Epoch 5/10, Batch 243/883, Training Loss: 0.8277
Epoch 5/10, Batch 244/883, Training Loss: 0.8489
Epoch 5/10, Batch 245/883, Training Loss: 0.6963
Epoch 5/10, Batch 246/883, Training Loss: 1.0077
Epoch 5/10, Batch 247/883, Training Loss: 0.8844
Epoch 5/10, Batch 248/883, Training Loss: 0.8701
Epoch 5/10, Batch 249/883, Training Loss: 0.9137
Epoch 5/10, Batch 250/883, Training Loss: 1.1981
Epoch 5/10, Batch 251/883, Training Loss: 0.5602
Epoch 5/10, Batch 252/883, Training Loss: 0.7060
Epoch 5/10, Batch 253/883, Training Loss: 0.4913
Epoch 5/10, Batch 254/883, Training Loss: 0.8500
Epoch 5/10, Batch 255/883, Training Loss: 0.5695
Epoch 5/10, Batch 256/883, Training Loss: 0.6333
Epoch 5/10, Batch 257/883, Training Loss: 0.9613
Epoch 5/10, Batch 258/883, Training Loss: 0.8305
Epoch 5/10, Batch 259/883, Training Loss: 0.7606
Epoch 5/10, Batch 260/883, Training Loss: 0.6002
Epoch 5/10, Batch 261/883, Training Loss: 0.6987
Epoch 5/10, Batch 262/883, Training Loss: 0.6202
Epoch 5/10, Batch 263/883, Training Loss: 0.6680
Epoch 5/10, Batch 264/883, Training Loss: 1.3638
Epoch 5/10, Batch 265/883, Training Loss: 0.7009
Epoch 5/10, Batch 266/883, Training Loss: 0.6872
Epoch 5/10, Batch 267/883, Training Loss: 0.7165
Epoch 5/10, Batch 268/883, Training Loss: 0.9503
Epoch 5/10, Batch 269/883, Training Loss: 0.5937
Epoch 5/10, Batch 270/883, Training Loss: 0.6675
Epoch 5/10, Batch 271/883, Training Loss: 0.7468
Epoch 5/10, Batch 272/883, Training Loss: 0.6079
Epoch 5/10, Batch 273/883, Training Loss: 0.7854
Epoch 5/10, Batch 274/883, Training Loss: 0.7027
Epoch 5/10, Batch 275/883, Training Loss: 0.8878
Epoch 5/10, Batch 276/883, Training Loss: 0.6246
Epoch 5/10, Batch 277/883, Training Loss: 0.6279
Epoch 5/10, Batch 278/883, Training Loss: 0.5260
Epoch 5/10, Batch 279/883, Training Loss: 0.5963
Epoch 5/10, Batch 280/883, Training Loss: 1.1547
Epoch 5/10, Batch 281/883, Training Loss: 0.5242
Epoch 5/10, Batch 282/883, Training Loss: 0.6064
Epoch 5/10, Batch 283/883, Training Loss: 0.9004
Epoch 5/10, Batch 284/883, Training Loss: 0.6729
Epoch 5/10, Batch 285/883, Training Loss: 0.4771
Epoch 5/10, Batch 286/883, Training Loss: 0.7204
Epoch 5/10, Batch 287/883, Training Loss: 0.4111
Epoch 5/10, Batch 288/883, Training Loss: 1.0086
Epoch 5/10, Batch 289/883, Training Loss: 0.4873
Epoch 5/10, Batch 290/883, Training Loss: 0.9191
Epoch 5/10, Batch 291/883, Training Loss: 0.8116
Epoch 5/10, Batch 292/883, Training Loss: 0.7146
Epoch 5/10, Batch 293/883, Training Loss: 0.9905
Epoch 5/10, Batch 294/883, Training Loss: 0.3953
Epoch 5/10, Batch 295/883, Training Loss: 0.7443
Epoch 5/10, Batch 296/883, Training Loss: 0.7805
Epoch 5/10, Batch 297/883, Training Loss: 0.7927
Epoch 5/10, Batch 298/883, Training Loss: 1.0730
Epoch 5/10, Batch 299/883, Training Loss: 0.6803
Epoch 5/10, Batch 300/883, Training Loss: 0.9904
Epoch 5/10, Batch 301/883, Training Loss: 0.9717
Epoch 5/10, Batch 302/883, Training Loss: 0.8282
Epoch 5/10, Batch 303/883, Training Loss: 0.6122
Epoch 5/10, Batch 304/883, Training Loss: 0.7070
Epoch 5/10, Batch 305/883, Training Loss: 0.5456
Epoch 5/10, Batch 306/883, Training Loss: 0.7721
Epoch 5/10, Batch 307/883, Training Loss: 0.5390
Epoch 5/10, Batch 308/883, Training Loss: 0.7396
Epoch 5/10, Batch 309/883, Training Loss: 0.7231
Epoch 5/10, Batch 310/883, Training Loss: 0.5028
Epoch 5/10, Batch 311/883, Training Loss: 0.6546
Epoch 5/10, Batch 312/883, Training Loss: 0.7655
Epoch 5/10, Batch 313/883, Training Loss: 0.8939
Epoch 5/10, Batch 314/883, Training Loss: 0.5899
Epoch 5/10, Batch 315/883, Training Loss: 1.0029
Epoch 5/10, Batch 316/883, Training Loss: 0.7420
Epoch 5/10, Batch 317/883, Training Loss: 0.7626
Epoch 5/10, Batch 318/883, Training Loss: 0.6312
Epoch 5/10, Batch 319/883, Training Loss: 0.9518
Epoch 5/10, Batch 320/883, Training Loss: 0.8966
Epoch 5/10, Batch 321/883, Training Loss: 0.7416
Epoch 5/10, Batch 322/883, Training Loss: 0.6024
Epoch 5/10, Batch 323/883, Training Loss: 0.6817
Epoch 5/10, Batch 324/883, Training Loss: 0.7596
Epoch 5/10, Batch 325/883, Training Loss: 0.6189
Epoch 5/10, Batch 326/883, Training Loss: 0.5214
Epoch 5/10, Batch 327/883, Training Loss: 0.6356
Epoch 5/10, Batch 328/883, Training Loss: 0.9457
Epoch 5/10, Batch 329/883, Training Loss: 0.5801
Epoch 5/10, Batch 330/883, Training Loss: 0.5895
Epoch 5/10, Batch 331/883, Training Loss: 0.6686
Epoch 5/10, Batch 332/883, Training Loss: 0.8258
Epoch 5/10, Batch 333/883, Training Loss: 0.7745
Epoch 5/10, Batch 334/883, Training Loss: 0.8909
Epoch 5/10, Batch 335/883, Training Loss: 0.6835
Epoch 5/10, Batch 336/883, Training Loss: 0.6523
Epoch 5/10, Batch 337/883, Training Loss: 0.8035
Epoch 5/10, Batch 338/883, Training Loss: 0.9737
Epoch 5/10, Batch 339/883, Training Loss: 0.4269
Epoch 5/10, Batch 340/883, Training Loss: 0.7869
Epoch 5/10, Batch 341/883, Training Loss: 0.6701
Epoch 5/10, Batch 342/883, Training Loss: 0.6042
Epoch 5/10, Batch 343/883, Training Loss: 0.8663
Epoch 5/10, Batch 344/883, Training Loss: 0.6644
Epoch 5/10, Batch 345/883, Training Loss: 0.9008
Epoch 5/10, Batch 346/883, Training Loss: 0.8087
Epoch 5/10, Batch 347/883, Training Loss: 0.6880
Epoch 5/10, Batch 348/883, Training Loss: 0.9434
Epoch 5/10, Batch 349/883, Training Loss: 0.5888
Epoch 5/10, Batch 350/883, Training Loss: 0.9559
Epoch 5/10, Batch 351/883, Training Loss: 0.7529
Epoch 5/10, Batch 352/883, Training Loss: 0.8669
Epoch 5/10, Batch 353/883, Training Loss: 0.7447
Epoch 5/10, Batch 354/883, Training Loss: 0.8063
Epoch 5/10, Batch 355/883, Training Loss: 0.4746
Epoch 5/10, Batch 356/883, Training Loss: 0.6888
Epoch 5/10, Batch 357/883, Training Loss: 0.5167
Epoch 5/10, Batch 358/883, Training Loss: 0.5127
Epoch 5/10, Batch 359/883, Training Loss: 0.7073
Epoch 5/10, Batch 360/883, Training Loss: 0.6061
Epoch 5/10, Batch 361/883, Training Loss: 0.7352
Epoch 5/10, Batch 362/883, Training Loss: 0.4975
Epoch 5/10, Batch 363/883, Training Loss: 0.7432
Epoch 5/10, Batch 364/883, Training Loss: 0.5655
Epoch 5/10, Batch 365/883, Training Loss: 0.6536
Epoch 5/10, Batch 366/883, Training Loss: 0.8694
Epoch 5/10, Batch 367/883, Training Loss: 0.7800
Epoch 5/10, Batch 368/883, Training Loss: 0.4814
Epoch 5/10, Batch 369/883, Training Loss: 0.6935
Epoch 5/10, Batch 370/883, Training Loss: 0.6706
Epoch 5/10, Batch 371/883, Training Loss: 0.7593
Epoch 5/10, Batch 372/883, Training Loss: 0.7452
Epoch 5/10, Batch 373/883, Training Loss: 0.6916
Epoch 5/10, Batch 374/883, Training Loss: 0.9331
Epoch 5/10, Batch 375/883, Training Loss: 0.7937
Epoch 5/10, Batch 376/883, Training Loss: 0.8065
Epoch 5/10, Batch 377/883, Training Loss: 0.9045
Epoch 5/10, Batch 378/883, Training Loss: 0.9426
Epoch 5/10, Batch 379/883, Training Loss: 0.6199
Epoch 5/10, Batch 380/883, Training Loss: 0.5902
Epoch 5/10, Batch 381/883, Training Loss: 1.0669
Epoch 5/10, Batch 382/883, Training Loss: 0.5312
Epoch 5/10, Batch 383/883, Training Loss: 0.6792
Epoch 5/10, Batch 384/883, Training Loss: 0.6357
Epoch 5/10, Batch 385/883, Training Loss: 0.9046
Epoch 5/10, Batch 386/883, Training Loss: 0.9655
Epoch 5/10, Batch 387/883, Training Loss: 0.6429
Epoch 5/10, Batch 388/883, Training Loss: 0.7932
Epoch 5/10, Batch 389/883, Training Loss: 0.4925
Epoch 5/10, Batch 390/883, Training Loss: 0.6776
Epoch 5/10, Batch 391/883, Training Loss: 0.5880
Epoch 5/10, Batch 392/883, Training Loss: 0.5864
Epoch 5/10, Batch 393/883, Training Loss: 0.7777
Epoch 5/10, Batch 394/883, Training Loss: 0.6696
Epoch 5/10, Batch 395/883, Training Loss: 1.1972
Epoch 5/10, Batch 396/883, Training Loss: 0.7814
Epoch 5/10, Batch 397/883, Training Loss: 0.8605
Epoch 5/10, Batch 398/883, Training Loss: 0.6937
Epoch 5/10, Batch 399/883, Training Loss: 0.7969
Epoch 5/10, Batch 400/883, Training Loss: 0.7355
Epoch 5/10, Batch 401/883, Training Loss: 0.8014
Epoch 5/10, Batch 402/883, Training Loss: 0.5213
Epoch 5/10, Batch 403/883, Training Loss: 0.7376
Epoch 5/10, Batch 404/883, Training Loss: 0.8623
Epoch 5/10, Batch 405/883, Training Loss: 0.7103
Epoch 5/10, Batch 406/883, Training Loss: 0.6852
Epoch 5/10, Batch 407/883, Training Loss: 0.8102
Epoch 5/10, Batch 408/883, Training Loss: 0.5222
Epoch 5/10, Batch 409/883, Training Loss: 0.7815
Epoch 5/10, Batch 410/883, Training Loss: 0.9281
Epoch 5/10, Batch 411/883, Training Loss: 0.4238
Epoch 5/10, Batch 412/883, Training Loss: 0.4931
Epoch 5/10, Batch 413/883, Training Loss: 0.8329
Epoch 5/10, Batch 414/883, Training Loss: 0.5337
Epoch 5/10, Batch 415/883, Training Loss: 0.8738
Epoch 5/10, Batch 416/883, Training Loss: 0.6295
Epoch 5/10, Batch 417/883, Training Loss: 0.6860
Epoch 5/10, Batch 418/883, Training Loss: 0.8969
Epoch 5/10, Batch 419/883, Training Loss: 0.6633
Epoch 5/10, Batch 420/883, Training Loss: 0.8847
Epoch 5/10, Batch 421/883, Training Loss: 0.8440
Epoch 5/10, Batch 422/883, Training Loss: 0.8586
Epoch 5/10, Batch 423/883, Training Loss: 0.7694
Epoch 5/10, Batch 424/883, Training Loss: 0.6383
Epoch 5/10, Batch 425/883, Training Loss: 0.6254
Epoch 5/10, Batch 426/883, Training Loss: 0.6175
Epoch 5/10, Batch 427/883, Training Loss: 0.8326
Epoch 5/10, Batch 428/883, Training Loss: 0.7140
Epoch 5/10, Batch 429/883, Training Loss: 0.6755
Epoch 5/10, Batch 430/883, Training Loss: 0.7034
Epoch 5/10, Batch 431/883, Training Loss: 0.8646
Epoch 5/10, Batch 432/883, Training Loss: 0.6984
Epoch 5/10, Batch 433/883, Training Loss: 0.7106
Epoch 5/10, Batch 434/883, Training Loss: 0.6017
Epoch 5/10, Batch 435/883, Training Loss: 0.6320
Epoch 5/10, Batch 436/883, Training Loss: 0.7343
Epoch 5/10, Batch 437/883, Training Loss: 0.8504
Epoch 5/10, Batch 438/883, Training Loss: 0.5947
Epoch 5/10, Batch 439/883, Training Loss: 0.7067
Epoch 5/10, Batch 440/883, Training Loss: 0.6223
Epoch 5/10, Batch 441/883, Training Loss: 0.5560
Epoch 5/10, Batch 442/883, Training Loss: 1.0414
Epoch 5/10, Batch 443/883, Training Loss: 0.7122
Epoch 5/10, Batch 444/883, Training Loss: 0.5710
Epoch 5/10, Batch 445/883, Training Loss: 0.7413
Epoch 5/10, Batch 446/883, Training Loss: 0.5702
Epoch 5/10, Batch 447/883, Training Loss: 0.6557
Epoch 5/10, Batch 448/883, Training Loss: 0.7596
Epoch 5/10, Batch 449/883, Training Loss: 0.7041
Epoch 5/10, Batch 450/883, Training Loss: 0.7916
Epoch 5/10, Batch 451/883, Training Loss: 0.5823
Epoch 5/10, Batch 452/883, Training Loss: 0.5064
Epoch 5/10, Batch 453/883, Training Loss: 1.1017
Epoch 5/10, Batch 454/883, Training Loss: 0.6518
Epoch 5/10, Batch 455/883, Training Loss: 0.7410
Epoch 5/10, Batch 456/883, Training Loss: 0.8796
Epoch 5/10, Batch 457/883, Training Loss: 0.6305
Epoch 5/10, Batch 458/883, Training Loss: 0.4576
Epoch 5/10, Batch 459/883, Training Loss: 0.9728
Epoch 5/10, Batch 460/883, Training Loss: 0.4702
Epoch 5/10, Batch 461/883, Training Loss: 0.4561
Epoch 5/10, Batch 462/883, Training Loss: 0.7318
Epoch 5/10, Batch 463/883, Training Loss: 0.9041
Epoch 5/10, Batch 464/883, Training Loss: 1.1659
Epoch 5/10, Batch 465/883, Training Loss: 0.6120
Epoch 5/10, Batch 466/883, Training Loss: 0.5924
Epoch 5/10, Batch 467/883, Training Loss: 1.1595
Epoch 5/10, Batch 468/883, Training Loss: 0.6997
Epoch 5/10, Batch 469/883, Training Loss: 0.7619
Epoch 5/10, Batch 470/883, Training Loss: 1.1390
Epoch 5/10, Batch 471/883, Training Loss: 0.9317
Epoch 5/10, Batch 472/883, Training Loss: 0.6358
Epoch 5/10, Batch 473/883, Training Loss: 0.8249
Epoch 5/10, Batch 474/883, Training Loss: 0.8782
Epoch 5/10, Batch 475/883, Training Loss: 0.7034
Epoch 5/10, Batch 476/883, Training Loss: 0.6902
Epoch 5/10, Batch 477/883, Training Loss: 0.8653
Epoch 5/10, Batch 478/883, Training Loss: 0.5737
Epoch 5/10, Batch 479/883, Training Loss: 0.8163
Epoch 5/10, Batch 480/883, Training Loss: 0.8460
Epoch 5/10, Batch 481/883, Training Loss: 0.6604
Epoch 5/10, Batch 482/883, Training Loss: 0.6292
Epoch 5/10, Batch 483/883, Training Loss: 0.5002
Epoch 5/10, Batch 484/883, Training Loss: 0.7897
Epoch 5/10, Batch 485/883, Training Loss: 0.8790
Epoch 5/10, Batch 486/883, Training Loss: 1.1688
Epoch 5/10, Batch 487/883, Training Loss: 0.7620
Epoch 5/10, Batch 488/883, Training Loss: 0.6731
Epoch 5/10, Batch 489/883, Training Loss: 0.7918
Epoch 5/10, Batch 490/883, Training Loss: 0.8858
Epoch 5/10, Batch 491/883, Training Loss: 0.7134
Epoch 5/10, Batch 492/883, Training Loss: 0.8831
Epoch 5/10, Batch 493/883, Training Loss: 0.6617
Epoch 5/10, Batch 494/883, Training Loss: 0.7288
Epoch 5/10, Batch 495/883, Training Loss: 0.8276
Epoch 5/10, Batch 496/883, Training Loss: 0.7393
Epoch 5/10, Batch 497/883, Training Loss: 0.6087
Epoch 5/10, Batch 498/883, Training Loss: 0.8456
Epoch 5/10, Batch 499/883, Training Loss: 0.6668
Epoch 5/10, Batch 500/883, Training Loss: 0.7220
Epoch 5/10, Batch 501/883, Training Loss: 0.7518
Epoch 5/10, Batch 502/883, Training Loss: 0.5576
Epoch 5/10, Batch 503/883, Training Loss: 0.6078
Epoch 5/10, Batch 504/883, Training Loss: 0.8213
Epoch 5/10, Batch 505/883, Training Loss: 0.8415
Epoch 5/10, Batch 506/883, Training Loss: 0.6459
Epoch 5/10, Batch 507/883, Training Loss: 0.8300
Epoch 5/10, Batch 508/883, Training Loss: 0.8871
Epoch 5/10, Batch 509/883, Training Loss: 0.8491
Epoch 5/10, Batch 510/883, Training Loss: 0.7792
Epoch 5/10, Batch 511/883, Training Loss: 0.6312
Epoch 5/10, Batch 512/883, Training Loss: 0.6461
Epoch 5/10, Batch 513/883, Training Loss: 0.8958
Epoch 5/10, Batch 514/883, Training Loss: 0.6043
Epoch 5/10, Batch 515/883, Training Loss: 0.6368
Epoch 5/10, Batch 516/883, Training Loss: 0.7423
Epoch 5/10, Batch 517/883, Training Loss: 0.6745
Epoch 5/10, Batch 518/883, Training Loss: 1.0408
Epoch 5/10, Batch 519/883, Training Loss: 0.9430
Epoch 5/10, Batch 520/883, Training Loss: 0.9540
Epoch 5/10, Batch 521/883, Training Loss: 0.7630
Epoch 5/10, Batch 522/883, Training Loss: 0.8571
Epoch 5/10, Batch 523/883, Training Loss: 0.7988
Epoch 5/10, Batch 524/883, Training Loss: 0.8031
Epoch 5/10, Batch 525/883, Training Loss: 0.5987
Epoch 5/10, Batch 526/883, Training Loss: 0.5611
Epoch 5/10, Batch 527/883, Training Loss: 0.7428
Epoch 5/10, Batch 528/883, Training Loss: 0.8175
Epoch 5/10, Batch 529/883, Training Loss: 0.9363
Epoch 5/10, Batch 530/883, Training Loss: 0.8746
Epoch 5/10, Batch 531/883, Training Loss: 0.5339
Epoch 5/10, Batch 532/883, Training Loss: 0.8169
Epoch 5/10, Batch 533/883, Training Loss: 0.5368
Epoch 5/10, Batch 534/883, Training Loss: 0.6582
Epoch 5/10, Batch 535/883, Training Loss: 0.9504
Epoch 5/10, Batch 536/883, Training Loss: 0.6325
Epoch 5/10, Batch 537/883, Training Loss: 0.4895
Epoch 5/10, Batch 538/883, Training Loss: 0.7680
Epoch 5/10, Batch 539/883, Training Loss: 0.7481
Epoch 5/10, Batch 540/883, Training Loss: 0.7020
Epoch 5/10, Batch 541/883, Training Loss: 0.5518
Epoch 5/10, Batch 542/883, Training Loss: 0.5625
Epoch 5/10, Batch 543/883, Training Loss: 0.6951
Epoch 5/10, Batch 544/883, Training Loss: 0.7400
Epoch 5/10, Batch 545/883, Training Loss: 0.7012
Epoch 5/10, Batch 546/883, Training Loss: 0.6327
Epoch 5/10, Batch 547/883, Training Loss: 0.7570
Epoch 5/10, Batch 548/883, Training Loss: 0.7515
Epoch 5/10, Batch 549/883, Training Loss: 0.6670
Epoch 5/10, Batch 550/883, Training Loss: 0.9420
Epoch 5/10, Batch 551/883, Training Loss: 0.6465
Epoch 5/10, Batch 552/883, Training Loss: 0.8407
Epoch 5/10, Batch 553/883, Training Loss: 0.6382
Epoch 5/10, Batch 554/883, Training Loss: 0.5956
Epoch 5/10, Batch 555/883, Training Loss: 0.6981
Epoch 5/10, Batch 556/883, Training Loss: 0.6932
Epoch 5/10, Batch 557/883, Training Loss: 0.4881
Epoch 5/10, Batch 558/883, Training Loss: 0.9256
Epoch 5/10, Batch 559/883, Training Loss: 0.6046
Epoch 5/10, Batch 560/883, Training Loss: 1.0106
Epoch 5/10, Batch 561/883, Training Loss: 0.6509
Epoch 5/10, Batch 562/883, Training Loss: 0.7298
Epoch 5/10, Batch 563/883, Training Loss: 0.7674
Epoch 5/10, Batch 564/883, Training Loss: 0.7837
Epoch 5/10, Batch 565/883, Training Loss: 1.0372
Epoch 5/10, Batch 566/883, Training Loss: 0.7357
Epoch 5/10, Batch 567/883, Training Loss: 0.9116
Epoch 5/10, Batch 568/883, Training Loss: 0.8127
Epoch 5/10, Batch 569/883, Training Loss: 0.7289
Epoch 5/10, Batch 570/883, Training Loss: 0.6845
Epoch 5/10, Batch 571/883, Training Loss: 0.8536
Epoch 5/10, Batch 572/883, Training Loss: 0.7023
Epoch 5/10, Batch 573/883, Training Loss: 0.4844
Epoch 5/10, Batch 574/883, Training Loss: 1.0205
Epoch 5/10, Batch 575/883, Training Loss: 0.7874
Epoch 5/10, Batch 576/883, Training Loss: 1.0108
Epoch 5/10, Batch 577/883, Training Loss: 0.5166
Epoch 5/10, Batch 578/883, Training Loss: 0.7370
Epoch 5/10, Batch 579/883, Training Loss: 1.0566
Epoch 5/10, Batch 580/883, Training Loss: 0.5817
Epoch 5/10, Batch 581/883, Training Loss: 0.8880
Epoch 5/10, Batch 582/883, Training Loss: 0.5454
Epoch 5/10, Batch 583/883, Training Loss: 0.6959
Epoch 5/10, Batch 584/883, Training Loss: 0.5575
Epoch 5/10, Batch 585/883, Training Loss: 0.6274
Epoch 5/10, Batch 586/883, Training Loss: 0.4899
Epoch 5/10, Batch 587/883, Training Loss: 0.7244
Epoch 5/10, Batch 588/883, Training Loss: 0.8388
Epoch 5/10, Batch 589/883, Training Loss: 0.9316
Epoch 5/10, Batch 590/883, Training Loss: 0.8293
Epoch 5/10, Batch 591/883, Training Loss: 0.6956
Epoch 5/10, Batch 592/883, Training Loss: 0.7202
Epoch 5/10, Batch 593/883, Training Loss: 0.6422
Epoch 5/10, Batch 594/883, Training Loss: 0.8969
Epoch 5/10, Batch 595/883, Training Loss: 0.6716
Epoch 5/10, Batch 596/883, Training Loss: 0.7632
Epoch 5/10, Batch 597/883, Training Loss: 0.7441
Epoch 5/10, Batch 598/883, Training Loss: 0.9574
Epoch 5/10, Batch 599/883, Training Loss: 0.5933
Epoch 5/10, Batch 600/883, Training Loss: 0.8028
Epoch 5/10, Batch 601/883, Training Loss: 0.6103
Epoch 5/10, Batch 602/883, Training Loss: 0.7523
Epoch 5/10, Batch 603/883, Training Loss: 0.9064
Epoch 5/10, Batch 604/883, Training Loss: 0.8030
Epoch 5/10, Batch 605/883, Training Loss: 0.7145
Epoch 5/10, Batch 606/883, Training Loss: 0.7610
Epoch 5/10, Batch 607/883, Training Loss: 0.8305
Epoch 5/10, Batch 608/883, Training Loss: 0.8458
Epoch 5/10, Batch 609/883, Training Loss: 0.7183
Epoch 5/10, Batch 610/883, Training Loss: 0.7656
Epoch 5/10, Batch 611/883, Training Loss: 0.6333
Epoch 5/10, Batch 612/883, Training Loss: 0.6489
Epoch 5/10, Batch 613/883, Training Loss: 0.6995
Epoch 5/10, Batch 614/883, Training Loss: 1.0675
Epoch 5/10, Batch 615/883, Training Loss: 0.8091
Epoch 5/10, Batch 616/883, Training Loss: 1.0734
Epoch 5/10, Batch 617/883, Training Loss: 0.9243
Epoch 5/10, Batch 618/883, Training Loss: 0.7149
Epoch 5/10, Batch 619/883, Training Loss: 0.8405
Epoch 5/10, Batch 620/883, Training Loss: 0.7482
Epoch 5/10, Batch 621/883, Training Loss: 0.5558
Epoch 5/10, Batch 622/883, Training Loss: 0.6439
Epoch 5/10, Batch 623/883, Training Loss: 0.7152
Epoch 5/10, Batch 624/883, Training Loss: 1.0121
Epoch 5/10, Batch 625/883, Training Loss: 0.7043
Epoch 5/10, Batch 626/883, Training Loss: 0.9162
Epoch 5/10, Batch 627/883, Training Loss: 0.8224
Epoch 5/10, Batch 628/883, Training Loss: 0.6368
Epoch 5/10, Batch 629/883, Training Loss: 0.7222
Epoch 5/10, Batch 630/883, Training Loss: 0.5930
Epoch 5/10, Batch 631/883, Training Loss: 0.6915
Epoch 5/10, Batch 632/883, Training Loss: 0.5910
Epoch 5/10, Batch 633/883, Training Loss: 0.7960
Epoch 5/10, Batch 634/883, Training Loss: 1.0810
Epoch 5/10, Batch 635/883, Training Loss: 0.6415
Epoch 5/10, Batch 636/883, Training Loss: 0.6161
Epoch 5/10, Batch 637/883, Training Loss: 0.7658
Epoch 5/10, Batch 638/883, Training Loss: 0.6854
Epoch 5/10, Batch 639/883, Training Loss: 0.5550
Epoch 5/10, Batch 640/883, Training Loss: 0.4457
Epoch 5/10, Batch 641/883, Training Loss: 0.6023
Epoch 5/10, Batch 642/883, Training Loss: 0.8133
Epoch 5/10, Batch 643/883, Training Loss: 0.6098
Epoch 5/10, Batch 644/883, Training Loss: 0.5745
Epoch 5/10, Batch 645/883, Training Loss: 0.6012
Epoch 5/10, Batch 646/883, Training Loss: 0.6244
Epoch 5/10, Batch 647/883, Training Loss: 0.8515
Epoch 5/10, Batch 648/883, Training Loss: 0.8345
Epoch 5/10, Batch 649/883, Training Loss: 0.6066
Epoch 5/10, Batch 650/883, Training Loss: 0.6289
Epoch 5/10, Batch 651/883, Training Loss: 0.6606
Epoch 5/10, Batch 652/883, Training Loss: 0.7570
Epoch 5/10, Batch 653/883, Training Loss: 0.6790
Epoch 5/10, Batch 654/883, Training Loss: 0.9463
Epoch 5/10, Batch 655/883, Training Loss: 0.6740
Epoch 5/10, Batch 656/883, Training Loss: 0.6292
Epoch 5/10, Batch 657/883, Training Loss: 0.7501
Epoch 5/10, Batch 658/883, Training Loss: 0.5016
Epoch 5/10, Batch 659/883, Training Loss: 0.8743
Epoch 5/10, Batch 660/883, Training Loss: 0.5366
Epoch 5/10, Batch 661/883, Training Loss: 0.5502
Epoch 5/10, Batch 662/883, Training Loss: 0.7876
Epoch 5/10, Batch 663/883, Training Loss: 0.7235
Epoch 5/10, Batch 664/883, Training Loss: 0.6927
Epoch 5/10, Batch 665/883, Training Loss: 0.7016
Epoch 5/10, Batch 666/883, Training Loss: 0.6529
Epoch 5/10, Batch 667/883, Training Loss: 0.6671
Epoch 5/10, Batch 668/883, Training Loss: 0.5760
Epoch 5/10, Batch 669/883, Training Loss: 0.7048
Epoch 5/10, Batch 670/883, Training Loss: 0.5377
Epoch 5/10, Batch 671/883, Training Loss: 1.1736
Epoch 5/10, Batch 672/883, Training Loss: 0.7581
Epoch 5/10, Batch 673/883, Training Loss: 0.5423
Epoch 5/10, Batch 674/883, Training Loss: 0.8079
Epoch 5/10, Batch 675/883, Training Loss: 0.7118
Epoch 5/10, Batch 676/883, Training Loss: 0.9450
Epoch 5/10, Batch 677/883, Training Loss: 0.6470
Epoch 5/10, Batch 678/883, Training Loss: 0.6293
Epoch 5/10, Batch 679/883, Training Loss: 0.9290
Epoch 5/10, Batch 680/883, Training Loss: 0.7767
Epoch 5/10, Batch 681/883, Training Loss: 0.4522
Epoch 5/10, Batch 682/883, Training Loss: 0.6233
Epoch 5/10, Batch 683/883, Training Loss: 0.7841
Epoch 5/10, Batch 684/883, Training Loss: 0.6996
Epoch 5/10, Batch 685/883, Training Loss: 0.5505
Epoch 5/10, Batch 686/883, Training Loss: 0.5221
Epoch 5/10, Batch 687/883, Training Loss: 0.8152
Epoch 5/10, Batch 688/883, Training Loss: 0.5909
Epoch 5/10, Batch 689/883, Training Loss: 0.7509
Epoch 5/10, Batch 690/883, Training Loss: 0.6513
Epoch 5/10, Batch 691/883, Training Loss: 0.6950
Epoch 5/10, Batch 692/883, Training Loss: 0.6976
Epoch 5/10, Batch 693/883, Training Loss: 0.5119
Epoch 5/10, Batch 694/883, Training Loss: 0.6165
Epoch 5/10, Batch 695/883, Training Loss: 0.7887
Epoch 5/10, Batch 696/883, Training Loss: 0.6283
Epoch 5/10, Batch 697/883, Training Loss: 1.1854
Epoch 5/10, Batch 698/883, Training Loss: 0.5562
Epoch 5/10, Batch 699/883, Training Loss: 0.8263
Epoch 5/10, Batch 700/883, Training Loss: 0.4979
Epoch 5/10, Batch 701/883, Training Loss: 0.8083
Epoch 5/10, Batch 702/883, Training Loss: 0.5908
Epoch 5/10, Batch 703/883, Training Loss: 0.6390
Epoch 5/10, Batch 704/883, Training Loss: 0.5142
Epoch 5/10, Batch 705/883, Training Loss: 0.6744
Epoch 5/10, Batch 706/883, Training Loss: 0.5185
Epoch 5/10, Batch 707/883, Training Loss: 0.6708
Epoch 5/10, Batch 708/883, Training Loss: 0.6181
Epoch 5/10, Batch 709/883, Training Loss: 0.9005
Epoch 5/10, Batch 710/883, Training Loss: 0.6161
Epoch 5/10, Batch 711/883, Training Loss: 0.6424
Epoch 5/10, Batch 712/883, Training Loss: 0.6416
Epoch 5/10, Batch 713/883, Training Loss: 0.6196
Epoch 5/10, Batch 714/883, Training Loss: 0.6832
Epoch 5/10, Batch 715/883, Training Loss: 0.7548
Epoch 5/10, Batch 716/883, Training Loss: 0.8223
Epoch 5/10, Batch 717/883, Training Loss: 0.8640
Epoch 5/10, Batch 718/883, Training Loss: 0.9501
Epoch 5/10, Batch 719/883, Training Loss: 0.8596
Epoch 5/10, Batch 720/883, Training Loss: 0.7592
Epoch 5/10, Batch 721/883, Training Loss: 0.6183
Epoch 5/10, Batch 722/883, Training Loss: 1.1821
Epoch 5/10, Batch 723/883, Training Loss: 0.8661
Epoch 5/10, Batch 724/883, Training Loss: 0.6512
Epoch 5/10, Batch 725/883, Training Loss: 0.5739
Epoch 5/10, Batch 726/883, Training Loss: 0.5942
Epoch 5/10, Batch 727/883, Training Loss: 0.6104
Epoch 5/10, Batch 728/883, Training Loss: 0.7835
Epoch 5/10, Batch 729/883, Training Loss: 0.5835
Epoch 5/10, Batch 730/883, Training Loss: 0.7426
Epoch 5/10, Batch 731/883, Training Loss: 0.7185
Epoch 5/10, Batch 732/883, Training Loss: 0.6715
Epoch 5/10, Batch 733/883, Training Loss: 0.5593
Epoch 5/10, Batch 734/883, Training Loss: 0.6736
Epoch 5/10, Batch 735/883, Training Loss: 0.4622
Epoch 5/10, Batch 736/883, Training Loss: 0.7525
Epoch 5/10, Batch 737/883, Training Loss: 0.8036
Epoch 5/10, Batch 738/883, Training Loss: 0.5138
Epoch 5/10, Batch 739/883, Training Loss: 1.1127
Epoch 5/10, Batch 740/883, Training Loss: 0.6153
Epoch 5/10, Batch 741/883, Training Loss: 1.1020
Epoch 5/10, Batch 742/883, Training Loss: 0.6280
Epoch 5/10, Batch 743/883, Training Loss: 0.7388
Epoch 5/10, Batch 744/883, Training Loss: 0.9236
Epoch 5/10, Batch 745/883, Training Loss: 0.7116
Epoch 5/10, Batch 746/883, Training Loss: 0.8835
Epoch 5/10, Batch 747/883, Training Loss: 1.0539
Epoch 5/10, Batch 748/883, Training Loss: 0.7834
Epoch 5/10, Batch 749/883, Training Loss: 0.7841
Epoch 5/10, Batch 750/883, Training Loss: 0.8998
Epoch 5/10, Batch 751/883, Training Loss: 0.9049
Epoch 5/10, Batch 752/883, Training Loss: 0.6667
Epoch 5/10, Batch 753/883, Training Loss: 0.6846
Epoch 5/10, Batch 754/883, Training Loss: 0.6591
Epoch 5/10, Batch 755/883, Training Loss: 0.5887
Epoch 5/10, Batch 756/883, Training Loss: 0.6436
Epoch 5/10, Batch 757/883, Training Loss: 0.7877
Epoch 5/10, Batch 758/883, Training Loss: 0.6290
Epoch 5/10, Batch 759/883, Training Loss: 0.7428
Epoch 5/10, Batch 760/883, Training Loss: 0.7197
Epoch 5/10, Batch 761/883, Training Loss: 0.8393
Epoch 5/10, Batch 762/883, Training Loss: 0.8536
Epoch 5/10, Batch 763/883, Training Loss: 0.6394
Epoch 5/10, Batch 764/883, Training Loss: 0.7020
Epoch 5/10, Batch 765/883, Training Loss: 1.1861
Epoch 5/10, Batch 766/883, Training Loss: 0.7577
Epoch 5/10, Batch 767/883, Training Loss: 0.4963
Epoch 5/10, Batch 768/883, Training Loss: 0.5965
Epoch 5/10, Batch 769/883, Training Loss: 0.6018
Epoch 5/10, Batch 770/883, Training Loss: 0.8874
Epoch 5/10, Batch 771/883, Training Loss: 0.7361
Epoch 5/10, Batch 772/883, Training Loss: 0.8874
Epoch 5/10, Batch 773/883, Training Loss: 0.9452
Epoch 5/10, Batch 774/883, Training Loss: 0.8572
Epoch 5/10, Batch 775/883, Training Loss: 0.6722
Epoch 5/10, Batch 776/883, Training Loss: 0.6835
Epoch 5/10, Batch 777/883, Training Loss: 0.6391
Epoch 5/10, Batch 778/883, Training Loss: 1.1407
Epoch 5/10, Batch 779/883, Training Loss: 0.5260
Epoch 5/10, Batch 780/883, Training Loss: 0.6806
Epoch 5/10, Batch 781/883, Training Loss: 0.7324
Epoch 5/10, Batch 782/883, Training Loss: 0.5732
Epoch 5/10, Batch 783/883, Training Loss: 0.6475
Epoch 5/10, Batch 784/883, Training Loss: 0.7016
Epoch 5/10, Batch 785/883, Training Loss: 0.6833
Epoch 5/10, Batch 786/883, Training Loss: 0.7702
Epoch 5/10, Batch 787/883, Training Loss: 0.8026
Epoch 5/10, Batch 788/883, Training Loss: 0.7485
Epoch 5/10, Batch 789/883, Training Loss: 0.6052
Epoch 5/10, Batch 790/883, Training Loss: 0.7654
Epoch 5/10, Batch 791/883, Training Loss: 0.6496
Epoch 5/10, Batch 792/883, Training Loss: 0.5656
Epoch 5/10, Batch 793/883, Training Loss: 0.6730
Epoch 5/10, Batch 794/883, Training Loss: 0.7568
Epoch 5/10, Batch 795/883, Training Loss: 0.7974
Epoch 5/10, Batch 796/883, Training Loss: 0.8528
Epoch 5/10, Batch 797/883, Training Loss: 0.6662
Epoch 5/10, Batch 798/883, Training Loss: 0.6769
Epoch 5/10, Batch 799/883, Training Loss: 0.8227
Epoch 5/10, Batch 800/883, Training Loss: 1.2765
Epoch 5/10, Batch 801/883, Training Loss: 0.6460
Epoch 5/10, Batch 802/883, Training Loss: 0.5021
Epoch 5/10, Batch 803/883, Training Loss: 0.7461
Epoch 5/10, Batch 804/883, Training Loss: 0.6221
Epoch 5/10, Batch 805/883, Training Loss: 0.5873
Epoch 5/10, Batch 806/883, Training Loss: 0.5695
Epoch 5/10, Batch 807/883, Training Loss: 0.5068
Epoch 5/10, Batch 808/883, Training Loss: 0.6473
Epoch 5/10, Batch 809/883, Training Loss: 0.8109
Epoch 5/10, Batch 810/883, Training Loss: 0.6671
Epoch 5/10, Batch 811/883, Training Loss: 0.4450
Epoch 5/10, Batch 812/883, Training Loss: 0.7343
Epoch 5/10, Batch 813/883, Training Loss: 0.9026
Epoch 5/10, Batch 814/883, Training Loss: 0.6634
Epoch 5/10, Batch 815/883, Training Loss: 0.7048
Epoch 5/10, Batch 816/883, Training Loss: 0.9580
Epoch 5/10, Batch 817/883, Training Loss: 0.5545
Epoch 5/10, Batch 818/883, Training Loss: 0.7795
Epoch 5/10, Batch 819/883, Training Loss: 0.6218
Epoch 5/10, Batch 820/883, Training Loss: 0.8903
Epoch 5/10, Batch 821/883, Training Loss: 1.3670
Epoch 5/10, Batch 822/883, Training Loss: 0.7444
Epoch 5/10, Batch 823/883, Training Loss: 0.7855
Epoch 5/10, Batch 824/883, Training Loss: 0.6951
Epoch 5/10, Batch 825/883, Training Loss: 0.6032
Epoch 5/10, Batch 826/883, Training Loss: 0.4937
Epoch 5/10, Batch 827/883, Training Loss: 0.6106
Epoch 5/10, Batch 828/883, Training Loss: 0.6329
Epoch 5/10, Batch 829/883, Training Loss: 0.6102
Epoch 5/10, Batch 830/883, Training Loss: 0.7457
Epoch 5/10, Batch 831/883, Training Loss: 0.9820
Epoch 5/10, Batch 832/883, Training Loss: 0.6111
Epoch 5/10, Batch 833/883, Training Loss: 0.6684
Epoch 5/10, Batch 834/883, Training Loss: 0.9700
Epoch 5/10, Batch 835/883, Training Loss: 0.5396
Epoch 5/10, Batch 836/883, Training Loss: 0.6251
Epoch 5/10, Batch 837/883, Training Loss: 0.6913
Epoch 5/10, Batch 838/883, Training Loss: 0.6148
Epoch 5/10, Batch 839/883, Training Loss: 0.6553
Epoch 5/10, Batch 840/883, Training Loss: 0.9137
Epoch 5/10, Batch 841/883, Training Loss: 0.5763
Epoch 5/10, Batch 842/883, Training Loss: 0.7486
Epoch 5/10, Batch 843/883, Training Loss: 0.5081
Epoch 5/10, Batch 844/883, Training Loss: 0.8402
Epoch 5/10, Batch 845/883, Training Loss: 0.8583
Epoch 5/10, Batch 846/883, Training Loss: 0.6794
Epoch 5/10, Batch 847/883, Training Loss: 0.9621
Epoch 5/10, Batch 848/883, Training Loss: 0.7033
Epoch 5/10, Batch 849/883, Training Loss: 0.8430
Epoch 5/10, Batch 850/883, Training Loss: 0.7659
Epoch 5/10, Batch 851/883, Training Loss: 0.6730
Epoch 5/10, Batch 852/883, Training Loss: 0.6079
Epoch 5/10, Batch 853/883, Training Loss: 1.4037
Epoch 5/10, Batch 854/883, Training Loss: 0.5262
Epoch 5/10, Batch 855/883, Training Loss: 0.7755
Epoch 5/10, Batch 856/883, Training Loss: 0.8843
Epoch 5/10, Batch 857/883, Training Loss: 0.6351
Epoch 5/10, Batch 858/883, Training Loss: 0.7632
Epoch 5/10, Batch 859/883, Training Loss: 0.5749
Epoch 5/10, Batch 860/883, Training Loss: 0.4800
Epoch 5/10, Batch 861/883, Training Loss: 0.7096
Epoch 5/10, Batch 862/883, Training Loss: 0.5192
Epoch 5/10, Batch 863/883, Training Loss: 0.7490
Epoch 5/10, Batch 864/883, Training Loss: 0.6009
Epoch 5/10, Batch 865/883, Training Loss: 0.8748
Epoch 5/10, Batch 866/883, Training Loss: 0.9232
Epoch 5/10, Batch 867/883, Training Loss: 0.6366
Epoch 5/10, Batch 868/883, Training Loss: 0.4725
Epoch 5/10, Batch 869/883, Training Loss: 0.8110
Epoch 5/10, Batch 870/883, Training Loss: 0.7929
Epoch 5/10, Batch 871/883, Training Loss: 0.8746
Epoch 5/10, Batch 872/883, Training Loss: 1.0163
Epoch 5/10, Batch 873/883, Training Loss: 0.7192
Epoch 5/10, Batch 874/883, Training Loss: 0.6190
Epoch 5/10, Batch 875/883, Training Loss: 0.6098
Epoch 5/10, Batch 876/883, Training Loss: 0.6892
Epoch 5/10, Batch 877/883, Training Loss: 0.6927
Epoch 5/10, Batch 878/883, Training Loss: 0.5679
Epoch 5/10, Batch 879/883, Training Loss: 0.8942
Epoch 5/10, Batch 880/883, Training Loss: 0.5123
Epoch 5/10, Batch 881/883, Training Loss: 0.9601
Epoch 5/10, Batch 882/883, Training Loss: 0.7215
Epoch 5/10, Batch 883/883, Training Loss: 0.6036
Epoch 5/10, Training Loss: 0.7341, Validation Loss: 0.6919, Validation Accuracy: 0.6838
Epoch 6/10, Batch 1/883, Training Loss: 0.5606
Epoch 6/10, Batch 2/883, Training Loss: 0.8599
Epoch 6/10, Batch 3/883, Training Loss: 0.5011
Epoch 6/10, Batch 4/883, Training Loss: 0.5468
Epoch 6/10, Batch 5/883, Training Loss: 0.5315
Epoch 6/10, Batch 6/883, Training Loss: 0.5834
Epoch 6/10, Batch 7/883, Training Loss: 1.1506
Epoch 6/10, Batch 8/883, Training Loss: 0.7417
Epoch 6/10, Batch 9/883, Training Loss: 0.6686
Epoch 6/10, Batch 10/883, Training Loss: 0.4419
Epoch 6/10, Batch 11/883, Training Loss: 0.6529
Epoch 6/10, Batch 12/883, Training Loss: 0.8092
Epoch 6/10, Batch 13/883, Training Loss: 0.9117
Epoch 6/10, Batch 14/883, Training Loss: 0.7162
Epoch 6/10, Batch 15/883, Training Loss: 0.7569
Epoch 6/10, Batch 16/883, Training Loss: 0.7450
Epoch 6/10, Batch 17/883, Training Loss: 1.4737
Epoch 6/10, Batch 18/883, Training Loss: 0.9983
Epoch 6/10, Batch 19/883, Training Loss: 0.8541
Epoch 6/10, Batch 20/883, Training Loss: 1.0990
Epoch 6/10, Batch 21/883, Training Loss: 0.6237
Epoch 6/10, Batch 22/883, Training Loss: 0.7334
Epoch 6/10, Batch 23/883, Training Loss: 1.1591
Epoch 6/10, Batch 24/883, Training Loss: 0.6359
Epoch 6/10, Batch 25/883, Training Loss: 0.6881
Epoch 6/10, Batch 26/883, Training Loss: 0.5908
Epoch 6/10, Batch 27/883, Training Loss: 0.5371
Epoch 6/10, Batch 28/883, Training Loss: 0.8820
Epoch 6/10, Batch 29/883, Training Loss: 0.5924
Epoch 6/10, Batch 30/883, Training Loss: 0.6759
Epoch 6/10, Batch 31/883, Training Loss: 0.6517
Epoch 6/10, Batch 32/883, Training Loss: 0.6138
Epoch 6/10, Batch 33/883, Training Loss: 0.6634
Epoch 6/10, Batch 34/883, Training Loss: 0.7444
Epoch 6/10, Batch 35/883, Training Loss: 0.4187
Epoch 6/10, Batch 36/883, Training Loss: 0.5326
Epoch 6/10, Batch 37/883, Training Loss: 0.5441
Epoch 6/10, Batch 38/883, Training Loss: 0.8549
Epoch 6/10, Batch 39/883, Training Loss: 0.6351
Epoch 6/10, Batch 40/883, Training Loss: 0.6019
Epoch 6/10, Batch 41/883, Training Loss: 0.6119
Epoch 6/10, Batch 42/883, Training Loss: 0.6112
Epoch 6/10, Batch 43/883, Training Loss: 0.7353
Epoch 6/10, Batch 44/883, Training Loss: 0.4661
Epoch 6/10, Batch 45/883, Training Loss: 0.5187
Epoch 6/10, Batch 46/883, Training Loss: 1.1452
Epoch 6/10, Batch 47/883, Training Loss: 0.9157
Epoch 6/10, Batch 48/883, Training Loss: 0.6919
Epoch 6/10, Batch 49/883, Training Loss: 0.6812
Epoch 6/10, Batch 50/883, Training Loss: 0.6119
Epoch 6/10, Batch 51/883, Training Loss: 0.8283
Epoch 6/10, Batch 52/883, Training Loss: 0.8777
Epoch 6/10, Batch 53/883, Training Loss: 0.8506
Epoch 6/10, Batch 54/883, Training Loss: 0.4895
Epoch 6/10, Batch 55/883, Training Loss: 0.6943
Epoch 6/10, Batch 56/883, Training Loss: 0.9114
Epoch 6/10, Batch 57/883, Training Loss: 0.3868
Epoch 6/10, Batch 58/883, Training Loss: 0.5181
Epoch 6/10, Batch 59/883, Training Loss: 0.6292
Epoch 6/10, Batch 60/883, Training Loss: 0.8043
Epoch 6/10, Batch 61/883, Training Loss: 0.4202
Epoch 6/10, Batch 62/883, Training Loss: 0.6529
Epoch 6/10, Batch 63/883, Training Loss: 0.8326
Epoch 6/10, Batch 64/883, Training Loss: 0.5514
Epoch 6/10, Batch 65/883, Training Loss: 0.6877
Epoch 6/10, Batch 66/883, Training Loss: 0.8571
Epoch 6/10, Batch 67/883, Training Loss: 0.4781
Epoch 6/10, Batch 68/883, Training Loss: 0.3993
Epoch 6/10, Batch 69/883, Training Loss: 0.7611
Epoch 6/10, Batch 70/883, Training Loss: 0.6459
Epoch 6/10, Batch 71/883, Training Loss: 0.7450
Epoch 6/10, Batch 72/883, Training Loss: 0.5054
Epoch 6/10, Batch 73/883, Training Loss: 0.7303
Epoch 6/10, Batch 74/883, Training Loss: 0.6878
Epoch 6/10, Batch 75/883, Training Loss: 0.3612
Epoch 6/10, Batch 76/883, Training Loss: 0.7978
Epoch 6/10, Batch 77/883, Training Loss: 0.9768
Epoch 6/10, Batch 78/883, Training Loss: 0.6173
Epoch 6/10, Batch 79/883, Training Loss: 0.5918
Epoch 6/10, Batch 80/883, Training Loss: 0.5412
Epoch 6/10, Batch 81/883, Training Loss: 0.7169
Epoch 6/10, Batch 82/883, Training Loss: 0.6834
Epoch 6/10, Batch 83/883, Training Loss: 0.8783
Epoch 6/10, Batch 84/883, Training Loss: 0.8288
Epoch 6/10, Batch 85/883, Training Loss: 0.6810
Epoch 6/10, Batch 86/883, Training Loss: 0.8685
Epoch 6/10, Batch 87/883, Training Loss: 0.4877
Epoch 6/10, Batch 88/883, Training Loss: 0.9010
Epoch 6/10, Batch 89/883, Training Loss: 0.6675
Epoch 6/10, Batch 90/883, Training Loss: 1.0485
Epoch 6/10, Batch 91/883, Training Loss: 0.7381
Epoch 6/10, Batch 92/883, Training Loss: 1.1780
Epoch 6/10, Batch 93/883, Training Loss: 0.7718
Epoch 6/10, Batch 94/883, Training Loss: 0.9352
Epoch 6/10, Batch 95/883, Training Loss: 0.7936
Epoch 6/10, Batch 96/883, Training Loss: 0.7602
Epoch 6/10, Batch 97/883, Training Loss: 0.5845
Epoch 6/10, Batch 98/883, Training Loss: 0.6560
Epoch 6/10, Batch 99/883, Training Loss: 0.7779
Epoch 6/10, Batch 100/883, Training Loss: 0.5742
Epoch 6/10, Batch 101/883, Training Loss: 0.5973
Epoch 6/10, Batch 102/883, Training Loss: 0.5417
Epoch 6/10, Batch 103/883, Training Loss: 0.7390
Epoch 6/10, Batch 104/883, Training Loss: 0.7794
Epoch 6/10, Batch 105/883, Training Loss: 0.8844
Epoch 6/10, Batch 106/883, Training Loss: 0.4526
Epoch 6/10, Batch 107/883, Training Loss: 0.5866
Epoch 6/10, Batch 108/883, Training Loss: 0.6210
Epoch 6/10, Batch 109/883, Training Loss: 0.5275
Epoch 6/10, Batch 110/883, Training Loss: 0.9381
Epoch 6/10, Batch 111/883, Training Loss: 0.5629
Epoch 6/10, Batch 112/883, Training Loss: 0.6071
Epoch 6/10, Batch 113/883, Training Loss: 0.5158
Epoch 6/10, Batch 114/883, Training Loss: 0.4789
Epoch 6/10, Batch 115/883, Training Loss: 0.6940
Epoch 6/10, Batch 116/883, Training Loss: 0.8503
Epoch 6/10, Batch 117/883, Training Loss: 0.5127
Epoch 6/10, Batch 118/883, Training Loss: 0.6302
Epoch 6/10, Batch 119/883, Training Loss: 0.9078
Epoch 6/10, Batch 120/883, Training Loss: 0.6139
Epoch 6/10, Batch 121/883, Training Loss: 0.5454
Epoch 6/10, Batch 122/883, Training Loss: 0.5826
Epoch 6/10, Batch 123/883, Training Loss: 0.5910
Epoch 6/10, Batch 124/883, Training Loss: 0.8204
Epoch 6/10, Batch 125/883, Training Loss: 0.4917
Epoch 6/10, Batch 126/883, Training Loss: 0.5107
Epoch 6/10, Batch 127/883, Training Loss: 0.6544
Epoch 6/10, Batch 128/883, Training Loss: 0.9638
Epoch 6/10, Batch 129/883, Training Loss: 0.9687
Epoch 6/10, Batch 130/883, Training Loss: 0.6051
Epoch 6/10, Batch 131/883, Training Loss: 0.7300
Epoch 6/10, Batch 132/883, Training Loss: 0.6070
Epoch 6/10, Batch 133/883, Training Loss: 0.5722
Epoch 6/10, Batch 134/883, Training Loss: 0.4617
Epoch 6/10, Batch 135/883, Training Loss: 0.8073
Epoch 6/10, Batch 136/883, Training Loss: 0.4786
Epoch 6/10, Batch 137/883, Training Loss: 0.7194
Epoch 6/10, Batch 138/883, Training Loss: 0.9814
Epoch 6/10, Batch 139/883, Training Loss: 0.6727
Epoch 6/10, Batch 140/883, Training Loss: 0.6742
Epoch 6/10, Batch 141/883, Training Loss: 0.7045
Epoch 6/10, Batch 142/883, Training Loss: 0.6332
Epoch 6/10, Batch 143/883, Training Loss: 0.7122
Epoch 6/10, Batch 144/883, Training Loss: 1.1117
Epoch 6/10, Batch 145/883, Training Loss: 0.6426
Epoch 6/10, Batch 146/883, Training Loss: 0.8953
Epoch 6/10, Batch 147/883, Training Loss: 0.6275
Epoch 6/10, Batch 148/883, Training Loss: 0.7070
Epoch 6/10, Batch 149/883, Training Loss: 0.5881
Epoch 6/10, Batch 150/883, Training Loss: 1.0310
Epoch 6/10, Batch 151/883, Training Loss: 0.6270
Epoch 6/10, Batch 152/883, Training Loss: 0.4356
Epoch 6/10, Batch 153/883, Training Loss: 0.5233
Epoch 6/10, Batch 154/883, Training Loss: 0.6283
Epoch 6/10, Batch 155/883, Training Loss: 0.7464
Epoch 6/10, Batch 156/883, Training Loss: 0.6203
Epoch 6/10, Batch 157/883, Training Loss: 0.8550
Epoch 6/10, Batch 158/883, Training Loss: 0.9620
Epoch 6/10, Batch 159/883, Training Loss: 0.5324
Epoch 6/10, Batch 160/883, Training Loss: 0.6840
Epoch 6/10, Batch 161/883, Training Loss: 1.0507
Epoch 6/10, Batch 162/883, Training Loss: 0.5408
Epoch 6/10, Batch 163/883, Training Loss: 0.5580
Epoch 6/10, Batch 164/883, Training Loss: 0.5457
Epoch 6/10, Batch 165/883, Training Loss: 0.8346
Epoch 6/10, Batch 166/883, Training Loss: 0.7500
Epoch 6/10, Batch 167/883, Training Loss: 0.7075
Epoch 6/10, Batch 168/883, Training Loss: 0.4953
Epoch 6/10, Batch 169/883, Training Loss: 0.8894
Epoch 6/10, Batch 170/883, Training Loss: 0.6227
Epoch 6/10, Batch 171/883, Training Loss: 0.5676
Epoch 6/10, Batch 172/883, Training Loss: 0.8690
Epoch 6/10, Batch 173/883, Training Loss: 0.7943
Epoch 6/10, Batch 174/883, Training Loss: 0.6484
Epoch 6/10, Batch 175/883, Training Loss: 0.7883
Epoch 6/10, Batch 176/883, Training Loss: 0.6993
Epoch 6/10, Batch 177/883, Training Loss: 0.6528
Epoch 6/10, Batch 178/883, Training Loss: 1.0017
Epoch 6/10, Batch 179/883, Training Loss: 0.7813
Epoch 6/10, Batch 180/883, Training Loss: 0.5019
Epoch 6/10, Batch 181/883, Training Loss: 0.5279
Epoch 6/10, Batch 182/883, Training Loss: 0.9642
Epoch 6/10, Batch 183/883, Training Loss: 0.8146
Epoch 6/10, Batch 184/883, Training Loss: 0.6379
Epoch 6/10, Batch 185/883, Training Loss: 0.9674
Epoch 6/10, Batch 186/883, Training Loss: 0.8289
Epoch 6/10, Batch 187/883, Training Loss: 0.7662
Epoch 6/10, Batch 188/883, Training Loss: 0.7551
Epoch 6/10, Batch 189/883, Training Loss: 0.9516
Epoch 6/10, Batch 190/883, Training Loss: 0.6414
Epoch 6/10, Batch 191/883, Training Loss: 0.6607
Epoch 6/10, Batch 192/883, Training Loss: 0.7066
Epoch 6/10, Batch 193/883, Training Loss: 0.6373
Epoch 6/10, Batch 194/883, Training Loss: 0.5326
Epoch 6/10, Batch 195/883, Training Loss: 0.8818
Epoch 6/10, Batch 196/883, Training Loss: 0.4799
Epoch 6/10, Batch 197/883, Training Loss: 0.6172
Epoch 6/10, Batch 198/883, Training Loss: 0.5444
Epoch 6/10, Batch 199/883, Training Loss: 0.4283
Epoch 6/10, Batch 200/883, Training Loss: 0.7343
Epoch 6/10, Batch 201/883, Training Loss: 0.5824
Epoch 6/10, Batch 202/883, Training Loss: 0.6404
Epoch 6/10, Batch 203/883, Training Loss: 0.5595
Epoch 6/10, Batch 204/883, Training Loss: 0.8950
Epoch 6/10, Batch 205/883, Training Loss: 0.8787
Epoch 6/10, Batch 206/883, Training Loss: 0.4744
Epoch 6/10, Batch 207/883, Training Loss: 0.7307
Epoch 6/10, Batch 208/883, Training Loss: 0.6718
Epoch 6/10, Batch 209/883, Training Loss: 0.6351
Epoch 6/10, Batch 210/883, Training Loss: 0.6312
Epoch 6/10, Batch 211/883, Training Loss: 0.9111
Epoch 6/10, Batch 212/883, Training Loss: 0.5885
Epoch 6/10, Batch 213/883, Training Loss: 0.5251
Epoch 6/10, Batch 214/883, Training Loss: 0.8400
Epoch 6/10, Batch 215/883, Training Loss: 0.8065
Epoch 6/10, Batch 216/883, Training Loss: 0.5492
Epoch 6/10, Batch 217/883, Training Loss: 0.6676
Epoch 6/10, Batch 218/883, Training Loss: 0.5764
Epoch 6/10, Batch 219/883, Training Loss: 1.0289
Epoch 6/10, Batch 220/883, Training Loss: 0.7747
Epoch 6/10, Batch 221/883, Training Loss: 0.5013
Epoch 6/10, Batch 222/883, Training Loss: 0.6027
Epoch 6/10, Batch 223/883, Training Loss: 0.4831
Epoch 6/10, Batch 224/883, Training Loss: 0.8207
Epoch 6/10, Batch 225/883, Training Loss: 0.4706
Epoch 6/10, Batch 226/883, Training Loss: 0.8543
Epoch 6/10, Batch 227/883, Training Loss: 0.6859
Epoch 6/10, Batch 228/883, Training Loss: 0.6303
Epoch 6/10, Batch 229/883, Training Loss: 0.8958
Epoch 6/10, Batch 230/883, Training Loss: 0.3979
Epoch 6/10, Batch 231/883, Training Loss: 0.5685
Epoch 6/10, Batch 232/883, Training Loss: 0.5312
Epoch 6/10, Batch 233/883, Training Loss: 0.8691
Epoch 6/10, Batch 234/883, Training Loss: 0.7385
Epoch 6/10, Batch 235/883, Training Loss: 0.3970
Epoch 6/10, Batch 236/883, Training Loss: 0.6552
Epoch 6/10, Batch 237/883, Training Loss: 1.0326
Epoch 6/10, Batch 238/883, Training Loss: 0.4494
Epoch 6/10, Batch 239/883, Training Loss: 0.5910
Epoch 6/10, Batch 240/883, Training Loss: 0.5415
Epoch 6/10, Batch 241/883, Training Loss: 0.6055
Epoch 6/10, Batch 242/883, Training Loss: 0.5034
Epoch 6/10, Batch 243/883, Training Loss: 0.6459
Epoch 6/10, Batch 244/883, Training Loss: 0.5584
Epoch 6/10, Batch 245/883, Training Loss: 1.1711
Epoch 6/10, Batch 246/883, Training Loss: 0.4340
Epoch 6/10, Batch 247/883, Training Loss: 0.8867
Epoch 6/10, Batch 248/883, Training Loss: 0.6988
Epoch 6/10, Batch 249/883, Training Loss: 0.7481
Epoch 6/10, Batch 250/883, Training Loss: 0.4678
Epoch 6/10, Batch 251/883, Training Loss: 0.4727
Epoch 6/10, Batch 252/883, Training Loss: 0.5605
Epoch 6/10, Batch 253/883, Training Loss: 0.7033
Epoch 6/10, Batch 254/883, Training Loss: 0.8659
Epoch 6/10, Batch 255/883, Training Loss: 0.5150
Epoch 6/10, Batch 256/883, Training Loss: 0.7232
Epoch 6/10, Batch 257/883, Training Loss: 0.8743
Epoch 6/10, Batch 258/883, Training Loss: 0.9382
Epoch 6/10, Batch 259/883, Training Loss: 0.5785
Epoch 6/10, Batch 260/883, Training Loss: 0.8415
Epoch 6/10, Batch 261/883, Training Loss: 0.5762
Epoch 6/10, Batch 262/883, Training Loss: 0.7156
Epoch 6/10, Batch 263/883, Training Loss: 0.6396
Epoch 6/10, Batch 264/883, Training Loss: 0.7865
Epoch 6/10, Batch 265/883, Training Loss: 0.6288
Epoch 6/10, Batch 266/883, Training Loss: 0.8648
Epoch 6/10, Batch 267/883, Training Loss: 0.7424
Epoch 6/10, Batch 268/883, Training Loss: 0.7482
Epoch 6/10, Batch 269/883, Training Loss: 0.5364
Epoch 6/10, Batch 270/883, Training Loss: 0.7894
Epoch 6/10, Batch 271/883, Training Loss: 0.5312
Epoch 6/10, Batch 272/883, Training Loss: 0.4629
Epoch 6/10, Batch 273/883, Training Loss: 0.7528
Epoch 6/10, Batch 274/883, Training Loss: 0.4199
Epoch 6/10, Batch 275/883, Training Loss: 0.6294
Epoch 6/10, Batch 276/883, Training Loss: 0.6444
Epoch 6/10, Batch 277/883, Training Loss: 0.6980
Epoch 6/10, Batch 278/883, Training Loss: 0.6333
Epoch 6/10, Batch 279/883, Training Loss: 0.7073
Epoch 6/10, Batch 280/883, Training Loss: 1.0601
Epoch 6/10, Batch 281/883, Training Loss: 0.8810
Epoch 6/10, Batch 282/883, Training Loss: 0.4111
Epoch 6/10, Batch 283/883, Training Loss: 0.6833
Epoch 6/10, Batch 284/883, Training Loss: 0.9028
Epoch 6/10, Batch 285/883, Training Loss: 0.5261
Epoch 6/10, Batch 286/883, Training Loss: 0.4867
Epoch 6/10, Batch 287/883, Training Loss: 0.4059
Epoch 6/10, Batch 288/883, Training Loss: 0.8726
Epoch 6/10, Batch 289/883, Training Loss: 0.8288
Epoch 6/10, Batch 290/883, Training Loss: 0.5170
Epoch 6/10, Batch 291/883, Training Loss: 0.8092
Epoch 6/10, Batch 292/883, Training Loss: 0.6924
Epoch 6/10, Batch 293/883, Training Loss: 0.8726
Epoch 6/10, Batch 294/883, Training Loss: 0.6119
Epoch 6/10, Batch 295/883, Training Loss: 0.7082
Epoch 6/10, Batch 296/883, Training Loss: 0.7643
Epoch 6/10, Batch 297/883, Training Loss: 0.7441
Epoch 6/10, Batch 298/883, Training Loss: 0.5949
Epoch 6/10, Batch 299/883, Training Loss: 0.5661
Epoch 6/10, Batch 300/883, Training Loss: 0.4938
Epoch 6/10, Batch 301/883, Training Loss: 0.6122
Epoch 6/10, Batch 302/883, Training Loss: 0.6864
Epoch 6/10, Batch 303/883, Training Loss: 0.9769
Epoch 6/10, Batch 304/883, Training Loss: 0.6562
Epoch 6/10, Batch 305/883, Training Loss: 0.9051
Epoch 6/10, Batch 306/883, Training Loss: 0.5091
Epoch 6/10, Batch 307/883, Training Loss: 0.7763
Epoch 6/10, Batch 308/883, Training Loss: 0.6273
Epoch 6/10, Batch 309/883, Training Loss: 0.8075
Epoch 6/10, Batch 310/883, Training Loss: 0.6139
Epoch 6/10, Batch 311/883, Training Loss: 0.9094
Epoch 6/10, Batch 312/883, Training Loss: 0.6346
Epoch 6/10, Batch 313/883, Training Loss: 1.0796
Epoch 6/10, Batch 314/883, Training Loss: 0.8107
Epoch 6/10, Batch 315/883, Training Loss: 0.5066
Epoch 6/10, Batch 316/883, Training Loss: 0.8439
Epoch 6/10, Batch 317/883, Training Loss: 0.6927
Epoch 6/10, Batch 318/883, Training Loss: 0.6727
Epoch 6/10, Batch 319/883, Training Loss: 0.7311
Epoch 6/10, Batch 320/883, Training Loss: 0.6782
Epoch 6/10, Batch 321/883, Training Loss: 0.5708
Epoch 6/10, Batch 322/883, Training Loss: 0.8587
Epoch 6/10, Batch 323/883, Training Loss: 0.5432
Epoch 6/10, Batch 324/883, Training Loss: 0.7520
Epoch 6/10, Batch 325/883, Training Loss: 0.7202
Epoch 6/10, Batch 326/883, Training Loss: 0.7660
Epoch 6/10, Batch 327/883, Training Loss: 0.5587
Epoch 6/10, Batch 328/883, Training Loss: 0.6853
Epoch 6/10, Batch 329/883, Training Loss: 0.5988
Epoch 6/10, Batch 330/883, Training Loss: 0.5551
Epoch 6/10, Batch 331/883, Training Loss: 0.4682
Epoch 6/10, Batch 332/883, Training Loss: 0.8878
Epoch 6/10, Batch 333/883, Training Loss: 0.8406
Epoch 6/10, Batch 334/883, Training Loss: 0.8389
Epoch 6/10, Batch 335/883, Training Loss: 0.5401
Epoch 6/10, Batch 336/883, Training Loss: 0.4358
Epoch 6/10, Batch 337/883, Training Loss: 0.7412
Epoch 6/10, Batch 338/883, Training Loss: 0.5962
Epoch 6/10, Batch 339/883, Training Loss: 0.6495
Epoch 6/10, Batch 340/883, Training Loss: 0.5850
Epoch 6/10, Batch 341/883, Training Loss: 0.4475
Epoch 6/10, Batch 342/883, Training Loss: 0.5491
Epoch 6/10, Batch 343/883, Training Loss: 0.8814
Epoch 6/10, Batch 344/883, Training Loss: 0.6864
Epoch 6/10, Batch 345/883, Training Loss: 0.5899
Epoch 6/10, Batch 346/883, Training Loss: 0.6400
Epoch 6/10, Batch 347/883, Training Loss: 0.6216
Epoch 6/10, Batch 348/883, Training Loss: 0.6052
Epoch 6/10, Batch 349/883, Training Loss: 0.6750
Epoch 6/10, Batch 350/883, Training Loss: 0.5395
Epoch 6/10, Batch 351/883, Training Loss: 0.7677
Epoch 6/10, Batch 352/883, Training Loss: 0.7499
Epoch 6/10, Batch 353/883, Training Loss: 0.5673
Epoch 6/10, Batch 354/883, Training Loss: 0.6480
Epoch 6/10, Batch 355/883, Training Loss: 0.6751
Epoch 6/10, Batch 356/883, Training Loss: 0.6745
Epoch 6/10, Batch 357/883, Training Loss: 0.6833
Epoch 6/10, Batch 358/883, Training Loss: 0.7383
Epoch 6/10, Batch 359/883, Training Loss: 0.6600
Epoch 6/10, Batch 360/883, Training Loss: 0.7329
Epoch 6/10, Batch 361/883, Training Loss: 0.5351
Epoch 6/10, Batch 362/883, Training Loss: 0.6395
Epoch 6/10, Batch 363/883, Training Loss: 1.3124
Epoch 6/10, Batch 364/883, Training Loss: 0.9071
Epoch 6/10, Batch 365/883, Training Loss: 0.6309
Epoch 6/10, Batch 366/883, Training Loss: 0.4269
Epoch 6/10, Batch 367/883, Training Loss: 0.6273
Epoch 6/10, Batch 368/883, Training Loss: 0.5863
Epoch 6/10, Batch 369/883, Training Loss: 0.7562
Epoch 6/10, Batch 370/883, Training Loss: 1.1803
Epoch 6/10, Batch 371/883, Training Loss: 0.6171
Epoch 6/10, Batch 372/883, Training Loss: 0.6561
Epoch 6/10, Batch 373/883, Training Loss: 0.8485
Epoch 6/10, Batch 374/883, Training Loss: 0.8591
Epoch 6/10, Batch 375/883, Training Loss: 0.4917
Epoch 6/10, Batch 376/883, Training Loss: 0.5413
Epoch 6/10, Batch 377/883, Training Loss: 0.5688
Epoch 6/10, Batch 378/883, Training Loss: 0.4531
Epoch 6/10, Batch 379/883, Training Loss: 0.9832
Epoch 6/10, Batch 380/883, Training Loss: 0.5768
Epoch 6/10, Batch 381/883, Training Loss: 0.6339
Epoch 6/10, Batch 382/883, Training Loss: 0.6890
Epoch 6/10, Batch 383/883, Training Loss: 0.7433
Epoch 6/10, Batch 384/883, Training Loss: 0.8819
Epoch 6/10, Batch 385/883, Training Loss: 0.7376
Epoch 6/10, Batch 386/883, Training Loss: 0.6772
Epoch 6/10, Batch 387/883, Training Loss: 1.0563
Epoch 6/10, Batch 388/883, Training Loss: 0.8428
Epoch 6/10, Batch 389/883, Training Loss: 0.6126
Epoch 6/10, Batch 390/883, Training Loss: 0.7231
Epoch 6/10, Batch 391/883, Training Loss: 0.8198
Epoch 6/10, Batch 392/883, Training Loss: 0.5662
Epoch 6/10, Batch 393/883, Training Loss: 0.5400
Epoch 6/10, Batch 394/883, Training Loss: 0.7194
Epoch 6/10, Batch 395/883, Training Loss: 0.5838
Epoch 6/10, Batch 396/883, Training Loss: 0.7626
Epoch 6/10, Batch 397/883, Training Loss: 0.5344
Epoch 6/10, Batch 398/883, Training Loss: 0.5241
Epoch 6/10, Batch 399/883, Training Loss: 0.5981
Epoch 6/10, Batch 400/883, Training Loss: 0.9780
Epoch 6/10, Batch 401/883, Training Loss: 0.7092
Epoch 6/10, Batch 402/883, Training Loss: 0.6306
Epoch 6/10, Batch 403/883, Training Loss: 0.4596
Epoch 6/10, Batch 404/883, Training Loss: 0.8922
Epoch 6/10, Batch 405/883, Training Loss: 0.4956
Epoch 6/10, Batch 406/883, Training Loss: 0.8888
Epoch 6/10, Batch 407/883, Training Loss: 0.8302
Epoch 6/10, Batch 408/883, Training Loss: 0.6211
Epoch 6/10, Batch 409/883, Training Loss: 0.7744
Epoch 6/10, Batch 410/883, Training Loss: 0.7203
Epoch 6/10, Batch 411/883, Training Loss: 0.6582
Epoch 6/10, Batch 412/883, Training Loss: 0.9131
Epoch 6/10, Batch 413/883, Training Loss: 0.6564
Epoch 6/10, Batch 414/883, Training Loss: 0.5114
Epoch 6/10, Batch 415/883, Training Loss: 0.7859
Epoch 6/10, Batch 416/883, Training Loss: 0.6426
Epoch 6/10, Batch 417/883, Training Loss: 0.7920
Epoch 6/10, Batch 418/883, Training Loss: 0.7461
Epoch 6/10, Batch 419/883, Training Loss: 0.7779
Epoch 6/10, Batch 420/883, Training Loss: 0.7547
Epoch 6/10, Batch 421/883, Training Loss: 0.5991
Epoch 6/10, Batch 422/883, Training Loss: 0.6762
Epoch 6/10, Batch 423/883, Training Loss: 0.7294
Epoch 6/10, Batch 424/883, Training Loss: 0.6694
Epoch 6/10, Batch 425/883, Training Loss: 0.5047
Epoch 6/10, Batch 426/883, Training Loss: 0.4187
Epoch 6/10, Batch 427/883, Training Loss: 0.6401
Epoch 6/10, Batch 428/883, Training Loss: 1.1001
Epoch 6/10, Batch 429/883, Training Loss: 0.5494
Epoch 6/10, Batch 430/883, Training Loss: 0.5426
Epoch 6/10, Batch 431/883, Training Loss: 0.5244
Epoch 6/10, Batch 432/883, Training Loss: 0.7008
Epoch 6/10, Batch 433/883, Training Loss: 0.7854
Epoch 6/10, Batch 434/883, Training Loss: 0.5374
Epoch 6/10, Batch 435/883, Training Loss: 0.6234
Epoch 6/10, Batch 436/883, Training Loss: 1.1143
Epoch 6/10, Batch 437/883, Training Loss: 0.5502
Epoch 6/10, Batch 438/883, Training Loss: 1.4413
Epoch 6/10, Batch 439/883, Training Loss: 0.8574
Epoch 6/10, Batch 440/883, Training Loss: 0.8305
Epoch 6/10, Batch 441/883, Training Loss: 1.1084
Epoch 6/10, Batch 442/883, Training Loss: 0.5129
Epoch 6/10, Batch 443/883, Training Loss: 0.6480
Epoch 6/10, Batch 444/883, Training Loss: 0.7170
Epoch 6/10, Batch 445/883, Training Loss: 0.9349
Epoch 6/10, Batch 446/883, Training Loss: 1.2448
Epoch 6/10, Batch 447/883, Training Loss: 0.5331
Epoch 6/10, Batch 448/883, Training Loss: 0.7597
Epoch 6/10, Batch 449/883, Training Loss: 0.6917
Epoch 6/10, Batch 450/883, Training Loss: 0.6117
Epoch 6/10, Batch 451/883, Training Loss: 0.7509
Epoch 6/10, Batch 452/883, Training Loss: 0.6520
Epoch 6/10, Batch 453/883, Training Loss: 0.5851
Epoch 6/10, Batch 454/883, Training Loss: 0.7656
Epoch 6/10, Batch 455/883, Training Loss: 0.7204
Epoch 6/10, Batch 456/883, Training Loss: 1.0793
Epoch 6/10, Batch 457/883, Training Loss: 0.6959
Epoch 6/10, Batch 458/883, Training Loss: 0.8442
Epoch 6/10, Batch 459/883, Training Loss: 0.9991
Epoch 6/10, Batch 460/883, Training Loss: 0.6493
Epoch 6/10, Batch 461/883, Training Loss: 0.8203
Epoch 6/10, Batch 462/883, Training Loss: 0.8703
Epoch 6/10, Batch 463/883, Training Loss: 0.8481
Epoch 6/10, Batch 464/883, Training Loss: 0.8264
Epoch 6/10, Batch 465/883, Training Loss: 0.6411
Epoch 6/10, Batch 466/883, Training Loss: 0.6360
Epoch 6/10, Batch 467/883, Training Loss: 0.6492
Epoch 6/10, Batch 468/883, Training Loss: 0.7261
Epoch 6/10, Batch 469/883, Training Loss: 0.5199
Epoch 6/10, Batch 470/883, Training Loss: 0.7119
Epoch 6/10, Batch 471/883, Training Loss: 0.7210
Epoch 6/10, Batch 472/883, Training Loss: 0.7260
Epoch 6/10, Batch 473/883, Training Loss: 0.6179
Epoch 6/10, Batch 474/883, Training Loss: 0.5799
Epoch 6/10, Batch 475/883, Training Loss: 0.6901
Epoch 6/10, Batch 476/883, Training Loss: 0.6120
Epoch 6/10, Batch 477/883, Training Loss: 0.6148
Epoch 6/10, Batch 478/883, Training Loss: 0.7698
Epoch 6/10, Batch 479/883, Training Loss: 0.5167
Epoch 6/10, Batch 480/883, Training Loss: 0.6199
Epoch 6/10, Batch 481/883, Training Loss: 0.9676
Epoch 6/10, Batch 482/883, Training Loss: 0.5205
Epoch 6/10, Batch 483/883, Training Loss: 1.0518
Epoch 6/10, Batch 484/883, Training Loss: 0.6146
Epoch 6/10, Batch 485/883, Training Loss: 1.0653
Epoch 6/10, Batch 486/883, Training Loss: 0.7585
Epoch 6/10, Batch 487/883, Training Loss: 0.6726
Epoch 6/10, Batch 488/883, Training Loss: 0.5088
Epoch 6/10, Batch 489/883, Training Loss: 0.6802
Epoch 6/10, Batch 490/883, Training Loss: 1.0097
Epoch 6/10, Batch 491/883, Training Loss: 0.7073
Epoch 6/10, Batch 492/883, Training Loss: 0.4340
Epoch 6/10, Batch 493/883, Training Loss: 0.8390
Epoch 6/10, Batch 494/883, Training Loss: 0.8315
Epoch 6/10, Batch 495/883, Training Loss: 0.6366
Epoch 6/10, Batch 496/883, Training Loss: 0.6788
Epoch 6/10, Batch 497/883, Training Loss: 0.6897
Epoch 6/10, Batch 498/883, Training Loss: 0.9253
Epoch 6/10, Batch 499/883, Training Loss: 0.6125
Epoch 6/10, Batch 500/883, Training Loss: 0.8688
Epoch 6/10, Batch 501/883, Training Loss: 0.4655
Epoch 6/10, Batch 502/883, Training Loss: 0.4715
Epoch 6/10, Batch 503/883, Training Loss: 0.6252
Epoch 6/10, Batch 504/883, Training Loss: 0.6737
Epoch 6/10, Batch 505/883, Training Loss: 0.6449
Epoch 6/10, Batch 506/883, Training Loss: 0.5046
Epoch 6/10, Batch 507/883, Training Loss: 0.5844
Epoch 6/10, Batch 508/883, Training Loss: 0.7107
Epoch 6/10, Batch 509/883, Training Loss: 0.4373
Epoch 6/10, Batch 510/883, Training Loss: 0.7476
Epoch 6/10, Batch 511/883, Training Loss: 0.7663
Epoch 6/10, Batch 512/883, Training Loss: 0.5800
Epoch 6/10, Batch 513/883, Training Loss: 0.4802
Epoch 6/10, Batch 514/883, Training Loss: 0.7868
Epoch 6/10, Batch 515/883, Training Loss: 0.7596
Epoch 6/10, Batch 516/883, Training Loss: 0.7309
Epoch 6/10, Batch 517/883, Training Loss: 0.6721
Epoch 6/10, Batch 518/883, Training Loss: 0.6813
Epoch 6/10, Batch 519/883, Training Loss: 0.5436
Epoch 6/10, Batch 520/883, Training Loss: 0.7160
Epoch 6/10, Batch 521/883, Training Loss: 0.5095
Epoch 6/10, Batch 522/883, Training Loss: 0.6208
Epoch 6/10, Batch 523/883, Training Loss: 0.5724
Epoch 6/10, Batch 524/883, Training Loss: 0.5892
Epoch 6/10, Batch 525/883, Training Loss: 0.5937
Epoch 6/10, Batch 526/883, Training Loss: 0.7050
Epoch 6/10, Batch 527/883, Training Loss: 0.5970
Epoch 6/10, Batch 528/883, Training Loss: 0.4736
Epoch 6/10, Batch 529/883, Training Loss: 0.6030
Epoch 6/10, Batch 530/883, Training Loss: 0.6562
Epoch 6/10, Batch 531/883, Training Loss: 0.7177
Epoch 6/10, Batch 532/883, Training Loss: 0.8842
Epoch 6/10, Batch 533/883, Training Loss: 0.5767
Epoch 6/10, Batch 534/883, Training Loss: 0.7487
Epoch 6/10, Batch 535/883, Training Loss: 0.4851
Epoch 6/10, Batch 536/883, Training Loss: 0.4356
Epoch 6/10, Batch 537/883, Training Loss: 0.5093
Epoch 6/10, Batch 538/883, Training Loss: 0.5525
Epoch 6/10, Batch 539/883, Training Loss: 0.5615
Epoch 6/10, Batch 540/883, Training Loss: 0.5898
Epoch 6/10, Batch 541/883, Training Loss: 0.5358
Epoch 6/10, Batch 542/883, Training Loss: 0.5413
Epoch 6/10, Batch 543/883, Training Loss: 0.6672
Epoch 6/10, Batch 544/883, Training Loss: 0.4716
Epoch 6/10, Batch 545/883, Training Loss: 0.7891
Epoch 6/10, Batch 546/883, Training Loss: 0.6366
Epoch 6/10, Batch 547/883, Training Loss: 0.7803
Epoch 6/10, Batch 548/883, Training Loss: 0.7911
Epoch 6/10, Batch 549/883, Training Loss: 0.5733
Epoch 6/10, Batch 550/883, Training Loss: 0.5401
Epoch 6/10, Batch 551/883, Training Loss: 0.5566
Epoch 6/10, Batch 552/883, Training Loss: 0.7211
Epoch 6/10, Batch 553/883, Training Loss: 0.9335
Epoch 6/10, Batch 554/883, Training Loss: 0.7482
Epoch 6/10, Batch 555/883, Training Loss: 0.7712
Epoch 6/10, Batch 556/883, Training Loss: 0.7412
Epoch 6/10, Batch 557/883, Training Loss: 0.5126
Epoch 6/10, Batch 558/883, Training Loss: 0.5203
Epoch 6/10, Batch 559/883, Training Loss: 0.5091
Epoch 6/10, Batch 560/883, Training Loss: 0.6348
Epoch 6/10, Batch 561/883, Training Loss: 0.5024
Epoch 6/10, Batch 562/883, Training Loss: 0.8379
Epoch 6/10, Batch 563/883, Training Loss: 0.5547
Epoch 6/10, Batch 564/883, Training Loss: 0.6243
Epoch 6/10, Batch 565/883, Training Loss: 0.5981
Epoch 6/10, Batch 566/883, Training Loss: 0.5227
Epoch 6/10, Batch 567/883, Training Loss: 1.0271
Epoch 6/10, Batch 568/883, Training Loss: 0.6406
Epoch 6/10, Batch 569/883, Training Loss: 0.7537
Epoch 6/10, Batch 570/883, Training Loss: 0.5411
Epoch 6/10, Batch 571/883, Training Loss: 0.7144
Epoch 6/10, Batch 572/883, Training Loss: 0.7134
Epoch 6/10, Batch 573/883, Training Loss: 0.9859
Epoch 6/10, Batch 574/883, Training Loss: 0.7711
Epoch 6/10, Batch 575/883, Training Loss: 0.5676
Epoch 6/10, Batch 576/883, Training Loss: 0.7006
Epoch 6/10, Batch 577/883, Training Loss: 0.5845
Epoch 6/10, Batch 578/883, Training Loss: 0.6757
Epoch 6/10, Batch 579/883, Training Loss: 0.6069
Epoch 6/10, Batch 580/883, Training Loss: 0.6993
Epoch 6/10, Batch 581/883, Training Loss: 0.6498
Epoch 6/10, Batch 582/883, Training Loss: 0.8138
Epoch 6/10, Batch 583/883, Training Loss: 0.6808
Epoch 6/10, Batch 584/883, Training Loss: 0.6045
Epoch 6/10, Batch 585/883, Training Loss: 0.6369
Epoch 6/10, Batch 586/883, Training Loss: 0.6061
Epoch 6/10, Batch 587/883, Training Loss: 0.5030
Epoch 6/10, Batch 588/883, Training Loss: 0.8867
Epoch 6/10, Batch 589/883, Training Loss: 0.6371
Epoch 6/10, Batch 590/883, Training Loss: 0.6245
Epoch 6/10, Batch 591/883, Training Loss: 0.5278
Epoch 6/10, Batch 592/883, Training Loss: 0.5862
Epoch 6/10, Batch 593/883, Training Loss: 0.6597
Epoch 6/10, Batch 594/883, Training Loss: 0.8087
Epoch 6/10, Batch 595/883, Training Loss: 0.5489
Epoch 6/10, Batch 596/883, Training Loss: 0.6267
Epoch 6/10, Batch 597/883, Training Loss: 1.2330
Epoch 6/10, Batch 598/883, Training Loss: 1.1425
Epoch 6/10, Batch 599/883, Training Loss: 0.4927
Epoch 6/10, Batch 600/883, Training Loss: 0.5393
Epoch 6/10, Batch 601/883, Training Loss: 0.7328
Epoch 6/10, Batch 602/883, Training Loss: 0.8544
Epoch 6/10, Batch 603/883, Training Loss: 0.9038
Epoch 6/10, Batch 604/883, Training Loss: 0.6940
Epoch 6/10, Batch 605/883, Training Loss: 1.1913
Epoch 6/10, Batch 606/883, Training Loss: 0.6561
Epoch 6/10, Batch 607/883, Training Loss: 0.8792
Epoch 6/10, Batch 608/883, Training Loss: 0.5213
Epoch 6/10, Batch 609/883, Training Loss: 0.8452
Epoch 6/10, Batch 610/883, Training Loss: 0.4541
Epoch 6/10, Batch 611/883, Training Loss: 0.9336
Epoch 6/10, Batch 612/883, Training Loss: 0.7602
Epoch 6/10, Batch 613/883, Training Loss: 0.9647
Epoch 6/10, Batch 614/883, Training Loss: 1.0660
Epoch 6/10, Batch 615/883, Training Loss: 0.5936
Epoch 6/10, Batch 616/883, Training Loss: 0.7444
Epoch 6/10, Batch 617/883, Training Loss: 0.6529
Epoch 6/10, Batch 618/883, Training Loss: 0.5996
Epoch 6/10, Batch 619/883, Training Loss: 0.7680
Epoch 6/10, Batch 620/883, Training Loss: 0.5863
Epoch 6/10, Batch 621/883, Training Loss: 0.9096
Epoch 6/10, Batch 622/883, Training Loss: 0.8626
Epoch 6/10, Batch 623/883, Training Loss: 0.7417
Epoch 6/10, Batch 624/883, Training Loss: 0.8971
Epoch 6/10, Batch 625/883, Training Loss: 0.6360
Epoch 6/10, Batch 626/883, Training Loss: 0.8719
Epoch 6/10, Batch 627/883, Training Loss: 0.7810
Epoch 6/10, Batch 628/883, Training Loss: 0.9147
Epoch 6/10, Batch 629/883, Training Loss: 0.6313
Epoch 6/10, Batch 630/883, Training Loss: 0.5217
Epoch 6/10, Batch 631/883, Training Loss: 0.4819
Epoch 6/10, Batch 632/883, Training Loss: 0.6911
Epoch 6/10, Batch 633/883, Training Loss: 0.6939
Epoch 6/10, Batch 634/883, Training Loss: 0.6567
Epoch 6/10, Batch 635/883, Training Loss: 0.8509
Epoch 6/10, Batch 636/883, Training Loss: 0.7058
Epoch 6/10, Batch 637/883, Training Loss: 0.5126
Epoch 6/10, Batch 638/883, Training Loss: 0.6908
Epoch 6/10, Batch 639/883, Training Loss: 0.7204
Epoch 6/10, Batch 640/883, Training Loss: 0.4306
Epoch 6/10, Batch 641/883, Training Loss: 0.6931
Epoch 6/10, Batch 642/883, Training Loss: 0.6324
Epoch 6/10, Batch 643/883, Training Loss: 0.7345
Epoch 6/10, Batch 644/883, Training Loss: 0.5865
Epoch 6/10, Batch 645/883, Training Loss: 0.7652
Epoch 6/10, Batch 646/883, Training Loss: 0.6673
Epoch 6/10, Batch 647/883, Training Loss: 0.6356
Epoch 6/10, Batch 648/883, Training Loss: 0.5803
Epoch 6/10, Batch 649/883, Training Loss: 0.7968
Epoch 6/10, Batch 650/883, Training Loss: 0.8654
Epoch 6/10, Batch 651/883, Training Loss: 0.6520
Epoch 6/10, Batch 652/883, Training Loss: 0.5917
Epoch 6/10, Batch 653/883, Training Loss: 1.1188
Epoch 6/10, Batch 654/883, Training Loss: 0.5883
Epoch 6/10, Batch 655/883, Training Loss: 0.5287
Epoch 6/10, Batch 656/883, Training Loss: 1.0094
Epoch 6/10, Batch 657/883, Training Loss: 0.8749
Epoch 6/10, Batch 658/883, Training Loss: 0.6843
Epoch 6/10, Batch 659/883, Training Loss: 0.9131
Epoch 6/10, Batch 660/883, Training Loss: 0.4977
Epoch 6/10, Batch 661/883, Training Loss: 0.5952
Epoch 6/10, Batch 662/883, Training Loss: 0.6519
Epoch 6/10, Batch 663/883, Training Loss: 0.8127
Epoch 6/10, Batch 664/883, Training Loss: 0.8289
Epoch 6/10, Batch 665/883, Training Loss: 0.7849
Epoch 6/10, Batch 666/883, Training Loss: 0.6349
Epoch 6/10, Batch 667/883, Training Loss: 0.6998
Epoch 6/10, Batch 668/883, Training Loss: 0.5810
Epoch 6/10, Batch 669/883, Training Loss: 0.4121
Epoch 6/10, Batch 670/883, Training Loss: 0.7287
Epoch 6/10, Batch 671/883, Training Loss: 0.8826
Epoch 6/10, Batch 672/883, Training Loss: 0.4783
Epoch 6/10, Batch 673/883, Training Loss: 0.6389
Epoch 6/10, Batch 674/883, Training Loss: 0.6611
Epoch 6/10, Batch 675/883, Training Loss: 0.7065
Epoch 6/10, Batch 676/883, Training Loss: 0.9636
Epoch 6/10, Batch 677/883, Training Loss: 0.9027
Epoch 6/10, Batch 678/883, Training Loss: 0.6014
Epoch 6/10, Batch 679/883, Training Loss: 0.8613
Epoch 6/10, Batch 680/883, Training Loss: 0.6823
Epoch 6/10, Batch 681/883, Training Loss: 1.4114
Epoch 6/10, Batch 682/883, Training Loss: 0.5705
Epoch 6/10, Batch 683/883, Training Loss: 0.6578
Epoch 6/10, Batch 684/883, Training Loss: 0.6629
Epoch 6/10, Batch 685/883, Training Loss: 0.6902
Epoch 6/10, Batch 686/883, Training Loss: 0.6055
Epoch 6/10, Batch 687/883, Training Loss: 0.8019
Epoch 6/10, Batch 688/883, Training Loss: 1.1491
Epoch 6/10, Batch 689/883, Training Loss: 0.7084
Epoch 6/10, Batch 690/883, Training Loss: 0.9229
Epoch 6/10, Batch 691/883, Training Loss: 0.7352
Epoch 6/10, Batch 692/883, Training Loss: 0.8577
Epoch 6/10, Batch 693/883, Training Loss: 0.9086
Epoch 6/10, Batch 694/883, Training Loss: 0.7907
Epoch 6/10, Batch 695/883, Training Loss: 0.8529
Epoch 6/10, Batch 696/883, Training Loss: 0.5845
Epoch 6/10, Batch 697/883, Training Loss: 0.8791
Epoch 6/10, Batch 698/883, Training Loss: 1.1775
Epoch 6/10, Batch 699/883, Training Loss: 0.9361
Epoch 6/10, Batch 700/883, Training Loss: 0.5327
Epoch 6/10, Batch 701/883, Training Loss: 1.1390
Epoch 6/10, Batch 702/883, Training Loss: 0.6057
Epoch 6/10, Batch 703/883, Training Loss: 0.6631
Epoch 6/10, Batch 704/883, Training Loss: 0.7077
Epoch 6/10, Batch 705/883, Training Loss: 0.7357
Epoch 6/10, Batch 706/883, Training Loss: 0.6158
Epoch 6/10, Batch 707/883, Training Loss: 0.9121
Epoch 6/10, Batch 708/883, Training Loss: 0.7356
Epoch 6/10, Batch 709/883, Training Loss: 0.5710
Epoch 6/10, Batch 710/883, Training Loss: 0.6069
Epoch 6/10, Batch 711/883, Training Loss: 0.7440
Epoch 6/10, Batch 712/883, Training Loss: 0.6337
Epoch 6/10, Batch 713/883, Training Loss: 0.8193
Epoch 6/10, Batch 714/883, Training Loss: 0.5890
Epoch 6/10, Batch 715/883, Training Loss: 0.6945
Epoch 6/10, Batch 716/883, Training Loss: 0.5765
Epoch 6/10, Batch 717/883, Training Loss: 0.7507
Epoch 6/10, Batch 718/883, Training Loss: 0.7927
Epoch 6/10, Batch 719/883, Training Loss: 0.8036
Epoch 6/10, Batch 720/883, Training Loss: 0.6311
Epoch 6/10, Batch 721/883, Training Loss: 0.6885
Epoch 6/10, Batch 722/883, Training Loss: 0.5215
Epoch 6/10, Batch 723/883, Training Loss: 0.7250
Epoch 6/10, Batch 724/883, Training Loss: 0.7233
Epoch 6/10, Batch 725/883, Training Loss: 0.7594
Epoch 6/10, Batch 726/883, Training Loss: 0.4368
Epoch 6/10, Batch 727/883, Training Loss: 0.7669
Epoch 6/10, Batch 728/883, Training Loss: 0.9096
Epoch 6/10, Batch 729/883, Training Loss: 0.9468
Epoch 6/10, Batch 730/883, Training Loss: 0.7211
Epoch 6/10, Batch 731/883, Training Loss: 0.6749
Epoch 6/10, Batch 732/883, Training Loss: 0.5595
Epoch 6/10, Batch 733/883, Training Loss: 0.7575
Epoch 6/10, Batch 734/883, Training Loss: 0.7988
Epoch 6/10, Batch 735/883, Training Loss: 0.7466
Epoch 6/10, Batch 736/883, Training Loss: 0.6223
Epoch 6/10, Batch 737/883, Training Loss: 0.7378
Epoch 6/10, Batch 738/883, Training Loss: 0.6524
Epoch 6/10, Batch 739/883, Training Loss: 0.7975
Epoch 6/10, Batch 740/883, Training Loss: 0.7027
Epoch 6/10, Batch 741/883, Training Loss: 0.7519
Epoch 6/10, Batch 742/883, Training Loss: 0.6547
Epoch 6/10, Batch 743/883, Training Loss: 0.6977
Epoch 6/10, Batch 744/883, Training Loss: 0.8340
Epoch 6/10, Batch 745/883, Training Loss: 0.5546
Epoch 6/10, Batch 746/883, Training Loss: 0.5520
Epoch 6/10, Batch 747/883, Training Loss: 0.5285
Epoch 6/10, Batch 748/883, Training Loss: 0.4970
Epoch 6/10, Batch 749/883, Training Loss: 0.6887
Epoch 6/10, Batch 750/883, Training Loss: 0.9089
Epoch 6/10, Batch 751/883, Training Loss: 0.6843
Epoch 6/10, Batch 752/883, Training Loss: 0.8045
Epoch 6/10, Batch 753/883, Training Loss: 0.4691
Epoch 6/10, Batch 754/883, Training Loss: 0.6295
Epoch 6/10, Batch 755/883, Training Loss: 0.7275
Epoch 6/10, Batch 756/883, Training Loss: 0.7260
Epoch 6/10, Batch 757/883, Training Loss: 0.6944
Epoch 6/10, Batch 758/883, Training Loss: 0.7120
Epoch 6/10, Batch 759/883, Training Loss: 0.8514
Epoch 6/10, Batch 760/883, Training Loss: 0.4623
Epoch 6/10, Batch 761/883, Training Loss: 0.6717
Epoch 6/10, Batch 762/883, Training Loss: 0.4459
Epoch 6/10, Batch 763/883, Training Loss: 0.7076
Epoch 6/10, Batch 764/883, Training Loss: 0.7125
Epoch 6/10, Batch 765/883, Training Loss: 0.4898
Epoch 6/10, Batch 766/883, Training Loss: 0.6213
Epoch 6/10, Batch 767/883, Training Loss: 0.4897
Epoch 6/10, Batch 768/883, Training Loss: 0.7252
Epoch 6/10, Batch 769/883, Training Loss: 0.8459
Epoch 6/10, Batch 770/883, Training Loss: 0.7551
Epoch 6/10, Batch 771/883, Training Loss: 0.6059
Epoch 6/10, Batch 772/883, Training Loss: 0.7247
Epoch 6/10, Batch 773/883, Training Loss: 0.5759
Epoch 6/10, Batch 774/883, Training Loss: 0.6306
Epoch 6/10, Batch 775/883, Training Loss: 0.4631
Epoch 6/10, Batch 776/883, Training Loss: 0.7450
Epoch 6/10, Batch 777/883, Training Loss: 0.6402
Epoch 6/10, Batch 778/883, Training Loss: 0.4973
Epoch 6/10, Batch 779/883, Training Loss: 0.7034
Epoch 6/10, Batch 780/883, Training Loss: 0.4874
Epoch 6/10, Batch 781/883, Training Loss: 0.6304
Epoch 6/10, Batch 782/883, Training Loss: 0.7070
Epoch 6/10, Batch 783/883, Training Loss: 0.7470
Epoch 6/10, Batch 784/883, Training Loss: 0.8166
Epoch 6/10, Batch 785/883, Training Loss: 0.7816
Epoch 6/10, Batch 786/883, Training Loss: 0.8080
Epoch 6/10, Batch 787/883, Training Loss: 0.7629
Epoch 6/10, Batch 788/883, Training Loss: 0.8952
Epoch 6/10, Batch 789/883, Training Loss: 0.5346
Epoch 6/10, Batch 790/883, Training Loss: 0.7206
Epoch 6/10, Batch 791/883, Training Loss: 0.9159
Epoch 6/10, Batch 792/883, Training Loss: 0.5281
Epoch 6/10, Batch 793/883, Training Loss: 0.6254
Epoch 6/10, Batch 794/883, Training Loss: 0.5280
Epoch 6/10, Batch 795/883, Training Loss: 0.5726
Epoch 6/10, Batch 796/883, Training Loss: 0.5492
Epoch 6/10, Batch 797/883, Training Loss: 0.8039
Epoch 6/10, Batch 798/883, Training Loss: 0.7704
Epoch 6/10, Batch 799/883, Training Loss: 0.8304
Epoch 6/10, Batch 800/883, Training Loss: 0.6384
Epoch 6/10, Batch 801/883, Training Loss: 0.6966
Epoch 6/10, Batch 802/883, Training Loss: 0.6870
Epoch 6/10, Batch 803/883, Training Loss: 0.6163
Epoch 6/10, Batch 804/883, Training Loss: 0.4755
Epoch 6/10, Batch 805/883, Training Loss: 0.5915
Epoch 6/10, Batch 806/883, Training Loss: 0.7525
Epoch 6/10, Batch 807/883, Training Loss: 0.9150
Epoch 6/10, Batch 808/883, Training Loss: 0.6738
Epoch 6/10, Batch 809/883, Training Loss: 0.5579
Epoch 6/10, Batch 810/883, Training Loss: 0.6691
Epoch 6/10, Batch 811/883, Training Loss: 0.5784
Epoch 6/10, Batch 812/883, Training Loss: 0.6375
Epoch 6/10, Batch 813/883, Training Loss: 0.7538
Epoch 6/10, Batch 814/883, Training Loss: 0.9365
Epoch 6/10, Batch 815/883, Training Loss: 0.6900
Epoch 6/10, Batch 816/883, Training Loss: 0.7509
Epoch 6/10, Batch 817/883, Training Loss: 0.6503
Epoch 6/10, Batch 818/883, Training Loss: 1.0095
Epoch 6/10, Batch 819/883, Training Loss: 0.4897
Epoch 6/10, Batch 820/883, Training Loss: 0.5347
Epoch 6/10, Batch 821/883, Training Loss: 0.5169
Epoch 6/10, Batch 822/883, Training Loss: 0.4196
Epoch 6/10, Batch 823/883, Training Loss: 0.8192
Epoch 6/10, Batch 824/883, Training Loss: 0.5103
Epoch 6/10, Batch 825/883, Training Loss: 0.8105
Epoch 6/10, Batch 826/883, Training Loss: 0.5018
Epoch 6/10, Batch 827/883, Training Loss: 1.1434
Epoch 6/10, Batch 828/883, Training Loss: 0.4908
Epoch 6/10, Batch 829/883, Training Loss: 0.4221
Epoch 6/10, Batch 830/883, Training Loss: 0.4469
Epoch 6/10, Batch 831/883, Training Loss: 0.6459
Epoch 6/10, Batch 832/883, Training Loss: 0.6079
Epoch 6/10, Batch 833/883, Training Loss: 0.5382
Epoch 6/10, Batch 834/883, Training Loss: 0.6084
Epoch 6/10, Batch 835/883, Training Loss: 0.6036
Epoch 6/10, Batch 836/883, Training Loss: 0.5765
Epoch 6/10, Batch 837/883, Training Loss: 0.6540
Epoch 6/10, Batch 838/883, Training Loss: 0.6701
Epoch 6/10, Batch 839/883, Training Loss: 0.8435
Epoch 6/10, Batch 840/883, Training Loss: 0.9418
Epoch 6/10, Batch 841/883, Training Loss: 0.6133
Epoch 6/10, Batch 842/883, Training Loss: 0.5981
Epoch 6/10, Batch 843/883, Training Loss: 0.5747
Epoch 6/10, Batch 844/883, Training Loss: 0.5228
Epoch 6/10, Batch 845/883, Training Loss: 0.8823
Epoch 6/10, Batch 846/883, Training Loss: 0.8090
Epoch 6/10, Batch 847/883, Training Loss: 0.6560
Epoch 6/10, Batch 848/883, Training Loss: 0.6224
Epoch 6/10, Batch 849/883, Training Loss: 0.5601
Epoch 6/10, Batch 850/883, Training Loss: 0.6206
Epoch 6/10, Batch 851/883, Training Loss: 0.5153
Epoch 6/10, Batch 852/883, Training Loss: 0.7678
Epoch 6/10, Batch 853/883, Training Loss: 0.6241
Epoch 6/10, Batch 854/883, Training Loss: 0.4909
Epoch 6/10, Batch 855/883, Training Loss: 0.5861
Epoch 6/10, Batch 856/883, Training Loss: 0.7877
Epoch 6/10, Batch 857/883, Training Loss: 0.7300
Epoch 6/10, Batch 858/883, Training Loss: 0.4355
Epoch 6/10, Batch 859/883, Training Loss: 0.7296
Epoch 6/10, Batch 860/883, Training Loss: 0.6280
Epoch 6/10, Batch 861/883, Training Loss: 0.8509
Epoch 6/10, Batch 862/883, Training Loss: 1.0455
Epoch 6/10, Batch 863/883, Training Loss: 0.2804
Epoch 6/10, Batch 864/883, Training Loss: 0.7026
Epoch 6/10, Batch 865/883, Training Loss: 0.7517
Epoch 6/10, Batch 866/883, Training Loss: 0.9512
Epoch 6/10, Batch 867/883, Training Loss: 0.6388
Epoch 6/10, Batch 868/883, Training Loss: 0.7734
Epoch 6/10, Batch 869/883, Training Loss: 0.8466
Epoch 6/10, Batch 870/883, Training Loss: 0.5863
Epoch 6/10, Batch 871/883, Training Loss: 1.0652
Epoch 6/10, Batch 872/883, Training Loss: 0.4532
Epoch 6/10, Batch 873/883, Training Loss: 0.6095
Epoch 6/10, Batch 874/883, Training Loss: 0.4249
Epoch 6/10, Batch 875/883, Training Loss: 0.5628
Epoch 6/10, Batch 876/883, Training Loss: 0.7044
Epoch 6/10, Batch 877/883, Training Loss: 0.7924
Epoch 6/10, Batch 878/883, Training Loss: 0.7402
Epoch 6/10, Batch 879/883, Training Loss: 0.8212
Epoch 6/10, Batch 880/883, Training Loss: 0.5794
Epoch 6/10, Batch 881/883, Training Loss: 0.8141
Epoch 6/10, Batch 882/883, Training Loss: 0.5746
Epoch 6/10, Batch 883/883, Training Loss: 0.4131
Epoch 6/10, Training Loss: 0.6950, Validation Loss: 0.6974, Validation Accuracy: 0.6654
Epoch 7/10, Batch 1/883, Training Loss: 0.6653
Epoch 7/10, Batch 2/883, Training Loss: 0.4004
Epoch 7/10, Batch 3/883, Training Loss: 0.8757
Epoch 7/10, Batch 4/883, Training Loss: 0.7207
Epoch 7/10, Batch 5/883, Training Loss: 0.5516
Epoch 7/10, Batch 6/883, Training Loss: 0.6645
Epoch 7/10, Batch 7/883, Training Loss: 0.8139
Epoch 7/10, Batch 8/883, Training Loss: 0.9196
Epoch 7/10, Batch 9/883, Training Loss: 0.7610
Epoch 7/10, Batch 10/883, Training Loss: 0.8049
Epoch 7/10, Batch 11/883, Training Loss: 0.9522
Epoch 7/10, Batch 12/883, Training Loss: 1.0637
Epoch 7/10, Batch 13/883, Training Loss: 0.5025
Epoch 7/10, Batch 14/883, Training Loss: 0.5515
Epoch 7/10, Batch 15/883, Training Loss: 0.6219
Epoch 7/10, Batch 16/883, Training Loss: 0.5944
Epoch 7/10, Batch 17/883, Training Loss: 0.7015
Epoch 7/10, Batch 18/883, Training Loss: 0.7073
Epoch 7/10, Batch 19/883, Training Loss: 0.6966
Epoch 7/10, Batch 20/883, Training Loss: 0.6202
Epoch 7/10, Batch 21/883, Training Loss: 0.7090
Epoch 7/10, Batch 22/883, Training Loss: 0.6505
Epoch 7/10, Batch 23/883, Training Loss: 0.6700
Epoch 7/10, Batch 24/883, Training Loss: 0.6389
Epoch 7/10, Batch 25/883, Training Loss: 0.5631
Epoch 7/10, Batch 26/883, Training Loss: 0.7675
Epoch 7/10, Batch 27/883, Training Loss: 0.5813
Epoch 7/10, Batch 28/883, Training Loss: 0.8360
Epoch 7/10, Batch 29/883, Training Loss: 0.6017
Epoch 7/10, Batch 30/883, Training Loss: 0.5175
Epoch 7/10, Batch 31/883, Training Loss: 0.6595
Epoch 7/10, Batch 32/883, Training Loss: 0.5890
Epoch 7/10, Batch 33/883, Training Loss: 0.5734
Epoch 7/10, Batch 34/883, Training Loss: 0.6875
Epoch 7/10, Batch 35/883, Training Loss: 0.5608
Epoch 7/10, Batch 36/883, Training Loss: 0.6538
Epoch 7/10, Batch 37/883, Training Loss: 0.4371
Epoch 7/10, Batch 38/883, Training Loss: 0.9829
Epoch 7/10, Batch 39/883, Training Loss: 0.6091
Epoch 7/10, Batch 40/883, Training Loss: 0.5719
Epoch 7/10, Batch 41/883, Training Loss: 0.5137
Epoch 7/10, Batch 42/883, Training Loss: 0.7426
Epoch 7/10, Batch 43/883, Training Loss: 0.6072
Epoch 7/10, Batch 44/883, Training Loss: 0.9057
Epoch 7/10, Batch 45/883, Training Loss: 0.6244
Epoch 7/10, Batch 46/883, Training Loss: 0.7047
Epoch 7/10, Batch 47/883, Training Loss: 0.8175
Epoch 7/10, Batch 48/883, Training Loss: 0.6198
Epoch 7/10, Batch 49/883, Training Loss: 0.8996
Epoch 7/10, Batch 50/883, Training Loss: 0.7087
Epoch 7/10, Batch 51/883, Training Loss: 0.6905
Epoch 7/10, Batch 52/883, Training Loss: 0.6160
Epoch 7/10, Batch 53/883, Training Loss: 0.5798
Epoch 7/10, Batch 54/883, Training Loss: 0.6293
Epoch 7/10, Batch 55/883, Training Loss: 0.8603
Epoch 7/10, Batch 56/883, Training Loss: 0.6447
Epoch 7/10, Batch 57/883, Training Loss: 0.7249
Epoch 7/10, Batch 58/883, Training Loss: 0.5001
Epoch 7/10, Batch 59/883, Training Loss: 0.7119
Epoch 7/10, Batch 60/883, Training Loss: 0.4306
Epoch 7/10, Batch 61/883, Training Loss: 0.6627
Epoch 7/10, Batch 62/883, Training Loss: 0.8030
Epoch 7/10, Batch 63/883, Training Loss: 0.6176
Epoch 7/10, Batch 64/883, Training Loss: 0.4788
Epoch 7/10, Batch 65/883, Training Loss: 0.7329
Epoch 7/10, Batch 66/883, Training Loss: 0.7842
Epoch 7/10, Batch 67/883, Training Loss: 0.6436
Epoch 7/10, Batch 68/883, Training Loss: 0.6561
Epoch 7/10, Batch 69/883, Training Loss: 0.9319
Epoch 7/10, Batch 70/883, Training Loss: 0.7243
Epoch 7/10, Batch 71/883, Training Loss: 0.5839
Epoch 7/10, Batch 72/883, Training Loss: 0.6218
Epoch 7/10, Batch 73/883, Training Loss: 0.6170
Epoch 7/10, Batch 74/883, Training Loss: 0.7538
Epoch 7/10, Batch 75/883, Training Loss: 0.6147
Epoch 7/10, Batch 76/883, Training Loss: 0.6863
Epoch 7/10, Batch 77/883, Training Loss: 0.7631
Epoch 7/10, Batch 78/883, Training Loss: 0.5544
Epoch 7/10, Batch 79/883, Training Loss: 0.9617
Epoch 7/10, Batch 80/883, Training Loss: 0.5068
Epoch 7/10, Batch 81/883, Training Loss: 0.7511
Epoch 7/10, Batch 82/883, Training Loss: 0.5574
Epoch 7/10, Batch 83/883, Training Loss: 0.6045
Epoch 7/10, Batch 84/883, Training Loss: 1.0163
Epoch 7/10, Batch 85/883, Training Loss: 0.7128
Epoch 7/10, Batch 86/883, Training Loss: 0.7320
Epoch 7/10, Batch 87/883, Training Loss: 0.7648
Epoch 7/10, Batch 88/883, Training Loss: 0.3407
Epoch 7/10, Batch 89/883, Training Loss: 0.4015
Epoch 7/10, Batch 90/883, Training Loss: 0.4461
Epoch 7/10, Batch 91/883, Training Loss: 0.9462
Epoch 7/10, Batch 92/883, Training Loss: 0.3489
Epoch 7/10, Batch 93/883, Training Loss: 0.7821
Epoch 7/10, Batch 94/883, Training Loss: 0.8072
Epoch 7/10, Batch 95/883, Training Loss: 0.6837
Epoch 7/10, Batch 96/883, Training Loss: 0.7948
Epoch 7/10, Batch 97/883, Training Loss: 0.5144
Epoch 7/10, Batch 98/883, Training Loss: 0.9262
Epoch 7/10, Batch 99/883, Training Loss: 1.0424
Epoch 7/10, Batch 100/883, Training Loss: 0.5591
Epoch 7/10, Batch 101/883, Training Loss: 0.6491
Epoch 7/10, Batch 102/883, Training Loss: 0.4445
Epoch 7/10, Batch 103/883, Training Loss: 0.7120
Epoch 7/10, Batch 104/883, Training Loss: 0.4226
Epoch 7/10, Batch 105/883, Training Loss: 0.9460
Epoch 7/10, Batch 106/883, Training Loss: 0.6409
Epoch 7/10, Batch 107/883, Training Loss: 0.7560
Epoch 7/10, Batch 108/883, Training Loss: 1.0895
Epoch 7/10, Batch 109/883, Training Loss: 0.8067
Epoch 7/10, Batch 110/883, Training Loss: 0.6919
Epoch 7/10, Batch 111/883, Training Loss: 0.7584
Epoch 7/10, Batch 112/883, Training Loss: 0.5475
Epoch 7/10, Batch 113/883, Training Loss: 0.4439
Epoch 7/10, Batch 114/883, Training Loss: 0.7056
Epoch 7/10, Batch 115/883, Training Loss: 0.5481
Epoch 7/10, Batch 116/883, Training Loss: 0.7507
Epoch 7/10, Batch 117/883, Training Loss: 0.6112
Epoch 7/10, Batch 118/883, Training Loss: 0.7735
Epoch 7/10, Batch 119/883, Training Loss: 0.5083
Epoch 7/10, Batch 120/883, Training Loss: 0.6363
Epoch 7/10, Batch 121/883, Training Loss: 0.4482
Epoch 7/10, Batch 122/883, Training Loss: 0.7331
Epoch 7/10, Batch 123/883, Training Loss: 0.7247
Epoch 7/10, Batch 124/883, Training Loss: 1.0321
Epoch 7/10, Batch 125/883, Training Loss: 0.3489
Epoch 7/10, Batch 126/883, Training Loss: 0.7920
Epoch 7/10, Batch 127/883, Training Loss: 0.7155
Epoch 7/10, Batch 128/883, Training Loss: 1.0224
Epoch 7/10, Batch 129/883, Training Loss: 0.5712
Epoch 7/10, Batch 130/883, Training Loss: 0.6202
Epoch 7/10, Batch 131/883, Training Loss: 0.6645
Epoch 7/10, Batch 132/883, Training Loss: 0.7870
Epoch 7/10, Batch 133/883, Training Loss: 0.6569
Epoch 7/10, Batch 134/883, Training Loss: 0.6007
Epoch 7/10, Batch 135/883, Training Loss: 0.6824
Epoch 7/10, Batch 136/883, Training Loss: 0.5667
Epoch 7/10, Batch 137/883, Training Loss: 0.7005
Epoch 7/10, Batch 138/883, Training Loss: 0.8081
Epoch 7/10, Batch 139/883, Training Loss: 0.6036
Epoch 7/10, Batch 140/883, Training Loss: 0.7194
Epoch 7/10, Batch 141/883, Training Loss: 0.8713
Epoch 7/10, Batch 142/883, Training Loss: 0.4935
Epoch 7/10, Batch 143/883, Training Loss: 0.5212
Epoch 7/10, Batch 144/883, Training Loss: 0.6997
Epoch 7/10, Batch 145/883, Training Loss: 0.5428
Epoch 7/10, Batch 146/883, Training Loss: 0.7914
Epoch 7/10, Batch 147/883, Training Loss: 0.5338
Epoch 7/10, Batch 148/883, Training Loss: 0.4620
Epoch 7/10, Batch 149/883, Training Loss: 0.5684
Epoch 7/10, Batch 150/883, Training Loss: 0.8022
Epoch 7/10, Batch 151/883, Training Loss: 0.5223
Epoch 7/10, Batch 152/883, Training Loss: 0.8307
Epoch 7/10, Batch 153/883, Training Loss: 0.4358
Epoch 7/10, Batch 154/883, Training Loss: 0.6386
Epoch 7/10, Batch 155/883, Training Loss: 0.6468
Epoch 7/10, Batch 156/883, Training Loss: 0.6509
Epoch 7/10, Batch 157/883, Training Loss: 0.8074
Epoch 7/10, Batch 158/883, Training Loss: 0.7307
Epoch 7/10, Batch 159/883, Training Loss: 0.5578
Epoch 7/10, Batch 160/883, Training Loss: 1.1917
Epoch 7/10, Batch 161/883, Training Loss: 0.5800
Epoch 7/10, Batch 162/883, Training Loss: 0.5446
Epoch 7/10, Batch 163/883, Training Loss: 0.6772
Epoch 7/10, Batch 164/883, Training Loss: 0.6574
Epoch 7/10, Batch 165/883, Training Loss: 0.6406
Epoch 7/10, Batch 166/883, Training Loss: 1.1806
Epoch 7/10, Batch 167/883, Training Loss: 0.3278
Epoch 7/10, Batch 168/883, Training Loss: 0.5104
Epoch 7/10, Batch 169/883, Training Loss: 0.7398
Epoch 7/10, Batch 170/883, Training Loss: 0.4758
Epoch 7/10, Batch 171/883, Training Loss: 0.5892
Epoch 7/10, Batch 172/883, Training Loss: 0.6700
Epoch 7/10, Batch 173/883, Training Loss: 0.7155
Epoch 7/10, Batch 174/883, Training Loss: 0.5535
Epoch 7/10, Batch 175/883, Training Loss: 0.5400
Epoch 7/10, Batch 176/883, Training Loss: 0.6055
Epoch 7/10, Batch 177/883, Training Loss: 0.7670
Epoch 7/10, Batch 178/883, Training Loss: 0.4758
Epoch 7/10, Batch 179/883, Training Loss: 1.1751
Epoch 7/10, Batch 180/883, Training Loss: 0.8755
Epoch 7/10, Batch 181/883, Training Loss: 0.5702
Epoch 7/10, Batch 182/883, Training Loss: 0.5711
Epoch 7/10, Batch 183/883, Training Loss: 0.4356
Epoch 7/10, Batch 184/883, Training Loss: 0.8578
Epoch 7/10, Batch 185/883, Training Loss: 0.5852
Epoch 7/10, Batch 186/883, Training Loss: 0.5297
Epoch 7/10, Batch 187/883, Training Loss: 0.5653
Epoch 7/10, Batch 188/883, Training Loss: 0.7135
Epoch 7/10, Batch 189/883, Training Loss: 0.4476
Epoch 7/10, Batch 190/883, Training Loss: 0.7607
Epoch 7/10, Batch 191/883, Training Loss: 0.5901
Epoch 7/10, Batch 192/883, Training Loss: 0.4606
Epoch 7/10, Batch 193/883, Training Loss: 0.7240
Epoch 7/10, Batch 194/883, Training Loss: 0.6114
Epoch 7/10, Batch 195/883, Training Loss: 0.6197
Epoch 7/10, Batch 196/883, Training Loss: 0.5236
Epoch 7/10, Batch 197/883, Training Loss: 0.6962
Epoch 7/10, Batch 198/883, Training Loss: 0.6092
Epoch 7/10, Batch 199/883, Training Loss: 0.3872
Epoch 7/10, Batch 200/883, Training Loss: 0.8867
Epoch 7/10, Batch 201/883, Training Loss: 0.4914
Epoch 7/10, Batch 202/883, Training Loss: 0.5988
Epoch 7/10, Batch 203/883, Training Loss: 0.5396
Epoch 7/10, Batch 204/883, Training Loss: 0.5806
Epoch 7/10, Batch 205/883, Training Loss: 1.0454
Epoch 7/10, Batch 206/883, Training Loss: 0.7765
Epoch 7/10, Batch 207/883, Training Loss: 0.4914
Epoch 7/10, Batch 208/883, Training Loss: 0.6146
Epoch 7/10, Batch 209/883, Training Loss: 0.5652
Epoch 7/10, Batch 210/883, Training Loss: 0.7023
Epoch 7/10, Batch 211/883, Training Loss: 0.6789
Epoch 7/10, Batch 212/883, Training Loss: 0.3454
Epoch 7/10, Batch 213/883, Training Loss: 0.5849
Epoch 7/10, Batch 214/883, Training Loss: 0.6072
Epoch 7/10, Batch 215/883, Training Loss: 0.5657
Epoch 7/10, Batch 216/883, Training Loss: 0.5790
Epoch 7/10, Batch 217/883, Training Loss: 0.7052
Epoch 7/10, Batch 218/883, Training Loss: 0.9522
Epoch 7/10, Batch 219/883, Training Loss: 0.6650
Epoch 7/10, Batch 220/883, Training Loss: 0.7138
Epoch 7/10, Batch 221/883, Training Loss: 0.6145
Epoch 7/10, Batch 222/883, Training Loss: 0.5835
Epoch 7/10, Batch 223/883, Training Loss: 0.5585
Epoch 7/10, Batch 224/883, Training Loss: 0.6869
Epoch 7/10, Batch 225/883, Training Loss: 0.5611
Epoch 7/10, Batch 226/883, Training Loss: 0.3791
Epoch 7/10, Batch 227/883, Training Loss: 0.8480
Epoch 7/10, Batch 228/883, Training Loss: 0.7994
Epoch 7/10, Batch 229/883, Training Loss: 0.6248
Epoch 7/10, Batch 230/883, Training Loss: 0.8593
Epoch 7/10, Batch 231/883, Training Loss: 0.6858
Epoch 7/10, Batch 232/883, Training Loss: 0.6521
Epoch 7/10, Batch 233/883, Training Loss: 0.7299
Epoch 7/10, Batch 234/883, Training Loss: 0.9211
Epoch 7/10, Batch 235/883, Training Loss: 0.5256
Epoch 7/10, Batch 236/883, Training Loss: 0.7180
Epoch 7/10, Batch 237/883, Training Loss: 0.6986
Epoch 7/10, Batch 238/883, Training Loss: 0.7608
Epoch 7/10, Batch 239/883, Training Loss: 0.5939
Epoch 7/10, Batch 240/883, Training Loss: 0.5279
Epoch 7/10, Batch 241/883, Training Loss: 0.6984
Epoch 7/10, Batch 242/883, Training Loss: 0.7521
Epoch 7/10, Batch 243/883, Training Loss: 0.8843
Epoch 7/10, Batch 244/883, Training Loss: 0.6664
Epoch 7/10, Batch 245/883, Training Loss: 0.5135
Epoch 7/10, Batch 246/883, Training Loss: 0.5997
Epoch 7/10, Batch 247/883, Training Loss: 0.4889
Epoch 7/10, Batch 248/883, Training Loss: 0.6776
Epoch 7/10, Batch 249/883, Training Loss: 0.6954
Epoch 7/10, Batch 250/883, Training Loss: 0.7267
Epoch 7/10, Batch 251/883, Training Loss: 0.6335
Epoch 7/10, Batch 252/883, Training Loss: 0.6826
Epoch 7/10, Batch 253/883, Training Loss: 0.4777
Epoch 7/10, Batch 254/883, Training Loss: 0.6263
Epoch 7/10, Batch 255/883, Training Loss: 0.7205
Epoch 7/10, Batch 256/883, Training Loss: 0.8961
Epoch 7/10, Batch 257/883, Training Loss: 0.9153
Epoch 7/10, Batch 258/883, Training Loss: 0.5810
Epoch 7/10, Batch 259/883, Training Loss: 0.7360
Epoch 7/10, Batch 260/883, Training Loss: 0.5823
Epoch 7/10, Batch 261/883, Training Loss: 0.9076
Epoch 7/10, Batch 262/883, Training Loss: 0.7603
Epoch 7/10, Batch 263/883, Training Loss: 0.5633
Epoch 7/10, Batch 264/883, Training Loss: 0.8014
Epoch 7/10, Batch 265/883, Training Loss: 0.5543
Epoch 7/10, Batch 266/883, Training Loss: 0.6106
Epoch 7/10, Batch 267/883, Training Loss: 0.6797
Epoch 7/10, Batch 268/883, Training Loss: 0.4414
Epoch 7/10, Batch 269/883, Training Loss: 0.8020
Epoch 7/10, Batch 270/883, Training Loss: 0.5700
Epoch 7/10, Batch 271/883, Training Loss: 1.1009
Epoch 7/10, Batch 272/883, Training Loss: 0.4566
Epoch 7/10, Batch 273/883, Training Loss: 0.7369
Epoch 7/10, Batch 274/883, Training Loss: 0.7861
Epoch 7/10, Batch 275/883, Training Loss: 0.7801
Epoch 7/10, Batch 276/883, Training Loss: 0.3763
Epoch 7/10, Batch 277/883, Training Loss: 0.4730
Epoch 7/10, Batch 278/883, Training Loss: 0.9631
Epoch 7/10, Batch 279/883, Training Loss: 0.7013
Epoch 7/10, Batch 280/883, Training Loss: 0.5374
Epoch 7/10, Batch 281/883, Training Loss: 0.6642
Epoch 7/10, Batch 282/883, Training Loss: 0.8797
Epoch 7/10, Batch 283/883, Training Loss: 0.4786
Epoch 7/10, Batch 284/883, Training Loss: 0.6865
Epoch 7/10, Batch 285/883, Training Loss: 0.5787
Epoch 7/10, Batch 286/883, Training Loss: 0.6406
Epoch 7/10, Batch 287/883, Training Loss: 0.9442
Epoch 7/10, Batch 288/883, Training Loss: 0.5101
Epoch 7/10, Batch 289/883, Training Loss: 0.4860
Epoch 7/10, Batch 290/883, Training Loss: 1.0962
Epoch 7/10, Batch 291/883, Training Loss: 0.7455
Epoch 7/10, Batch 292/883, Training Loss: 0.7133
Epoch 7/10, Batch 293/883, Training Loss: 0.5983
Epoch 7/10, Batch 294/883, Training Loss: 0.5676
Epoch 7/10, Batch 295/883, Training Loss: 0.7328
Epoch 7/10, Batch 296/883, Training Loss: 0.9649
Epoch 7/10, Batch 297/883, Training Loss: 0.6638
Epoch 7/10, Batch 298/883, Training Loss: 0.5218
Epoch 7/10, Batch 299/883, Training Loss: 0.5393
Epoch 7/10, Batch 300/883, Training Loss: 0.4947
Epoch 7/10, Batch 301/883, Training Loss: 0.6865
Epoch 7/10, Batch 302/883, Training Loss: 0.6361
Epoch 7/10, Batch 303/883, Training Loss: 0.6091
Epoch 7/10, Batch 304/883, Training Loss: 0.6739
Epoch 7/10, Batch 305/883, Training Loss: 0.7490
Epoch 7/10, Batch 306/883, Training Loss: 0.6299
Epoch 7/10, Batch 307/883, Training Loss: 0.7044
Epoch 7/10, Batch 308/883, Training Loss: 0.7102
Epoch 7/10, Batch 309/883, Training Loss: 0.6850
Epoch 7/10, Batch 310/883, Training Loss: 0.7302
Epoch 7/10, Batch 311/883, Training Loss: 0.6957
Epoch 7/10, Batch 312/883, Training Loss: 0.5779
Epoch 7/10, Batch 313/883, Training Loss: 0.8395
Epoch 7/10, Batch 314/883, Training Loss: 0.6632
Epoch 7/10, Batch 315/883, Training Loss: 0.7612
Epoch 7/10, Batch 316/883, Training Loss: 0.5871
Epoch 7/10, Batch 317/883, Training Loss: 1.1964
Epoch 7/10, Batch 318/883, Training Loss: 0.6469
Epoch 7/10, Batch 319/883, Training Loss: 0.7169
Epoch 7/10, Batch 320/883, Training Loss: 0.5777
Epoch 7/10, Batch 321/883, Training Loss: 0.5324
Epoch 7/10, Batch 322/883, Training Loss: 0.7707
Epoch 7/10, Batch 323/883, Training Loss: 0.8288
Epoch 7/10, Batch 324/883, Training Loss: 0.6837
Epoch 7/10, Batch 325/883, Training Loss: 0.7607
Epoch 7/10, Batch 326/883, Training Loss: 0.6698
Epoch 7/10, Batch 327/883, Training Loss: 0.4956
Epoch 7/10, Batch 328/883, Training Loss: 0.7231
Epoch 7/10, Batch 329/883, Training Loss: 0.5273
Epoch 7/10, Batch 330/883, Training Loss: 0.6499
Epoch 7/10, Batch 331/883, Training Loss: 0.5105
Epoch 7/10, Batch 332/883, Training Loss: 1.0990
Epoch 7/10, Batch 333/883, Training Loss: 0.8128
Epoch 7/10, Batch 334/883, Training Loss: 0.3995
Epoch 7/10, Batch 335/883, Training Loss: 0.4169
Epoch 7/10, Batch 336/883, Training Loss: 0.8967
Epoch 7/10, Batch 337/883, Training Loss: 0.5377
Epoch 7/10, Batch 338/883, Training Loss: 0.5424
Epoch 7/10, Batch 339/883, Training Loss: 0.7742
Epoch 7/10, Batch 340/883, Training Loss: 0.4667
Epoch 7/10, Batch 341/883, Training Loss: 0.6377
Epoch 7/10, Batch 342/883, Training Loss: 0.5745
Epoch 7/10, Batch 343/883, Training Loss: 0.4162
Epoch 7/10, Batch 344/883, Training Loss: 0.5214
Epoch 7/10, Batch 345/883, Training Loss: 0.5497
Epoch 7/10, Batch 346/883, Training Loss: 0.5762
Epoch 7/10, Batch 347/883, Training Loss: 0.4045
Epoch 7/10, Batch 348/883, Training Loss: 0.4685
Epoch 7/10, Batch 349/883, Training Loss: 0.5207
Epoch 7/10, Batch 350/883, Training Loss: 0.5752
Epoch 7/10, Batch 351/883, Training Loss: 0.5408
Epoch 7/10, Batch 352/883, Training Loss: 0.6683
Epoch 7/10, Batch 353/883, Training Loss: 0.6615
Epoch 7/10, Batch 354/883, Training Loss: 0.5472
Epoch 7/10, Batch 355/883, Training Loss: 0.7343
Epoch 7/10, Batch 356/883, Training Loss: 0.7490
Epoch 7/10, Batch 357/883, Training Loss: 0.8967
Epoch 7/10, Batch 358/883, Training Loss: 0.7681
Epoch 7/10, Batch 359/883, Training Loss: 0.4296
Epoch 7/10, Batch 360/883, Training Loss: 0.9268
Epoch 7/10, Batch 361/883, Training Loss: 0.7056
Epoch 7/10, Batch 362/883, Training Loss: 0.6256
Epoch 7/10, Batch 363/883, Training Loss: 0.6872
Epoch 7/10, Batch 364/883, Training Loss: 0.7819
Epoch 7/10, Batch 365/883, Training Loss: 0.8878
Epoch 7/10, Batch 366/883, Training Loss: 0.8243
Epoch 7/10, Batch 367/883, Training Loss: 1.0586
Epoch 7/10, Batch 368/883, Training Loss: 0.7023
Epoch 7/10, Batch 369/883, Training Loss: 0.5925
Epoch 7/10, Batch 370/883, Training Loss: 0.6605
Epoch 7/10, Batch 371/883, Training Loss: 0.5229
Epoch 7/10, Batch 372/883, Training Loss: 0.5400
Epoch 7/10, Batch 373/883, Training Loss: 0.4310
Epoch 7/10, Batch 374/883, Training Loss: 0.4724
Epoch 7/10, Batch 375/883, Training Loss: 0.6776
Epoch 7/10, Batch 376/883, Training Loss: 0.7025
Epoch 7/10, Batch 377/883, Training Loss: 0.6644
Epoch 7/10, Batch 378/883, Training Loss: 0.6585
Epoch 7/10, Batch 379/883, Training Loss: 0.5980
Epoch 7/10, Batch 380/883, Training Loss: 0.7113
Epoch 7/10, Batch 381/883, Training Loss: 0.5264
Epoch 7/10, Batch 382/883, Training Loss: 0.5751
Epoch 7/10, Batch 383/883, Training Loss: 0.7823
Epoch 7/10, Batch 384/883, Training Loss: 1.0324
Epoch 7/10, Batch 385/883, Training Loss: 0.6597
Epoch 7/10, Batch 386/883, Training Loss: 0.5611
Epoch 7/10, Batch 387/883, Training Loss: 0.4583
Epoch 7/10, Batch 388/883, Training Loss: 0.5829
Epoch 7/10, Batch 389/883, Training Loss: 0.9620
Epoch 7/10, Batch 390/883, Training Loss: 0.5750
Epoch 7/10, Batch 391/883, Training Loss: 0.8627
Epoch 7/10, Batch 392/883, Training Loss: 0.6222
Epoch 7/10, Batch 393/883, Training Loss: 0.8396
Epoch 7/10, Batch 394/883, Training Loss: 0.5625
Epoch 7/10, Batch 395/883, Training Loss: 0.6812
Epoch 7/10, Batch 396/883, Training Loss: 0.3827
Epoch 7/10, Batch 397/883, Training Loss: 0.6569
Epoch 7/10, Batch 398/883, Training Loss: 0.5429
Epoch 7/10, Batch 399/883, Training Loss: 0.9156
Epoch 7/10, Batch 400/883, Training Loss: 0.4989
Epoch 7/10, Batch 401/883, Training Loss: 0.7664
Epoch 7/10, Batch 402/883, Training Loss: 0.6707
Epoch 7/10, Batch 403/883, Training Loss: 0.6799
Epoch 7/10, Batch 404/883, Training Loss: 0.5242
Epoch 7/10, Batch 405/883, Training Loss: 0.6411
Epoch 7/10, Batch 406/883, Training Loss: 0.5545
Epoch 7/10, Batch 407/883, Training Loss: 0.8622
Epoch 7/10, Batch 408/883, Training Loss: 0.5765
Epoch 7/10, Batch 409/883, Training Loss: 0.8445
Epoch 7/10, Batch 410/883, Training Loss: 0.4833
Epoch 7/10, Batch 411/883, Training Loss: 0.7834
Epoch 7/10, Batch 412/883, Training Loss: 0.5558
Epoch 7/10, Batch 413/883, Training Loss: 0.7861
Epoch 7/10, Batch 414/883, Training Loss: 0.6341
Epoch 7/10, Batch 415/883, Training Loss: 0.8047
Epoch 7/10, Batch 416/883, Training Loss: 0.7430
Epoch 7/10, Batch 417/883, Training Loss: 0.8659
Epoch 7/10, Batch 418/883, Training Loss: 0.5302
Epoch 7/10, Batch 419/883, Training Loss: 0.9291
Epoch 7/10, Batch 420/883, Training Loss: 0.7434
Epoch 7/10, Batch 421/883, Training Loss: 0.5044
Epoch 7/10, Batch 422/883, Training Loss: 0.5695
Epoch 7/10, Batch 423/883, Training Loss: 0.6291
Epoch 7/10, Batch 424/883, Training Loss: 0.6104
Epoch 7/10, Batch 425/883, Training Loss: 0.6852
Epoch 7/10, Batch 426/883, Training Loss: 0.5800
Epoch 7/10, Batch 427/883, Training Loss: 0.7986
Epoch 7/10, Batch 428/883, Training Loss: 0.8040
Epoch 7/10, Batch 429/883, Training Loss: 0.8057
Epoch 7/10, Batch 430/883, Training Loss: 0.7119
Epoch 7/10, Batch 431/883, Training Loss: 0.4510
Epoch 7/10, Batch 432/883, Training Loss: 0.5653
Epoch 7/10, Batch 433/883, Training Loss: 0.7493
Epoch 7/10, Batch 434/883, Training Loss: 0.7434
Epoch 7/10, Batch 435/883, Training Loss: 1.0346
Epoch 7/10, Batch 436/883, Training Loss: 0.5780
Epoch 7/10, Batch 437/883, Training Loss: 0.5954
Epoch 7/10, Batch 438/883, Training Loss: 0.4793
Epoch 7/10, Batch 439/883, Training Loss: 0.9148
Epoch 7/10, Batch 440/883, Training Loss: 0.8592
Epoch 7/10, Batch 441/883, Training Loss: 1.0093
Epoch 7/10, Batch 442/883, Training Loss: 0.7402
Epoch 7/10, Batch 443/883, Training Loss: 0.4201
Epoch 7/10, Batch 444/883, Training Loss: 0.7708
Epoch 7/10, Batch 445/883, Training Loss: 0.7416
Epoch 7/10, Batch 446/883, Training Loss: 0.7899
Epoch 7/10, Batch 447/883, Training Loss: 0.4821
Epoch 7/10, Batch 448/883, Training Loss: 0.5755
Epoch 7/10, Batch 449/883, Training Loss: 0.4711
Epoch 7/10, Batch 450/883, Training Loss: 0.4183
Epoch 7/10, Batch 451/883, Training Loss: 0.5124
Epoch 7/10, Batch 452/883, Training Loss: 0.4333
Epoch 7/10, Batch 453/883, Training Loss: 0.9138
Epoch 7/10, Batch 454/883, Training Loss: 0.7471
Epoch 7/10, Batch 455/883, Training Loss: 0.7610
Epoch 7/10, Batch 456/883, Training Loss: 0.5864
Epoch 7/10, Batch 457/883, Training Loss: 0.6925
Epoch 7/10, Batch 458/883, Training Loss: 0.9607
Epoch 7/10, Batch 459/883, Training Loss: 0.6087
Epoch 7/10, Batch 460/883, Training Loss: 0.6713
Epoch 7/10, Batch 461/883, Training Loss: 0.5153
Epoch 7/10, Batch 462/883, Training Loss: 0.7262
Epoch 7/10, Batch 463/883, Training Loss: 0.6094
Epoch 7/10, Batch 464/883, Training Loss: 0.6957
Epoch 7/10, Batch 465/883, Training Loss: 0.7645
Epoch 7/10, Batch 466/883, Training Loss: 0.5733
Epoch 7/10, Batch 467/883, Training Loss: 0.5896
Epoch 7/10, Batch 468/883, Training Loss: 0.3934
Epoch 7/10, Batch 469/883, Training Loss: 0.5191
Epoch 7/10, Batch 470/883, Training Loss: 0.5447
Epoch 7/10, Batch 471/883, Training Loss: 0.4051
Epoch 7/10, Batch 472/883, Training Loss: 0.7205
Epoch 7/10, Batch 473/883, Training Loss: 0.7465
Epoch 7/10, Batch 474/883, Training Loss: 0.7604
Epoch 7/10, Batch 475/883, Training Loss: 0.7050
Epoch 7/10, Batch 476/883, Training Loss: 0.5612
Epoch 7/10, Batch 477/883, Training Loss: 0.7313
Epoch 7/10, Batch 478/883, Training Loss: 0.7065
Epoch 7/10, Batch 479/883, Training Loss: 0.4656
Epoch 7/10, Batch 480/883, Training Loss: 0.9832
Epoch 7/10, Batch 481/883, Training Loss: 0.5032
Epoch 7/10, Batch 482/883, Training Loss: 0.5781
Epoch 7/10, Batch 483/883, Training Loss: 0.7320
Epoch 7/10, Batch 484/883, Training Loss: 0.5274
Epoch 7/10, Batch 485/883, Training Loss: 0.6795
Epoch 7/10, Batch 486/883, Training Loss: 0.8326
Epoch 7/10, Batch 487/883, Training Loss: 0.8057
Epoch 7/10, Batch 488/883, Training Loss: 0.6177
Epoch 7/10, Batch 489/883, Training Loss: 0.7339
Epoch 7/10, Batch 490/883, Training Loss: 0.6098
Epoch 7/10, Batch 491/883, Training Loss: 0.5388
Epoch 7/10, Batch 492/883, Training Loss: 0.8679
Epoch 7/10, Batch 493/883, Training Loss: 0.5573
Epoch 7/10, Batch 494/883, Training Loss: 0.6188
Epoch 7/10, Batch 495/883, Training Loss: 0.5968
Epoch 7/10, Batch 496/883, Training Loss: 0.4981
Epoch 7/10, Batch 497/883, Training Loss: 0.6212
Epoch 7/10, Batch 498/883, Training Loss: 0.7818
Epoch 7/10, Batch 499/883, Training Loss: 0.9385
Epoch 7/10, Batch 500/883, Training Loss: 0.4803
Epoch 7/10, Batch 501/883, Training Loss: 0.8929
Epoch 7/10, Batch 502/883, Training Loss: 0.9025
Epoch 7/10, Batch 503/883, Training Loss: 0.8854
Epoch 7/10, Batch 504/883, Training Loss: 0.8016
Epoch 7/10, Batch 505/883, Training Loss: 0.7130
Epoch 7/10, Batch 506/883, Training Loss: 0.7205
Epoch 7/10, Batch 507/883, Training Loss: 0.5306
Epoch 7/10, Batch 508/883, Training Loss: 0.5464
Epoch 7/10, Batch 509/883, Training Loss: 0.5029
Epoch 7/10, Batch 510/883, Training Loss: 0.7788
Epoch 7/10, Batch 511/883, Training Loss: 0.5636
Epoch 7/10, Batch 512/883, Training Loss: 0.6693
Epoch 7/10, Batch 513/883, Training Loss: 0.5084
Epoch 7/10, Batch 514/883, Training Loss: 0.9220
Epoch 7/10, Batch 515/883, Training Loss: 0.6901
Epoch 7/10, Batch 516/883, Training Loss: 0.6435
Epoch 7/10, Batch 517/883, Training Loss: 0.6044
Epoch 7/10, Batch 518/883, Training Loss: 0.7132
Epoch 7/10, Batch 519/883, Training Loss: 0.7087
Epoch 7/10, Batch 520/883, Training Loss: 1.1418
Epoch 7/10, Batch 521/883, Training Loss: 0.5168
Epoch 7/10, Batch 522/883, Training Loss: 0.5982
Epoch 7/10, Batch 523/883, Training Loss: 0.5668
Epoch 7/10, Batch 524/883, Training Loss: 0.6885
Epoch 7/10, Batch 525/883, Training Loss: 0.7117
Epoch 7/10, Batch 526/883, Training Loss: 0.6621
Epoch 7/10, Batch 527/883, Training Loss: 0.7177
Epoch 7/10, Batch 528/883, Training Loss: 0.6707
Epoch 7/10, Batch 529/883, Training Loss: 0.5951
Epoch 7/10, Batch 530/883, Training Loss: 0.8634
Epoch 7/10, Batch 531/883, Training Loss: 0.4497
Epoch 7/10, Batch 532/883, Training Loss: 0.6078
Epoch 7/10, Batch 533/883, Training Loss: 0.7365
Epoch 7/10, Batch 534/883, Training Loss: 0.5461
Epoch 7/10, Batch 535/883, Training Loss: 0.6090
Epoch 7/10, Batch 536/883, Training Loss: 0.6745
Epoch 7/10, Batch 537/883, Training Loss: 0.3874
Epoch 7/10, Batch 538/883, Training Loss: 0.5935
Epoch 7/10, Batch 539/883, Training Loss: 0.5198
Epoch 7/10, Batch 540/883, Training Loss: 0.7199
Epoch 7/10, Batch 541/883, Training Loss: 0.7877
Epoch 7/10, Batch 542/883, Training Loss: 0.5498
Epoch 7/10, Batch 543/883, Training Loss: 0.5154
Epoch 7/10, Batch 544/883, Training Loss: 0.6565
Epoch 7/10, Batch 545/883, Training Loss: 0.9812
Epoch 7/10, Batch 546/883, Training Loss: 0.8807
Epoch 7/10, Batch 547/883, Training Loss: 0.6135
Epoch 7/10, Batch 548/883, Training Loss: 0.7613
Epoch 7/10, Batch 549/883, Training Loss: 0.4288
Epoch 7/10, Batch 550/883, Training Loss: 0.4691
Epoch 7/10, Batch 551/883, Training Loss: 0.6969
Epoch 7/10, Batch 552/883, Training Loss: 0.4929
Epoch 7/10, Batch 553/883, Training Loss: 0.4197
Epoch 7/10, Batch 554/883, Training Loss: 0.6040
Epoch 7/10, Batch 555/883, Training Loss: 0.5188
Epoch 7/10, Batch 556/883, Training Loss: 1.0393
Epoch 7/10, Batch 557/883, Training Loss: 0.5938
Epoch 7/10, Batch 558/883, Training Loss: 0.4575
Epoch 7/10, Batch 559/883, Training Loss: 0.5588
Epoch 7/10, Batch 560/883, Training Loss: 0.4481
Epoch 7/10, Batch 561/883, Training Loss: 0.6579
Epoch 7/10, Batch 562/883, Training Loss: 0.6511
Epoch 7/10, Batch 563/883, Training Loss: 0.5418
Epoch 7/10, Batch 564/883, Training Loss: 0.5528
Epoch 7/10, Batch 565/883, Training Loss: 0.7842
Epoch 7/10, Batch 566/883, Training Loss: 0.8487
Epoch 7/10, Batch 567/883, Training Loss: 0.4628
Epoch 7/10, Batch 568/883, Training Loss: 0.7047
Epoch 7/10, Batch 569/883, Training Loss: 0.5281
Epoch 7/10, Batch 570/883, Training Loss: 0.6013
Epoch 7/10, Batch 571/883, Training Loss: 0.3706
Epoch 7/10, Batch 572/883, Training Loss: 0.8113
Epoch 7/10, Batch 573/883, Training Loss: 0.6378
Epoch 7/10, Batch 574/883, Training Loss: 0.4864
Epoch 7/10, Batch 575/883, Training Loss: 0.6424
Epoch 7/10, Batch 576/883, Training Loss: 0.4092
Epoch 7/10, Batch 577/883, Training Loss: 0.5517
Epoch 7/10, Batch 578/883, Training Loss: 0.5238
Epoch 7/10, Batch 579/883, Training Loss: 0.3719
Epoch 7/10, Batch 580/883, Training Loss: 0.4493
Epoch 7/10, Batch 581/883, Training Loss: 0.5482
Epoch 7/10, Batch 582/883, Training Loss: 0.7319
Epoch 7/10, Batch 583/883, Training Loss: 0.7677
Epoch 7/10, Batch 584/883, Training Loss: 0.5004
Epoch 7/10, Batch 585/883, Training Loss: 0.6780
Epoch 7/10, Batch 586/883, Training Loss: 0.6993
Epoch 7/10, Batch 587/883, Training Loss: 0.5532
Epoch 7/10, Batch 588/883, Training Loss: 0.6114
Epoch 7/10, Batch 589/883, Training Loss: 0.5213
Epoch 7/10, Batch 590/883, Training Loss: 0.5166
Epoch 7/10, Batch 591/883, Training Loss: 0.3591
Epoch 7/10, Batch 592/883, Training Loss: 0.7182
Epoch 7/10, Batch 593/883, Training Loss: 0.6649
Epoch 7/10, Batch 594/883, Training Loss: 0.4739
Epoch 7/10, Batch 595/883, Training Loss: 0.5009
Epoch 7/10, Batch 596/883, Training Loss: 0.6119
Epoch 7/10, Batch 597/883, Training Loss: 0.6909
Epoch 7/10, Batch 598/883, Training Loss: 0.8399
Epoch 7/10, Batch 599/883, Training Loss: 0.5416
Epoch 7/10, Batch 600/883, Training Loss: 0.6922
Epoch 7/10, Batch 601/883, Training Loss: 0.7194
Epoch 7/10, Batch 602/883, Training Loss: 0.5023
Epoch 7/10, Batch 603/883, Training Loss: 0.7085
Epoch 7/10, Batch 604/883, Training Loss: 0.7360
Epoch 7/10, Batch 605/883, Training Loss: 0.6716
Epoch 7/10, Batch 606/883, Training Loss: 0.5645
Epoch 7/10, Batch 607/883, Training Loss: 0.7292
Epoch 7/10, Batch 608/883, Training Loss: 0.4110
Epoch 7/10, Batch 609/883, Training Loss: 0.7174
Epoch 7/10, Batch 610/883, Training Loss: 0.9617
Epoch 7/10, Batch 611/883, Training Loss: 0.4565
Epoch 7/10, Batch 612/883, Training Loss: 0.8706
Epoch 7/10, Batch 613/883, Training Loss: 0.6459
Epoch 7/10, Batch 614/883, Training Loss: 0.8221
Epoch 7/10, Batch 615/883, Training Loss: 0.7535
Epoch 7/10, Batch 616/883, Training Loss: 0.5932
Epoch 7/10, Batch 617/883, Training Loss: 0.5352
Epoch 7/10, Batch 618/883, Training Loss: 0.6662
Epoch 7/10, Batch 619/883, Training Loss: 0.4970
Epoch 7/10, Batch 620/883, Training Loss: 0.4478
Epoch 7/10, Batch 621/883, Training Loss: 0.9732
Epoch 7/10, Batch 622/883, Training Loss: 0.4480
Epoch 7/10, Batch 623/883, Training Loss: 0.7629
Epoch 7/10, Batch 624/883, Training Loss: 0.4967
Epoch 7/10, Batch 625/883, Training Loss: 0.6238
Epoch 7/10, Batch 626/883, Training Loss: 0.5596
Epoch 7/10, Batch 627/883, Training Loss: 0.7938
Epoch 7/10, Batch 628/883, Training Loss: 0.4167
Epoch 7/10, Batch 629/883, Training Loss: 0.7798
Epoch 7/10, Batch 630/883, Training Loss: 0.5896
Epoch 7/10, Batch 631/883, Training Loss: 0.6635
Epoch 7/10, Batch 632/883, Training Loss: 0.4690
Epoch 7/10, Batch 633/883, Training Loss: 0.5300
Epoch 7/10, Batch 634/883, Training Loss: 0.4819
Epoch 7/10, Batch 635/883, Training Loss: 0.5523
Epoch 7/10, Batch 636/883, Training Loss: 0.7039
Epoch 7/10, Batch 637/883, Training Loss: 0.5183
Epoch 7/10, Batch 638/883, Training Loss: 0.4800
Epoch 7/10, Batch 639/883, Training Loss: 0.8049
Epoch 7/10, Batch 640/883, Training Loss: 0.4128
Epoch 7/10, Batch 641/883, Training Loss: 0.7344
Epoch 7/10, Batch 642/883, Training Loss: 0.6304
Epoch 7/10, Batch 643/883, Training Loss: 0.3913
Epoch 7/10, Batch 644/883, Training Loss: 0.6753
Epoch 7/10, Batch 645/883, Training Loss: 0.5103
Epoch 7/10, Batch 646/883, Training Loss: 0.4233
Epoch 7/10, Batch 647/883, Training Loss: 0.4116
Epoch 7/10, Batch 648/883, Training Loss: 0.3187
Epoch 7/10, Batch 649/883, Training Loss: 0.6541
Epoch 7/10, Batch 650/883, Training Loss: 0.7834
Epoch 7/10, Batch 651/883, Training Loss: 0.5296
Epoch 7/10, Batch 652/883, Training Loss: 0.4579
Epoch 7/10, Batch 653/883, Training Loss: 0.4739
Epoch 7/10, Batch 654/883, Training Loss: 0.6430
Epoch 7/10, Batch 655/883, Training Loss: 0.6897
Epoch 7/10, Batch 656/883, Training Loss: 0.3784
Epoch 7/10, Batch 657/883, Training Loss: 0.8830
Epoch 7/10, Batch 658/883, Training Loss: 0.6777
Epoch 7/10, Batch 659/883, Training Loss: 0.6973
Epoch 7/10, Batch 660/883, Training Loss: 0.4010
Epoch 7/10, Batch 661/883, Training Loss: 0.3505
Epoch 7/10, Batch 662/883, Training Loss: 0.7421
Epoch 7/10, Batch 663/883, Training Loss: 0.5986
Epoch 7/10, Batch 664/883, Training Loss: 0.8267
Epoch 7/10, Batch 665/883, Training Loss: 0.6790
Epoch 7/10, Batch 666/883, Training Loss: 1.4639
Epoch 7/10, Batch 667/883, Training Loss: 0.6688
Epoch 7/10, Batch 668/883, Training Loss: 0.6764
Epoch 7/10, Batch 669/883, Training Loss: 0.6389
Epoch 7/10, Batch 670/883, Training Loss: 0.5205
Epoch 7/10, Batch 671/883, Training Loss: 0.5857
Epoch 7/10, Batch 672/883, Training Loss: 0.5722
Epoch 7/10, Batch 673/883, Training Loss: 0.6547
Epoch 7/10, Batch 674/883, Training Loss: 0.8058
Epoch 7/10, Batch 675/883, Training Loss: 0.5096
Epoch 7/10, Batch 676/883, Training Loss: 0.4672
Epoch 7/10, Batch 677/883, Training Loss: 0.4107
Epoch 7/10, Batch 678/883, Training Loss: 0.4805
Epoch 7/10, Batch 679/883, Training Loss: 0.6052
Epoch 7/10, Batch 680/883, Training Loss: 0.6621
Epoch 7/10, Batch 681/883, Training Loss: 0.7403
Epoch 7/10, Batch 682/883, Training Loss: 0.7203
Epoch 7/10, Batch 683/883, Training Loss: 0.4552
Epoch 7/10, Batch 684/883, Training Loss: 0.8158
Epoch 7/10, Batch 685/883, Training Loss: 0.5115
Epoch 7/10, Batch 686/883, Training Loss: 0.5903
Epoch 7/10, Batch 687/883, Training Loss: 0.9966
Epoch 7/10, Batch 688/883, Training Loss: 0.6640
Epoch 7/10, Batch 689/883, Training Loss: 0.5017
Epoch 7/10, Batch 690/883, Training Loss: 0.7633
Epoch 7/10, Batch 691/883, Training Loss: 0.3706
Epoch 7/10, Batch 692/883, Training Loss: 0.3886
Epoch 7/10, Batch 693/883, Training Loss: 0.5094
Epoch 7/10, Batch 694/883, Training Loss: 0.5418
Epoch 7/10, Batch 695/883, Training Loss: 0.5931
Epoch 7/10, Batch 696/883, Training Loss: 0.9194
Epoch 7/10, Batch 697/883, Training Loss: 0.8461
Epoch 7/10, Batch 698/883, Training Loss: 0.6459
Epoch 7/10, Batch 699/883, Training Loss: 0.7961
Epoch 7/10, Batch 700/883, Training Loss: 0.6236
Epoch 7/10, Batch 701/883, Training Loss: 0.8232
Epoch 7/10, Batch 702/883, Training Loss: 0.4812
Epoch 7/10, Batch 703/883, Training Loss: 0.9563
Epoch 7/10, Batch 704/883, Training Loss: 1.3190
Epoch 7/10, Batch 705/883, Training Loss: 0.6230
Epoch 7/10, Batch 706/883, Training Loss: 0.5974
Epoch 7/10, Batch 707/883, Training Loss: 0.4765
Epoch 7/10, Batch 708/883, Training Loss: 0.5845
Epoch 7/10, Batch 709/883, Training Loss: 0.5794
Epoch 7/10, Batch 710/883, Training Loss: 0.4208
Epoch 7/10, Batch 711/883, Training Loss: 0.6261
Epoch 7/10, Batch 712/883, Training Loss: 0.5780
Epoch 7/10, Batch 713/883, Training Loss: 0.9516
Epoch 7/10, Batch 714/883, Training Loss: 0.7847
Epoch 7/10, Batch 715/883, Training Loss: 0.5549
Epoch 7/10, Batch 716/883, Training Loss: 0.6255
Epoch 7/10, Batch 717/883, Training Loss: 0.9656
Epoch 7/10, Batch 718/883, Training Loss: 0.5660
Epoch 7/10, Batch 719/883, Training Loss: 0.8265
Epoch 7/10, Batch 720/883, Training Loss: 0.4733
Epoch 7/10, Batch 721/883, Training Loss: 0.6640
Epoch 7/10, Batch 722/883, Training Loss: 0.8273
Epoch 7/10, Batch 723/883, Training Loss: 0.7023
Epoch 7/10, Batch 724/883, Training Loss: 0.5835
Epoch 7/10, Batch 725/883, Training Loss: 0.9103
Epoch 7/10, Batch 726/883, Training Loss: 0.4933
Epoch 7/10, Batch 727/883, Training Loss: 0.6895
Epoch 7/10, Batch 728/883, Training Loss: 0.7508
Epoch 7/10, Batch 729/883, Training Loss: 0.5082
Epoch 7/10, Batch 730/883, Training Loss: 0.6878
Epoch 7/10, Batch 731/883, Training Loss: 0.4597
Epoch 7/10, Batch 732/883, Training Loss: 0.6833
Epoch 7/10, Batch 733/883, Training Loss: 0.6180
Epoch 7/10, Batch 734/883, Training Loss: 0.4215
Epoch 7/10, Batch 735/883, Training Loss: 0.6226
Epoch 7/10, Batch 736/883, Training Loss: 0.8496
Epoch 7/10, Batch 737/883, Training Loss: 0.6692
Epoch 7/10, Batch 738/883, Training Loss: 0.7683
Epoch 7/10, Batch 739/883, Training Loss: 0.4944
Epoch 7/10, Batch 740/883, Training Loss: 0.7213
Epoch 7/10, Batch 741/883, Training Loss: 0.6315
Epoch 7/10, Batch 742/883, Training Loss: 0.4346
Epoch 7/10, Batch 743/883, Training Loss: 0.5364
Epoch 7/10, Batch 744/883, Training Loss: 0.4323
Epoch 7/10, Batch 745/883, Training Loss: 0.8634
Epoch 7/10, Batch 746/883, Training Loss: 0.5591
Epoch 7/10, Batch 747/883, Training Loss: 0.5860
Epoch 7/10, Batch 748/883, Training Loss: 0.4536
Epoch 7/10, Batch 749/883, Training Loss: 0.5437
Epoch 7/10, Batch 750/883, Training Loss: 0.9434
Epoch 7/10, Batch 751/883, Training Loss: 0.7847
Epoch 7/10, Batch 752/883, Training Loss: 0.8186
Epoch 7/10, Batch 753/883, Training Loss: 1.0487
Epoch 7/10, Batch 754/883, Training Loss: 0.5340
Epoch 7/10, Batch 755/883, Training Loss: 0.7942
Epoch 7/10, Batch 756/883, Training Loss: 0.6237
Epoch 7/10, Batch 757/883, Training Loss: 1.0024
Epoch 7/10, Batch 758/883, Training Loss: 0.7410
Epoch 7/10, Batch 759/883, Training Loss: 0.4381
Epoch 7/10, Batch 760/883, Training Loss: 0.4769
Epoch 7/10, Batch 761/883, Training Loss: 0.6549
Epoch 7/10, Batch 762/883, Training Loss: 0.4944
Epoch 7/10, Batch 763/883, Training Loss: 0.6143
Epoch 7/10, Batch 764/883, Training Loss: 0.6735
Epoch 7/10, Batch 765/883, Training Loss: 0.5128
Epoch 7/10, Batch 766/883, Training Loss: 0.5142
Epoch 7/10, Batch 767/883, Training Loss: 0.6345
Epoch 7/10, Batch 768/883, Training Loss: 0.6780
Epoch 7/10, Batch 769/883, Training Loss: 0.6504
Epoch 7/10, Batch 770/883, Training Loss: 0.7241
Epoch 7/10, Batch 771/883, Training Loss: 0.9025
Epoch 7/10, Batch 772/883, Training Loss: 0.3941
Epoch 7/10, Batch 773/883, Training Loss: 0.4817
Epoch 7/10, Batch 774/883, Training Loss: 0.4717
Epoch 7/10, Batch 775/883, Training Loss: 0.7403
Epoch 7/10, Batch 776/883, Training Loss: 0.9732
Epoch 7/10, Batch 777/883, Training Loss: 0.5947
Epoch 7/10, Batch 778/883, Training Loss: 0.8536
Epoch 7/10, Batch 779/883, Training Loss: 0.5718
Epoch 7/10, Batch 780/883, Training Loss: 0.6059
Epoch 7/10, Batch 781/883, Training Loss: 0.5656
Epoch 7/10, Batch 782/883, Training Loss: 0.6796
Epoch 7/10, Batch 783/883, Training Loss: 0.8004
Epoch 7/10, Batch 784/883, Training Loss: 0.8295
Epoch 7/10, Batch 785/883, Training Loss: 0.8376
Epoch 7/10, Batch 786/883, Training Loss: 0.4971
Epoch 7/10, Batch 787/883, Training Loss: 0.6472
Epoch 7/10, Batch 788/883, Training Loss: 0.6469
Epoch 7/10, Batch 789/883, Training Loss: 0.9654
Epoch 7/10, Batch 790/883, Training Loss: 0.6402
Epoch 7/10, Batch 791/883, Training Loss: 0.6996
Epoch 7/10, Batch 792/883, Training Loss: 0.5123
Epoch 7/10, Batch 793/883, Training Loss: 0.6536
Epoch 7/10, Batch 794/883, Training Loss: 0.8341
Epoch 7/10, Batch 795/883, Training Loss: 0.6581
Epoch 7/10, Batch 796/883, Training Loss: 0.9786
Epoch 7/10, Batch 797/883, Training Loss: 0.8774
Epoch 7/10, Batch 798/883, Training Loss: 0.7345
Epoch 7/10, Batch 799/883, Training Loss: 0.3748
Epoch 7/10, Batch 800/883, Training Loss: 0.7338
Epoch 7/10, Batch 801/883, Training Loss: 0.9182
Epoch 7/10, Batch 802/883, Training Loss: 0.7800
Epoch 7/10, Batch 803/883, Training Loss: 1.2102
Epoch 7/10, Batch 804/883, Training Loss: 0.4330
Epoch 7/10, Batch 805/883, Training Loss: 0.8648
Epoch 7/10, Batch 806/883, Training Loss: 0.6774
Epoch 7/10, Batch 807/883, Training Loss: 0.7692
Epoch 7/10, Batch 808/883, Training Loss: 0.3930
Epoch 7/10, Batch 809/883, Training Loss: 0.7784
Epoch 7/10, Batch 810/883, Training Loss: 0.6135
Epoch 7/10, Batch 811/883, Training Loss: 0.4239
Epoch 7/10, Batch 812/883, Training Loss: 0.6269
Epoch 7/10, Batch 813/883, Training Loss: 0.5836
Epoch 7/10, Batch 814/883, Training Loss: 0.5664
Epoch 7/10, Batch 815/883, Training Loss: 0.4327
Epoch 7/10, Batch 816/883, Training Loss: 0.7877
Epoch 7/10, Batch 817/883, Training Loss: 0.5375
Epoch 7/10, Batch 818/883, Training Loss: 1.2469
Epoch 7/10, Batch 819/883, Training Loss: 0.5858
Epoch 7/10, Batch 820/883, Training Loss: 0.7447
Epoch 7/10, Batch 821/883, Training Loss: 0.4279
Epoch 7/10, Batch 822/883, Training Loss: 0.5665
Epoch 7/10, Batch 823/883, Training Loss: 0.6291
Epoch 7/10, Batch 824/883, Training Loss: 0.5527
Epoch 7/10, Batch 825/883, Training Loss: 0.9913
Epoch 7/10, Batch 826/883, Training Loss: 0.5829
Epoch 7/10, Batch 827/883, Training Loss: 0.8555
Epoch 7/10, Batch 828/883, Training Loss: 0.6076
Epoch 7/10, Batch 829/883, Training Loss: 0.4818
Epoch 7/10, Batch 830/883, Training Loss: 0.7688
Epoch 7/10, Batch 831/883, Training Loss: 0.7723
Epoch 7/10, Batch 832/883, Training Loss: 1.0719
Epoch 7/10, Batch 833/883, Training Loss: 0.5427
Epoch 7/10, Batch 834/883, Training Loss: 0.6498
Epoch 7/10, Batch 835/883, Training Loss: 0.8645
Epoch 7/10, Batch 836/883, Training Loss: 0.6993
Epoch 7/10, Batch 837/883, Training Loss: 0.6868
Epoch 7/10, Batch 838/883, Training Loss: 0.8520
Epoch 7/10, Batch 839/883, Training Loss: 0.4416
Epoch 7/10, Batch 840/883, Training Loss: 0.4867
Epoch 7/10, Batch 841/883, Training Loss: 0.5217
Epoch 7/10, Batch 842/883, Training Loss: 0.5964
Epoch 7/10, Batch 843/883, Training Loss: 0.4521
Epoch 7/10, Batch 844/883, Training Loss: 0.5380
Epoch 7/10, Batch 845/883, Training Loss: 0.8041
Epoch 7/10, Batch 846/883, Training Loss: 0.8231
Epoch 7/10, Batch 847/883, Training Loss: 0.9564
Epoch 7/10, Batch 848/883, Training Loss: 0.7074
Epoch 7/10, Batch 849/883, Training Loss: 0.6504
Epoch 7/10, Batch 850/883, Training Loss: 0.9996
Epoch 7/10, Batch 851/883, Training Loss: 0.5545
Epoch 7/10, Batch 852/883, Training Loss: 0.5099
Epoch 7/10, Batch 853/883, Training Loss: 0.6030
Epoch 7/10, Batch 854/883, Training Loss: 0.5625
Epoch 7/10, Batch 855/883, Training Loss: 0.9916
Epoch 7/10, Batch 856/883, Training Loss: 0.4528
Epoch 7/10, Batch 857/883, Training Loss: 1.0177
Epoch 7/10, Batch 858/883, Training Loss: 0.5413
Epoch 7/10, Batch 859/883, Training Loss: 0.4423
Epoch 7/10, Batch 860/883, Training Loss: 0.7020
Epoch 7/10, Batch 861/883, Training Loss: 0.4806
Epoch 7/10, Batch 862/883, Training Loss: 0.5375
Epoch 7/10, Batch 863/883, Training Loss: 0.6601
Epoch 7/10, Batch 864/883, Training Loss: 0.5659
Epoch 7/10, Batch 865/883, Training Loss: 0.6661
Epoch 7/10, Batch 866/883, Training Loss: 0.4586
Epoch 7/10, Batch 867/883, Training Loss: 0.4948
Epoch 7/10, Batch 868/883, Training Loss: 0.6442
Epoch 7/10, Batch 869/883, Training Loss: 0.6795
Epoch 7/10, Batch 870/883, Training Loss: 0.4449
Epoch 7/10, Batch 871/883, Training Loss: 0.8780
Epoch 7/10, Batch 872/883, Training Loss: 0.5505
Epoch 7/10, Batch 873/883, Training Loss: 1.0452
Epoch 7/10, Batch 874/883, Training Loss: 0.7225
Epoch 7/10, Batch 875/883, Training Loss: 0.4431
Epoch 7/10, Batch 876/883, Training Loss: 0.6998
Epoch 7/10, Batch 877/883, Training Loss: 0.9558
Epoch 7/10, Batch 878/883, Training Loss: 0.4744
Epoch 7/10, Batch 879/883, Training Loss: 0.6015
Epoch 7/10, Batch 880/883, Training Loss: 0.5302
Epoch 7/10, Batch 881/883, Training Loss: 0.7653
Epoch 7/10, Batch 882/883, Training Loss: 0.6553
Epoch 7/10, Batch 883/883, Training Loss: 0.3458
Epoch 7/10, Training Loss: 0.6586, Validation Loss: 0.6359, Validation Accuracy: 0.7183
Epoch 8/10, Batch 1/883, Training Loss: 0.6986
Epoch 8/10, Batch 2/883, Training Loss: 0.6010
Epoch 8/10, Batch 3/883, Training Loss: 0.6699
Epoch 8/10, Batch 4/883, Training Loss: 0.6095
Epoch 8/10, Batch 5/883, Training Loss: 0.4532
Epoch 8/10, Batch 6/883, Training Loss: 0.7242
Epoch 8/10, Batch 7/883, Training Loss: 0.4661
Epoch 8/10, Batch 8/883, Training Loss: 0.6726
Epoch 8/10, Batch 9/883, Training Loss: 0.6670
Epoch 8/10, Batch 10/883, Training Loss: 0.8323
Epoch 8/10, Batch 11/883, Training Loss: 0.8899
Epoch 8/10, Batch 12/883, Training Loss: 0.3561
Epoch 8/10, Batch 13/883, Training Loss: 0.5957
Epoch 8/10, Batch 14/883, Training Loss: 0.4437
Epoch 8/10, Batch 15/883, Training Loss: 0.5957
Epoch 8/10, Batch 16/883, Training Loss: 0.4794
Epoch 8/10, Batch 17/883, Training Loss: 0.8968
Epoch 8/10, Batch 18/883, Training Loss: 0.4041
Epoch 8/10, Batch 19/883, Training Loss: 0.4530
Epoch 8/10, Batch 20/883, Training Loss: 0.6889
Epoch 8/10, Batch 21/883, Training Loss: 0.7198
Epoch 8/10, Batch 22/883, Training Loss: 0.6064
Epoch 8/10, Batch 23/883, Training Loss: 0.7331
Epoch 8/10, Batch 24/883, Training Loss: 0.5443
Epoch 8/10, Batch 25/883, Training Loss: 0.5538
Epoch 8/10, Batch 26/883, Training Loss: 0.7577
Epoch 8/10, Batch 27/883, Training Loss: 0.4634
Epoch 8/10, Batch 28/883, Training Loss: 0.5695
Epoch 8/10, Batch 29/883, Training Loss: 0.5393
Epoch 8/10, Batch 30/883, Training Loss: 0.7109
Epoch 8/10, Batch 31/883, Training Loss: 0.5282
Epoch 8/10, Batch 32/883, Training Loss: 0.6617
Epoch 8/10, Batch 33/883, Training Loss: 0.5047
Epoch 8/10, Batch 34/883, Training Loss: 0.6215
Epoch 8/10, Batch 35/883, Training Loss: 0.7765
Epoch 8/10, Batch 36/883, Training Loss: 0.6370
Epoch 8/10, Batch 37/883, Training Loss: 0.7476
Epoch 8/10, Batch 38/883, Training Loss: 0.7537
Epoch 8/10, Batch 39/883, Training Loss: 0.8184
Epoch 8/10, Batch 40/883, Training Loss: 0.7231
Epoch 8/10, Batch 41/883, Training Loss: 0.4271
Epoch 8/10, Batch 42/883, Training Loss: 0.7291
Epoch 8/10, Batch 43/883, Training Loss: 0.6336
Epoch 8/10, Batch 44/883, Training Loss: 0.6950
Epoch 8/10, Batch 45/883, Training Loss: 0.4783
Epoch 8/10, Batch 46/883, Training Loss: 0.2934
Epoch 8/10, Batch 47/883, Training Loss: 0.9001
Epoch 8/10, Batch 48/883, Training Loss: 0.5993
Epoch 8/10, Batch 49/883, Training Loss: 0.7804
Epoch 8/10, Batch 50/883, Training Loss: 0.5826
Epoch 8/10, Batch 51/883, Training Loss: 0.5344
Epoch 8/10, Batch 52/883, Training Loss: 0.5563
Epoch 8/10, Batch 53/883, Training Loss: 0.3984
Epoch 8/10, Batch 54/883, Training Loss: 0.7137
Epoch 8/10, Batch 55/883, Training Loss: 0.6785
Epoch 8/10, Batch 56/883, Training Loss: 0.3474
Epoch 8/10, Batch 57/883, Training Loss: 0.9447
Epoch 8/10, Batch 58/883, Training Loss: 0.5214
Epoch 8/10, Batch 59/883, Training Loss: 0.6066
Epoch 8/10, Batch 60/883, Training Loss: 0.4315
Epoch 8/10, Batch 61/883, Training Loss: 0.4773
Epoch 8/10, Batch 62/883, Training Loss: 0.5344
Epoch 8/10, Batch 63/883, Training Loss: 0.5462
Epoch 8/10, Batch 64/883, Training Loss: 0.3490
Epoch 8/10, Batch 65/883, Training Loss: 0.5992
Epoch 8/10, Batch 66/883, Training Loss: 0.4940
Epoch 8/10, Batch 67/883, Training Loss: 0.7792
Epoch 8/10, Batch 68/883, Training Loss: 0.6491
Epoch 8/10, Batch 69/883, Training Loss: 0.7515
Epoch 8/10, Batch 70/883, Training Loss: 0.5813
Epoch 8/10, Batch 71/883, Training Loss: 0.4370
Epoch 8/10, Batch 72/883, Training Loss: 0.5107
Epoch 8/10, Batch 73/883, Training Loss: 0.4833
Epoch 8/10, Batch 74/883, Training Loss: 0.4191
Epoch 8/10, Batch 75/883, Training Loss: 0.6254
Epoch 8/10, Batch 76/883, Training Loss: 0.8726
Epoch 8/10, Batch 77/883, Training Loss: 0.5645
Epoch 8/10, Batch 78/883, Training Loss: 0.8615
Epoch 8/10, Batch 79/883, Training Loss: 0.6132
Epoch 8/10, Batch 80/883, Training Loss: 0.7724
Epoch 8/10, Batch 81/883, Training Loss: 0.3659
Epoch 8/10, Batch 82/883, Training Loss: 0.4647
Epoch 8/10, Batch 83/883, Training Loss: 0.5564
Epoch 8/10, Batch 84/883, Training Loss: 0.5690
Epoch 8/10, Batch 85/883, Training Loss: 0.5200
Epoch 8/10, Batch 86/883, Training Loss: 0.3497
Epoch 8/10, Batch 87/883, Training Loss: 0.7194
Epoch 8/10, Batch 88/883, Training Loss: 0.5395
Epoch 8/10, Batch 89/883, Training Loss: 0.9728
Epoch 8/10, Batch 90/883, Training Loss: 0.5707
Epoch 8/10, Batch 91/883, Training Loss: 0.7940
Epoch 8/10, Batch 92/883, Training Loss: 0.4929
Epoch 8/10, Batch 93/883, Training Loss: 0.4320
Epoch 8/10, Batch 94/883, Training Loss: 0.7025
Epoch 8/10, Batch 95/883, Training Loss: 0.8220
Epoch 8/10, Batch 96/883, Training Loss: 0.8587
Epoch 8/10, Batch 97/883, Training Loss: 0.6844
Epoch 8/10, Batch 98/883, Training Loss: 0.7452
Epoch 8/10, Batch 99/883, Training Loss: 0.2490
Epoch 8/10, Batch 100/883, Training Loss: 0.7417
Epoch 8/10, Batch 101/883, Training Loss: 1.1072
Epoch 8/10, Batch 102/883, Training Loss: 0.7564
Epoch 8/10, Batch 103/883, Training Loss: 0.7308
Epoch 8/10, Batch 104/883, Training Loss: 0.7500
Epoch 8/10, Batch 105/883, Training Loss: 0.5741
Epoch 8/10, Batch 106/883, Training Loss: 0.5627
Epoch 8/10, Batch 107/883, Training Loss: 0.5550
Epoch 8/10, Batch 108/883, Training Loss: 0.6015
Epoch 8/10, Batch 109/883, Training Loss: 0.4897
Epoch 8/10, Batch 110/883, Training Loss: 0.5905
Epoch 8/10, Batch 111/883, Training Loss: 0.8925
Epoch 8/10, Batch 112/883, Training Loss: 0.7023
Epoch 8/10, Batch 113/883, Training Loss: 0.6697
Epoch 8/10, Batch 114/883, Training Loss: 1.2376
Epoch 8/10, Batch 115/883, Training Loss: 0.3786
Epoch 8/10, Batch 116/883, Training Loss: 0.3497
Epoch 8/10, Batch 117/883, Training Loss: 0.4867
Epoch 8/10, Batch 118/883, Training Loss: 0.5951
Epoch 8/10, Batch 119/883, Training Loss: 0.5353
Epoch 8/10, Batch 120/883, Training Loss: 0.6192
Epoch 8/10, Batch 121/883, Training Loss: 0.4723
Epoch 8/10, Batch 122/883, Training Loss: 0.7093
Epoch 8/10, Batch 123/883, Training Loss: 0.6479
Epoch 8/10, Batch 124/883, Training Loss: 0.9667
Epoch 8/10, Batch 125/883, Training Loss: 0.4194
Epoch 8/10, Batch 126/883, Training Loss: 0.4855
Epoch 8/10, Batch 127/883, Training Loss: 0.7322
Epoch 8/10, Batch 128/883, Training Loss: 0.3468
Epoch 8/10, Batch 129/883, Training Loss: 0.5847
Epoch 8/10, Batch 130/883, Training Loss: 0.5442
Epoch 8/10, Batch 131/883, Training Loss: 0.3983
Epoch 8/10, Batch 132/883, Training Loss: 0.6164
Epoch 8/10, Batch 133/883, Training Loss: 0.6431
Epoch 8/10, Batch 134/883, Training Loss: 0.8097
Epoch 8/10, Batch 135/883, Training Loss: 0.5044
Epoch 8/10, Batch 136/883, Training Loss: 0.3790
Epoch 8/10, Batch 137/883, Training Loss: 0.4676
Epoch 8/10, Batch 138/883, Training Loss: 0.8604
Epoch 8/10, Batch 139/883, Training Loss: 0.8456
Epoch 8/10, Batch 140/883, Training Loss: 1.0040
Epoch 8/10, Batch 141/883, Training Loss: 0.7053
Epoch 8/10, Batch 142/883, Training Loss: 0.6999
Epoch 8/10, Batch 143/883, Training Loss: 0.6074
Epoch 8/10, Batch 144/883, Training Loss: 0.3353
Epoch 8/10, Batch 145/883, Training Loss: 0.4548
Epoch 8/10, Batch 146/883, Training Loss: 0.7284
Epoch 8/10, Batch 147/883, Training Loss: 0.6416
Epoch 8/10, Batch 148/883, Training Loss: 1.0383
Epoch 8/10, Batch 149/883, Training Loss: 0.7208
Epoch 8/10, Batch 150/883, Training Loss: 0.8568
Epoch 8/10, Batch 151/883, Training Loss: 0.5970
Epoch 8/10, Batch 152/883, Training Loss: 0.8465
Epoch 8/10, Batch 153/883, Training Loss: 0.5198
Epoch 8/10, Batch 154/883, Training Loss: 0.4627
Epoch 8/10, Batch 155/883, Training Loss: 0.5732
Epoch 8/10, Batch 156/883, Training Loss: 0.6149
Epoch 8/10, Batch 157/883, Training Loss: 0.4550
Epoch 8/10, Batch 158/883, Training Loss: 0.7223
Epoch 8/10, Batch 159/883, Training Loss: 0.6399
Epoch 8/10, Batch 160/883, Training Loss: 0.8403
Epoch 8/10, Batch 161/883, Training Loss: 0.5712
Epoch 8/10, Batch 162/883, Training Loss: 0.6080
Epoch 8/10, Batch 163/883, Training Loss: 0.5616
Epoch 8/10, Batch 164/883, Training Loss: 0.7064
Epoch 8/10, Batch 165/883, Training Loss: 0.4716
Epoch 8/10, Batch 166/883, Training Loss: 0.3749
Epoch 8/10, Batch 167/883, Training Loss: 0.6894
Epoch 8/10, Batch 168/883, Training Loss: 0.4395
Epoch 8/10, Batch 169/883, Training Loss: 0.7713
Epoch 8/10, Batch 170/883, Training Loss: 1.0596
Epoch 8/10, Batch 171/883, Training Loss: 0.6149
Epoch 8/10, Batch 172/883, Training Loss: 0.5121
Epoch 8/10, Batch 173/883, Training Loss: 0.6460
Epoch 8/10, Batch 174/883, Training Loss: 0.5708
Epoch 8/10, Batch 175/883, Training Loss: 0.8072
Epoch 8/10, Batch 176/883, Training Loss: 0.6749
Epoch 8/10, Batch 177/883, Training Loss: 0.9042
Epoch 8/10, Batch 178/883, Training Loss: 0.8130
Epoch 8/10, Batch 179/883, Training Loss: 0.4147
Epoch 8/10, Batch 180/883, Training Loss: 0.5401
Epoch 8/10, Batch 181/883, Training Loss: 0.8454
Epoch 8/10, Batch 182/883, Training Loss: 0.5731
Epoch 8/10, Batch 183/883, Training Loss: 0.5077
Epoch 8/10, Batch 184/883, Training Loss: 0.6899
Epoch 8/10, Batch 185/883, Training Loss: 0.6243
Epoch 8/10, Batch 186/883, Training Loss: 0.4324
Epoch 8/10, Batch 187/883, Training Loss: 0.7521
Epoch 8/10, Batch 188/883, Training Loss: 0.8463
Epoch 8/10, Batch 189/883, Training Loss: 1.0025
Epoch 8/10, Batch 190/883, Training Loss: 0.6241
Epoch 8/10, Batch 191/883, Training Loss: 0.8162
Epoch 8/10, Batch 192/883, Training Loss: 0.7630
Epoch 8/10, Batch 193/883, Training Loss: 0.5156
Epoch 8/10, Batch 194/883, Training Loss: 0.9138
Epoch 8/10, Batch 195/883, Training Loss: 0.5619
Epoch 8/10, Batch 196/883, Training Loss: 0.5985
Epoch 8/10, Batch 197/883, Training Loss: 0.6679
Epoch 8/10, Batch 198/883, Training Loss: 0.7192
Epoch 8/10, Batch 199/883, Training Loss: 0.5039
Epoch 8/10, Batch 200/883, Training Loss: 0.7508
Epoch 8/10, Batch 201/883, Training Loss: 0.5864
Epoch 8/10, Batch 202/883, Training Loss: 0.6729
Epoch 8/10, Batch 203/883, Training Loss: 0.7469
Epoch 8/10, Batch 204/883, Training Loss: 0.6414
Epoch 8/10, Batch 205/883, Training Loss: 0.7870
Epoch 8/10, Batch 206/883, Training Loss: 0.7550
Epoch 8/10, Batch 207/883, Training Loss: 0.6211
Epoch 8/10, Batch 208/883, Training Loss: 0.6465
Epoch 8/10, Batch 209/883, Training Loss: 0.4393
Epoch 8/10, Batch 210/883, Training Loss: 0.7407
Epoch 8/10, Batch 211/883, Training Loss: 1.1063
Epoch 8/10, Batch 212/883, Training Loss: 0.8680
Epoch 8/10, Batch 213/883, Training Loss: 0.5987
Epoch 8/10, Batch 214/883, Training Loss: 0.4889
Epoch 8/10, Batch 215/883, Training Loss: 0.3989
Epoch 8/10, Batch 216/883, Training Loss: 0.4227
Epoch 8/10, Batch 217/883, Training Loss: 0.7391
Epoch 8/10, Batch 218/883, Training Loss: 0.4578
Epoch 8/10, Batch 219/883, Training Loss: 0.6482
Epoch 8/10, Batch 220/883, Training Loss: 0.3876
Epoch 8/10, Batch 221/883, Training Loss: 0.6990
Epoch 8/10, Batch 222/883, Training Loss: 0.6685
Epoch 8/10, Batch 223/883, Training Loss: 0.9340
Epoch 8/10, Batch 224/883, Training Loss: 0.5229
Epoch 8/10, Batch 225/883, Training Loss: 0.4705
Epoch 8/10, Batch 226/883, Training Loss: 0.5621
Epoch 8/10, Batch 227/883, Training Loss: 0.9832
Epoch 8/10, Batch 228/883, Training Loss: 0.4056
Epoch 8/10, Batch 229/883, Training Loss: 0.6719
Epoch 8/10, Batch 230/883, Training Loss: 0.8211
Epoch 8/10, Batch 231/883, Training Loss: 0.5785
Epoch 8/10, Batch 232/883, Training Loss: 0.5899
Epoch 8/10, Batch 233/883, Training Loss: 0.8257
Epoch 8/10, Batch 234/883, Training Loss: 0.5693
Epoch 8/10, Batch 235/883, Training Loss: 0.4046
Epoch 8/10, Batch 236/883, Training Loss: 0.4022
Epoch 8/10, Batch 237/883, Training Loss: 0.7009
Epoch 8/10, Batch 238/883, Training Loss: 0.4932
Epoch 8/10, Batch 239/883, Training Loss: 0.4787
Epoch 8/10, Batch 240/883, Training Loss: 0.8192
Epoch 8/10, Batch 241/883, Training Loss: 0.6108
Epoch 8/10, Batch 242/883, Training Loss: 0.5642
Epoch 8/10, Batch 243/883, Training Loss: 0.4063
Epoch 8/10, Batch 244/883, Training Loss: 0.5798
Epoch 8/10, Batch 245/883, Training Loss: 0.8509
Epoch 8/10, Batch 246/883, Training Loss: 0.4357
Epoch 8/10, Batch 247/883, Training Loss: 0.6226
Epoch 8/10, Batch 248/883, Training Loss: 0.4937
Epoch 8/10, Batch 249/883, Training Loss: 0.6732
Epoch 8/10, Batch 250/883, Training Loss: 0.5466
Epoch 8/10, Batch 251/883, Training Loss: 0.6139
Epoch 8/10, Batch 252/883, Training Loss: 0.4679
Epoch 8/10, Batch 253/883, Training Loss: 0.4080
Epoch 8/10, Batch 254/883, Training Loss: 0.4776
Epoch 8/10, Batch 255/883, Training Loss: 0.5371
Epoch 8/10, Batch 256/883, Training Loss: 0.6038
Epoch 8/10, Batch 257/883, Training Loss: 1.1765
Epoch 8/10, Batch 258/883, Training Loss: 0.6159
Epoch 8/10, Batch 259/883, Training Loss: 0.6913
Epoch 8/10, Batch 260/883, Training Loss: 0.6731
Epoch 8/10, Batch 261/883, Training Loss: 0.5656
Epoch 8/10, Batch 262/883, Training Loss: 0.4154
Epoch 8/10, Batch 263/883, Training Loss: 0.5057
Epoch 8/10, Batch 264/883, Training Loss: 0.3751
Epoch 8/10, Batch 265/883, Training Loss: 0.5074
Epoch 8/10, Batch 266/883, Training Loss: 0.5386
Epoch 8/10, Batch 267/883, Training Loss: 0.4776
Epoch 8/10, Batch 268/883, Training Loss: 0.4333
Epoch 8/10, Batch 269/883, Training Loss: 0.5305
Epoch 8/10, Batch 270/883, Training Loss: 0.9453
Epoch 8/10, Batch 271/883, Training Loss: 0.6416
Epoch 8/10, Batch 272/883, Training Loss: 0.4515
Epoch 8/10, Batch 273/883, Training Loss: 0.8611
Epoch 8/10, Batch 274/883, Training Loss: 0.5741
Epoch 8/10, Batch 275/883, Training Loss: 0.4534
Epoch 8/10, Batch 276/883, Training Loss: 0.7461
Epoch 8/10, Batch 277/883, Training Loss: 0.5863
Epoch 8/10, Batch 278/883, Training Loss: 0.7946
Epoch 8/10, Batch 279/883, Training Loss: 1.2891
Epoch 8/10, Batch 280/883, Training Loss: 0.6360
Epoch 8/10, Batch 281/883, Training Loss: 0.5635
Epoch 8/10, Batch 282/883, Training Loss: 0.8299
Epoch 8/10, Batch 283/883, Training Loss: 0.9073
Epoch 8/10, Batch 284/883, Training Loss: 0.7742
Epoch 8/10, Batch 285/883, Training Loss: 0.8427
Epoch 8/10, Batch 286/883, Training Loss: 0.4392
Epoch 8/10, Batch 287/883, Training Loss: 0.4775
Epoch 8/10, Batch 288/883, Training Loss: 0.6558
Epoch 8/10, Batch 289/883, Training Loss: 0.4753
Epoch 8/10, Batch 290/883, Training Loss: 0.6226
Epoch 8/10, Batch 291/883, Training Loss: 1.0972
Epoch 8/10, Batch 292/883, Training Loss: 0.6575
Epoch 8/10, Batch 293/883, Training Loss: 0.5461
Epoch 8/10, Batch 294/883, Training Loss: 0.6484
Epoch 8/10, Batch 295/883, Training Loss: 0.5067
Epoch 8/10, Batch 296/883, Training Loss: 0.6964
Epoch 8/10, Batch 297/883, Training Loss: 0.3536
Epoch 8/10, Batch 298/883, Training Loss: 0.8060
Epoch 8/10, Batch 299/883, Training Loss: 0.5337
Epoch 8/10, Batch 300/883, Training Loss: 0.5370
Epoch 8/10, Batch 301/883, Training Loss: 0.7823
Epoch 8/10, Batch 302/883, Training Loss: 0.6203
Epoch 8/10, Batch 303/883, Training Loss: 0.3470
Epoch 8/10, Batch 304/883, Training Loss: 0.7771
Epoch 8/10, Batch 305/883, Training Loss: 0.6373
Epoch 8/10, Batch 306/883, Training Loss: 0.6429
Epoch 8/10, Batch 307/883, Training Loss: 1.3558
Epoch 8/10, Batch 308/883, Training Loss: 0.9607
Epoch 8/10, Batch 309/883, Training Loss: 0.7110
Epoch 8/10, Batch 310/883, Training Loss: 0.6393
Epoch 8/10, Batch 311/883, Training Loss: 0.6005
Epoch 8/10, Batch 312/883, Training Loss: 0.5841
Epoch 8/10, Batch 313/883, Training Loss: 0.5198
Epoch 8/10, Batch 314/883, Training Loss: 0.5583
Epoch 8/10, Batch 315/883, Training Loss: 0.5640
Epoch 8/10, Batch 316/883, Training Loss: 0.6548
Epoch 8/10, Batch 317/883, Training Loss: 0.5005
Epoch 8/10, Batch 318/883, Training Loss: 0.5327
Epoch 8/10, Batch 319/883, Training Loss: 0.7280
Epoch 8/10, Batch 320/883, Training Loss: 0.4729
Epoch 8/10, Batch 321/883, Training Loss: 0.8610
Epoch 8/10, Batch 322/883, Training Loss: 0.4412
Epoch 8/10, Batch 323/883, Training Loss: 0.4928
Epoch 8/10, Batch 324/883, Training Loss: 0.8039
Epoch 8/10, Batch 325/883, Training Loss: 0.7379
Epoch 8/10, Batch 326/883, Training Loss: 0.5302
Epoch 8/10, Batch 327/883, Training Loss: 0.7094
Epoch 8/10, Batch 328/883, Training Loss: 0.4087
Epoch 8/10, Batch 329/883, Training Loss: 0.3873
Epoch 8/10, Batch 330/883, Training Loss: 0.6289
Epoch 8/10, Batch 331/883, Training Loss: 0.7567
Epoch 8/10, Batch 332/883, Training Loss: 0.4630
Epoch 8/10, Batch 333/883, Training Loss: 0.6062
Epoch 8/10, Batch 334/883, Training Loss: 0.6263
Epoch 8/10, Batch 335/883, Training Loss: 0.5811
Epoch 8/10, Batch 336/883, Training Loss: 0.6736
Epoch 8/10, Batch 337/883, Training Loss: 0.9110
Epoch 8/10, Batch 338/883, Training Loss: 0.7505
Epoch 8/10, Batch 339/883, Training Loss: 0.5299
Epoch 8/10, Batch 340/883, Training Loss: 0.4403
Epoch 8/10, Batch 341/883, Training Loss: 0.8745
Epoch 8/10, Batch 342/883, Training Loss: 0.4934
Epoch 8/10, Batch 343/883, Training Loss: 0.5960
Epoch 8/10, Batch 344/883, Training Loss: 0.6154
Epoch 8/10, Batch 345/883, Training Loss: 0.6798
Epoch 8/10, Batch 346/883, Training Loss: 0.5578
Epoch 8/10, Batch 347/883, Training Loss: 0.5424
Epoch 8/10, Batch 348/883, Training Loss: 0.5735
Epoch 8/10, Batch 349/883, Training Loss: 0.5636
Epoch 8/10, Batch 350/883, Training Loss: 0.6575
Epoch 8/10, Batch 351/883, Training Loss: 0.8312
Epoch 8/10, Batch 352/883, Training Loss: 0.5166
Epoch 8/10, Batch 353/883, Training Loss: 0.8204
Epoch 8/10, Batch 354/883, Training Loss: 0.4719
Epoch 8/10, Batch 355/883, Training Loss: 0.8242
Epoch 8/10, Batch 356/883, Training Loss: 0.5143
Epoch 8/10, Batch 357/883, Training Loss: 0.8558
Epoch 8/10, Batch 358/883, Training Loss: 0.8083
Epoch 8/10, Batch 359/883, Training Loss: 0.7233
Epoch 8/10, Batch 360/883, Training Loss: 0.8934
Epoch 8/10, Batch 361/883, Training Loss: 0.5603
Epoch 8/10, Batch 362/883, Training Loss: 0.9711
Epoch 8/10, Batch 363/883, Training Loss: 0.4415
Epoch 8/10, Batch 364/883, Training Loss: 0.5808
Epoch 8/10, Batch 365/883, Training Loss: 0.5125
Epoch 8/10, Batch 366/883, Training Loss: 0.9227
Epoch 8/10, Batch 367/883, Training Loss: 0.4969
Epoch 8/10, Batch 368/883, Training Loss: 0.5769
Epoch 8/10, Batch 369/883, Training Loss: 0.5732
Epoch 8/10, Batch 370/883, Training Loss: 0.5345
Epoch 8/10, Batch 371/883, Training Loss: 0.5517
Epoch 8/10, Batch 372/883, Training Loss: 0.5864
Epoch 8/10, Batch 373/883, Training Loss: 0.6257
Epoch 8/10, Batch 374/883, Training Loss: 0.5007
Epoch 8/10, Batch 375/883, Training Loss: 0.5168
Epoch 8/10, Batch 376/883, Training Loss: 0.5261
Epoch 8/10, Batch 377/883, Training Loss: 0.7334
Epoch 8/10, Batch 378/883, Training Loss: 0.7359
Epoch 8/10, Batch 379/883, Training Loss: 0.4215
Epoch 8/10, Batch 380/883, Training Loss: 0.9341
Epoch 8/10, Batch 381/883, Training Loss: 0.6007
Epoch 8/10, Batch 382/883, Training Loss: 0.7559
Epoch 8/10, Batch 383/883, Training Loss: 0.7967
Epoch 8/10, Batch 384/883, Training Loss: 0.5208
Epoch 8/10, Batch 385/883, Training Loss: 0.8102
Epoch 8/10, Batch 386/883, Training Loss: 0.6227
Epoch 8/10, Batch 387/883, Training Loss: 0.7828
Epoch 8/10, Batch 388/883, Training Loss: 0.8509
Epoch 8/10, Batch 389/883, Training Loss: 0.8284
Epoch 8/10, Batch 390/883, Training Loss: 0.5638
Epoch 8/10, Batch 391/883, Training Loss: 0.6974
Epoch 8/10, Batch 392/883, Training Loss: 0.5356
Epoch 8/10, Batch 393/883, Training Loss: 0.6970
Epoch 8/10, Batch 394/883, Training Loss: 1.1034
Epoch 8/10, Batch 395/883, Training Loss: 1.1235
Epoch 8/10, Batch 396/883, Training Loss: 0.5249
Epoch 8/10, Batch 397/883, Training Loss: 0.8212
Epoch 8/10, Batch 398/883, Training Loss: 0.4979
Epoch 8/10, Batch 399/883, Training Loss: 0.8304
Epoch 8/10, Batch 400/883, Training Loss: 0.7778
Epoch 8/10, Batch 401/883, Training Loss: 0.4467
Epoch 8/10, Batch 402/883, Training Loss: 0.8064
Epoch 8/10, Batch 403/883, Training Loss: 0.5697
Epoch 8/10, Batch 404/883, Training Loss: 0.4795
Epoch 8/10, Batch 405/883, Training Loss: 0.5402
Epoch 8/10, Batch 406/883, Training Loss: 0.4173
Epoch 8/10, Batch 407/883, Training Loss: 0.6546
Epoch 8/10, Batch 408/883, Training Loss: 0.4580
Epoch 8/10, Batch 409/883, Training Loss: 0.4770
Epoch 8/10, Batch 410/883, Training Loss: 0.8387
Epoch 8/10, Batch 411/883, Training Loss: 0.7347
Epoch 8/10, Batch 412/883, Training Loss: 0.7182
Epoch 8/10, Batch 413/883, Training Loss: 0.5019
Epoch 8/10, Batch 414/883, Training Loss: 0.5772
Epoch 8/10, Batch 415/883, Training Loss: 0.5849
Epoch 8/10, Batch 416/883, Training Loss: 0.4670
Epoch 8/10, Batch 417/883, Training Loss: 0.5348
Epoch 8/10, Batch 418/883, Training Loss: 0.6574
Epoch 8/10, Batch 419/883, Training Loss: 0.7200
Epoch 8/10, Batch 420/883, Training Loss: 0.5006
Epoch 8/10, Batch 421/883, Training Loss: 0.4382
Epoch 8/10, Batch 422/883, Training Loss: 0.7326
Epoch 8/10, Batch 423/883, Training Loss: 0.7122
Epoch 8/10, Batch 424/883, Training Loss: 0.4263
Epoch 8/10, Batch 425/883, Training Loss: 0.3635
Epoch 8/10, Batch 426/883, Training Loss: 0.6087
Epoch 8/10, Batch 427/883, Training Loss: 0.4023
Epoch 8/10, Batch 428/883, Training Loss: 0.5700
Epoch 8/10, Batch 429/883, Training Loss: 0.3326
Epoch 8/10, Batch 430/883, Training Loss: 0.5425
Epoch 8/10, Batch 431/883, Training Loss: 0.7539
Epoch 8/10, Batch 432/883, Training Loss: 0.4810
Epoch 8/10, Batch 433/883, Training Loss: 0.4011
Epoch 8/10, Batch 434/883, Training Loss: 0.5145
Epoch 8/10, Batch 435/883, Training Loss: 0.5621
Epoch 8/10, Batch 436/883, Training Loss: 0.5254
Epoch 8/10, Batch 437/883, Training Loss: 0.4871
Epoch 8/10, Batch 438/883, Training Loss: 0.5769
Epoch 8/10, Batch 439/883, Training Loss: 0.7209
Epoch 8/10, Batch 440/883, Training Loss: 0.7381
Epoch 8/10, Batch 441/883, Training Loss: 0.7057
Epoch 8/10, Batch 442/883, Training Loss: 0.4153
Epoch 8/10, Batch 443/883, Training Loss: 0.4922
Epoch 8/10, Batch 444/883, Training Loss: 0.7548
Epoch 8/10, Batch 445/883, Training Loss: 0.6038
Epoch 8/10, Batch 446/883, Training Loss: 0.7416
Epoch 8/10, Batch 447/883, Training Loss: 1.0415
Epoch 8/10, Batch 448/883, Training Loss: 0.4867
Epoch 8/10, Batch 449/883, Training Loss: 0.6334
Epoch 8/10, Batch 450/883, Training Loss: 0.3430
Epoch 8/10, Batch 451/883, Training Loss: 0.9821
Epoch 8/10, Batch 452/883, Training Loss: 0.6826
Epoch 8/10, Batch 453/883, Training Loss: 0.6388
Epoch 8/10, Batch 454/883, Training Loss: 0.5754
Epoch 8/10, Batch 455/883, Training Loss: 0.5346
Epoch 8/10, Batch 456/883, Training Loss: 0.7194
Epoch 8/10, Batch 457/883, Training Loss: 0.5442
Epoch 8/10, Batch 458/883, Training Loss: 0.9135
Epoch 8/10, Batch 459/883, Training Loss: 0.3945
Epoch 8/10, Batch 460/883, Training Loss: 0.7000
Epoch 8/10, Batch 461/883, Training Loss: 0.5187
Epoch 8/10, Batch 462/883, Training Loss: 0.3915
Epoch 8/10, Batch 463/883, Training Loss: 0.5571
Epoch 8/10, Batch 464/883, Training Loss: 0.5695
Epoch 8/10, Batch 465/883, Training Loss: 0.8100
Epoch 8/10, Batch 466/883, Training Loss: 0.5724
Epoch 8/10, Batch 467/883, Training Loss: 0.4407
Epoch 8/10, Batch 468/883, Training Loss: 0.4869
Epoch 8/10, Batch 469/883, Training Loss: 0.3263
Epoch 8/10, Batch 470/883, Training Loss: 0.3524
Epoch 8/10, Batch 471/883, Training Loss: 0.4705
Epoch 8/10, Batch 472/883, Training Loss: 0.3940
Epoch 8/10, Batch 473/883, Training Loss: 0.6408
Epoch 8/10, Batch 474/883, Training Loss: 0.4221
Epoch 8/10, Batch 475/883, Training Loss: 0.5755
Epoch 8/10, Batch 476/883, Training Loss: 0.8151
Epoch 8/10, Batch 477/883, Training Loss: 0.5793
Epoch 8/10, Batch 478/883, Training Loss: 0.7727
Epoch 8/10, Batch 479/883, Training Loss: 0.6768
Epoch 8/10, Batch 480/883, Training Loss: 0.4195
Epoch 8/10, Batch 481/883, Training Loss: 0.5255
Epoch 8/10, Batch 482/883, Training Loss: 0.5199
Epoch 8/10, Batch 483/883, Training Loss: 0.6208
Epoch 8/10, Batch 484/883, Training Loss: 0.4922
Epoch 8/10, Batch 485/883, Training Loss: 0.4425
Epoch 8/10, Batch 486/883, Training Loss: 0.6914
Epoch 8/10, Batch 487/883, Training Loss: 0.5042
Epoch 8/10, Batch 488/883, Training Loss: 0.8411
Epoch 8/10, Batch 489/883, Training Loss: 0.6793
Epoch 8/10, Batch 490/883, Training Loss: 0.3660
Epoch 8/10, Batch 491/883, Training Loss: 0.4537
Epoch 8/10, Batch 492/883, Training Loss: 0.9202
Epoch 8/10, Batch 493/883, Training Loss: 0.6624
Epoch 8/10, Batch 494/883, Training Loss: 0.7476
Epoch 8/10, Batch 495/883, Training Loss: 0.4115
Epoch 8/10, Batch 496/883, Training Loss: 0.9214
Epoch 8/10, Batch 497/883, Training Loss: 0.3216
Epoch 8/10, Batch 498/883, Training Loss: 0.4311
Epoch 8/10, Batch 499/883, Training Loss: 0.6975
Epoch 8/10, Batch 500/883, Training Loss: 0.7093
Epoch 8/10, Batch 501/883, Training Loss: 0.8306
Epoch 8/10, Batch 502/883, Training Loss: 0.9133
Epoch 8/10, Batch 503/883, Training Loss: 0.3127
Epoch 8/10, Batch 504/883, Training Loss: 0.5563
Epoch 8/10, Batch 505/883, Training Loss: 0.9532
Epoch 8/10, Batch 506/883, Training Loss: 1.0622
Epoch 8/10, Batch 507/883, Training Loss: 0.6016
Epoch 8/10, Batch 508/883, Training Loss: 0.6294
Epoch 8/10, Batch 509/883, Training Loss: 0.7084
Epoch 8/10, Batch 510/883, Training Loss: 0.7872
Epoch 8/10, Batch 511/883, Training Loss: 0.4918
Epoch 8/10, Batch 512/883, Training Loss: 0.5212
Epoch 8/10, Batch 513/883, Training Loss: 0.7004
Epoch 8/10, Batch 514/883, Training Loss: 0.4491
Epoch 8/10, Batch 515/883, Training Loss: 0.6876
Epoch 8/10, Batch 516/883, Training Loss: 0.6088
Epoch 8/10, Batch 517/883, Training Loss: 0.7093
Epoch 8/10, Batch 518/883, Training Loss: 0.4142
Epoch 8/10, Batch 519/883, Training Loss: 0.5701
Epoch 8/10, Batch 520/883, Training Loss: 0.4639
Epoch 8/10, Batch 521/883, Training Loss: 0.7893
Epoch 8/10, Batch 522/883, Training Loss: 0.9345
Epoch 8/10, Batch 523/883, Training Loss: 0.6316
Epoch 8/10, Batch 524/883, Training Loss: 0.5210
Epoch 8/10, Batch 525/883, Training Loss: 0.7301
Epoch 8/10, Batch 526/883, Training Loss: 0.5747
Epoch 8/10, Batch 527/883, Training Loss: 0.6245
Epoch 8/10, Batch 528/883, Training Loss: 0.3894
Epoch 8/10, Batch 529/883, Training Loss: 0.6815
Epoch 8/10, Batch 530/883, Training Loss: 0.4890
Epoch 8/10, Batch 531/883, Training Loss: 0.4563
Epoch 8/10, Batch 532/883, Training Loss: 0.8808
Epoch 8/10, Batch 533/883, Training Loss: 0.4263
Epoch 8/10, Batch 534/883, Training Loss: 0.4808
Epoch 8/10, Batch 535/883, Training Loss: 0.6664
Epoch 8/10, Batch 536/883, Training Loss: 0.5193
Epoch 8/10, Batch 537/883, Training Loss: 0.8350
Epoch 8/10, Batch 538/883, Training Loss: 0.9055
Epoch 8/10, Batch 539/883, Training Loss: 0.5057
Epoch 8/10, Batch 540/883, Training Loss: 0.3458
Epoch 8/10, Batch 541/883, Training Loss: 0.5620
Epoch 8/10, Batch 542/883, Training Loss: 0.5661
Epoch 8/10, Batch 543/883, Training Loss: 0.7272
Epoch 8/10, Batch 544/883, Training Loss: 0.8823
Epoch 8/10, Batch 545/883, Training Loss: 0.5422
Epoch 8/10, Batch 546/883, Training Loss: 0.5939
Epoch 8/10, Batch 547/883, Training Loss: 0.5979
Epoch 8/10, Batch 548/883, Training Loss: 0.7317
Epoch 8/10, Batch 549/883, Training Loss: 0.6159
Epoch 8/10, Batch 550/883, Training Loss: 0.9621
Epoch 8/10, Batch 551/883, Training Loss: 0.4618
Epoch 8/10, Batch 552/883, Training Loss: 0.5032
Epoch 8/10, Batch 553/883, Training Loss: 0.7463
Epoch 8/10, Batch 554/883, Training Loss: 0.9055
Epoch 8/10, Batch 555/883, Training Loss: 0.6106
Epoch 8/10, Batch 556/883, Training Loss: 0.5156
Epoch 8/10, Batch 557/883, Training Loss: 0.6312
Epoch 8/10, Batch 558/883, Training Loss: 0.6589
Epoch 8/10, Batch 559/883, Training Loss: 0.7651
Epoch 8/10, Batch 560/883, Training Loss: 0.7108
Epoch 8/10, Batch 561/883, Training Loss: 0.3695
Epoch 8/10, Batch 562/883, Training Loss: 0.4216
Epoch 8/10, Batch 563/883, Training Loss: 0.6492
Epoch 8/10, Batch 564/883, Training Loss: 0.6584
Epoch 8/10, Batch 565/883, Training Loss: 0.7705
Epoch 8/10, Batch 566/883, Training Loss: 0.6228
Epoch 8/10, Batch 567/883, Training Loss: 0.5178
Epoch 8/10, Batch 568/883, Training Loss: 0.6355
Epoch 8/10, Batch 569/883, Training Loss: 0.7209
Epoch 8/10, Batch 570/883, Training Loss: 0.9711
Epoch 8/10, Batch 571/883, Training Loss: 0.3770
Epoch 8/10, Batch 572/883, Training Loss: 0.4182
Epoch 8/10, Batch 573/883, Training Loss: 0.7205
Epoch 8/10, Batch 574/883, Training Loss: 0.8394
Epoch 8/10, Batch 575/883, Training Loss: 0.5106
Epoch 8/10, Batch 576/883, Training Loss: 0.5625
Epoch 8/10, Batch 577/883, Training Loss: 0.7922
Epoch 8/10, Batch 578/883, Training Loss: 0.4829
Epoch 8/10, Batch 579/883, Training Loss: 0.3891
Epoch 8/10, Batch 580/883, Training Loss: 0.7200
Epoch 8/10, Batch 581/883, Training Loss: 0.3620
Epoch 8/10, Batch 582/883, Training Loss: 0.9026
Epoch 8/10, Batch 583/883, Training Loss: 0.3737
Epoch 8/10, Batch 584/883, Training Loss: 0.5389
Epoch 8/10, Batch 585/883, Training Loss: 1.0831
Epoch 8/10, Batch 586/883, Training Loss: 0.4083
Epoch 8/10, Batch 587/883, Training Loss: 0.5873
Epoch 8/10, Batch 588/883, Training Loss: 1.1269
Epoch 8/10, Batch 589/883, Training Loss: 0.5542
Epoch 8/10, Batch 590/883, Training Loss: 0.7091
Epoch 8/10, Batch 591/883, Training Loss: 0.6375
Epoch 8/10, Batch 592/883, Training Loss: 0.4847
Epoch 8/10, Batch 593/883, Training Loss: 0.5663
Epoch 8/10, Batch 594/883, Training Loss: 0.7498
Epoch 8/10, Batch 595/883, Training Loss: 0.5344
Epoch 8/10, Batch 596/883, Training Loss: 1.0738
Epoch 8/10, Batch 597/883, Training Loss: 0.6049
Epoch 8/10, Batch 598/883, Training Loss: 0.4238
Epoch 8/10, Batch 599/883, Training Loss: 0.6425
Epoch 8/10, Batch 600/883, Training Loss: 0.3667
Epoch 8/10, Batch 601/883, Training Loss: 0.7045
Epoch 8/10, Batch 602/883, Training Loss: 0.5935
Epoch 8/10, Batch 603/883, Training Loss: 0.3385
Epoch 8/10, Batch 604/883, Training Loss: 0.7698
Epoch 8/10, Batch 605/883, Training Loss: 0.4978
Epoch 8/10, Batch 606/883, Training Loss: 0.6150
Epoch 8/10, Batch 607/883, Training Loss: 0.5106
Epoch 8/10, Batch 608/883, Training Loss: 0.7807
Epoch 8/10, Batch 609/883, Training Loss: 0.9459
Epoch 8/10, Batch 610/883, Training Loss: 0.9315
Epoch 8/10, Batch 611/883, Training Loss: 0.6432
Epoch 8/10, Batch 612/883, Training Loss: 0.4957
Epoch 8/10, Batch 613/883, Training Loss: 0.5093
Epoch 8/10, Batch 614/883, Training Loss: 0.6985
Epoch 8/10, Batch 615/883, Training Loss: 0.7075
Epoch 8/10, Batch 616/883, Training Loss: 0.6764
Epoch 8/10, Batch 617/883, Training Loss: 0.4570
Epoch 8/10, Batch 618/883, Training Loss: 0.6387
Epoch 8/10, Batch 619/883, Training Loss: 0.6936
Epoch 8/10, Batch 620/883, Training Loss: 0.4844
Epoch 8/10, Batch 621/883, Training Loss: 0.3867
Epoch 8/10, Batch 622/883, Training Loss: 0.5630
Epoch 8/10, Batch 623/883, Training Loss: 0.5497
Epoch 8/10, Batch 624/883, Training Loss: 0.7337
Epoch 8/10, Batch 625/883, Training Loss: 0.5591
Epoch 8/10, Batch 626/883, Training Loss: 0.6934
Epoch 8/10, Batch 627/883, Training Loss: 0.5785
Epoch 8/10, Batch 628/883, Training Loss: 0.7851
Epoch 8/10, Batch 629/883, Training Loss: 0.4601
Epoch 8/10, Batch 630/883, Training Loss: 0.4639
Epoch 8/10, Batch 631/883, Training Loss: 0.5719
Epoch 8/10, Batch 632/883, Training Loss: 0.9306
Epoch 8/10, Batch 633/883, Training Loss: 0.4741
Epoch 8/10, Batch 634/883, Training Loss: 0.4474
Epoch 8/10, Batch 635/883, Training Loss: 0.6243
Epoch 8/10, Batch 636/883, Training Loss: 0.5260
Epoch 8/10, Batch 637/883, Training Loss: 0.3419
Epoch 8/10, Batch 638/883, Training Loss: 0.9691
Epoch 8/10, Batch 639/883, Training Loss: 0.4250
Epoch 8/10, Batch 640/883, Training Loss: 0.6459
Epoch 8/10, Batch 641/883, Training Loss: 0.8513
Epoch 8/10, Batch 642/883, Training Loss: 0.4878
Epoch 8/10, Batch 643/883, Training Loss: 0.7005
Epoch 8/10, Batch 644/883, Training Loss: 0.4180
Epoch 8/10, Batch 645/883, Training Loss: 0.3970
Epoch 8/10, Batch 646/883, Training Loss: 0.6396
Epoch 8/10, Batch 647/883, Training Loss: 0.6438
Epoch 8/10, Batch 648/883, Training Loss: 0.5833
Epoch 8/10, Batch 649/883, Training Loss: 0.4784
Epoch 8/10, Batch 650/883, Training Loss: 0.5032
Epoch 8/10, Batch 651/883, Training Loss: 1.0577
Epoch 8/10, Batch 652/883, Training Loss: 0.6997
Epoch 8/10, Batch 653/883, Training Loss: 0.5303
Epoch 8/10, Batch 654/883, Training Loss: 0.5261
Epoch 8/10, Batch 655/883, Training Loss: 0.3909
Epoch 8/10, Batch 656/883, Training Loss: 0.6197
Epoch 8/10, Batch 657/883, Training Loss: 0.5246
Epoch 8/10, Batch 658/883, Training Loss: 0.4334
Epoch 8/10, Batch 659/883, Training Loss: 0.6814
Epoch 8/10, Batch 660/883, Training Loss: 0.5452
Epoch 8/10, Batch 661/883, Training Loss: 0.5499
Epoch 8/10, Batch 662/883, Training Loss: 0.5173
Epoch 8/10, Batch 663/883, Training Loss: 0.9527
Epoch 8/10, Batch 664/883, Training Loss: 0.4140
Epoch 8/10, Batch 665/883, Training Loss: 0.7168
Epoch 8/10, Batch 666/883, Training Loss: 0.5448
Epoch 8/10, Batch 667/883, Training Loss: 0.8996
Epoch 8/10, Batch 668/883, Training Loss: 0.5629
Epoch 8/10, Batch 669/883, Training Loss: 0.6296
Epoch 8/10, Batch 670/883, Training Loss: 0.8436
Epoch 8/10, Batch 671/883, Training Loss: 0.8372
Epoch 8/10, Batch 672/883, Training Loss: 0.5185
Epoch 8/10, Batch 673/883, Training Loss: 0.6466
Epoch 8/10, Batch 674/883, Training Loss: 0.4783
Epoch 8/10, Batch 675/883, Training Loss: 0.4550
Epoch 8/10, Batch 676/883, Training Loss: 0.6669
Epoch 8/10, Batch 677/883, Training Loss: 0.5865
Epoch 8/10, Batch 678/883, Training Loss: 0.6194
Epoch 8/10, Batch 679/883, Training Loss: 0.3715
Epoch 8/10, Batch 680/883, Training Loss: 0.6135
Epoch 8/10, Batch 681/883, Training Loss: 0.6298
Epoch 8/10, Batch 682/883, Training Loss: 0.4390
Epoch 8/10, Batch 683/883, Training Loss: 0.6975
Epoch 8/10, Batch 684/883, Training Loss: 0.5652
Epoch 8/10, Batch 685/883, Training Loss: 0.8225
Epoch 8/10, Batch 686/883, Training Loss: 0.5667
Epoch 8/10, Batch 687/883, Training Loss: 0.4684
Epoch 8/10, Batch 688/883, Training Loss: 0.4119
Epoch 8/10, Batch 689/883, Training Loss: 0.5960
Epoch 8/10, Batch 690/883, Training Loss: 0.4953
Epoch 8/10, Batch 691/883, Training Loss: 0.5960
Epoch 8/10, Batch 692/883, Training Loss: 0.4645
Epoch 8/10, Batch 693/883, Training Loss: 0.6222
Epoch 8/10, Batch 694/883, Training Loss: 0.6810
Epoch 8/10, Batch 695/883, Training Loss: 0.4755
Epoch 8/10, Batch 696/883, Training Loss: 0.4168
Epoch 8/10, Batch 697/883, Training Loss: 0.7401
Epoch 8/10, Batch 698/883, Training Loss: 0.4999
Epoch 8/10, Batch 699/883, Training Loss: 0.7527
Epoch 8/10, Batch 700/883, Training Loss: 0.4386
Epoch 8/10, Batch 701/883, Training Loss: 0.5603
Epoch 8/10, Batch 702/883, Training Loss: 0.4783
Epoch 8/10, Batch 703/883, Training Loss: 0.7645
Epoch 8/10, Batch 704/883, Training Loss: 0.6206
Epoch 8/10, Batch 705/883, Training Loss: 0.6471
Epoch 8/10, Batch 706/883, Training Loss: 0.6547
Epoch 8/10, Batch 707/883, Training Loss: 0.3856
Epoch 8/10, Batch 708/883, Training Loss: 0.3452
Epoch 8/10, Batch 709/883, Training Loss: 0.9233
Epoch 8/10, Batch 710/883, Training Loss: 0.5455
Epoch 8/10, Batch 711/883, Training Loss: 0.5263
Epoch 8/10, Batch 712/883, Training Loss: 0.6693
Epoch 8/10, Batch 713/883, Training Loss: 0.6018
Epoch 8/10, Batch 714/883, Training Loss: 0.8224
Epoch 8/10, Batch 715/883, Training Loss: 0.4845
Epoch 8/10, Batch 716/883, Training Loss: 1.0287
Epoch 8/10, Batch 717/883, Training Loss: 0.7646
Epoch 8/10, Batch 718/883, Training Loss: 0.3188
Epoch 8/10, Batch 719/883, Training Loss: 0.4157
Epoch 8/10, Batch 720/883, Training Loss: 0.5724
Epoch 8/10, Batch 721/883, Training Loss: 0.8527
Epoch 8/10, Batch 722/883, Training Loss: 0.5150
Epoch 8/10, Batch 723/883, Training Loss: 0.4855
Epoch 8/10, Batch 724/883, Training Loss: 0.5893
Epoch 8/10, Batch 725/883, Training Loss: 0.9386
Epoch 8/10, Batch 726/883, Training Loss: 0.6774
Epoch 8/10, Batch 727/883, Training Loss: 0.9418
Epoch 8/10, Batch 728/883, Training Loss: 1.0740
Epoch 8/10, Batch 729/883, Training Loss: 0.8016
Epoch 8/10, Batch 730/883, Training Loss: 0.5505
Epoch 8/10, Batch 731/883, Training Loss: 0.4319
Epoch 8/10, Batch 732/883, Training Loss: 0.6982
Epoch 8/10, Batch 733/883, Training Loss: 0.8005
Epoch 8/10, Batch 734/883, Training Loss: 0.5627
Epoch 8/10, Batch 735/883, Training Loss: 0.4993
Epoch 8/10, Batch 736/883, Training Loss: 0.5082
Epoch 8/10, Batch 737/883, Training Loss: 0.4959
Epoch 8/10, Batch 738/883, Training Loss: 0.6279
Epoch 8/10, Batch 739/883, Training Loss: 0.9007
Epoch 8/10, Batch 740/883, Training Loss: 0.4541
Epoch 8/10, Batch 741/883, Training Loss: 0.6317
Epoch 8/10, Batch 742/883, Training Loss: 0.7329
Epoch 8/10, Batch 743/883, Training Loss: 0.9197
Epoch 8/10, Batch 744/883, Training Loss: 0.6763
Epoch 8/10, Batch 745/883, Training Loss: 0.5943
Epoch 8/10, Batch 746/883, Training Loss: 0.4787
Epoch 8/10, Batch 747/883, Training Loss: 0.4470
Epoch 8/10, Batch 748/883, Training Loss: 0.7760
Epoch 8/10, Batch 749/883, Training Loss: 0.4240
Epoch 8/10, Batch 750/883, Training Loss: 0.6328
Epoch 8/10, Batch 751/883, Training Loss: 0.9767
Epoch 8/10, Batch 752/883, Training Loss: 0.3929
Epoch 8/10, Batch 753/883, Training Loss: 0.5487
Epoch 8/10, Batch 754/883, Training Loss: 0.6139
Epoch 8/10, Batch 755/883, Training Loss: 0.5298
Epoch 8/10, Batch 756/883, Training Loss: 0.5519
Epoch 8/10, Batch 757/883, Training Loss: 0.5406
Epoch 8/10, Batch 758/883, Training Loss: 0.7226
Epoch 8/10, Batch 759/883, Training Loss: 0.7413
Epoch 8/10, Batch 760/883, Training Loss: 0.4663
Epoch 8/10, Batch 761/883, Training Loss: 0.5950
Epoch 8/10, Batch 762/883, Training Loss: 0.3874
Epoch 8/10, Batch 763/883, Training Loss: 0.4864
Epoch 8/10, Batch 764/883, Training Loss: 0.5915
Epoch 8/10, Batch 765/883, Training Loss: 0.4177
Epoch 8/10, Batch 766/883, Training Loss: 0.7618
Epoch 8/10, Batch 767/883, Training Loss: 0.9749
Epoch 8/10, Batch 768/883, Training Loss: 0.4863
Epoch 8/10, Batch 769/883, Training Loss: 0.9739
Epoch 8/10, Batch 770/883, Training Loss: 0.5545
Epoch 8/10, Batch 771/883, Training Loss: 0.6945
Epoch 8/10, Batch 772/883, Training Loss: 0.3898
Epoch 8/10, Batch 773/883, Training Loss: 0.6186
Epoch 8/10, Batch 774/883, Training Loss: 0.4552
Epoch 8/10, Batch 775/883, Training Loss: 0.6357
Epoch 8/10, Batch 776/883, Training Loss: 0.4147
Epoch 8/10, Batch 777/883, Training Loss: 0.3417
Epoch 8/10, Batch 778/883, Training Loss: 0.5403
Epoch 8/10, Batch 779/883, Training Loss: 0.8604
Epoch 8/10, Batch 780/883, Training Loss: 0.5974
Epoch 8/10, Batch 781/883, Training Loss: 0.5048
Epoch 8/10, Batch 782/883, Training Loss: 0.9949
Epoch 8/10, Batch 783/883, Training Loss: 1.0072
Epoch 8/10, Batch 784/883, Training Loss: 0.8119
Epoch 8/10, Batch 785/883, Training Loss: 0.5682
Epoch 8/10, Batch 786/883, Training Loss: 0.4514
Epoch 8/10, Batch 787/883, Training Loss: 0.6316
Epoch 8/10, Batch 788/883, Training Loss: 0.6754
Epoch 8/10, Batch 789/883, Training Loss: 0.6094
Epoch 8/10, Batch 790/883, Training Loss: 0.8527
Epoch 8/10, Batch 791/883, Training Loss: 0.4801
Epoch 8/10, Batch 792/883, Training Loss: 0.5947
Epoch 8/10, Batch 793/883, Training Loss: 0.6863
Epoch 8/10, Batch 794/883, Training Loss: 0.6555
Epoch 8/10, Batch 795/883, Training Loss: 0.7726
Epoch 8/10, Batch 796/883, Training Loss: 0.6096
Epoch 8/10, Batch 797/883, Training Loss: 0.7946
Epoch 8/10, Batch 798/883, Training Loss: 0.6182
Epoch 8/10, Batch 799/883, Training Loss: 0.5126
Epoch 8/10, Batch 800/883, Training Loss: 0.5102
Epoch 8/10, Batch 801/883, Training Loss: 0.6568
Epoch 8/10, Batch 802/883, Training Loss: 0.8917
Epoch 8/10, Batch 803/883, Training Loss: 0.9372
Epoch 8/10, Batch 804/883, Training Loss: 0.4790
Epoch 8/10, Batch 805/883, Training Loss: 0.5955
Epoch 8/10, Batch 806/883, Training Loss: 0.5389
Epoch 8/10, Batch 807/883, Training Loss: 0.6978
Epoch 8/10, Batch 808/883, Training Loss: 0.6562
Epoch 8/10, Batch 809/883, Training Loss: 0.3927
Epoch 8/10, Batch 810/883, Training Loss: 0.5974
Epoch 8/10, Batch 811/883, Training Loss: 0.8985
Epoch 8/10, Batch 812/883, Training Loss: 0.5843
Epoch 8/10, Batch 813/883, Training Loss: 1.2293
Epoch 8/10, Batch 814/883, Training Loss: 0.4620
Epoch 8/10, Batch 815/883, Training Loss: 0.8136
Epoch 8/10, Batch 816/883, Training Loss: 0.6993
Epoch 8/10, Batch 817/883, Training Loss: 0.7526
Epoch 8/10, Batch 818/883, Training Loss: 0.6921
Epoch 8/10, Batch 819/883, Training Loss: 0.9254
Epoch 8/10, Batch 820/883, Training Loss: 0.4645
Epoch 8/10, Batch 821/883, Training Loss: 0.5927
Epoch 8/10, Batch 822/883, Training Loss: 0.7877
Epoch 8/10, Batch 823/883, Training Loss: 0.5517
Epoch 8/10, Batch 824/883, Training Loss: 0.6949
Epoch 8/10, Batch 825/883, Training Loss: 0.8833
Epoch 8/10, Batch 826/883, Training Loss: 0.6374
Epoch 8/10, Batch 827/883, Training Loss: 0.6230
Epoch 8/10, Batch 828/883, Training Loss: 0.4852
Epoch 8/10, Batch 829/883, Training Loss: 0.8689
Epoch 8/10, Batch 830/883, Training Loss: 0.7252
Epoch 8/10, Batch 831/883, Training Loss: 0.6364
Epoch 8/10, Batch 832/883, Training Loss: 0.5081
Epoch 8/10, Batch 833/883, Training Loss: 0.5460
Epoch 8/10, Batch 834/883, Training Loss: 0.7756
Epoch 8/10, Batch 835/883, Training Loss: 0.6327
Epoch 8/10, Batch 836/883, Training Loss: 0.4690
Epoch 8/10, Batch 837/883, Training Loss: 0.5847
Epoch 8/10, Batch 838/883, Training Loss: 0.4206
Epoch 8/10, Batch 839/883, Training Loss: 0.8420
Epoch 8/10, Batch 840/883, Training Loss: 0.7334
Epoch 8/10, Batch 841/883, Training Loss: 0.5024
Epoch 8/10, Batch 842/883, Training Loss: 0.8122
Epoch 8/10, Batch 843/883, Training Loss: 0.4605
Epoch 8/10, Batch 844/883, Training Loss: 0.4799
Epoch 8/10, Batch 845/883, Training Loss: 0.7081
Epoch 8/10, Batch 846/883, Training Loss: 0.4535
Epoch 8/10, Batch 847/883, Training Loss: 0.5159
Epoch 8/10, Batch 848/883, Training Loss: 0.5288
Epoch 8/10, Batch 849/883, Training Loss: 0.9744
Epoch 8/10, Batch 850/883, Training Loss: 0.7756
Epoch 8/10, Batch 851/883, Training Loss: 0.9389
Epoch 8/10, Batch 852/883, Training Loss: 0.4313
Epoch 8/10, Batch 853/883, Training Loss: 0.3916
Epoch 8/10, Batch 854/883, Training Loss: 0.8032
Epoch 8/10, Batch 855/883, Training Loss: 0.3596
Epoch 8/10, Batch 856/883, Training Loss: 0.7289
Epoch 8/10, Batch 857/883, Training Loss: 0.5964
Epoch 8/10, Batch 858/883, Training Loss: 0.5792
Epoch 8/10, Batch 859/883, Training Loss: 0.8254
Epoch 8/10, Batch 860/883, Training Loss: 0.4829
Epoch 8/10, Batch 861/883, Training Loss: 0.5473
Epoch 8/10, Batch 862/883, Training Loss: 0.4923
Epoch 8/10, Batch 863/883, Training Loss: 0.5077
Epoch 8/10, Batch 864/883, Training Loss: 0.5084
Epoch 8/10, Batch 865/883, Training Loss: 0.5409
Epoch 8/10, Batch 866/883, Training Loss: 0.3638
Epoch 8/10, Batch 867/883, Training Loss: 0.6439
Epoch 8/10, Batch 868/883, Training Loss: 0.4763
Epoch 8/10, Batch 869/883, Training Loss: 0.7088
Epoch 8/10, Batch 870/883, Training Loss: 0.7370
Epoch 8/10, Batch 871/883, Training Loss: 0.8877
Epoch 8/10, Batch 872/883, Training Loss: 0.6737
Epoch 8/10, Batch 873/883, Training Loss: 0.6876
Epoch 8/10, Batch 874/883, Training Loss: 0.7655
Epoch 8/10, Batch 875/883, Training Loss: 0.7251
Epoch 8/10, Batch 876/883, Training Loss: 0.7109
Epoch 8/10, Batch 877/883, Training Loss: 0.6345
Epoch 8/10, Batch 878/883, Training Loss: 0.5741
Epoch 8/10, Batch 879/883, Training Loss: 1.4903
Epoch 8/10, Batch 880/883, Training Loss: 0.6993
Epoch 8/10, Batch 881/883, Training Loss: 0.4970
Epoch 8/10, Batch 882/883, Training Loss: 0.4289
Epoch 8/10, Batch 883/883, Training Loss: 0.5877
Epoch 8/10, Training Loss: 0.6273, Validation Loss: 0.5671, Validation Accuracy: 0.7493
Epoch 9/10, Batch 1/883, Training Loss: 0.6198
Epoch 9/10, Batch 2/883, Training Loss: 0.7614
Epoch 9/10, Batch 3/883, Training Loss: 0.5280
Epoch 9/10, Batch 4/883, Training Loss: 0.7488
Epoch 9/10, Batch 5/883, Training Loss: 0.6349
Epoch 9/10, Batch 6/883, Training Loss: 0.6473
Epoch 9/10, Batch 7/883, Training Loss: 0.5576
Epoch 9/10, Batch 8/883, Training Loss: 0.6161
Epoch 9/10, Batch 9/883, Training Loss: 0.6577
Epoch 9/10, Batch 10/883, Training Loss: 0.3237
Epoch 9/10, Batch 11/883, Training Loss: 0.7323
Epoch 9/10, Batch 12/883, Training Loss: 0.6753
Epoch 9/10, Batch 13/883, Training Loss: 0.5904
Epoch 9/10, Batch 14/883, Training Loss: 0.4870
Epoch 9/10, Batch 15/883, Training Loss: 0.6149
Epoch 9/10, Batch 16/883, Training Loss: 0.7980
Epoch 9/10, Batch 17/883, Training Loss: 0.5662
Epoch 9/10, Batch 18/883, Training Loss: 0.4703
Epoch 9/10, Batch 19/883, Training Loss: 0.8006
Epoch 9/10, Batch 20/883, Training Loss: 0.5249
Epoch 9/10, Batch 21/883, Training Loss: 0.5384
Epoch 9/10, Batch 22/883, Training Loss: 1.3574
Epoch 9/10, Batch 23/883, Training Loss: 0.4863
Epoch 9/10, Batch 24/883, Training Loss: 0.8399
Epoch 9/10, Batch 25/883, Training Loss: 0.4624
Epoch 9/10, Batch 26/883, Training Loss: 1.1728
Epoch 9/10, Batch 27/883, Training Loss: 0.5938
Epoch 9/10, Batch 28/883, Training Loss: 0.3865
Epoch 9/10, Batch 29/883, Training Loss: 0.6544
Epoch 9/10, Batch 30/883, Training Loss: 0.4354
Epoch 9/10, Batch 31/883, Training Loss: 0.6459
Epoch 9/10, Batch 32/883, Training Loss: 0.5069
Epoch 9/10, Batch 33/883, Training Loss: 0.4908
Epoch 9/10, Batch 34/883, Training Loss: 0.6686
Epoch 9/10, Batch 35/883, Training Loss: 0.5391
Epoch 9/10, Batch 36/883, Training Loss: 0.4186
Epoch 9/10, Batch 37/883, Training Loss: 0.5338
Epoch 9/10, Batch 38/883, Training Loss: 0.4562
Epoch 9/10, Batch 39/883, Training Loss: 0.6172
Epoch 9/10, Batch 40/883, Training Loss: 0.6438
Epoch 9/10, Batch 41/883, Training Loss: 0.5917
Epoch 9/10, Batch 42/883, Training Loss: 0.8021
Epoch 9/10, Batch 43/883, Training Loss: 0.7443
Epoch 9/10, Batch 44/883, Training Loss: 0.9797
Epoch 9/10, Batch 45/883, Training Loss: 0.5817
Epoch 9/10, Batch 46/883, Training Loss: 0.5313
Epoch 9/10, Batch 47/883, Training Loss: 0.5339
Epoch 9/10, Batch 48/883, Training Loss: 0.6824
Epoch 9/10, Batch 49/883, Training Loss: 0.5469
Epoch 9/10, Batch 50/883, Training Loss: 0.3598
Epoch 9/10, Batch 51/883, Training Loss: 0.7250
Epoch 9/10, Batch 52/883, Training Loss: 0.5383
Epoch 9/10, Batch 53/883, Training Loss: 0.3488
Epoch 9/10, Batch 54/883, Training Loss: 0.6341
Epoch 9/10, Batch 55/883, Training Loss: 0.7123
Epoch 9/10, Batch 56/883, Training Loss: 0.4340
Epoch 9/10, Batch 57/883, Training Loss: 1.0845
Epoch 9/10, Batch 58/883, Training Loss: 0.6686
Epoch 9/10, Batch 59/883, Training Loss: 0.4365
Epoch 9/10, Batch 60/883, Training Loss: 0.6838
Epoch 9/10, Batch 61/883, Training Loss: 0.7343
Epoch 9/10, Batch 62/883, Training Loss: 0.6270
Epoch 9/10, Batch 63/883, Training Loss: 0.4173
Epoch 9/10, Batch 64/883, Training Loss: 0.5987
Epoch 9/10, Batch 65/883, Training Loss: 0.7538
Epoch 9/10, Batch 66/883, Training Loss: 0.4890
Epoch 9/10, Batch 67/883, Training Loss: 0.6518
Epoch 9/10, Batch 68/883, Training Loss: 0.5659
Epoch 9/10, Batch 69/883, Training Loss: 0.5707
Epoch 9/10, Batch 70/883, Training Loss: 0.4515
Epoch 9/10, Batch 71/883, Training Loss: 0.7056
Epoch 9/10, Batch 72/883, Training Loss: 0.4208
Epoch 9/10, Batch 73/883, Training Loss: 0.7914
Epoch 9/10, Batch 74/883, Training Loss: 0.5609
Epoch 9/10, Batch 75/883, Training Loss: 0.4598
Epoch 9/10, Batch 76/883, Training Loss: 0.5841
Epoch 9/10, Batch 77/883, Training Loss: 0.3763
Epoch 9/10, Batch 78/883, Training Loss: 0.6571
Epoch 9/10, Batch 79/883, Training Loss: 0.9196
Epoch 9/10, Batch 80/883, Training Loss: 0.8867
Epoch 9/10, Batch 81/883, Training Loss: 0.7206
Epoch 9/10, Batch 82/883, Training Loss: 0.5955
Epoch 9/10, Batch 83/883, Training Loss: 0.4837
Epoch 9/10, Batch 84/883, Training Loss: 0.5322
Epoch 9/10, Batch 85/883, Training Loss: 0.6658
Epoch 9/10, Batch 86/883, Training Loss: 0.3871
Epoch 9/10, Batch 87/883, Training Loss: 0.5495
Epoch 9/10, Batch 88/883, Training Loss: 0.7990
Epoch 9/10, Batch 89/883, Training Loss: 0.4373
Epoch 9/10, Batch 90/883, Training Loss: 0.2965
Epoch 9/10, Batch 91/883, Training Loss: 0.4520
Epoch 9/10, Batch 92/883, Training Loss: 0.5366
Epoch 9/10, Batch 93/883, Training Loss: 0.3036
Epoch 9/10, Batch 94/883, Training Loss: 0.8817
Epoch 9/10, Batch 95/883, Training Loss: 0.7056
Epoch 9/10, Batch 96/883, Training Loss: 0.5883
Epoch 9/10, Batch 97/883, Training Loss: 0.6616
Epoch 9/10, Batch 98/883, Training Loss: 0.5608
Epoch 9/10, Batch 99/883, Training Loss: 0.4863
Epoch 9/10, Batch 100/883, Training Loss: 0.5063
Epoch 9/10, Batch 101/883, Training Loss: 0.4950
Epoch 9/10, Batch 102/883, Training Loss: 0.4408
Epoch 9/10, Batch 103/883, Training Loss: 0.4981
Epoch 9/10, Batch 104/883, Training Loss: 0.4276
Epoch 9/10, Batch 105/883, Training Loss: 0.7875
Epoch 9/10, Batch 106/883, Training Loss: 1.0030
Epoch 9/10, Batch 107/883, Training Loss: 0.5210
Epoch 9/10, Batch 108/883, Training Loss: 0.7347
Epoch 9/10, Batch 109/883, Training Loss: 0.3576
Epoch 9/10, Batch 110/883, Training Loss: 0.6535
Epoch 9/10, Batch 111/883, Training Loss: 0.5725
Epoch 9/10, Batch 112/883, Training Loss: 0.5414
Epoch 9/10, Batch 113/883, Training Loss: 0.4524
Epoch 9/10, Batch 114/883, Training Loss: 0.8220
Epoch 9/10, Batch 115/883, Training Loss: 0.6853
Epoch 9/10, Batch 116/883, Training Loss: 0.3568
Epoch 9/10, Batch 117/883, Training Loss: 0.4884
Epoch 9/10, Batch 118/883, Training Loss: 0.7151
Epoch 9/10, Batch 119/883, Training Loss: 0.4922
Epoch 9/10, Batch 120/883, Training Loss: 0.5392
Epoch 9/10, Batch 121/883, Training Loss: 0.5034
Epoch 9/10, Batch 122/883, Training Loss: 0.5843
Epoch 9/10, Batch 123/883, Training Loss: 0.4197
Epoch 9/10, Batch 124/883, Training Loss: 0.3071
Epoch 9/10, Batch 125/883, Training Loss: 0.6865
Epoch 9/10, Batch 126/883, Training Loss: 0.7831
Epoch 9/10, Batch 127/883, Training Loss: 0.6086
Epoch 9/10, Batch 128/883, Training Loss: 0.5159
Epoch 9/10, Batch 129/883, Training Loss: 0.4317
Epoch 9/10, Batch 130/883, Training Loss: 0.5251
Epoch 9/10, Batch 131/883, Training Loss: 0.6650
Epoch 9/10, Batch 132/883, Training Loss: 0.3361
Epoch 9/10, Batch 133/883, Training Loss: 0.3150
Epoch 9/10, Batch 134/883, Training Loss: 0.3836
Epoch 9/10, Batch 135/883, Training Loss: 0.5511
Epoch 9/10, Batch 136/883, Training Loss: 0.4380
Epoch 9/10, Batch 137/883, Training Loss: 0.8857
Epoch 9/10, Batch 138/883, Training Loss: 0.6189
Epoch 9/10, Batch 139/883, Training Loss: 0.8103
Epoch 9/10, Batch 140/883, Training Loss: 1.1660
Epoch 9/10, Batch 141/883, Training Loss: 0.5674
Epoch 9/10, Batch 142/883, Training Loss: 0.6943
Epoch 9/10, Batch 143/883, Training Loss: 0.8023
Epoch 9/10, Batch 144/883, Training Loss: 0.6913
Epoch 9/10, Batch 145/883, Training Loss: 0.6148
Epoch 9/10, Batch 146/883, Training Loss: 0.5784
Epoch 9/10, Batch 147/883, Training Loss: 0.4781
Epoch 9/10, Batch 148/883, Training Loss: 0.4244
Epoch 9/10, Batch 149/883, Training Loss: 0.5503
Epoch 9/10, Batch 150/883, Training Loss: 0.7468
Epoch 9/10, Batch 151/883, Training Loss: 0.4254
Epoch 9/10, Batch 152/883, Training Loss: 0.8106
Epoch 9/10, Batch 153/883, Training Loss: 0.3208
Epoch 9/10, Batch 154/883, Training Loss: 0.6652
Epoch 9/10, Batch 155/883, Training Loss: 0.3604
Epoch 9/10, Batch 156/883, Training Loss: 0.4064
Epoch 9/10, Batch 157/883, Training Loss: 0.5993
Epoch 9/10, Batch 158/883, Training Loss: 0.6779
Epoch 9/10, Batch 159/883, Training Loss: 0.5902
Epoch 9/10, Batch 160/883, Training Loss: 0.4959
Epoch 9/10, Batch 161/883, Training Loss: 0.5544
Epoch 9/10, Batch 162/883, Training Loss: 0.9490
Epoch 9/10, Batch 163/883, Training Loss: 0.4809
Epoch 9/10, Batch 164/883, Training Loss: 0.5257
Epoch 9/10, Batch 165/883, Training Loss: 0.3593
Epoch 9/10, Batch 166/883, Training Loss: 0.4122
Epoch 9/10, Batch 167/883, Training Loss: 0.7837
Epoch 9/10, Batch 168/883, Training Loss: 0.5314
Epoch 9/10, Batch 169/883, Training Loss: 0.4565
Epoch 9/10, Batch 170/883, Training Loss: 0.6143
Epoch 9/10, Batch 171/883, Training Loss: 0.6511
Epoch 9/10, Batch 172/883, Training Loss: 0.6011
Epoch 9/10, Batch 173/883, Training Loss: 0.6059
Epoch 9/10, Batch 174/883, Training Loss: 0.6736
Epoch 9/10, Batch 175/883, Training Loss: 0.5219
Epoch 9/10, Batch 176/883, Training Loss: 0.4521
Epoch 9/10, Batch 177/883, Training Loss: 0.7826
Epoch 9/10, Batch 178/883, Training Loss: 0.5744
Epoch 9/10, Batch 179/883, Training Loss: 0.6847
Epoch 9/10, Batch 180/883, Training Loss: 0.6791
Epoch 9/10, Batch 181/883, Training Loss: 0.4675
Epoch 9/10, Batch 182/883, Training Loss: 0.4409
Epoch 9/10, Batch 183/883, Training Loss: 0.3929
Epoch 9/10, Batch 184/883, Training Loss: 0.5464
Epoch 9/10, Batch 185/883, Training Loss: 1.0658
Epoch 9/10, Batch 186/883, Training Loss: 0.4364
Epoch 9/10, Batch 187/883, Training Loss: 1.3069
Epoch 9/10, Batch 188/883, Training Loss: 0.8228
Epoch 9/10, Batch 189/883, Training Loss: 0.4616
Epoch 9/10, Batch 190/883, Training Loss: 0.5685
Epoch 9/10, Batch 191/883, Training Loss: 0.4436
Epoch 9/10, Batch 192/883, Training Loss: 0.6482
Epoch 9/10, Batch 193/883, Training Loss: 0.4693
Epoch 9/10, Batch 194/883, Training Loss: 0.6738
Epoch 9/10, Batch 195/883, Training Loss: 0.4535
Epoch 9/10, Batch 196/883, Training Loss: 0.5102
Epoch 9/10, Batch 197/883, Training Loss: 0.6558
Epoch 9/10, Batch 198/883, Training Loss: 0.6011
Epoch 9/10, Batch 199/883, Training Loss: 0.3332
Epoch 9/10, Batch 200/883, Training Loss: 0.6711
Epoch 9/10, Batch 201/883, Training Loss: 0.3972
Epoch 9/10, Batch 202/883, Training Loss: 0.7022
Epoch 9/10, Batch 203/883, Training Loss: 0.4887
Epoch 9/10, Batch 204/883, Training Loss: 0.7701
Epoch 9/10, Batch 205/883, Training Loss: 0.6227
Epoch 9/10, Batch 206/883, Training Loss: 0.7961
Epoch 9/10, Batch 207/883, Training Loss: 0.4277
Epoch 9/10, Batch 208/883, Training Loss: 0.4465
Epoch 9/10, Batch 209/883, Training Loss: 0.4588
Epoch 9/10, Batch 210/883, Training Loss: 0.6501
Epoch 9/10, Batch 211/883, Training Loss: 1.0628
Epoch 9/10, Batch 212/883, Training Loss: 0.7513
Epoch 9/10, Batch 213/883, Training Loss: 0.5839
Epoch 9/10, Batch 214/883, Training Loss: 0.6774
Epoch 9/10, Batch 215/883, Training Loss: 0.5975
Epoch 9/10, Batch 216/883, Training Loss: 0.5213
Epoch 9/10, Batch 217/883, Training Loss: 0.5420
Epoch 9/10, Batch 218/883, Training Loss: 0.6976
Epoch 9/10, Batch 219/883, Training Loss: 0.6400
Epoch 9/10, Batch 220/883, Training Loss: 0.7362
Epoch 9/10, Batch 221/883, Training Loss: 0.9052
Epoch 9/10, Batch 222/883, Training Loss: 0.5328
Epoch 9/10, Batch 223/883, Training Loss: 0.4959
Epoch 9/10, Batch 224/883, Training Loss: 0.4433
Epoch 9/10, Batch 225/883, Training Loss: 0.4027
Epoch 9/10, Batch 226/883, Training Loss: 0.5484
Epoch 9/10, Batch 227/883, Training Loss: 0.5660
Epoch 9/10, Batch 228/883, Training Loss: 0.4775
Epoch 9/10, Batch 229/883, Training Loss: 0.4572
Epoch 9/10, Batch 230/883, Training Loss: 0.6958
Epoch 9/10, Batch 231/883, Training Loss: 0.5008
Epoch 9/10, Batch 232/883, Training Loss: 0.3762
Epoch 9/10, Batch 233/883, Training Loss: 0.5638
Epoch 9/10, Batch 234/883, Training Loss: 0.5539
Epoch 9/10, Batch 235/883, Training Loss: 0.3741
Epoch 9/10, Batch 236/883, Training Loss: 0.7670
Epoch 9/10, Batch 237/883, Training Loss: 0.4749
Epoch 9/10, Batch 238/883, Training Loss: 0.6327
Epoch 9/10, Batch 239/883, Training Loss: 0.5556
Epoch 9/10, Batch 240/883, Training Loss: 0.4399
Epoch 9/10, Batch 241/883, Training Loss: 0.4501
Epoch 9/10, Batch 242/883, Training Loss: 1.1540
Epoch 9/10, Batch 243/883, Training Loss: 0.4954
Epoch 9/10, Batch 244/883, Training Loss: 0.4961
Epoch 9/10, Batch 245/883, Training Loss: 0.3747
Epoch 9/10, Batch 246/883, Training Loss: 0.5307
Epoch 9/10, Batch 247/883, Training Loss: 0.3966
Epoch 9/10, Batch 248/883, Training Loss: 0.6119
Epoch 9/10, Batch 249/883, Training Loss: 0.6450
Epoch 9/10, Batch 250/883, Training Loss: 0.6070
Epoch 9/10, Batch 251/883, Training Loss: 0.5119
Epoch 9/10, Batch 252/883, Training Loss: 0.5615
Epoch 9/10, Batch 253/883, Training Loss: 0.9572
Epoch 9/10, Batch 254/883, Training Loss: 0.4970
Epoch 9/10, Batch 255/883, Training Loss: 0.4463
Epoch 9/10, Batch 256/883, Training Loss: 0.6930
Epoch 9/10, Batch 257/883, Training Loss: 0.5216
Epoch 9/10, Batch 258/883, Training Loss: 0.8207
Epoch 9/10, Batch 259/883, Training Loss: 0.3616
Epoch 9/10, Batch 260/883, Training Loss: 0.4134
Epoch 9/10, Batch 261/883, Training Loss: 0.5185
Epoch 9/10, Batch 262/883, Training Loss: 0.4949
Epoch 9/10, Batch 263/883, Training Loss: 0.6286
Epoch 9/10, Batch 264/883, Training Loss: 0.7681
Epoch 9/10, Batch 265/883, Training Loss: 0.5853
Epoch 9/10, Batch 266/883, Training Loss: 0.5664
Epoch 9/10, Batch 267/883, Training Loss: 0.5252
Epoch 9/10, Batch 268/883, Training Loss: 0.7272
Epoch 9/10, Batch 269/883, Training Loss: 0.8560
Epoch 9/10, Batch 270/883, Training Loss: 0.5533
Epoch 9/10, Batch 271/883, Training Loss: 0.6003
Epoch 9/10, Batch 272/883, Training Loss: 0.7802
Epoch 9/10, Batch 273/883, Training Loss: 0.3708
Epoch 9/10, Batch 274/883, Training Loss: 0.5279
Epoch 9/10, Batch 275/883, Training Loss: 0.7818
Epoch 9/10, Batch 276/883, Training Loss: 0.6981
Epoch 9/10, Batch 277/883, Training Loss: 0.3445
Epoch 9/10, Batch 278/883, Training Loss: 0.4671
Epoch 9/10, Batch 279/883, Training Loss: 0.5188
Epoch 9/10, Batch 280/883, Training Loss: 0.7436
Epoch 9/10, Batch 281/883, Training Loss: 0.5251
Epoch 9/10, Batch 282/883, Training Loss: 0.4904
Epoch 9/10, Batch 283/883, Training Loss: 0.3312
Epoch 9/10, Batch 284/883, Training Loss: 0.9731
Epoch 9/10, Batch 285/883, Training Loss: 0.5684
Epoch 9/10, Batch 286/883, Training Loss: 0.3919
Epoch 9/10, Batch 287/883, Training Loss: 0.7669
Epoch 9/10, Batch 288/883, Training Loss: 0.5877
Epoch 9/10, Batch 289/883, Training Loss: 0.4158
Epoch 9/10, Batch 290/883, Training Loss: 0.6931
Epoch 9/10, Batch 291/883, Training Loss: 0.5335
Epoch 9/10, Batch 292/883, Training Loss: 0.2388
Epoch 9/10, Batch 293/883, Training Loss: 0.4767
Epoch 9/10, Batch 294/883, Training Loss: 0.5419
Epoch 9/10, Batch 295/883, Training Loss: 0.4855
Epoch 9/10, Batch 296/883, Training Loss: 0.6041
Epoch 9/10, Batch 297/883, Training Loss: 1.3066
Epoch 9/10, Batch 298/883, Training Loss: 0.5850
Epoch 9/10, Batch 299/883, Training Loss: 0.6016
Epoch 9/10, Batch 300/883, Training Loss: 0.5727
Epoch 9/10, Batch 301/883, Training Loss: 0.8216
Epoch 9/10, Batch 302/883, Training Loss: 0.8416
Epoch 9/10, Batch 303/883, Training Loss: 0.7900
Epoch 9/10, Batch 304/883, Training Loss: 0.5405
Epoch 9/10, Batch 305/883, Training Loss: 0.5262
Epoch 9/10, Batch 306/883, Training Loss: 0.8432
Epoch 9/10, Batch 307/883, Training Loss: 0.7572
Epoch 9/10, Batch 308/883, Training Loss: 0.5240
Epoch 9/10, Batch 309/883, Training Loss: 0.6691
Epoch 9/10, Batch 310/883, Training Loss: 0.6013
Epoch 9/10, Batch 311/883, Training Loss: 0.3051
Epoch 9/10, Batch 312/883, Training Loss: 0.4820
Epoch 9/10, Batch 313/883, Training Loss: 0.7502
Epoch 9/10, Batch 314/883, Training Loss: 0.6282
Epoch 9/10, Batch 315/883, Training Loss: 0.8634
Epoch 9/10, Batch 316/883, Training Loss: 0.5830
Epoch 9/10, Batch 317/883, Training Loss: 0.3709
Epoch 9/10, Batch 318/883, Training Loss: 1.0000
Epoch 9/10, Batch 319/883, Training Loss: 0.4493
Epoch 9/10, Batch 320/883, Training Loss: 0.8051
Epoch 9/10, Batch 321/883, Training Loss: 0.7104
Epoch 9/10, Batch 322/883, Training Loss: 0.6566
Epoch 9/10, Batch 323/883, Training Loss: 0.5588
Epoch 9/10, Batch 324/883, Training Loss: 0.4147
Epoch 9/10, Batch 325/883, Training Loss: 0.5888
Epoch 9/10, Batch 326/883, Training Loss: 0.2801
Epoch 9/10, Batch 327/883, Training Loss: 0.4071
Epoch 9/10, Batch 328/883, Training Loss: 0.6259
Epoch 9/10, Batch 329/883, Training Loss: 0.4621
Epoch 9/10, Batch 330/883, Training Loss: 0.5329
Epoch 9/10, Batch 331/883, Training Loss: 0.5794
Epoch 9/10, Batch 332/883, Training Loss: 0.5195
Epoch 9/10, Batch 333/883, Training Loss: 0.5230
Epoch 9/10, Batch 334/883, Training Loss: 1.0025
Epoch 9/10, Batch 335/883, Training Loss: 0.3443
Epoch 9/10, Batch 336/883, Training Loss: 0.8155
Epoch 9/10, Batch 337/883, Training Loss: 0.4167
Epoch 9/10, Batch 338/883, Training Loss: 0.5052
Epoch 9/10, Batch 339/883, Training Loss: 0.8386
Epoch 9/10, Batch 340/883, Training Loss: 0.7053
Epoch 9/10, Batch 341/883, Training Loss: 0.6065
Epoch 9/10, Batch 342/883, Training Loss: 0.4996
Epoch 9/10, Batch 343/883, Training Loss: 0.4118
Epoch 9/10, Batch 344/883, Training Loss: 0.7467
Epoch 9/10, Batch 345/883, Training Loss: 0.3760
Epoch 9/10, Batch 346/883, Training Loss: 0.5697
Epoch 9/10, Batch 347/883, Training Loss: 0.3987
Epoch 9/10, Batch 348/883, Training Loss: 0.4773
Epoch 9/10, Batch 349/883, Training Loss: 0.5129
Epoch 9/10, Batch 350/883, Training Loss: 0.8203
Epoch 9/10, Batch 351/883, Training Loss: 0.4747
Epoch 9/10, Batch 352/883, Training Loss: 0.8836
Epoch 9/10, Batch 353/883, Training Loss: 0.8483
Epoch 9/10, Batch 354/883, Training Loss: 0.5407
Epoch 9/10, Batch 355/883, Training Loss: 0.8874
Epoch 9/10, Batch 356/883, Training Loss: 0.6413
Epoch 9/10, Batch 357/883, Training Loss: 0.3886
Epoch 9/10, Batch 358/883, Training Loss: 0.6152
Epoch 9/10, Batch 359/883, Training Loss: 0.5591
Epoch 9/10, Batch 360/883, Training Loss: 0.4984
Epoch 9/10, Batch 361/883, Training Loss: 0.5297
Epoch 9/10, Batch 362/883, Training Loss: 0.4397
Epoch 9/10, Batch 363/883, Training Loss: 0.4239
Epoch 9/10, Batch 364/883, Training Loss: 0.5370
Epoch 9/10, Batch 365/883, Training Loss: 0.4995
Epoch 9/10, Batch 366/883, Training Loss: 0.5525
Epoch 9/10, Batch 367/883, Training Loss: 0.8951
Epoch 9/10, Batch 368/883, Training Loss: 0.3164
Epoch 9/10, Batch 369/883, Training Loss: 0.8067
Epoch 9/10, Batch 370/883, Training Loss: 0.3794
Epoch 9/10, Batch 371/883, Training Loss: 0.5785
Epoch 9/10, Batch 372/883, Training Loss: 0.6269
Epoch 9/10, Batch 373/883, Training Loss: 0.5645
Epoch 9/10, Batch 374/883, Training Loss: 0.6994
Epoch 9/10, Batch 375/883, Training Loss: 0.3998
Epoch 9/10, Batch 376/883, Training Loss: 0.2893
Epoch 9/10, Batch 377/883, Training Loss: 0.6501
Epoch 9/10, Batch 378/883, Training Loss: 0.8029
Epoch 9/10, Batch 379/883, Training Loss: 0.6617
Epoch 9/10, Batch 380/883, Training Loss: 0.8497
Epoch 9/10, Batch 381/883, Training Loss: 0.7573
Epoch 9/10, Batch 382/883, Training Loss: 0.6341
Epoch 9/10, Batch 383/883, Training Loss: 0.7264
Epoch 9/10, Batch 384/883, Training Loss: 0.4783
Epoch 9/10, Batch 385/883, Training Loss: 0.8274
Epoch 9/10, Batch 386/883, Training Loss: 0.7873
Epoch 9/10, Batch 387/883, Training Loss: 0.5271
Epoch 9/10, Batch 388/883, Training Loss: 0.6589
Epoch 9/10, Batch 389/883, Training Loss: 0.5655
Epoch 9/10, Batch 390/883, Training Loss: 0.6952
Epoch 9/10, Batch 391/883, Training Loss: 0.2707
Epoch 9/10, Batch 392/883, Training Loss: 0.5068
Epoch 9/10, Batch 393/883, Training Loss: 0.6417
Epoch 9/10, Batch 394/883, Training Loss: 0.6503
Epoch 9/10, Batch 395/883, Training Loss: 0.4004
Epoch 9/10, Batch 396/883, Training Loss: 0.5029
Epoch 9/10, Batch 397/883, Training Loss: 0.7076
Epoch 9/10, Batch 398/883, Training Loss: 0.5086
Epoch 9/10, Batch 399/883, Training Loss: 0.5785
Epoch 9/10, Batch 400/883, Training Loss: 0.4496
Epoch 9/10, Batch 401/883, Training Loss: 0.6353
Epoch 9/10, Batch 402/883, Training Loss: 0.7802
Epoch 9/10, Batch 403/883, Training Loss: 0.4032
Epoch 9/10, Batch 404/883, Training Loss: 0.6319
Epoch 9/10, Batch 405/883, Training Loss: 0.4095
Epoch 9/10, Batch 406/883, Training Loss: 0.7371
Epoch 9/10, Batch 407/883, Training Loss: 0.6090
Epoch 9/10, Batch 408/883, Training Loss: 0.5026
Epoch 9/10, Batch 409/883, Training Loss: 0.4533
Epoch 9/10, Batch 410/883, Training Loss: 0.7327
Epoch 9/10, Batch 411/883, Training Loss: 0.8227
Epoch 9/10, Batch 412/883, Training Loss: 0.4719
Epoch 9/10, Batch 413/883, Training Loss: 0.5214
Epoch 9/10, Batch 414/883, Training Loss: 0.7834
Epoch 9/10, Batch 415/883, Training Loss: 0.4527
Epoch 9/10, Batch 416/883, Training Loss: 0.3445
Epoch 9/10, Batch 417/883, Training Loss: 0.3142
Epoch 9/10, Batch 418/883, Training Loss: 0.3851
Epoch 9/10, Batch 419/883, Training Loss: 0.6068
Epoch 9/10, Batch 420/883, Training Loss: 0.8790
Epoch 9/10, Batch 421/883, Training Loss: 0.5298
Epoch 9/10, Batch 422/883, Training Loss: 0.3302
Epoch 9/10, Batch 423/883, Training Loss: 0.4746
Epoch 9/10, Batch 424/883, Training Loss: 0.5014
Epoch 9/10, Batch 425/883, Training Loss: 0.5633
Epoch 9/10, Batch 426/883, Training Loss: 0.6487
Epoch 9/10, Batch 427/883, Training Loss: 0.5998
Epoch 9/10, Batch 428/883, Training Loss: 0.8766
Epoch 9/10, Batch 429/883, Training Loss: 0.7302
Epoch 9/10, Batch 430/883, Training Loss: 0.6133
Epoch 9/10, Batch 431/883, Training Loss: 0.3447
Epoch 9/10, Batch 432/883, Training Loss: 0.6171
Epoch 9/10, Batch 433/883, Training Loss: 0.5355
Epoch 9/10, Batch 434/883, Training Loss: 0.4732
Epoch 9/10, Batch 435/883, Training Loss: 0.5832
Epoch 9/10, Batch 436/883, Training Loss: 0.7486
Epoch 9/10, Batch 437/883, Training Loss: 0.4685
Epoch 9/10, Batch 438/883, Training Loss: 1.2550
Epoch 9/10, Batch 439/883, Training Loss: 0.4571
Epoch 9/10, Batch 440/883, Training Loss: 0.6841
Epoch 9/10, Batch 441/883, Training Loss: 0.7006
Epoch 9/10, Batch 442/883, Training Loss: 0.4890
Epoch 9/10, Batch 443/883, Training Loss: 0.5705
Epoch 9/10, Batch 444/883, Training Loss: 0.9121
Epoch 9/10, Batch 445/883, Training Loss: 0.5146
Epoch 9/10, Batch 446/883, Training Loss: 0.6614
Epoch 9/10, Batch 447/883, Training Loss: 0.8139
Epoch 9/10, Batch 448/883, Training Loss: 0.5343
Epoch 9/10, Batch 449/883, Training Loss: 0.5248
Epoch 9/10, Batch 450/883, Training Loss: 0.4700
Epoch 9/10, Batch 451/883, Training Loss: 0.6290
Epoch 9/10, Batch 452/883, Training Loss: 0.5132
Epoch 9/10, Batch 453/883, Training Loss: 0.8747
Epoch 9/10, Batch 454/883, Training Loss: 0.6388
Epoch 9/10, Batch 455/883, Training Loss: 0.3620
Epoch 9/10, Batch 456/883, Training Loss: 0.5437
Epoch 9/10, Batch 457/883, Training Loss: 1.1173
Epoch 9/10, Batch 458/883, Training Loss: 0.6970
Epoch 9/10, Batch 459/883, Training Loss: 0.5615
Epoch 9/10, Batch 460/883, Training Loss: 0.5298
Epoch 9/10, Batch 461/883, Training Loss: 0.5267
Epoch 9/10, Batch 462/883, Training Loss: 0.5784
Epoch 9/10, Batch 463/883, Training Loss: 0.7704
Epoch 9/10, Batch 464/883, Training Loss: 0.5070
Epoch 9/10, Batch 465/883, Training Loss: 0.4645
Epoch 9/10, Batch 466/883, Training Loss: 0.4299
Epoch 9/10, Batch 467/883, Training Loss: 0.5233
Epoch 9/10, Batch 468/883, Training Loss: 0.4879
Epoch 9/10, Batch 469/883, Training Loss: 0.7154
Epoch 9/10, Batch 470/883, Training Loss: 0.7444
Epoch 9/10, Batch 471/883, Training Loss: 0.4874
Epoch 9/10, Batch 472/883, Training Loss: 0.3660
Epoch 9/10, Batch 473/883, Training Loss: 0.7534
Epoch 9/10, Batch 474/883, Training Loss: 0.4685
Epoch 9/10, Batch 475/883, Training Loss: 0.3147
Epoch 9/10, Batch 476/883, Training Loss: 0.4616
Epoch 9/10, Batch 477/883, Training Loss: 0.6922
Epoch 9/10, Batch 478/883, Training Loss: 0.4040
Epoch 9/10, Batch 479/883, Training Loss: 0.5722
Epoch 9/10, Batch 480/883, Training Loss: 0.4796
Epoch 9/10, Batch 481/883, Training Loss: 0.4874
Epoch 9/10, Batch 482/883, Training Loss: 0.7147
Epoch 9/10, Batch 483/883, Training Loss: 0.3621
Epoch 9/10, Batch 484/883, Training Loss: 0.4987
Epoch 9/10, Batch 485/883, Training Loss: 0.5426
Epoch 9/10, Batch 486/883, Training Loss: 0.6252
Epoch 9/10, Batch 487/883, Training Loss: 0.4719
Epoch 9/10, Batch 488/883, Training Loss: 0.5933
Epoch 9/10, Batch 489/883, Training Loss: 0.4788
Epoch 9/10, Batch 490/883, Training Loss: 0.6100
Epoch 9/10, Batch 491/883, Training Loss: 0.5522
Epoch 9/10, Batch 492/883, Training Loss: 0.6749
Epoch 9/10, Batch 493/883, Training Loss: 0.4178
Epoch 9/10, Batch 494/883, Training Loss: 0.4289
Epoch 9/10, Batch 495/883, Training Loss: 0.3432
Epoch 9/10, Batch 496/883, Training Loss: 0.6768
Epoch 9/10, Batch 497/883, Training Loss: 0.4374
Epoch 9/10, Batch 498/883, Training Loss: 0.9504
Epoch 9/10, Batch 499/883, Training Loss: 0.8683
Epoch 9/10, Batch 500/883, Training Loss: 0.5996
Epoch 9/10, Batch 501/883, Training Loss: 0.6485
Epoch 9/10, Batch 502/883, Training Loss: 0.4316
Epoch 9/10, Batch 503/883, Training Loss: 0.4845
Epoch 9/10, Batch 504/883, Training Loss: 0.4648
Epoch 9/10, Batch 505/883, Training Loss: 0.4975
Epoch 9/10, Batch 506/883, Training Loss: 0.5015
Epoch 9/10, Batch 507/883, Training Loss: 0.5730
Epoch 9/10, Batch 508/883, Training Loss: 0.5155
Epoch 9/10, Batch 509/883, Training Loss: 0.9360
Epoch 9/10, Batch 510/883, Training Loss: 0.5974
Epoch 9/10, Batch 511/883, Training Loss: 0.3360
Epoch 9/10, Batch 512/883, Training Loss: 0.6215
Epoch 9/10, Batch 513/883, Training Loss: 0.7455
Epoch 9/10, Batch 514/883, Training Loss: 0.6195
Epoch 9/10, Batch 515/883, Training Loss: 0.5443
Epoch 9/10, Batch 516/883, Training Loss: 0.2648
Epoch 9/10, Batch 517/883, Training Loss: 0.4001
Epoch 9/10, Batch 518/883, Training Loss: 0.3399
Epoch 9/10, Batch 519/883, Training Loss: 0.4710
Epoch 9/10, Batch 520/883, Training Loss: 0.9475
Epoch 9/10, Batch 521/883, Training Loss: 0.5806
Epoch 9/10, Batch 522/883, Training Loss: 0.8505
Epoch 9/10, Batch 523/883, Training Loss: 1.0066
Epoch 9/10, Batch 524/883, Training Loss: 0.5668
Epoch 9/10, Batch 525/883, Training Loss: 0.3357
Epoch 9/10, Batch 526/883, Training Loss: 0.3883
Epoch 9/10, Batch 527/883, Training Loss: 0.8424
Epoch 9/10, Batch 528/883, Training Loss: 0.4974
Epoch 9/10, Batch 529/883, Training Loss: 0.3773
Epoch 9/10, Batch 530/883, Training Loss: 0.7557
Epoch 9/10, Batch 531/883, Training Loss: 0.4371
Epoch 9/10, Batch 532/883, Training Loss: 0.4941
Epoch 9/10, Batch 533/883, Training Loss: 0.7792
Epoch 9/10, Batch 534/883, Training Loss: 0.4573
Epoch 9/10, Batch 535/883, Training Loss: 0.3879
Epoch 9/10, Batch 536/883, Training Loss: 0.5636
Epoch 9/10, Batch 537/883, Training Loss: 0.4290
Epoch 9/10, Batch 538/883, Training Loss: 0.2591
Epoch 9/10, Batch 539/883, Training Loss: 0.8175
Epoch 9/10, Batch 540/883, Training Loss: 0.5769
Epoch 9/10, Batch 541/883, Training Loss: 0.6109
Epoch 9/10, Batch 542/883, Training Loss: 0.2947
Epoch 9/10, Batch 543/883, Training Loss: 0.5780
Epoch 9/10, Batch 544/883, Training Loss: 0.3780
Epoch 9/10, Batch 545/883, Training Loss: 0.4690
Epoch 9/10, Batch 546/883, Training Loss: 0.4312
Epoch 9/10, Batch 547/883, Training Loss: 0.5152
Epoch 9/10, Batch 548/883, Training Loss: 0.4201
Epoch 9/10, Batch 549/883, Training Loss: 0.7370
Epoch 9/10, Batch 550/883, Training Loss: 0.5797
Epoch 9/10, Batch 551/883, Training Loss: 0.4195
Epoch 9/10, Batch 552/883, Training Loss: 0.8319
Epoch 9/10, Batch 553/883, Training Loss: 0.3688
Epoch 9/10, Batch 554/883, Training Loss: 0.5552
Epoch 9/10, Batch 555/883, Training Loss: 0.5606
Epoch 9/10, Batch 556/883, Training Loss: 0.7814
Epoch 9/10, Batch 557/883, Training Loss: 1.0357
Epoch 9/10, Batch 558/883, Training Loss: 1.1770
Epoch 9/10, Batch 559/883, Training Loss: 0.5371
Epoch 9/10, Batch 560/883, Training Loss: 0.8070
Epoch 9/10, Batch 561/883, Training Loss: 0.5175
Epoch 9/10, Batch 562/883, Training Loss: 0.2698
Epoch 9/10, Batch 563/883, Training Loss: 0.7846
Epoch 9/10, Batch 564/883, Training Loss: 0.6146
Epoch 9/10, Batch 565/883, Training Loss: 1.0076
Epoch 9/10, Batch 566/883, Training Loss: 0.2633
Epoch 9/10, Batch 567/883, Training Loss: 0.4474
Epoch 9/10, Batch 568/883, Training Loss: 0.7526
Epoch 9/10, Batch 569/883, Training Loss: 0.7194
Epoch 9/10, Batch 570/883, Training Loss: 0.7899
Epoch 9/10, Batch 571/883, Training Loss: 0.6242
Epoch 9/10, Batch 572/883, Training Loss: 0.6424
Epoch 9/10, Batch 573/883, Training Loss: 0.4708
Epoch 9/10, Batch 574/883, Training Loss: 0.9003
Epoch 9/10, Batch 575/883, Training Loss: 0.5714
Epoch 9/10, Batch 576/883, Training Loss: 1.0302
Epoch 9/10, Batch 577/883, Training Loss: 0.6981
Epoch 9/10, Batch 578/883, Training Loss: 0.7851
Epoch 9/10, Batch 579/883, Training Loss: 0.5170
Epoch 9/10, Batch 580/883, Training Loss: 0.4755
Epoch 9/10, Batch 581/883, Training Loss: 0.5734
Epoch 9/10, Batch 582/883, Training Loss: 0.7327
Epoch 9/10, Batch 583/883, Training Loss: 0.4693
Epoch 9/10, Batch 584/883, Training Loss: 0.6273
Epoch 9/10, Batch 585/883, Training Loss: 0.5544
Epoch 9/10, Batch 586/883, Training Loss: 0.3508
Epoch 9/10, Batch 587/883, Training Loss: 0.4915
Epoch 9/10, Batch 588/883, Training Loss: 0.6328
Epoch 9/10, Batch 589/883, Training Loss: 0.4640
Epoch 9/10, Batch 590/883, Training Loss: 0.5927
Epoch 9/10, Batch 591/883, Training Loss: 0.4532
Epoch 9/10, Batch 592/883, Training Loss: 0.7061
Epoch 9/10, Batch 593/883, Training Loss: 0.6037
Epoch 9/10, Batch 594/883, Training Loss: 0.4553
Epoch 9/10, Batch 595/883, Training Loss: 0.6797
Epoch 9/10, Batch 596/883, Training Loss: 1.1140
Epoch 9/10, Batch 597/883, Training Loss: 0.5015
Epoch 9/10, Batch 598/883, Training Loss: 0.9080
Epoch 9/10, Batch 599/883, Training Loss: 0.4486
Epoch 9/10, Batch 600/883, Training Loss: 0.6017
Epoch 9/10, Batch 601/883, Training Loss: 0.9742
Epoch 9/10, Batch 602/883, Training Loss: 0.4400
Epoch 9/10, Batch 603/883, Training Loss: 0.5196
Epoch 9/10, Batch 604/883, Training Loss: 0.4047
Epoch 9/10, Batch 605/883, Training Loss: 0.6157
Epoch 9/10, Batch 606/883, Training Loss: 0.7161
Epoch 9/10, Batch 607/883, Training Loss: 0.7243
Epoch 9/10, Batch 608/883, Training Loss: 0.5865
Epoch 9/10, Batch 609/883, Training Loss: 0.4092
Epoch 9/10, Batch 610/883, Training Loss: 0.4890
Epoch 9/10, Batch 611/883, Training Loss: 0.3866
Epoch 9/10, Batch 612/883, Training Loss: 0.5832
Epoch 9/10, Batch 613/883, Training Loss: 0.4397
Epoch 9/10, Batch 614/883, Training Loss: 0.5298
Epoch 9/10, Batch 615/883, Training Loss: 0.5446
Epoch 9/10, Batch 616/883, Training Loss: 0.3640
Epoch 9/10, Batch 617/883, Training Loss: 0.7293
Epoch 9/10, Batch 618/883, Training Loss: 0.7355
Epoch 9/10, Batch 619/883, Training Loss: 0.5351
Epoch 9/10, Batch 620/883, Training Loss: 0.8780
Epoch 9/10, Batch 621/883, Training Loss: 0.5818
Epoch 9/10, Batch 622/883, Training Loss: 0.5731
Epoch 9/10, Batch 623/883, Training Loss: 0.6249
Epoch 9/10, Batch 624/883, Training Loss: 0.5893
Epoch 9/10, Batch 625/883, Training Loss: 1.0214
Epoch 9/10, Batch 626/883, Training Loss: 0.9880
Epoch 9/10, Batch 627/883, Training Loss: 0.5158
Epoch 9/10, Batch 628/883, Training Loss: 0.4745
Epoch 9/10, Batch 629/883, Training Loss: 0.7877
Epoch 9/10, Batch 630/883, Training Loss: 0.5112
Epoch 9/10, Batch 631/883, Training Loss: 0.6591
Epoch 9/10, Batch 632/883, Training Loss: 0.3038
Epoch 9/10, Batch 633/883, Training Loss: 0.4749
Epoch 9/10, Batch 634/883, Training Loss: 0.5453
Epoch 9/10, Batch 635/883, Training Loss: 0.9227
Epoch 9/10, Batch 636/883, Training Loss: 0.5324
Epoch 9/10, Batch 637/883, Training Loss: 0.4045
Epoch 9/10, Batch 638/883, Training Loss: 0.9544
Epoch 9/10, Batch 639/883, Training Loss: 0.5551
Epoch 9/10, Batch 640/883, Training Loss: 0.6103
Epoch 9/10, Batch 641/883, Training Loss: 0.5248
Epoch 9/10, Batch 642/883, Training Loss: 0.5912
Epoch 9/10, Batch 643/883, Training Loss: 0.6009
Epoch 9/10, Batch 644/883, Training Loss: 0.5275
Epoch 9/10, Batch 645/883, Training Loss: 0.6293
Epoch 9/10, Batch 646/883, Training Loss: 0.6543
Epoch 9/10, Batch 647/883, Training Loss: 0.7247
Epoch 9/10, Batch 648/883, Training Loss: 0.6406
Epoch 9/10, Batch 649/883, Training Loss: 0.9730
Epoch 9/10, Batch 650/883, Training Loss: 0.5407
Epoch 9/10, Batch 651/883, Training Loss: 0.5454
Epoch 9/10, Batch 652/883, Training Loss: 0.4941
Epoch 9/10, Batch 653/883, Training Loss: 0.5979
Epoch 9/10, Batch 654/883, Training Loss: 0.3982
Epoch 9/10, Batch 655/883, Training Loss: 0.3791
Epoch 9/10, Batch 656/883, Training Loss: 0.5996
Epoch 9/10, Batch 657/883, Training Loss: 0.4425
Epoch 9/10, Batch 658/883, Training Loss: 0.6695
Epoch 9/10, Batch 659/883, Training Loss: 0.4930
Epoch 9/10, Batch 660/883, Training Loss: 0.3933
Epoch 9/10, Batch 661/883, Training Loss: 0.4126
Epoch 9/10, Batch 662/883, Training Loss: 0.7372
Epoch 9/10, Batch 663/883, Training Loss: 0.9199
Epoch 9/10, Batch 664/883, Training Loss: 0.2832
Epoch 9/10, Batch 665/883, Training Loss: 0.6960
Epoch 9/10, Batch 666/883, Training Loss: 0.7413
Epoch 9/10, Batch 667/883, Training Loss: 0.5991
Epoch 9/10, Batch 668/883, Training Loss: 0.7279
Epoch 9/10, Batch 669/883, Training Loss: 0.8503
Epoch 9/10, Batch 670/883, Training Loss: 0.6574
Epoch 9/10, Batch 671/883, Training Loss: 0.5957
Epoch 9/10, Batch 672/883, Training Loss: 0.5980
Epoch 9/10, Batch 673/883, Training Loss: 0.6681
Epoch 9/10, Batch 674/883, Training Loss: 0.6920
Epoch 9/10, Batch 675/883, Training Loss: 0.3028
Epoch 9/10, Batch 676/883, Training Loss: 0.6333
Epoch 9/10, Batch 677/883, Training Loss: 0.3698
Epoch 9/10, Batch 678/883, Training Loss: 0.7758
Epoch 9/10, Batch 679/883, Training Loss: 0.5851
Epoch 9/10, Batch 680/883, Training Loss: 0.8769
Epoch 9/10, Batch 681/883, Training Loss: 0.5858
Epoch 9/10, Batch 682/883, Training Loss: 0.3313
Epoch 9/10, Batch 683/883, Training Loss: 0.6303
Epoch 9/10, Batch 684/883, Training Loss: 0.7988
Epoch 9/10, Batch 685/883, Training Loss: 0.6002
Epoch 9/10, Batch 686/883, Training Loss: 0.4622
Epoch 9/10, Batch 687/883, Training Loss: 0.3779
Epoch 9/10, Batch 688/883, Training Loss: 0.6379
Epoch 9/10, Batch 689/883, Training Loss: 0.7008
Epoch 9/10, Batch 690/883, Training Loss: 0.5732
Epoch 9/10, Batch 691/883, Training Loss: 0.4767
Epoch 9/10, Batch 692/883, Training Loss: 0.6255
Epoch 9/10, Batch 693/883, Training Loss: 0.5674
Epoch 9/10, Batch 694/883, Training Loss: 0.4087
Epoch 9/10, Batch 695/883, Training Loss: 0.3302
Epoch 9/10, Batch 696/883, Training Loss: 0.7209
Epoch 9/10, Batch 697/883, Training Loss: 0.4527
Epoch 9/10, Batch 698/883, Training Loss: 0.6194
Epoch 9/10, Batch 699/883, Training Loss: 0.4397
Epoch 9/10, Batch 700/883, Training Loss: 0.3758
Epoch 9/10, Batch 701/883, Training Loss: 0.5566
Epoch 9/10, Batch 702/883, Training Loss: 0.6172
Epoch 9/10, Batch 703/883, Training Loss: 0.7695
Epoch 9/10, Batch 704/883, Training Loss: 0.6742
Epoch 9/10, Batch 705/883, Training Loss: 0.5791
Epoch 9/10, Batch 706/883, Training Loss: 0.4300
Epoch 9/10, Batch 707/883, Training Loss: 0.6237
Epoch 9/10, Batch 708/883, Training Loss: 0.6373
Epoch 9/10, Batch 709/883, Training Loss: 0.5968
Epoch 9/10, Batch 710/883, Training Loss: 0.5789
Epoch 9/10, Batch 711/883, Training Loss: 0.4494
Epoch 9/10, Batch 712/883, Training Loss: 0.7567
Epoch 9/10, Batch 713/883, Training Loss: 0.4200
Epoch 9/10, Batch 714/883, Training Loss: 0.4443
Epoch 9/10, Batch 715/883, Training Loss: 0.4488
Epoch 9/10, Batch 716/883, Training Loss: 0.6458
Epoch 9/10, Batch 717/883, Training Loss: 0.6883
Epoch 9/10, Batch 718/883, Training Loss: 0.4732
Epoch 9/10, Batch 719/883, Training Loss: 0.4997
Epoch 9/10, Batch 720/883, Training Loss: 0.8972
Epoch 9/10, Batch 721/883, Training Loss: 0.5664
Epoch 9/10, Batch 722/883, Training Loss: 0.8285
Epoch 9/10, Batch 723/883, Training Loss: 0.4284
Epoch 9/10, Batch 724/883, Training Loss: 0.7163
Epoch 9/10, Batch 725/883, Training Loss: 0.8115
Epoch 9/10, Batch 726/883, Training Loss: 0.8012
Epoch 9/10, Batch 727/883, Training Loss: 0.3920
Epoch 9/10, Batch 728/883, Training Loss: 1.0088
Epoch 9/10, Batch 729/883, Training Loss: 0.8352
Epoch 9/10, Batch 730/883, Training Loss: 0.5944
Epoch 9/10, Batch 731/883, Training Loss: 0.3924
Epoch 9/10, Batch 732/883, Training Loss: 0.6071
Epoch 9/10, Batch 733/883, Training Loss: 0.6117
Epoch 9/10, Batch 734/883, Training Loss: 0.5528
Epoch 9/10, Batch 735/883, Training Loss: 0.4360
Epoch 9/10, Batch 736/883, Training Loss: 0.3609
Epoch 9/10, Batch 737/883, Training Loss: 0.5007
Epoch 9/10, Batch 738/883, Training Loss: 0.8719
Epoch 9/10, Batch 739/883, Training Loss: 0.3222
Epoch 9/10, Batch 740/883, Training Loss: 1.0175
Epoch 9/10, Batch 741/883, Training Loss: 0.3599
Epoch 9/10, Batch 742/883, Training Loss: 0.6223
Epoch 9/10, Batch 743/883, Training Loss: 0.4520
Epoch 9/10, Batch 744/883, Training Loss: 0.4553
Epoch 9/10, Batch 745/883, Training Loss: 0.5920
Epoch 9/10, Batch 746/883, Training Loss: 0.4594
Epoch 9/10, Batch 747/883, Training Loss: 0.5986
Epoch 9/10, Batch 748/883, Training Loss: 0.9490
Epoch 9/10, Batch 749/883, Training Loss: 0.8704
Epoch 9/10, Batch 750/883, Training Loss: 0.5251
Epoch 9/10, Batch 751/883, Training Loss: 0.3910
Epoch 9/10, Batch 752/883, Training Loss: 0.6105
Epoch 9/10, Batch 753/883, Training Loss: 0.5795
Epoch 9/10, Batch 754/883, Training Loss: 0.7962
Epoch 9/10, Batch 755/883, Training Loss: 0.6275
Epoch 9/10, Batch 756/883, Training Loss: 0.4179
Epoch 9/10, Batch 757/883, Training Loss: 0.5388
Epoch 9/10, Batch 758/883, Training Loss: 0.4228
Epoch 9/10, Batch 759/883, Training Loss: 0.7484
Epoch 9/10, Batch 760/883, Training Loss: 0.2796
Epoch 9/10, Batch 761/883, Training Loss: 0.5832
Epoch 9/10, Batch 762/883, Training Loss: 0.4011
Epoch 9/10, Batch 763/883, Training Loss: 0.6220
Epoch 9/10, Batch 764/883, Training Loss: 0.5275
Epoch 9/10, Batch 765/883, Training Loss: 0.7786
Epoch 9/10, Batch 766/883, Training Loss: 0.5184
Epoch 9/10, Batch 767/883, Training Loss: 0.5634
Epoch 9/10, Batch 768/883, Training Loss: 0.6700
Epoch 9/10, Batch 769/883, Training Loss: 0.4786
Epoch 9/10, Batch 770/883, Training Loss: 0.8945
Epoch 9/10, Batch 771/883, Training Loss: 0.3961
Epoch 9/10, Batch 772/883, Training Loss: 0.6386
Epoch 9/10, Batch 773/883, Training Loss: 0.5278
Epoch 9/10, Batch 774/883, Training Loss: 0.6312
Epoch 9/10, Batch 775/883, Training Loss: 0.5484
Epoch 9/10, Batch 776/883, Training Loss: 0.4063
Epoch 9/10, Batch 777/883, Training Loss: 0.3392
Epoch 9/10, Batch 778/883, Training Loss: 0.5037
Epoch 9/10, Batch 779/883, Training Loss: 0.3295
Epoch 9/10, Batch 780/883, Training Loss: 0.3277
Epoch 9/10, Batch 781/883, Training Loss: 0.5344
Epoch 9/10, Batch 782/883, Training Loss: 0.6977
Epoch 9/10, Batch 783/883, Training Loss: 0.5485
Epoch 9/10, Batch 784/883, Training Loss: 0.6822
Epoch 9/10, Batch 785/883, Training Loss: 0.5904
Epoch 9/10, Batch 786/883, Training Loss: 0.3987
Epoch 9/10, Batch 787/883, Training Loss: 0.8477
Epoch 9/10, Batch 788/883, Training Loss: 0.7011
Epoch 9/10, Batch 789/883, Training Loss: 0.3850
Epoch 9/10, Batch 790/883, Training Loss: 0.5936
Epoch 9/10, Batch 791/883, Training Loss: 0.4731
Epoch 9/10, Batch 792/883, Training Loss: 0.2868
Epoch 9/10, Batch 793/883, Training Loss: 0.5230
Epoch 9/10, Batch 794/883, Training Loss: 0.4477
Epoch 9/10, Batch 795/883, Training Loss: 0.5804
Epoch 9/10, Batch 796/883, Training Loss: 0.8663
Epoch 9/10, Batch 797/883, Training Loss: 0.5835
Epoch 9/10, Batch 798/883, Training Loss: 0.8857
Epoch 9/10, Batch 799/883, Training Loss: 0.5269
Epoch 9/10, Batch 800/883, Training Loss: 0.6724
Epoch 9/10, Batch 801/883, Training Loss: 0.4916
Epoch 9/10, Batch 802/883, Training Loss: 0.8130
Epoch 9/10, Batch 803/883, Training Loss: 0.5185
Epoch 9/10, Batch 804/883, Training Loss: 0.4860
Epoch 9/10, Batch 805/883, Training Loss: 0.3837
Epoch 9/10, Batch 806/883, Training Loss: 0.6577
Epoch 9/10, Batch 807/883, Training Loss: 0.5665
Epoch 9/10, Batch 808/883, Training Loss: 0.6875
Epoch 9/10, Batch 809/883, Training Loss: 0.8937
Epoch 9/10, Batch 810/883, Training Loss: 0.4354
Epoch 9/10, Batch 811/883, Training Loss: 0.5265
Epoch 9/10, Batch 812/883, Training Loss: 0.7377
Epoch 9/10, Batch 813/883, Training Loss: 0.4423
Epoch 9/10, Batch 814/883, Training Loss: 0.5031
Epoch 9/10, Batch 815/883, Training Loss: 0.5203
Epoch 9/10, Batch 816/883, Training Loss: 0.4917
Epoch 9/10, Batch 817/883, Training Loss: 0.4262
Epoch 9/10, Batch 818/883, Training Loss: 0.4743
Epoch 9/10, Batch 819/883, Training Loss: 1.0761
Epoch 9/10, Batch 820/883, Training Loss: 0.4769
Epoch 9/10, Batch 821/883, Training Loss: 1.0472
Epoch 9/10, Batch 822/883, Training Loss: 0.6374
Epoch 9/10, Batch 823/883, Training Loss: 0.5926
Epoch 9/10, Batch 824/883, Training Loss: 0.2881
Epoch 9/10, Batch 825/883, Training Loss: 0.6305
Epoch 9/10, Batch 826/883, Training Loss: 0.5809
Epoch 9/10, Batch 827/883, Training Loss: 0.4856
Epoch 9/10, Batch 828/883, Training Loss: 0.7638
Epoch 9/10, Batch 829/883, Training Loss: 0.4993
Epoch 9/10, Batch 830/883, Training Loss: 0.6431
Epoch 9/10, Batch 831/883, Training Loss: 0.7674
Epoch 9/10, Batch 832/883, Training Loss: 0.5664
Epoch 9/10, Batch 833/883, Training Loss: 0.6309
Epoch 9/10, Batch 834/883, Training Loss: 0.8614
Epoch 9/10, Batch 835/883, Training Loss: 0.4702
Epoch 9/10, Batch 836/883, Training Loss: 0.4753
Epoch 9/10, Batch 837/883, Training Loss: 0.5817
Epoch 9/10, Batch 838/883, Training Loss: 0.5215
Epoch 9/10, Batch 839/883, Training Loss: 0.6146
Epoch 9/10, Batch 840/883, Training Loss: 0.4495
Epoch 9/10, Batch 841/883, Training Loss: 0.6122
Epoch 9/10, Batch 842/883, Training Loss: 0.6308
Epoch 9/10, Batch 843/883, Training Loss: 0.6962
Epoch 9/10, Batch 844/883, Training Loss: 0.5299
Epoch 9/10, Batch 845/883, Training Loss: 0.5587
Epoch 9/10, Batch 846/883, Training Loss: 0.5478
Epoch 9/10, Batch 847/883, Training Loss: 0.5571
Epoch 9/10, Batch 848/883, Training Loss: 0.5256
Epoch 9/10, Batch 849/883, Training Loss: 0.4364
Epoch 9/10, Batch 850/883, Training Loss: 0.4133
Epoch 9/10, Batch 851/883, Training Loss: 0.4575
Epoch 9/10, Batch 852/883, Training Loss: 0.4208
Epoch 9/10, Batch 853/883, Training Loss: 0.6532
Epoch 9/10, Batch 854/883, Training Loss: 0.5293
Epoch 9/10, Batch 855/883, Training Loss: 0.3993
Epoch 9/10, Batch 856/883, Training Loss: 0.8331
Epoch 9/10, Batch 857/883, Training Loss: 0.7395
Epoch 9/10, Batch 858/883, Training Loss: 0.5195
Epoch 9/10, Batch 859/883, Training Loss: 0.6498
Epoch 9/10, Batch 860/883, Training Loss: 0.5551
Epoch 9/10, Batch 861/883, Training Loss: 0.5569
Epoch 9/10, Batch 862/883, Training Loss: 0.5497
Epoch 9/10, Batch 863/883, Training Loss: 0.6730
Epoch 9/10, Batch 864/883, Training Loss: 0.4929
Epoch 9/10, Batch 865/883, Training Loss: 0.6260
Epoch 9/10, Batch 866/883, Training Loss: 0.5963
Epoch 9/10, Batch 867/883, Training Loss: 0.6587
Epoch 9/10, Batch 868/883, Training Loss: 0.3214
Epoch 9/10, Batch 869/883, Training Loss: 0.5948
Epoch 9/10, Batch 870/883, Training Loss: 0.7235
Epoch 9/10, Batch 871/883, Training Loss: 0.5681
Epoch 9/10, Batch 872/883, Training Loss: 0.5133
Epoch 9/10, Batch 873/883, Training Loss: 0.5211
Epoch 9/10, Batch 874/883, Training Loss: 0.8064
Epoch 9/10, Batch 875/883, Training Loss: 0.6154
Epoch 9/10, Batch 876/883, Training Loss: 0.6659
Epoch 9/10, Batch 877/883, Training Loss: 0.5476
Epoch 9/10, Batch 878/883, Training Loss: 0.6021
Epoch 9/10, Batch 879/883, Training Loss: 0.5399
Epoch 9/10, Batch 880/883, Training Loss: 0.5827
Epoch 9/10, Batch 881/883, Training Loss: 0.8807
Epoch 9/10, Batch 882/883, Training Loss: 0.5403
Epoch 9/10, Batch 883/883, Training Loss: 0.2738
Epoch 9/10, Training Loss: 0.5900, Validation Loss: 0.6287, Validation Accuracy: 0.7231
Epoch 10/10, Batch 1/883, Training Loss: 0.6240
Epoch 10/10, Batch 2/883, Training Loss: 0.4735
Epoch 10/10, Batch 3/883, Training Loss: 0.2241
Epoch 10/10, Batch 4/883, Training Loss: 0.6712
Epoch 10/10, Batch 5/883, Training Loss: 1.0383
Epoch 10/10, Batch 6/883, Training Loss: 0.7660
Epoch 10/10, Batch 7/883, Training Loss: 0.3729
Epoch 10/10, Batch 8/883, Training Loss: 0.4360
Epoch 10/10, Batch 9/883, Training Loss: 0.4184
Epoch 10/10, Batch 10/883, Training Loss: 0.5972
Epoch 10/10, Batch 11/883, Training Loss: 0.7105
Epoch 10/10, Batch 12/883, Training Loss: 0.3054
Epoch 10/10, Batch 13/883, Training Loss: 0.3728
Epoch 10/10, Batch 14/883, Training Loss: 0.4981
Epoch 10/10, Batch 15/883, Training Loss: 0.3469
Epoch 10/10, Batch 16/883, Training Loss: 0.7559
Epoch 10/10, Batch 17/883, Training Loss: 0.5314
Epoch 10/10, Batch 18/883, Training Loss: 0.5823
Epoch 10/10, Batch 19/883, Training Loss: 0.6707
Epoch 10/10, Batch 20/883, Training Loss: 0.5794
Epoch 10/10, Batch 21/883, Training Loss: 0.2938
Epoch 10/10, Batch 22/883, Training Loss: 0.3439
Epoch 10/10, Batch 23/883, Training Loss: 0.3645
Epoch 10/10, Batch 24/883, Training Loss: 0.4282
Epoch 10/10, Batch 25/883, Training Loss: 0.3655
Epoch 10/10, Batch 26/883, Training Loss: 0.7874
Epoch 10/10, Batch 27/883, Training Loss: 0.9791
Epoch 10/10, Batch 28/883, Training Loss: 0.3328
Epoch 10/10, Batch 29/883, Training Loss: 0.3341
Epoch 10/10, Batch 30/883, Training Loss: 0.5028
Epoch 10/10, Batch 31/883, Training Loss: 0.7525
Epoch 10/10, Batch 32/883, Training Loss: 0.3341
Epoch 10/10, Batch 33/883, Training Loss: 0.3502
Epoch 10/10, Batch 34/883, Training Loss: 0.5901
Epoch 10/10, Batch 35/883, Training Loss: 1.0314
Epoch 10/10, Batch 36/883, Training Loss: 0.7518
Epoch 10/10, Batch 37/883, Training Loss: 0.5358
Epoch 10/10, Batch 38/883, Training Loss: 0.5130
Epoch 10/10, Batch 39/883, Training Loss: 0.8251
Epoch 10/10, Batch 40/883, Training Loss: 0.6608
Epoch 10/10, Batch 41/883, Training Loss: 0.8492
Epoch 10/10, Batch 42/883, Training Loss: 0.6008
Epoch 10/10, Batch 43/883, Training Loss: 0.8441
Epoch 10/10, Batch 44/883, Training Loss: 0.5850
Epoch 10/10, Batch 45/883, Training Loss: 0.5046
Epoch 10/10, Batch 46/883, Training Loss: 0.9936
Epoch 10/10, Batch 47/883, Training Loss: 0.3160
Epoch 10/10, Batch 48/883, Training Loss: 0.4496
Epoch 10/10, Batch 49/883, Training Loss: 0.6704
Epoch 10/10, Batch 50/883, Training Loss: 0.3850
Epoch 10/10, Batch 51/883, Training Loss: 0.4674
Epoch 10/10, Batch 52/883, Training Loss: 0.6108
Epoch 10/10, Batch 53/883, Training Loss: 0.4979
Epoch 10/10, Batch 54/883, Training Loss: 0.6087
Epoch 10/10, Batch 55/883, Training Loss: 0.3773
Epoch 10/10, Batch 56/883, Training Loss: 0.6621
Epoch 10/10, Batch 57/883, Training Loss: 0.6906
Epoch 10/10, Batch 58/883, Training Loss: 0.4545
Epoch 10/10, Batch 59/883, Training Loss: 0.4562
Epoch 10/10, Batch 60/883, Training Loss: 0.5311
Epoch 10/10, Batch 61/883, Training Loss: 0.6384
Epoch 10/10, Batch 62/883, Training Loss: 0.6775
Epoch 10/10, Batch 63/883, Training Loss: 0.5486
Epoch 10/10, Batch 64/883, Training Loss: 1.1085
Epoch 10/10, Batch 65/883, Training Loss: 0.4527
Epoch 10/10, Batch 66/883, Training Loss: 0.3812
Epoch 10/10, Batch 67/883, Training Loss: 0.5349
Epoch 10/10, Batch 68/883, Training Loss: 0.7439
Epoch 10/10, Batch 69/883, Training Loss: 0.3744
Epoch 10/10, Batch 70/883, Training Loss: 0.4045
Epoch 10/10, Batch 71/883, Training Loss: 0.6542
Epoch 10/10, Batch 72/883, Training Loss: 1.0114
Epoch 10/10, Batch 73/883, Training Loss: 0.3830
Epoch 10/10, Batch 74/883, Training Loss: 0.4723
Epoch 10/10, Batch 75/883, Training Loss: 0.8818
Epoch 10/10, Batch 76/883, Training Loss: 0.6051
Epoch 10/10, Batch 77/883, Training Loss: 0.3969
Epoch 10/10, Batch 78/883, Training Loss: 0.9818
Epoch 10/10, Batch 79/883, Training Loss: 0.5339
Epoch 10/10, Batch 80/883, Training Loss: 0.5491
Epoch 10/10, Batch 81/883, Training Loss: 0.6062
Epoch 10/10, Batch 82/883, Training Loss: 0.5369
Epoch 10/10, Batch 83/883, Training Loss: 0.5555
Epoch 10/10, Batch 84/883, Training Loss: 0.5357
Epoch 10/10, Batch 85/883, Training Loss: 0.6862
Epoch 10/10, Batch 86/883, Training Loss: 0.4780
Epoch 10/10, Batch 87/883, Training Loss: 0.3595
Epoch 10/10, Batch 88/883, Training Loss: 0.3333
Epoch 10/10, Batch 89/883, Training Loss: 0.4589
Epoch 10/10, Batch 90/883, Training Loss: 0.5397
Epoch 10/10, Batch 91/883, Training Loss: 0.7051
Epoch 10/10, Batch 92/883, Training Loss: 0.4619
Epoch 10/10, Batch 93/883, Training Loss: 0.5735
Epoch 10/10, Batch 94/883, Training Loss: 0.5457
Epoch 10/10, Batch 95/883, Training Loss: 0.6686
Epoch 10/10, Batch 96/883, Training Loss: 0.4108
Epoch 10/10, Batch 97/883, Training Loss: 0.6979
Epoch 10/10, Batch 98/883, Training Loss: 0.9157
Epoch 10/10, Batch 99/883, Training Loss: 0.7073
Epoch 10/10, Batch 100/883, Training Loss: 0.7539
Epoch 10/10, Batch 101/883, Training Loss: 0.5726
Epoch 10/10, Batch 102/883, Training Loss: 0.3210
Epoch 10/10, Batch 103/883, Training Loss: 0.5920
Epoch 10/10, Batch 104/883, Training Loss: 0.8051
Epoch 10/10, Batch 105/883, Training Loss: 0.4490
Epoch 10/10, Batch 106/883, Training Loss: 0.3193
Epoch 10/10, Batch 107/883, Training Loss: 0.5502
Epoch 10/10, Batch 108/883, Training Loss: 0.7785
Epoch 10/10, Batch 109/883, Training Loss: 0.6921
Epoch 10/10, Batch 110/883, Training Loss: 0.4038
Epoch 10/10, Batch 111/883, Training Loss: 0.7942
Epoch 10/10, Batch 112/883, Training Loss: 0.3377
Epoch 10/10, Batch 113/883, Training Loss: 0.5740
Epoch 10/10, Batch 114/883, Training Loss: 0.6541
Epoch 10/10, Batch 115/883, Training Loss: 0.3057
Epoch 10/10, Batch 116/883, Training Loss: 0.7962
Epoch 10/10, Batch 117/883, Training Loss: 0.3908
Epoch 10/10, Batch 118/883, Training Loss: 0.5764
Epoch 10/10, Batch 119/883, Training Loss: 0.4056
Epoch 10/10, Batch 120/883, Training Loss: 0.4304
Epoch 10/10, Batch 121/883, Training Loss: 0.5083
Epoch 10/10, Batch 122/883, Training Loss: 0.3738
Epoch 10/10, Batch 123/883, Training Loss: 0.3417
Epoch 10/10, Batch 124/883, Training Loss: 0.4090
Epoch 10/10, Batch 125/883, Training Loss: 0.6893
Epoch 10/10, Batch 126/883, Training Loss: 0.4172
Epoch 10/10, Batch 127/883, Training Loss: 0.7535
Epoch 10/10, Batch 128/883, Training Loss: 0.7560
Epoch 10/10, Batch 129/883, Training Loss: 0.3090
Epoch 10/10, Batch 130/883, Training Loss: 0.5878
Epoch 10/10, Batch 131/883, Training Loss: 0.3059
Epoch 10/10, Batch 132/883, Training Loss: 0.4494
Epoch 10/10, Batch 133/883, Training Loss: 0.4681
Epoch 10/10, Batch 134/883, Training Loss: 0.9895
Epoch 10/10, Batch 135/883, Training Loss: 0.6166
Epoch 10/10, Batch 136/883, Training Loss: 0.9257
Epoch 10/10, Batch 137/883, Training Loss: 0.6441
Epoch 10/10, Batch 138/883, Training Loss: 0.7610
Epoch 10/10, Batch 139/883, Training Loss: 0.4132
Epoch 10/10, Batch 140/883, Training Loss: 0.6923
Epoch 10/10, Batch 141/883, Training Loss: 0.4372
Epoch 10/10, Batch 142/883, Training Loss: 0.5805
Epoch 10/10, Batch 143/883, Training Loss: 0.8757
Epoch 10/10, Batch 144/883, Training Loss: 0.4830
Epoch 10/10, Batch 145/883, Training Loss: 0.7331
Epoch 10/10, Batch 146/883, Training Loss: 0.4488
Epoch 10/10, Batch 147/883, Training Loss: 0.7804
Epoch 10/10, Batch 148/883, Training Loss: 0.4218
Epoch 10/10, Batch 149/883, Training Loss: 0.6026
Epoch 10/10, Batch 150/883, Training Loss: 0.2711
Epoch 10/10, Batch 151/883, Training Loss: 0.5569
Epoch 10/10, Batch 152/883, Training Loss: 0.4819
Epoch 10/10, Batch 153/883, Training Loss: 0.6330
Epoch 10/10, Batch 154/883, Training Loss: 0.2952
Epoch 10/10, Batch 155/883, Training Loss: 0.6366
Epoch 10/10, Batch 156/883, Training Loss: 0.3596
Epoch 10/10, Batch 157/883, Training Loss: 0.4480
Epoch 10/10, Batch 158/883, Training Loss: 0.5328
Epoch 10/10, Batch 159/883, Training Loss: 0.7224
Epoch 10/10, Batch 160/883, Training Loss: 0.5116
Epoch 10/10, Batch 161/883, Training Loss: 0.3272
Epoch 10/10, Batch 162/883, Training Loss: 0.3804
Epoch 10/10, Batch 163/883, Training Loss: 0.6879
Epoch 10/10, Batch 164/883, Training Loss: 0.3432
Epoch 10/10, Batch 165/883, Training Loss: 0.6134
Epoch 10/10, Batch 166/883, Training Loss: 0.6492
Epoch 10/10, Batch 167/883, Training Loss: 0.5416
Epoch 10/10, Batch 168/883, Training Loss: 0.7012
Epoch 10/10, Batch 169/883, Training Loss: 0.5183
Epoch 10/10, Batch 170/883, Training Loss: 1.0603
Epoch 10/10, Batch 171/883, Training Loss: 0.8786
Epoch 10/10, Batch 172/883, Training Loss: 0.3757
Epoch 10/10, Batch 173/883, Training Loss: 0.3951
Epoch 10/10, Batch 174/883, Training Loss: 0.6112
Epoch 10/10, Batch 175/883, Training Loss: 0.6030
Epoch 10/10, Batch 176/883, Training Loss: 0.5545
Epoch 10/10, Batch 177/883, Training Loss: 0.6217
Epoch 10/10, Batch 178/883, Training Loss: 0.9193
Epoch 10/10, Batch 179/883, Training Loss: 0.5266
Epoch 10/10, Batch 180/883, Training Loss: 0.5750
Epoch 10/10, Batch 181/883, Training Loss: 0.9063
Epoch 10/10, Batch 182/883, Training Loss: 0.7849
Epoch 10/10, Batch 183/883, Training Loss: 0.4699
Epoch 10/10, Batch 184/883, Training Loss: 0.4376
Epoch 10/10, Batch 185/883, Training Loss: 0.4344
Epoch 10/10, Batch 186/883, Training Loss: 0.3895
Epoch 10/10, Batch 187/883, Training Loss: 0.5619
Epoch 10/10, Batch 188/883, Training Loss: 0.3228
Epoch 10/10, Batch 189/883, Training Loss: 0.5420
Epoch 10/10, Batch 190/883, Training Loss: 0.4588
Epoch 10/10, Batch 191/883, Training Loss: 0.7016
Epoch 10/10, Batch 192/883, Training Loss: 0.4148
Epoch 10/10, Batch 193/883, Training Loss: 1.0204
Epoch 10/10, Batch 194/883, Training Loss: 0.3748
Epoch 10/10, Batch 195/883, Training Loss: 0.7856
Epoch 10/10, Batch 196/883, Training Loss: 0.6861
Epoch 10/10, Batch 197/883, Training Loss: 0.5473
Epoch 10/10, Batch 198/883, Training Loss: 0.4082
Epoch 10/10, Batch 199/883, Training Loss: 0.8828
Epoch 10/10, Batch 200/883, Training Loss: 0.4734
Epoch 10/10, Batch 201/883, Training Loss: 0.5436
Epoch 10/10, Batch 202/883, Training Loss: 0.5708
Epoch 10/10, Batch 203/883, Training Loss: 0.5544
Epoch 10/10, Batch 204/883, Training Loss: 0.4208
Epoch 10/10, Batch 205/883, Training Loss: 0.8018
Epoch 10/10, Batch 206/883, Training Loss: 0.4065
Epoch 10/10, Batch 207/883, Training Loss: 0.6679
Epoch 10/10, Batch 208/883, Training Loss: 0.3236
Epoch 10/10, Batch 209/883, Training Loss: 0.6684
Epoch 10/10, Batch 210/883, Training Loss: 0.4536
Epoch 10/10, Batch 211/883, Training Loss: 0.5684
Epoch 10/10, Batch 212/883, Training Loss: 0.7308
Epoch 10/10, Batch 213/883, Training Loss: 0.2888
Epoch 10/10, Batch 214/883, Training Loss: 0.5289
Epoch 10/10, Batch 215/883, Training Loss: 0.4907
Epoch 10/10, Batch 216/883, Training Loss: 0.5897
Epoch 10/10, Batch 217/883, Training Loss: 0.7474
Epoch 10/10, Batch 218/883, Training Loss: 0.5321
Epoch 10/10, Batch 219/883, Training Loss: 0.7247
Epoch 10/10, Batch 220/883, Training Loss: 0.3181
Epoch 10/10, Batch 221/883, Training Loss: 0.4009
Epoch 10/10, Batch 222/883, Training Loss: 0.4236
Epoch 10/10, Batch 223/883, Training Loss: 0.5563
Epoch 10/10, Batch 224/883, Training Loss: 0.2307
Epoch 10/10, Batch 225/883, Training Loss: 0.5722
Epoch 10/10, Batch 226/883, Training Loss: 0.5306
Epoch 10/10, Batch 227/883, Training Loss: 0.6988
Epoch 10/10, Batch 228/883, Training Loss: 0.5859
Epoch 10/10, Batch 229/883, Training Loss: 0.4230
Epoch 10/10, Batch 230/883, Training Loss: 0.7917
Epoch 10/10, Batch 231/883, Training Loss: 0.7911
Epoch 10/10, Batch 232/883, Training Loss: 0.3446
Epoch 10/10, Batch 233/883, Training Loss: 0.4573
Epoch 10/10, Batch 234/883, Training Loss: 0.3327
Epoch 10/10, Batch 235/883, Training Loss: 0.5163
Epoch 10/10, Batch 236/883, Training Loss: 0.4918
Epoch 10/10, Batch 237/883, Training Loss: 0.3978
Epoch 10/10, Batch 238/883, Training Loss: 0.7800
Epoch 10/10, Batch 239/883, Training Loss: 0.5179
Epoch 10/10, Batch 240/883, Training Loss: 0.4316
Epoch 10/10, Batch 241/883, Training Loss: 0.5343
Epoch 10/10, Batch 242/883, Training Loss: 0.4583
Epoch 10/10, Batch 243/883, Training Loss: 0.3936
Epoch 10/10, Batch 244/883, Training Loss: 0.5257
Epoch 10/10, Batch 245/883, Training Loss: 0.4103
Epoch 10/10, Batch 246/883, Training Loss: 0.6660
Epoch 10/10, Batch 247/883, Training Loss: 0.5818
Epoch 10/10, Batch 248/883, Training Loss: 0.5227
Epoch 10/10, Batch 249/883, Training Loss: 0.3392
Epoch 10/10, Batch 250/883, Training Loss: 0.5823
Epoch 10/10, Batch 251/883, Training Loss: 0.4556
Epoch 10/10, Batch 252/883, Training Loss: 0.7252
Epoch 10/10, Batch 253/883, Training Loss: 0.4026
Epoch 10/10, Batch 254/883, Training Loss: 0.4778
Epoch 10/10, Batch 255/883, Training Loss: 0.4410
Epoch 10/10, Batch 256/883, Training Loss: 0.3611
Epoch 10/10, Batch 257/883, Training Loss: 0.7339
Epoch 10/10, Batch 258/883, Training Loss: 0.4829
Epoch 10/10, Batch 259/883, Training Loss: 0.3768
Epoch 10/10, Batch 260/883, Training Loss: 0.4485
Epoch 10/10, Batch 261/883, Training Loss: 0.9187
Epoch 10/10, Batch 262/883, Training Loss: 0.4133
Epoch 10/10, Batch 263/883, Training Loss: 0.4734
Epoch 10/10, Batch 264/883, Training Loss: 0.4909
Epoch 10/10, Batch 265/883, Training Loss: 0.7438
Epoch 10/10, Batch 266/883, Training Loss: 0.6287
Epoch 10/10, Batch 267/883, Training Loss: 0.3102
Epoch 10/10, Batch 268/883, Training Loss: 0.4300
Epoch 10/10, Batch 269/883, Training Loss: 1.2345
Epoch 10/10, Batch 270/883, Training Loss: 0.4912
Epoch 10/10, Batch 271/883, Training Loss: 0.5186
Epoch 10/10, Batch 272/883, Training Loss: 0.2896
Epoch 10/10, Batch 273/883, Training Loss: 0.3075
Epoch 10/10, Batch 274/883, Training Loss: 0.4969
Epoch 10/10, Batch 275/883, Training Loss: 0.3723
Epoch 10/10, Batch 276/883, Training Loss: 0.4941
Epoch 10/10, Batch 277/883, Training Loss: 0.6559
Epoch 10/10, Batch 278/883, Training Loss: 0.6214
Epoch 10/10, Batch 279/883, Training Loss: 0.4864
Epoch 10/10, Batch 280/883, Training Loss: 0.3490
Epoch 10/10, Batch 281/883, Training Loss: 0.4930
Epoch 10/10, Batch 282/883, Training Loss: 0.6113
Epoch 10/10, Batch 283/883, Training Loss: 0.8208
Epoch 10/10, Batch 284/883, Training Loss: 0.6140
Epoch 10/10, Batch 285/883, Training Loss: 0.4776
Epoch 10/10, Batch 286/883, Training Loss: 0.7100
Epoch 10/10, Batch 287/883, Training Loss: 0.6046
Epoch 10/10, Batch 288/883, Training Loss: 0.5261
Epoch 10/10, Batch 289/883, Training Loss: 0.4112
Epoch 10/10, Batch 290/883, Training Loss: 0.4615
Epoch 10/10, Batch 291/883, Training Loss: 0.8461
Epoch 10/10, Batch 292/883, Training Loss: 0.7447
Epoch 10/10, Batch 293/883, Training Loss: 0.8066
Epoch 10/10, Batch 294/883, Training Loss: 0.6058
Epoch 10/10, Batch 295/883, Training Loss: 0.3475
Epoch 10/10, Batch 296/883, Training Loss: 0.3719
Epoch 10/10, Batch 297/883, Training Loss: 0.6752
Epoch 10/10, Batch 298/883, Training Loss: 0.8159
Epoch 10/10, Batch 299/883, Training Loss: 0.4928
Epoch 10/10, Batch 300/883, Training Loss: 0.4508
Epoch 10/10, Batch 301/883, Training Loss: 0.6688
Epoch 10/10, Batch 302/883, Training Loss: 0.5816
Epoch 10/10, Batch 303/883, Training Loss: 0.8436
Epoch 10/10, Batch 304/883, Training Loss: 0.2880
Epoch 10/10, Batch 305/883, Training Loss: 1.0022
Epoch 10/10, Batch 306/883, Training Loss: 0.4739
Epoch 10/10, Batch 307/883, Training Loss: 0.7408
Epoch 10/10, Batch 308/883, Training Loss: 0.2706
Epoch 10/10, Batch 309/883, Training Loss: 0.4279
Epoch 10/10, Batch 310/883, Training Loss: 0.7360
Epoch 10/10, Batch 311/883, Training Loss: 0.8282
Epoch 10/10, Batch 312/883, Training Loss: 0.8115
Epoch 10/10, Batch 313/883, Training Loss: 0.5255
Epoch 10/10, Batch 314/883, Training Loss: 0.5521
Epoch 10/10, Batch 315/883, Training Loss: 0.6750
Epoch 10/10, Batch 316/883, Training Loss: 0.5336
Epoch 10/10, Batch 317/883, Training Loss: 0.5755
Epoch 10/10, Batch 318/883, Training Loss: 0.7310
Epoch 10/10, Batch 319/883, Training Loss: 0.5387
Epoch 10/10, Batch 320/883, Training Loss: 0.5011
Epoch 10/10, Batch 321/883, Training Loss: 0.4246
Epoch 10/10, Batch 322/883, Training Loss: 0.3329
Epoch 10/10, Batch 323/883, Training Loss: 1.1538
Epoch 10/10, Batch 324/883, Training Loss: 0.6143
Epoch 10/10, Batch 325/883, Training Loss: 0.9604
Epoch 10/10, Batch 326/883, Training Loss: 0.5466
Epoch 10/10, Batch 327/883, Training Loss: 0.4788
Epoch 10/10, Batch 328/883, Training Loss: 0.5300
Epoch 10/10, Batch 329/883, Training Loss: 0.6130
Epoch 10/10, Batch 330/883, Training Loss: 0.3973
Epoch 10/10, Batch 331/883, Training Loss: 0.5375
Epoch 10/10, Batch 332/883, Training Loss: 0.6495
Epoch 10/10, Batch 333/883, Training Loss: 0.6631
Epoch 10/10, Batch 334/883, Training Loss: 0.2638
Epoch 10/10, Batch 335/883, Training Loss: 0.5757
Epoch 10/10, Batch 336/883, Training Loss: 0.7301
Epoch 10/10, Batch 337/883, Training Loss: 0.4455
Epoch 10/10, Batch 338/883, Training Loss: 0.6135
Epoch 10/10, Batch 339/883, Training Loss: 0.2677
Epoch 10/10, Batch 340/883, Training Loss: 0.4310
Epoch 10/10, Batch 341/883, Training Loss: 0.5176
Epoch 10/10, Batch 342/883, Training Loss: 0.4085
Epoch 10/10, Batch 343/883, Training Loss: 0.2494
Epoch 10/10, Batch 344/883, Training Loss: 0.6148
Epoch 10/10, Batch 345/883, Training Loss: 0.6263
Epoch 10/10, Batch 346/883, Training Loss: 0.3994
Epoch 10/10, Batch 347/883, Training Loss: 0.5900
Epoch 10/10, Batch 348/883, Training Loss: 0.5133
Epoch 10/10, Batch 349/883, Training Loss: 0.6803
Epoch 10/10, Batch 350/883, Training Loss: 0.4769
Epoch 10/10, Batch 351/883, Training Loss: 0.3895
Epoch 10/10, Batch 352/883, Training Loss: 1.0600
Epoch 10/10, Batch 353/883, Training Loss: 0.4793
Epoch 10/10, Batch 354/883, Training Loss: 0.3525
Epoch 10/10, Batch 355/883, Training Loss: 0.4289
Epoch 10/10, Batch 356/883, Training Loss: 0.6788
Epoch 10/10, Batch 357/883, Training Loss: 0.5529
Epoch 10/10, Batch 358/883, Training Loss: 0.4789
Epoch 10/10, Batch 359/883, Training Loss: 0.4557
Epoch 10/10, Batch 360/883, Training Loss: 0.3588
Epoch 10/10, Batch 361/883, Training Loss: 0.9141
Epoch 10/10, Batch 362/883, Training Loss: 0.7019
Epoch 10/10, Batch 363/883, Training Loss: 0.5933
Epoch 10/10, Batch 364/883, Training Loss: 0.5809
Epoch 10/10, Batch 365/883, Training Loss: 0.5280
Epoch 10/10, Batch 366/883, Training Loss: 0.6216
Epoch 10/10, Batch 367/883, Training Loss: 0.5707
Epoch 10/10, Batch 368/883, Training Loss: 0.4794
Epoch 10/10, Batch 369/883, Training Loss: 0.5367
Epoch 10/10, Batch 370/883, Training Loss: 0.8389
Epoch 10/10, Batch 371/883, Training Loss: 0.4764
Epoch 10/10, Batch 372/883, Training Loss: 0.5106
Epoch 10/10, Batch 373/883, Training Loss: 0.3076
Epoch 10/10, Batch 374/883, Training Loss: 0.5040
Epoch 10/10, Batch 375/883, Training Loss: 0.4333
Epoch 10/10, Batch 376/883, Training Loss: 0.8947
Epoch 10/10, Batch 377/883, Training Loss: 1.0009
Epoch 10/10, Batch 378/883, Training Loss: 0.6308
Epoch 10/10, Batch 379/883, Training Loss: 0.3440
Epoch 10/10, Batch 380/883, Training Loss: 1.1171
Epoch 10/10, Batch 381/883, Training Loss: 0.6165
Epoch 10/10, Batch 382/883, Training Loss: 0.7008
Epoch 10/10, Batch 383/883, Training Loss: 0.5678
Epoch 10/10, Batch 384/883, Training Loss: 0.8009
Epoch 10/10, Batch 385/883, Training Loss: 0.8217
Epoch 10/10, Batch 386/883, Training Loss: 0.6869
Epoch 10/10, Batch 387/883, Training Loss: 0.7656
Epoch 10/10, Batch 388/883, Training Loss: 0.5296
Epoch 10/10, Batch 389/883, Training Loss: 0.8158
Epoch 10/10, Batch 390/883, Training Loss: 0.2836
Epoch 10/10, Batch 391/883, Training Loss: 0.3570
Epoch 10/10, Batch 392/883, Training Loss: 0.4616
Epoch 10/10, Batch 393/883, Training Loss: 0.4475
Epoch 10/10, Batch 394/883, Training Loss: 0.3793
Epoch 10/10, Batch 395/883, Training Loss: 0.4728
Epoch 10/10, Batch 396/883, Training Loss: 0.7776
Epoch 10/10, Batch 397/883, Training Loss: 0.4237
Epoch 10/10, Batch 398/883, Training Loss: 0.4483
Epoch 10/10, Batch 399/883, Training Loss: 0.4098
Epoch 10/10, Batch 400/883, Training Loss: 0.8922
Epoch 10/10, Batch 401/883, Training Loss: 0.5917
Epoch 10/10, Batch 402/883, Training Loss: 0.6459
Epoch 10/10, Batch 403/883, Training Loss: 0.3704
Epoch 10/10, Batch 404/883, Training Loss: 0.9582
Epoch 10/10, Batch 405/883, Training Loss: 0.4544
Epoch 10/10, Batch 406/883, Training Loss: 0.7722
Epoch 10/10, Batch 407/883, Training Loss: 0.4723
Epoch 10/10, Batch 408/883, Training Loss: 0.4428
Epoch 10/10, Batch 409/883, Training Loss: 0.6344
Epoch 10/10, Batch 410/883, Training Loss: 0.5829
Epoch 10/10, Batch 411/883, Training Loss: 0.4156
Epoch 10/10, Batch 412/883, Training Loss: 0.5519
Epoch 10/10, Batch 413/883, Training Loss: 0.8302
Epoch 10/10, Batch 414/883, Training Loss: 0.7731
Epoch 10/10, Batch 415/883, Training Loss: 0.4005
Epoch 10/10, Batch 416/883, Training Loss: 0.8053
Epoch 10/10, Batch 417/883, Training Loss: 0.7513
Epoch 10/10, Batch 418/883, Training Loss: 0.4943
Epoch 10/10, Batch 419/883, Training Loss: 0.5869
Epoch 10/10, Batch 420/883, Training Loss: 0.5274
Epoch 10/10, Batch 421/883, Training Loss: 0.7243
Epoch 10/10, Batch 422/883, Training Loss: 0.4415
Epoch 10/10, Batch 423/883, Training Loss: 0.6189
Epoch 10/10, Batch 424/883, Training Loss: 0.5131
Epoch 10/10, Batch 425/883, Training Loss: 0.3584
Epoch 10/10, Batch 426/883, Training Loss: 0.7802
Epoch 10/10, Batch 427/883, Training Loss: 0.2164
Epoch 10/10, Batch 428/883, Training Loss: 0.9241
Epoch 10/10, Batch 429/883, Training Loss: 0.4448
Epoch 10/10, Batch 430/883, Training Loss: 0.5943
Epoch 10/10, Batch 431/883, Training Loss: 0.5580
Epoch 10/10, Batch 432/883, Training Loss: 0.6506
Epoch 10/10, Batch 433/883, Training Loss: 0.7244
Epoch 10/10, Batch 434/883, Training Loss: 0.4429
Epoch 10/10, Batch 435/883, Training Loss: 0.4181
Epoch 10/10, Batch 436/883, Training Loss: 0.6330
Epoch 10/10, Batch 437/883, Training Loss: 0.3282
Epoch 10/10, Batch 438/883, Training Loss: 0.7814
Epoch 10/10, Batch 439/883, Training Loss: 0.6818
Epoch 10/10, Batch 440/883, Training Loss: 0.5901
Epoch 10/10, Batch 441/883, Training Loss: 0.6826
Epoch 10/10, Batch 442/883, Training Loss: 0.5125
Epoch 10/10, Batch 443/883, Training Loss: 0.3608
Epoch 10/10, Batch 444/883, Training Loss: 0.7026
Epoch 10/10, Batch 445/883, Training Loss: 0.8154
Epoch 10/10, Batch 446/883, Training Loss: 0.3843
Epoch 10/10, Batch 447/883, Training Loss: 0.5710
Epoch 10/10, Batch 448/883, Training Loss: 0.5513
Epoch 10/10, Batch 449/883, Training Loss: 0.6192
Epoch 10/10, Batch 450/883, Training Loss: 0.3868
Epoch 10/10, Batch 451/883, Training Loss: 0.4483
Epoch 10/10, Batch 452/883, Training Loss: 0.5686
Epoch 10/10, Batch 453/883, Training Loss: 0.8148
Epoch 10/10, Batch 454/883, Training Loss: 0.4865
Epoch 10/10, Batch 455/883, Training Loss: 0.5074
Epoch 10/10, Batch 456/883, Training Loss: 0.6746
Epoch 10/10, Batch 457/883, Training Loss: 0.4950
Epoch 10/10, Batch 458/883, Training Loss: 0.7405
Epoch 10/10, Batch 459/883, Training Loss: 0.6159
Epoch 10/10, Batch 460/883, Training Loss: 0.7860
Epoch 10/10, Batch 461/883, Training Loss: 0.3978
Epoch 10/10, Batch 462/883, Training Loss: 0.4978
Epoch 10/10, Batch 463/883, Training Loss: 0.5331
Epoch 10/10, Batch 464/883, Training Loss: 0.4923
Epoch 10/10, Batch 465/883, Training Loss: 0.7044
Epoch 10/10, Batch 466/883, Training Loss: 0.7622
Epoch 10/10, Batch 467/883, Training Loss: 0.5294
Epoch 10/10, Batch 468/883, Training Loss: 0.8228
Epoch 10/10, Batch 469/883, Training Loss: 0.5349
Epoch 10/10, Batch 470/883, Training Loss: 0.8934
Epoch 10/10, Batch 471/883, Training Loss: 0.8416
Epoch 10/10, Batch 472/883, Training Loss: 0.5146
Epoch 10/10, Batch 473/883, Training Loss: 0.6079
Epoch 10/10, Batch 474/883, Training Loss: 0.8318
Epoch 10/10, Batch 475/883, Training Loss: 0.9952
Epoch 10/10, Batch 476/883, Training Loss: 0.6752
Epoch 10/10, Batch 477/883, Training Loss: 0.5141
Epoch 10/10, Batch 478/883, Training Loss: 0.5242
Epoch 10/10, Batch 479/883, Training Loss: 0.9643
Epoch 10/10, Batch 480/883, Training Loss: 0.6905
Epoch 10/10, Batch 481/883, Training Loss: 0.3050
Epoch 10/10, Batch 482/883, Training Loss: 0.4240
Epoch 10/10, Batch 483/883, Training Loss: 0.4004
Epoch 10/10, Batch 484/883, Training Loss: 0.7103
Epoch 10/10, Batch 485/883, Training Loss: 0.6304
Epoch 10/10, Batch 486/883, Training Loss: 0.4309
Epoch 10/10, Batch 487/883, Training Loss: 0.5013
Epoch 10/10, Batch 488/883, Training Loss: 0.5325
Epoch 10/10, Batch 489/883, Training Loss: 0.7678
Epoch 10/10, Batch 490/883, Training Loss: 0.3999
Epoch 10/10, Batch 491/883, Training Loss: 0.5860
Epoch 10/10, Batch 492/883, Training Loss: 0.3638
Epoch 10/10, Batch 493/883, Training Loss: 0.6015
Epoch 10/10, Batch 494/883, Training Loss: 0.5134
Epoch 10/10, Batch 495/883, Training Loss: 0.6263
Epoch 10/10, Batch 496/883, Training Loss: 0.7612
Epoch 10/10, Batch 497/883, Training Loss: 0.6224
Epoch 10/10, Batch 498/883, Training Loss: 0.2990
Epoch 10/10, Batch 499/883, Training Loss: 0.4766
Epoch 10/10, Batch 500/883, Training Loss: 0.5644
Epoch 10/10, Batch 501/883, Training Loss: 0.4017
Epoch 10/10, Batch 502/883, Training Loss: 0.5433
Epoch 10/10, Batch 503/883, Training Loss: 0.4501
Epoch 10/10, Batch 504/883, Training Loss: 0.5500
Epoch 10/10, Batch 505/883, Training Loss: 0.5367
Epoch 10/10, Batch 506/883, Training Loss: 0.4580
Epoch 10/10, Batch 507/883, Training Loss: 0.6118
Epoch 10/10, Batch 508/883, Training Loss: 0.6533
Epoch 10/10, Batch 509/883, Training Loss: 0.5455
Epoch 10/10, Batch 510/883, Training Loss: 0.3636
Epoch 10/10, Batch 511/883, Training Loss: 0.6189
Epoch 10/10, Batch 512/883, Training Loss: 0.6819
Epoch 10/10, Batch 513/883, Training Loss: 0.6485
Epoch 10/10, Batch 514/883, Training Loss: 0.5515
Epoch 10/10, Batch 515/883, Training Loss: 0.3373
Epoch 10/10, Batch 516/883, Training Loss: 0.5770
Epoch 10/10, Batch 517/883, Training Loss: 0.4192
Epoch 10/10, Batch 518/883, Training Loss: 0.5655
Epoch 10/10, Batch 519/883, Training Loss: 0.5569
Epoch 10/10, Batch 520/883, Training Loss: 0.5095
Epoch 10/10, Batch 521/883, Training Loss: 0.3077
Epoch 10/10, Batch 522/883, Training Loss: 0.5345
Epoch 10/10, Batch 523/883, Training Loss: 0.6400
Epoch 10/10, Batch 524/883, Training Loss: 0.3080
Epoch 10/10, Batch 525/883, Training Loss: 0.7088
Epoch 10/10, Batch 526/883, Training Loss: 0.5812
Epoch 10/10, Batch 527/883, Training Loss: 0.6522
Epoch 10/10, Batch 528/883, Training Loss: 0.4693
Epoch 10/10, Batch 529/883, Training Loss: 0.4813
Epoch 10/10, Batch 530/883, Training Loss: 0.8451
Epoch 10/10, Batch 531/883, Training Loss: 0.4642
Epoch 10/10, Batch 532/883, Training Loss: 0.5742
Epoch 10/10, Batch 533/883, Training Loss: 0.3833
Epoch 10/10, Batch 534/883, Training Loss: 0.4711
Epoch 10/10, Batch 535/883, Training Loss: 0.7128
Epoch 10/10, Batch 536/883, Training Loss: 0.5882
Epoch 10/10, Batch 537/883, Training Loss: 0.7085
Epoch 10/10, Batch 538/883, Training Loss: 0.6364
Epoch 10/10, Batch 539/883, Training Loss: 0.3986
Epoch 10/10, Batch 540/883, Training Loss: 0.4137
Epoch 10/10, Batch 541/883, Training Loss: 0.7521
Epoch 10/10, Batch 542/883, Training Loss: 0.6066
Epoch 10/10, Batch 543/883, Training Loss: 0.7837
Epoch 10/10, Batch 544/883, Training Loss: 0.6006
Epoch 10/10, Batch 545/883, Training Loss: 0.2719
Epoch 10/10, Batch 546/883, Training Loss: 0.5276
Epoch 10/10, Batch 547/883, Training Loss: 0.4381
Epoch 10/10, Batch 548/883, Training Loss: 0.7832
Epoch 10/10, Batch 549/883, Training Loss: 0.3467
Epoch 10/10, Batch 550/883, Training Loss: 0.4537
Epoch 10/10, Batch 551/883, Training Loss: 0.2933
Epoch 10/10, Batch 552/883, Training Loss: 0.5666
Epoch 10/10, Batch 553/883, Training Loss: 0.7307
Epoch 10/10, Batch 554/883, Training Loss: 0.3570
Epoch 10/10, Batch 555/883, Training Loss: 0.4550
Epoch 10/10, Batch 556/883, Training Loss: 0.5646
Epoch 10/10, Batch 557/883, Training Loss: 0.3145
Epoch 10/10, Batch 558/883, Training Loss: 0.2182
Epoch 10/10, Batch 559/883, Training Loss: 0.7311
Epoch 10/10, Batch 560/883, Training Loss: 0.6580
Epoch 10/10, Batch 561/883, Training Loss: 0.4739
Epoch 10/10, Batch 562/883, Training Loss: 0.4348
Epoch 10/10, Batch 563/883, Training Loss: 0.4006
Epoch 10/10, Batch 564/883, Training Loss: 0.7286
Epoch 10/10, Batch 565/883, Training Loss: 0.4052
Epoch 10/10, Batch 566/883, Training Loss: 0.4913
Epoch 10/10, Batch 567/883, Training Loss: 0.5684
Epoch 10/10, Batch 568/883, Training Loss: 0.4678
Epoch 10/10, Batch 569/883, Training Loss: 0.4527
Epoch 10/10, Batch 570/883, Training Loss: 0.3821
Epoch 10/10, Batch 571/883, Training Loss: 0.5995
Epoch 10/10, Batch 572/883, Training Loss: 0.5294
Epoch 10/10, Batch 573/883, Training Loss: 0.3021
Epoch 10/10, Batch 574/883, Training Loss: 0.4869
Epoch 10/10, Batch 575/883, Training Loss: 0.5061
Epoch 10/10, Batch 576/883, Training Loss: 0.2746
Epoch 10/10, Batch 577/883, Training Loss: 0.6826
Epoch 10/10, Batch 578/883, Training Loss: 0.8499
Epoch 10/10, Batch 579/883, Training Loss: 0.4515
Epoch 10/10, Batch 580/883, Training Loss: 0.4264
Epoch 10/10, Batch 581/883, Training Loss: 0.9442
Epoch 10/10, Batch 582/883, Training Loss: 0.4656
Epoch 10/10, Batch 583/883, Training Loss: 0.8969
Epoch 10/10, Batch 584/883, Training Loss: 1.1452
Epoch 10/10, Batch 585/883, Training Loss: 0.5072
Epoch 10/10, Batch 586/883, Training Loss: 0.4283
Epoch 10/10, Batch 587/883, Training Loss: 0.9660
Epoch 10/10, Batch 588/883, Training Loss: 0.3667
Epoch 10/10, Batch 589/883, Training Loss: 0.3512
Epoch 10/10, Batch 590/883, Training Loss: 0.5352
Epoch 10/10, Batch 591/883, Training Loss: 0.7875
Epoch 10/10, Batch 592/883, Training Loss: 0.2398
Epoch 10/10, Batch 593/883, Training Loss: 0.5310
Epoch 10/10, Batch 594/883, Training Loss: 0.5154
Epoch 10/10, Batch 595/883, Training Loss: 0.5113
Epoch 10/10, Batch 596/883, Training Loss: 0.4469
Epoch 10/10, Batch 597/883, Training Loss: 0.5552
Epoch 10/10, Batch 598/883, Training Loss: 0.7253
Epoch 10/10, Batch 599/883, Training Loss: 0.5145
Epoch 10/10, Batch 600/883, Training Loss: 0.7154
Epoch 10/10, Batch 601/883, Training Loss: 0.6636
Epoch 10/10, Batch 602/883, Training Loss: 0.6686
Epoch 10/10, Batch 603/883, Training Loss: 0.5668
Epoch 10/10, Batch 604/883, Training Loss: 0.4552
Epoch 10/10, Batch 605/883, Training Loss: 0.5056
Epoch 10/10, Batch 606/883, Training Loss: 0.5972
Epoch 10/10, Batch 607/883, Training Loss: 0.3704
Epoch 10/10, Batch 608/883, Training Loss: 0.8764
Epoch 10/10, Batch 609/883, Training Loss: 0.7954
Epoch 10/10, Batch 610/883, Training Loss: 0.6175
Epoch 10/10, Batch 611/883, Training Loss: 0.4719
Epoch 10/10, Batch 612/883, Training Loss: 0.6490
Epoch 10/10, Batch 613/883, Training Loss: 0.6157
Epoch 10/10, Batch 614/883, Training Loss: 0.3352
Epoch 10/10, Batch 615/883, Training Loss: 0.4934
Epoch 10/10, Batch 616/883, Training Loss: 0.4197
Epoch 10/10, Batch 617/883, Training Loss: 0.4220
Epoch 10/10, Batch 618/883, Training Loss: 0.4873
Epoch 10/10, Batch 619/883, Training Loss: 0.4127
Epoch 10/10, Batch 620/883, Training Loss: 0.5797
Epoch 10/10, Batch 621/883, Training Loss: 0.3965
Epoch 10/10, Batch 622/883, Training Loss: 0.6901
Epoch 10/10, Batch 623/883, Training Loss: 0.7086
Epoch 10/10, Batch 624/883, Training Loss: 0.4115
Epoch 10/10, Batch 625/883, Training Loss: 0.6631
Epoch 10/10, Batch 626/883, Training Loss: 0.4230
Epoch 10/10, Batch 627/883, Training Loss: 0.4767
Epoch 10/10, Batch 628/883, Training Loss: 0.6391
Epoch 10/10, Batch 629/883, Training Loss: 0.6156
Epoch 10/10, Batch 630/883, Training Loss: 0.7146
Epoch 10/10, Batch 631/883, Training Loss: 0.4630
Epoch 10/10, Batch 632/883, Training Loss: 0.4491
Epoch 10/10, Batch 633/883, Training Loss: 0.4285
Epoch 10/10, Batch 634/883, Training Loss: 0.5872
Epoch 10/10, Batch 635/883, Training Loss: 0.7487
Epoch 10/10, Batch 636/883, Training Loss: 0.7394
Epoch 10/10, Batch 637/883, Training Loss: 0.3881
Epoch 10/10, Batch 638/883, Training Loss: 0.4980
Epoch 10/10, Batch 639/883, Training Loss: 0.9965
Epoch 10/10, Batch 640/883, Training Loss: 0.5519
Epoch 10/10, Batch 641/883, Training Loss: 0.3351
Epoch 10/10, Batch 642/883, Training Loss: 0.7063
Epoch 10/10, Batch 643/883, Training Loss: 0.7410
Epoch 10/10, Batch 644/883, Training Loss: 0.6786
Epoch 10/10, Batch 645/883, Training Loss: 0.7070
Epoch 10/10, Batch 646/883, Training Loss: 0.6096
Epoch 10/10, Batch 647/883, Training Loss: 0.7031
Epoch 10/10, Batch 648/883, Training Loss: 0.7035
Epoch 10/10, Batch 649/883, Training Loss: 0.2588
Epoch 10/10, Batch 650/883, Training Loss: 0.4152
Epoch 10/10, Batch 651/883, Training Loss: 0.3860
Epoch 10/10, Batch 652/883, Training Loss: 0.5517
Epoch 10/10, Batch 653/883, Training Loss: 0.4765
Epoch 10/10, Batch 654/883, Training Loss: 0.4298
Epoch 10/10, Batch 655/883, Training Loss: 0.4351
Epoch 10/10, Batch 656/883, Training Loss: 0.4953
Epoch 10/10, Batch 657/883, Training Loss: 0.6376
Epoch 10/10, Batch 658/883, Training Loss: 0.4998
Epoch 10/10, Batch 659/883, Training Loss: 0.6712
Epoch 10/10, Batch 660/883, Training Loss: 0.3797
Epoch 10/10, Batch 661/883, Training Loss: 0.4081
Epoch 10/10, Batch 662/883, Training Loss: 0.7939
Epoch 10/10, Batch 663/883, Training Loss: 0.4039
Epoch 10/10, Batch 664/883, Training Loss: 0.5238
Epoch 10/10, Batch 665/883, Training Loss: 0.3050
Epoch 10/10, Batch 666/883, Training Loss: 0.4211
Epoch 10/10, Batch 667/883, Training Loss: 0.4555
Epoch 10/10, Batch 668/883, Training Loss: 0.6870
Epoch 10/10, Batch 669/883, Training Loss: 0.4878
Epoch 10/10, Batch 670/883, Training Loss: 0.4488
Epoch 10/10, Batch 671/883, Training Loss: 0.5197
Epoch 10/10, Batch 672/883, Training Loss: 0.3424
Epoch 10/10, Batch 673/883, Training Loss: 0.4079
Epoch 10/10, Batch 674/883, Training Loss: 0.5257
Epoch 10/10, Batch 675/883, Training Loss: 0.4950
Epoch 10/10, Batch 676/883, Training Loss: 0.3198
Epoch 10/10, Batch 677/883, Training Loss: 0.6193
Epoch 10/10, Batch 678/883, Training Loss: 0.7077
Epoch 10/10, Batch 679/883, Training Loss: 0.4539
Epoch 10/10, Batch 680/883, Training Loss: 0.6270
Epoch 10/10, Batch 681/883, Training Loss: 0.7623
Epoch 10/10, Batch 682/883, Training Loss: 0.3442
Epoch 10/10, Batch 683/883, Training Loss: 0.3190
Epoch 10/10, Batch 684/883, Training Loss: 0.5938
Epoch 10/10, Batch 685/883, Training Loss: 0.3055
Epoch 10/10, Batch 686/883, Training Loss: 0.3526
Epoch 10/10, Batch 687/883, Training Loss: 0.6773
Epoch 10/10, Batch 688/883, Training Loss: 0.5236
Epoch 10/10, Batch 689/883, Training Loss: 0.3609
Epoch 10/10, Batch 690/883, Training Loss: 0.4735
Epoch 10/10, Batch 691/883, Training Loss: 0.6674
Epoch 10/10, Batch 692/883, Training Loss: 0.3899
Epoch 10/10, Batch 693/883, Training Loss: 0.8222
Epoch 10/10, Batch 694/883, Training Loss: 0.5824
Epoch 10/10, Batch 695/883, Training Loss: 1.1878
Epoch 10/10, Batch 696/883, Training Loss: 0.6941
Epoch 10/10, Batch 697/883, Training Loss: 0.6817
Epoch 10/10, Batch 698/883, Training Loss: 0.6300
Epoch 10/10, Batch 699/883, Training Loss: 0.4405
Epoch 10/10, Batch 700/883, Training Loss: 0.3731
Epoch 10/10, Batch 701/883, Training Loss: 0.4872
Epoch 10/10, Batch 702/883, Training Loss: 0.3077
Epoch 10/10, Batch 703/883, Training Loss: 0.3576
Epoch 10/10, Batch 704/883, Training Loss: 0.5375
Epoch 10/10, Batch 705/883, Training Loss: 0.6186
Epoch 10/10, Batch 706/883, Training Loss: 0.9203
Epoch 10/10, Batch 707/883, Training Loss: 0.5941
Epoch 10/10, Batch 708/883, Training Loss: 0.6354
Epoch 10/10, Batch 709/883, Training Loss: 0.7959
Epoch 10/10, Batch 710/883, Training Loss: 0.6241
Epoch 10/10, Batch 711/883, Training Loss: 0.4817
Epoch 10/10, Batch 712/883, Training Loss: 0.5104
Epoch 10/10, Batch 713/883, Training Loss: 0.4610
Epoch 10/10, Batch 714/883, Training Loss: 0.4287
Epoch 10/10, Batch 715/883, Training Loss: 0.4934
Epoch 10/10, Batch 716/883, Training Loss: 0.5898
Epoch 10/10, Batch 717/883, Training Loss: 0.4768
Epoch 10/10, Batch 718/883, Training Loss: 0.6260
Epoch 10/10, Batch 719/883, Training Loss: 0.8885
Epoch 10/10, Batch 720/883, Training Loss: 0.7647
Epoch 10/10, Batch 721/883, Training Loss: 0.5445
Epoch 10/10, Batch 722/883, Training Loss: 0.2735
Epoch 10/10, Batch 723/883, Training Loss: 0.6239
Epoch 10/10, Batch 724/883, Training Loss: 0.6453
Epoch 10/10, Batch 725/883, Training Loss: 0.5892
Epoch 10/10, Batch 726/883, Training Loss: 0.5489
Epoch 10/10, Batch 727/883, Training Loss: 0.6430
Epoch 10/10, Batch 728/883, Training Loss: 0.7455
Epoch 10/10, Batch 729/883, Training Loss: 0.7133
Epoch 10/10, Batch 730/883, Training Loss: 0.9908
Epoch 10/10, Batch 731/883, Training Loss: 0.7269
Epoch 10/10, Batch 732/883, Training Loss: 0.7690
Epoch 10/10, Batch 733/883, Training Loss: 0.3844
Epoch 10/10, Batch 734/883, Training Loss: 0.4303
Epoch 10/10, Batch 735/883, Training Loss: 0.7469
Epoch 10/10, Batch 736/883, Training Loss: 0.5945
Epoch 10/10, Batch 737/883, Training Loss: 0.3059
Epoch 10/10, Batch 738/883, Training Loss: 0.7030
Epoch 10/10, Batch 739/883, Training Loss: 0.6148
Epoch 10/10, Batch 740/883, Training Loss: 0.7337
Epoch 10/10, Batch 741/883, Training Loss: 0.5116
Epoch 10/10, Batch 742/883, Training Loss: 0.4462
Epoch 10/10, Batch 743/883, Training Loss: 0.2223
Epoch 10/10, Batch 744/883, Training Loss: 0.6558
Epoch 10/10, Batch 745/883, Training Loss: 0.3318
Epoch 10/10, Batch 746/883, Training Loss: 1.0872
Epoch 10/10, Batch 747/883, Training Loss: 0.3982
Epoch 10/10, Batch 748/883, Training Loss: 0.5153
Epoch 10/10, Batch 749/883, Training Loss: 0.5791
Epoch 10/10, Batch 750/883, Training Loss: 0.4046
Epoch 10/10, Batch 751/883, Training Loss: 0.3639
Epoch 10/10, Batch 752/883, Training Loss: 0.5885
Epoch 10/10, Batch 753/883, Training Loss: 0.6854
Epoch 10/10, Batch 754/883, Training Loss: 0.4528
Epoch 10/10, Batch 755/883, Training Loss: 0.7779
Epoch 10/10, Batch 756/883, Training Loss: 0.7185
Epoch 10/10, Batch 757/883, Training Loss: 0.4702
Epoch 10/10, Batch 758/883, Training Loss: 0.5570
Epoch 10/10, Batch 759/883, Training Loss: 0.4835
Epoch 10/10, Batch 760/883, Training Loss: 0.7386
Epoch 10/10, Batch 761/883, Training Loss: 0.8283
Epoch 10/10, Batch 762/883, Training Loss: 0.5022
Epoch 10/10, Batch 763/883, Training Loss: 0.7111
Epoch 10/10, Batch 764/883, Training Loss: 0.4030
Epoch 10/10, Batch 765/883, Training Loss: 0.6331
Epoch 10/10, Batch 766/883, Training Loss: 0.4555
Epoch 10/10, Batch 767/883, Training Loss: 0.8919
Epoch 10/10, Batch 768/883, Training Loss: 0.5243
Epoch 10/10, Batch 769/883, Training Loss: 0.5231
Epoch 10/10, Batch 770/883, Training Loss: 0.4632
Epoch 10/10, Batch 771/883, Training Loss: 0.4837
Epoch 10/10, Batch 772/883, Training Loss: 0.2982
Epoch 10/10, Batch 773/883, Training Loss: 1.0535
Epoch 10/10, Batch 774/883, Training Loss: 0.5509
Epoch 10/10, Batch 775/883, Training Loss: 0.5019
Epoch 10/10, Batch 776/883, Training Loss: 0.7489
Epoch 10/10, Batch 777/883, Training Loss: 0.6287
Epoch 10/10, Batch 778/883, Training Loss: 0.6286
Epoch 10/10, Batch 779/883, Training Loss: 0.4047
Epoch 10/10, Batch 780/883, Training Loss: 0.2514
Epoch 10/10, Batch 781/883, Training Loss: 0.3421
Epoch 10/10, Batch 782/883, Training Loss: 0.7166
Epoch 10/10, Batch 783/883, Training Loss: 0.5366
Epoch 10/10, Batch 784/883, Training Loss: 0.6520
Epoch 10/10, Batch 785/883, Training Loss: 0.3310
Epoch 10/10, Batch 786/883, Training Loss: 0.4093
Epoch 10/10, Batch 787/883, Training Loss: 0.6283
Epoch 10/10, Batch 788/883, Training Loss: 0.6649
Epoch 10/10, Batch 789/883, Training Loss: 0.6768
Epoch 10/10, Batch 790/883, Training Loss: 0.4315
Epoch 10/10, Batch 791/883, Training Loss: 0.5442
Epoch 10/10, Batch 792/883, Training Loss: 0.5318
Epoch 10/10, Batch 793/883, Training Loss: 0.9691
Epoch 10/10, Batch 794/883, Training Loss: 0.2756
Epoch 10/10, Batch 795/883, Training Loss: 0.6957
Epoch 10/10, Batch 796/883, Training Loss: 0.6433
Epoch 10/10, Batch 797/883, Training Loss: 0.6186
Epoch 10/10, Batch 798/883, Training Loss: 0.5261
Epoch 10/10, Batch 799/883, Training Loss: 0.2933
Epoch 10/10, Batch 800/883, Training Loss: 0.3674
Epoch 10/10, Batch 801/883, Training Loss: 0.4656
Epoch 10/10, Batch 802/883, Training Loss: 0.3734
Epoch 10/10, Batch 803/883, Training Loss: 0.5665
Epoch 10/10, Batch 804/883, Training Loss: 0.4253
Epoch 10/10, Batch 805/883, Training Loss: 0.3017
Epoch 10/10, Batch 806/883, Training Loss: 0.5178
Epoch 10/10, Batch 807/883, Training Loss: 0.4035
Epoch 10/10, Batch 808/883, Training Loss: 0.5578
Epoch 10/10, Batch 809/883, Training Loss: 0.5972
Epoch 10/10, Batch 810/883, Training Loss: 0.5489
Epoch 10/10, Batch 811/883, Training Loss: 0.7326
Epoch 10/10, Batch 812/883, Training Loss: 0.3552
Epoch 10/10, Batch 813/883, Training Loss: 0.6118
Epoch 10/10, Batch 814/883, Training Loss: 0.5136
Epoch 10/10, Batch 815/883, Training Loss: 0.4945
Epoch 10/10, Batch 816/883, Training Loss: 0.7491
Epoch 10/10, Batch 817/883, Training Loss: 0.8410
Epoch 10/10, Batch 818/883, Training Loss: 0.5901
Epoch 10/10, Batch 819/883, Training Loss: 0.4054
Epoch 10/10, Batch 820/883, Training Loss: 0.5855
Epoch 10/10, Batch 821/883, Training Loss: 1.1260
Epoch 10/10, Batch 822/883, Training Loss: 0.4885
Epoch 10/10, Batch 823/883, Training Loss: 0.2747
Epoch 10/10, Batch 824/883, Training Loss: 0.5616
Epoch 10/10, Batch 825/883, Training Loss: 0.5385
Epoch 10/10, Batch 826/883, Training Loss: 0.4360
Epoch 10/10, Batch 827/883, Training Loss: 0.5841
Epoch 10/10, Batch 828/883, Training Loss: 0.6365
Epoch 10/10, Batch 829/883, Training Loss: 0.4148
Epoch 10/10, Batch 830/883, Training Loss: 0.6781
Epoch 10/10, Batch 831/883, Training Loss: 0.4333
Epoch 10/10, Batch 832/883, Training Loss: 0.3643
Epoch 10/10, Batch 833/883, Training Loss: 0.7822
Epoch 10/10, Batch 834/883, Training Loss: 0.5884
Epoch 10/10, Batch 835/883, Training Loss: 0.2604
Epoch 10/10, Batch 836/883, Training Loss: 0.2766
Epoch 10/10, Batch 837/883, Training Loss: 0.4847
Epoch 10/10, Batch 838/883, Training Loss: 0.7503
Epoch 10/10, Batch 839/883, Training Loss: 0.4268
Epoch 10/10, Batch 840/883, Training Loss: 0.8207
Epoch 10/10, Batch 841/883, Training Loss: 0.3368
Epoch 10/10, Batch 842/883, Training Loss: 0.5906
Epoch 10/10, Batch 843/883, Training Loss: 0.3693
Epoch 10/10, Batch 844/883, Training Loss: 0.5013
Epoch 10/10, Batch 845/883, Training Loss: 0.3522
Epoch 10/10, Batch 846/883, Training Loss: 0.3556
Epoch 10/10, Batch 847/883, Training Loss: 0.3063
Epoch 10/10, Batch 848/883, Training Loss: 0.6377
Epoch 10/10, Batch 849/883, Training Loss: 0.3906
Epoch 10/10, Batch 850/883, Training Loss: 0.4396
Epoch 10/10, Batch 851/883, Training Loss: 0.5368
Epoch 10/10, Batch 852/883, Training Loss: 0.6438
Epoch 10/10, Batch 853/883, Training Loss: 0.7532
Epoch 10/10, Batch 854/883, Training Loss: 0.4696
Epoch 10/10, Batch 855/883, Training Loss: 0.5150
Epoch 10/10, Batch 856/883, Training Loss: 0.4939
Epoch 10/10, Batch 857/883, Training Loss: 0.8613
Epoch 10/10, Batch 858/883, Training Loss: 0.3512
Epoch 10/10, Batch 859/883, Training Loss: 0.5679
Epoch 10/10, Batch 860/883, Training Loss: 0.3063
Epoch 10/10, Batch 861/883, Training Loss: 0.1906
Epoch 10/10, Batch 862/883, Training Loss: 0.5612
Epoch 10/10, Batch 863/883, Training Loss: 0.4729
Epoch 10/10, Batch 864/883, Training Loss: 0.4619
Epoch 10/10, Batch 865/883, Training Loss: 0.3508
Epoch 10/10, Batch 866/883, Training Loss: 0.3925
Epoch 10/10, Batch 867/883, Training Loss: 0.4888
Epoch 10/10, Batch 868/883, Training Loss: 0.7779
Epoch 10/10, Batch 869/883, Training Loss: 0.7374
Epoch 10/10, Batch 870/883, Training Loss: 0.9886
Epoch 10/10, Batch 871/883, Training Loss: 0.5130
Epoch 10/10, Batch 872/883, Training Loss: 0.5847
Epoch 10/10, Batch 873/883, Training Loss: 0.3595
Epoch 10/10, Batch 874/883, Training Loss: 0.4627
Epoch 10/10, Batch 875/883, Training Loss: 0.6702
Epoch 10/10, Batch 876/883, Training Loss: 0.4420
Epoch 10/10, Batch 877/883, Training Loss: 0.5930
Epoch 10/10, Batch 878/883, Training Loss: 0.3837
Epoch 10/10, Batch 879/883, Training Loss: 0.4606
Epoch 10/10, Batch 880/883, Training Loss: 0.4659
Epoch 10/10, Batch 881/883, Training Loss: 0.8512
Epoch 10/10, Batch 882/883, Training Loss: 0.4614
Epoch 10/10, Batch 883/883, Training Loss: 1.0104
Epoch 10/10, Training Loss: 0.5602, Validation Loss: 0.5373, Validation Accuracy: 0.7686
Test Loss: 0.5260, Test Accuracy: 0.7717
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-04-27 20:03:40,605] Trial 1 finished with value: 0.5372937212316511 and parameters: {'batch_size': 16, 'learning_rate': 0.0046246250285297795, 'weight_decay': 7.14177291770463e-05}. Best is trial 0 with value: 0.2196991708036512.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/10, Batch 1/442, Training Loss: 1.6141
Epoch 1/10, Batch 2/442, Training Loss: 1.6847
Epoch 1/10, Batch 3/442, Training Loss: 3.2437
Epoch 1/10, Batch 4/442, Training Loss: 3.2642
Epoch 1/10, Batch 5/442, Training Loss: 1.8986
Epoch 1/10, Batch 6/442, Training Loss: 1.4026
Epoch 1/10, Batch 7/442, Training Loss: 1.2354
Epoch 1/10, Batch 8/442, Training Loss: 1.2611
Epoch 1/10, Batch 9/442, Training Loss: 1.3275
Epoch 1/10, Batch 10/442, Training Loss: 1.0095
Epoch 1/10, Batch 11/442, Training Loss: 0.9113
Epoch 1/10, Batch 12/442, Training Loss: 0.9545
Epoch 1/10, Batch 13/442, Training Loss: 2.0563
Epoch 1/10, Batch 14/442, Training Loss: 1.0994
Epoch 1/10, Batch 15/442, Training Loss: 1.0934
Epoch 1/10, Batch 16/442, Training Loss: 1.1653
Epoch 1/10, Batch 17/442, Training Loss: 0.9365
Epoch 1/10, Batch 18/442, Training Loss: 0.8920
Epoch 1/10, Batch 19/442, Training Loss: 0.9190
Epoch 1/10, Batch 20/442, Training Loss: 1.0503
Epoch 1/10, Batch 21/442, Training Loss: 1.3044
Epoch 1/10, Batch 22/442, Training Loss: 1.0709
Epoch 1/10, Batch 23/442, Training Loss: 1.5335
Epoch 1/10, Batch 24/442, Training Loss: 1.5156
Epoch 1/10, Batch 25/442, Training Loss: 0.9535
Epoch 1/10, Batch 26/442, Training Loss: 0.9843
Epoch 1/10, Batch 27/442, Training Loss: 1.0997
Epoch 1/10, Batch 28/442, Training Loss: 1.0088
Epoch 1/10, Batch 29/442, Training Loss: 1.0316
Epoch 1/10, Batch 30/442, Training Loss: 0.9378
Epoch 1/10, Batch 31/442, Training Loss: 1.0750
Epoch 1/10, Batch 32/442, Training Loss: 0.9394
Epoch 1/10, Batch 33/442, Training Loss: 1.0874
Epoch 1/10, Batch 34/442, Training Loss: 0.9284
Epoch 1/10, Batch 35/442, Training Loss: 1.0061
Epoch 1/10, Batch 36/442, Training Loss: 1.0783
Epoch 1/10, Batch 37/442, Training Loss: 0.9585
Epoch 1/10, Batch 38/442, Training Loss: 0.8482
Epoch 1/10, Batch 39/442, Training Loss: 1.0267
Epoch 1/10, Batch 40/442, Training Loss: 0.9252
Epoch 1/10, Batch 41/442, Training Loss: 0.8799
Epoch 1/10, Batch 42/442, Training Loss: 1.0258
Epoch 1/10, Batch 43/442, Training Loss: 1.0001
Epoch 1/10, Batch 44/442, Training Loss: 0.9105
Epoch 1/10, Batch 45/442, Training Loss: 0.9832
Epoch 1/10, Batch 46/442, Training Loss: 0.8908
Epoch 1/10, Batch 47/442, Training Loss: 1.0136
Epoch 1/10, Batch 48/442, Training Loss: 0.9155
Epoch 1/10, Batch 49/442, Training Loss: 1.1048
Epoch 1/10, Batch 50/442, Training Loss: 0.8250
Epoch 1/10, Batch 51/442, Training Loss: 0.9238
Epoch 1/10, Batch 52/442, Training Loss: 0.9713
Epoch 1/10, Batch 53/442, Training Loss: 0.8905
Epoch 1/10, Batch 54/442, Training Loss: 0.9555
Epoch 1/10, Batch 55/442, Training Loss: 0.9973
Epoch 1/10, Batch 56/442, Training Loss: 0.8741
Epoch 1/10, Batch 57/442, Training Loss: 1.1115
Epoch 1/10, Batch 58/442, Training Loss: 1.0069
Epoch 1/10, Batch 59/442, Training Loss: 0.9310
Epoch 1/10, Batch 60/442, Training Loss: 1.0039
Epoch 1/10, Batch 61/442, Training Loss: 1.0219
Epoch 1/10, Batch 62/442, Training Loss: 0.8988
Epoch 1/10, Batch 63/442, Training Loss: 0.8602
Epoch 1/10, Batch 64/442, Training Loss: 0.9306
Epoch 1/10, Batch 65/442, Training Loss: 1.0152
Epoch 1/10, Batch 66/442, Training Loss: 0.8527
Epoch 1/10, Batch 67/442, Training Loss: 1.0064
Epoch 1/10, Batch 68/442, Training Loss: 0.9437
Epoch 1/10, Batch 69/442, Training Loss: 1.2317
Epoch 1/10, Batch 70/442, Training Loss: 0.9385
Epoch 1/10, Batch 71/442, Training Loss: 1.0396
Epoch 1/10, Batch 72/442, Training Loss: 1.1045
Epoch 1/10, Batch 73/442, Training Loss: 0.9798
Epoch 1/10, Batch 74/442, Training Loss: 0.8359
Epoch 1/10, Batch 75/442, Training Loss: 0.9139
Epoch 1/10, Batch 76/442, Training Loss: 0.9561
Epoch 1/10, Batch 77/442, Training Loss: 1.1251
Epoch 1/10, Batch 78/442, Training Loss: 0.8281
Epoch 1/10, Batch 79/442, Training Loss: 0.9249
Epoch 1/10, Batch 80/442, Training Loss: 1.1945
Epoch 1/10, Batch 81/442, Training Loss: 0.9844
Epoch 1/10, Batch 82/442, Training Loss: 0.9195
Epoch 1/10, Batch 83/442, Training Loss: 1.0093
Epoch 1/10, Batch 84/442, Training Loss: 0.8087
Epoch 1/10, Batch 85/442, Training Loss: 0.8572
Epoch 1/10, Batch 86/442, Training Loss: 0.9221
Epoch 1/10, Batch 87/442, Training Loss: 0.9279
Epoch 1/10, Batch 88/442, Training Loss: 1.0327
Epoch 1/10, Batch 89/442, Training Loss: 1.0601
Epoch 1/10, Batch 90/442, Training Loss: 0.7921
Epoch 1/10, Batch 91/442, Training Loss: 0.6932
Epoch 1/10, Batch 92/442, Training Loss: 1.0914
Epoch 1/10, Batch 93/442, Training Loss: 0.7180
Epoch 1/10, Batch 94/442, Training Loss: 0.8808
Epoch 1/10, Batch 95/442, Training Loss: 0.7793
Epoch 1/10, Batch 96/442, Training Loss: 0.7359
Epoch 1/10, Batch 97/442, Training Loss: 0.8786
Epoch 1/10, Batch 98/442, Training Loss: 0.9214
Epoch 1/10, Batch 99/442, Training Loss: 0.6963
Epoch 1/10, Batch 100/442, Training Loss: 1.0258
Epoch 1/10, Batch 101/442, Training Loss: 0.9713
Epoch 1/10, Batch 102/442, Training Loss: 1.2159
Epoch 1/10, Batch 103/442, Training Loss: 0.9202
Epoch 1/10, Batch 104/442, Training Loss: 0.8087
Epoch 1/10, Batch 105/442, Training Loss: 1.0966
Epoch 1/10, Batch 106/442, Training Loss: 0.9024
Epoch 1/10, Batch 107/442, Training Loss: 1.0824
Epoch 1/10, Batch 108/442, Training Loss: 0.9420
Epoch 1/10, Batch 109/442, Training Loss: 0.9500
Epoch 1/10, Batch 110/442, Training Loss: 1.0085
Epoch 1/10, Batch 111/442, Training Loss: 0.8904
Epoch 1/10, Batch 112/442, Training Loss: 1.0996
Epoch 1/10, Batch 113/442, Training Loss: 0.9096
Epoch 1/10, Batch 114/442, Training Loss: 0.8072
Epoch 1/10, Batch 115/442, Training Loss: 0.7718
Epoch 1/10, Batch 116/442, Training Loss: 0.8260
Epoch 1/10, Batch 117/442, Training Loss: 0.9157
Epoch 1/10, Batch 118/442, Training Loss: 0.9165
Epoch 1/10, Batch 119/442, Training Loss: 0.9075
Epoch 1/10, Batch 120/442, Training Loss: 0.9271
Epoch 1/10, Batch 121/442, Training Loss: 0.8025
Epoch 1/10, Batch 122/442, Training Loss: 1.3242
Epoch 1/10, Batch 123/442, Training Loss: 0.7553
Epoch 1/10, Batch 124/442, Training Loss: 0.9223
Epoch 1/10, Batch 125/442, Training Loss: 0.9098
Epoch 1/10, Batch 126/442, Training Loss: 0.9364
Epoch 1/10, Batch 127/442, Training Loss: 1.0927
Epoch 1/10, Batch 128/442, Training Loss: 1.1065
Epoch 1/10, Batch 129/442, Training Loss: 1.0671
Epoch 1/10, Batch 130/442, Training Loss: 0.9036
Epoch 1/10, Batch 131/442, Training Loss: 0.8370
Epoch 1/10, Batch 132/442, Training Loss: 0.9841
Epoch 1/10, Batch 133/442, Training Loss: 1.2272
Epoch 1/10, Batch 134/442, Training Loss: 0.7912
Epoch 1/10, Batch 135/442, Training Loss: 0.8597
Epoch 1/10, Batch 136/442, Training Loss: 0.9419
Epoch 1/10, Batch 137/442, Training Loss: 0.8772
Epoch 1/10, Batch 138/442, Training Loss: 0.8460
Epoch 1/10, Batch 139/442, Training Loss: 0.8987
Epoch 1/10, Batch 140/442, Training Loss: 0.8006
Epoch 1/10, Batch 141/442, Training Loss: 0.7886
Epoch 1/10, Batch 142/442, Training Loss: 1.1847
Epoch 1/10, Batch 143/442, Training Loss: 0.8716
Epoch 1/10, Batch 144/442, Training Loss: 0.8081
Epoch 1/10, Batch 145/442, Training Loss: 0.9330
Epoch 1/10, Batch 146/442, Training Loss: 1.0640
Epoch 1/10, Batch 147/442, Training Loss: 1.1691
Epoch 1/10, Batch 148/442, Training Loss: 0.9188
Epoch 1/10, Batch 149/442, Training Loss: 1.0492
Epoch 1/10, Batch 150/442, Training Loss: 0.8155
Epoch 1/10, Batch 151/442, Training Loss: 0.8585
Epoch 1/10, Batch 152/442, Training Loss: 0.7749
Epoch 1/10, Batch 153/442, Training Loss: 0.8937
Epoch 1/10, Batch 154/442, Training Loss: 0.9255
Epoch 1/10, Batch 155/442, Training Loss: 0.7568
Epoch 1/10, Batch 156/442, Training Loss: 1.3156
Epoch 1/10, Batch 157/442, Training Loss: 0.8736
Epoch 1/10, Batch 158/442, Training Loss: 0.8220
Epoch 1/10, Batch 159/442, Training Loss: 0.7849
Epoch 1/10, Batch 160/442, Training Loss: 1.0268
Epoch 1/10, Batch 161/442, Training Loss: 0.6738
Epoch 1/10, Batch 162/442, Training Loss: 0.8845
Epoch 1/10, Batch 163/442, Training Loss: 0.7064
Epoch 1/10, Batch 164/442, Training Loss: 1.0893
Epoch 1/10, Batch 165/442, Training Loss: 0.9912
Epoch 1/10, Batch 166/442, Training Loss: 0.7168
Epoch 1/10, Batch 167/442, Training Loss: 1.0312
Epoch 1/10, Batch 168/442, Training Loss: 0.8647
Epoch 1/10, Batch 169/442, Training Loss: 0.8456
Epoch 1/10, Batch 170/442, Training Loss: 0.9118
Epoch 1/10, Batch 171/442, Training Loss: 0.8324
Epoch 1/10, Batch 172/442, Training Loss: 1.2015
Epoch 1/10, Batch 173/442, Training Loss: 1.1394
Epoch 1/10, Batch 174/442, Training Loss: 0.9428
Epoch 1/10, Batch 175/442, Training Loss: 1.1825
Epoch 1/10, Batch 176/442, Training Loss: 1.0333
Epoch 1/10, Batch 177/442, Training Loss: 0.7752
Epoch 1/10, Batch 178/442, Training Loss: 1.0296
Epoch 1/10, Batch 179/442, Training Loss: 0.8854
Epoch 1/10, Batch 180/442, Training Loss: 0.9450
Epoch 1/10, Batch 181/442, Training Loss: 0.9970
Epoch 1/10, Batch 182/442, Training Loss: 0.6984
Epoch 1/10, Batch 183/442, Training Loss: 1.0906
Epoch 1/10, Batch 184/442, Training Loss: 0.9780
Epoch 1/10, Batch 185/442, Training Loss: 1.0066
Epoch 1/10, Batch 186/442, Training Loss: 0.9840
Epoch 1/10, Batch 187/442, Training Loss: 0.9417
Epoch 1/10, Batch 188/442, Training Loss: 0.8934
Epoch 1/10, Batch 189/442, Training Loss: 1.0464
Epoch 1/10, Batch 190/442, Training Loss: 0.9468
Epoch 1/10, Batch 191/442, Training Loss: 0.7686
Epoch 1/10, Batch 192/442, Training Loss: 0.9448
Epoch 1/10, Batch 193/442, Training Loss: 0.9392
Epoch 1/10, Batch 194/442, Training Loss: 0.8968
Epoch 1/10, Batch 195/442, Training Loss: 1.0113
Epoch 1/10, Batch 196/442, Training Loss: 1.1572
Epoch 1/10, Batch 197/442, Training Loss: 0.8582
Epoch 1/10, Batch 198/442, Training Loss: 0.9378
Epoch 1/10, Batch 199/442, Training Loss: 0.7898
Epoch 1/10, Batch 200/442, Training Loss: 0.9203
Epoch 1/10, Batch 201/442, Training Loss: 0.7316
Epoch 1/10, Batch 202/442, Training Loss: 0.9780
Epoch 1/10, Batch 203/442, Training Loss: 1.2684
Epoch 1/10, Batch 204/442, Training Loss: 0.9229
Epoch 1/10, Batch 205/442, Training Loss: 1.0733
Epoch 1/10, Batch 206/442, Training Loss: 0.6933
Epoch 1/10, Batch 207/442, Training Loss: 0.9399
Epoch 1/10, Batch 208/442, Training Loss: 0.8538
Epoch 1/10, Batch 209/442, Training Loss: 0.7023
Epoch 1/10, Batch 210/442, Training Loss: 0.9537
Epoch 1/10, Batch 211/442, Training Loss: 1.3217
Epoch 1/10, Batch 212/442, Training Loss: 0.9007
Epoch 1/10, Batch 213/442, Training Loss: 0.8571
Epoch 1/10, Batch 214/442, Training Loss: 1.0015
Epoch 1/10, Batch 215/442, Training Loss: 0.7680
Epoch 1/10, Batch 216/442, Training Loss: 0.7758
Epoch 1/10, Batch 217/442, Training Loss: 0.8984
Epoch 1/10, Batch 218/442, Training Loss: 1.0530
Epoch 1/10, Batch 219/442, Training Loss: 1.0593
Epoch 1/10, Batch 220/442, Training Loss: 0.8765
Epoch 1/10, Batch 221/442, Training Loss: 0.7834
Epoch 1/10, Batch 222/442, Training Loss: 1.0751
Epoch 1/10, Batch 223/442, Training Loss: 0.9062
Epoch 1/10, Batch 224/442, Training Loss: 1.0383
Epoch 1/10, Batch 225/442, Training Loss: 0.7950
Epoch 1/10, Batch 226/442, Training Loss: 0.9868
Epoch 1/10, Batch 227/442, Training Loss: 0.9006
Epoch 1/10, Batch 228/442, Training Loss: 0.8361
Epoch 1/10, Batch 229/442, Training Loss: 0.8069
Epoch 1/10, Batch 230/442, Training Loss: 1.0500
Epoch 1/10, Batch 231/442, Training Loss: 1.2173
Epoch 1/10, Batch 232/442, Training Loss: 0.9482
Epoch 1/10, Batch 233/442, Training Loss: 0.8888
Epoch 1/10, Batch 234/442, Training Loss: 0.8262
Epoch 1/10, Batch 235/442, Training Loss: 0.9457
Epoch 1/10, Batch 236/442, Training Loss: 0.7976
Epoch 1/10, Batch 237/442, Training Loss: 0.7658
Epoch 1/10, Batch 238/442, Training Loss: 0.8312
Epoch 1/10, Batch 239/442, Training Loss: 1.0753
Epoch 1/10, Batch 240/442, Training Loss: 1.0423
Epoch 1/10, Batch 241/442, Training Loss: 0.7546
Epoch 1/10, Batch 242/442, Training Loss: 1.0230
Epoch 1/10, Batch 243/442, Training Loss: 0.6918
Epoch 1/10, Batch 244/442, Training Loss: 1.0483
Epoch 1/10, Batch 245/442, Training Loss: 0.7594
Epoch 1/10, Batch 246/442, Training Loss: 0.8478
Epoch 1/10, Batch 247/442, Training Loss: 0.8366
Epoch 1/10, Batch 248/442, Training Loss: 0.6911
Epoch 1/10, Batch 249/442, Training Loss: 0.9316
Epoch 1/10, Batch 250/442, Training Loss: 0.9520
Epoch 1/10, Batch 251/442, Training Loss: 0.8532
Epoch 1/10, Batch 252/442, Training Loss: 0.9050
Epoch 1/10, Batch 253/442, Training Loss: 0.7459
Epoch 1/10, Batch 254/442, Training Loss: 0.9833
Epoch 1/10, Batch 255/442, Training Loss: 0.8472
Epoch 1/10, Batch 256/442, Training Loss: 0.6133
Epoch 1/10, Batch 257/442, Training Loss: 0.8353
Epoch 1/10, Batch 258/442, Training Loss: 0.8656
Epoch 1/10, Batch 259/442, Training Loss: 0.7169
Epoch 1/10, Batch 260/442, Training Loss: 0.7752
Epoch 1/10, Batch 261/442, Training Loss: 0.9018
Epoch 1/10, Batch 262/442, Training Loss: 0.6876
Epoch 1/10, Batch 263/442, Training Loss: 0.8842
Epoch 1/10, Batch 264/442, Training Loss: 0.7161
Epoch 1/10, Batch 265/442, Training Loss: 0.9161
Epoch 1/10, Batch 266/442, Training Loss: 0.7061
Epoch 1/10, Batch 267/442, Training Loss: 0.6886
Epoch 1/10, Batch 268/442, Training Loss: 0.6741
Epoch 1/10, Batch 269/442, Training Loss: 1.1240
Epoch 1/10, Batch 270/442, Training Loss: 0.7343
Epoch 1/10, Batch 271/442, Training Loss: 0.7881
Epoch 1/10, Batch 272/442, Training Loss: 0.6348
Epoch 1/10, Batch 273/442, Training Loss: 0.9960
Epoch 1/10, Batch 274/442, Training Loss: 1.0281
Epoch 1/10, Batch 275/442, Training Loss: 0.8152
Epoch 1/10, Batch 276/442, Training Loss: 0.9330
Epoch 1/10, Batch 277/442, Training Loss: 1.1350
Epoch 1/10, Batch 278/442, Training Loss: 0.7810
Epoch 1/10, Batch 279/442, Training Loss: 0.8951
Epoch 1/10, Batch 280/442, Training Loss: 0.7843
Epoch 1/10, Batch 281/442, Training Loss: 0.6751
Epoch 1/10, Batch 282/442, Training Loss: 0.9098
Epoch 1/10, Batch 283/442, Training Loss: 1.0440
Epoch 1/10, Batch 284/442, Training Loss: 0.8355
Epoch 1/10, Batch 285/442, Training Loss: 0.7971
Epoch 1/10, Batch 286/442, Training Loss: 0.8992
Epoch 1/10, Batch 287/442, Training Loss: 0.8893
Epoch 1/10, Batch 288/442, Training Loss: 0.9209
Epoch 1/10, Batch 289/442, Training Loss: 0.8738
Epoch 1/10, Batch 290/442, Training Loss: 0.9515
Epoch 1/10, Batch 291/442, Training Loss: 0.7162
Epoch 1/10, Batch 292/442, Training Loss: 0.9776
Epoch 1/10, Batch 293/442, Training Loss: 1.0944
Epoch 1/10, Batch 294/442, Training Loss: 0.8974
Epoch 1/10, Batch 295/442, Training Loss: 0.8009
Epoch 1/10, Batch 296/442, Training Loss: 0.9177
Epoch 1/10, Batch 297/442, Training Loss: 0.8626
Epoch 1/10, Batch 298/442, Training Loss: 0.9144
Epoch 1/10, Batch 299/442, Training Loss: 1.0004
Epoch 1/10, Batch 300/442, Training Loss: 0.9402
Epoch 1/10, Batch 301/442, Training Loss: 0.8297
Epoch 1/10, Batch 302/442, Training Loss: 0.7811
Epoch 1/10, Batch 303/442, Training Loss: 0.7971
Epoch 1/10, Batch 304/442, Training Loss: 0.8433
Epoch 1/10, Batch 305/442, Training Loss: 0.8738
Epoch 1/10, Batch 306/442, Training Loss: 0.7342
Epoch 1/10, Batch 307/442, Training Loss: 0.8388
Epoch 1/10, Batch 308/442, Training Loss: 0.8257
Epoch 1/10, Batch 309/442, Training Loss: 0.8661
Epoch 1/10, Batch 310/442, Training Loss: 0.6699
Epoch 1/10, Batch 311/442, Training Loss: 0.8466
Epoch 1/10, Batch 312/442, Training Loss: 1.0668
Epoch 1/10, Batch 313/442, Training Loss: 0.6740
Epoch 1/10, Batch 314/442, Training Loss: 0.7586
Epoch 1/10, Batch 315/442, Training Loss: 0.8929
Epoch 1/10, Batch 316/442, Training Loss: 1.0379
Epoch 1/10, Batch 317/442, Training Loss: 0.9811
Epoch 1/10, Batch 318/442, Training Loss: 0.8545
Epoch 1/10, Batch 319/442, Training Loss: 0.7009
Epoch 1/10, Batch 320/442, Training Loss: 0.8540
Epoch 1/10, Batch 321/442, Training Loss: 0.7496
Epoch 1/10, Batch 322/442, Training Loss: 0.7512
Epoch 1/10, Batch 323/442, Training Loss: 0.7514
Epoch 1/10, Batch 324/442, Training Loss: 0.7330
Epoch 1/10, Batch 325/442, Training Loss: 0.9573
Epoch 1/10, Batch 326/442, Training Loss: 0.8544
Epoch 1/10, Batch 327/442, Training Loss: 0.8360
Epoch 1/10, Batch 328/442, Training Loss: 0.7280
Epoch 1/10, Batch 329/442, Training Loss: 0.8636
Epoch 1/10, Batch 330/442, Training Loss: 0.9193
Epoch 1/10, Batch 331/442, Training Loss: 0.7036
Epoch 1/10, Batch 332/442, Training Loss: 0.8415
Epoch 1/10, Batch 333/442, Training Loss: 0.9321
Epoch 1/10, Batch 334/442, Training Loss: 0.7681
Epoch 1/10, Batch 335/442, Training Loss: 0.7646
Epoch 1/10, Batch 336/442, Training Loss: 0.9969
Epoch 1/10, Batch 337/442, Training Loss: 0.7596
Epoch 1/10, Batch 338/442, Training Loss: 0.7310
Epoch 1/10, Batch 339/442, Training Loss: 0.6719
Epoch 1/10, Batch 340/442, Training Loss: 1.0159
Epoch 1/10, Batch 341/442, Training Loss: 0.8644
Epoch 1/10, Batch 342/442, Training Loss: 0.7800
Epoch 1/10, Batch 343/442, Training Loss: 0.8486
Epoch 1/10, Batch 344/442, Training Loss: 0.6907
Epoch 1/10, Batch 345/442, Training Loss: 0.6128
Epoch 1/10, Batch 346/442, Training Loss: 0.6782
Epoch 1/10, Batch 347/442, Training Loss: 0.8507
Epoch 1/10, Batch 348/442, Training Loss: 0.8646
Epoch 1/10, Batch 349/442, Training Loss: 0.6285
Epoch 1/10, Batch 350/442, Training Loss: 0.7068
Epoch 1/10, Batch 351/442, Training Loss: 0.7965
Epoch 1/10, Batch 352/442, Training Loss: 0.5715
Epoch 1/10, Batch 353/442, Training Loss: 0.6865
Epoch 1/10, Batch 354/442, Training Loss: 0.7433
Epoch 1/10, Batch 355/442, Training Loss: 0.8620
Epoch 1/10, Batch 356/442, Training Loss: 1.1180
Epoch 1/10, Batch 357/442, Training Loss: 0.6016
Epoch 1/10, Batch 358/442, Training Loss: 0.6719
Epoch 1/10, Batch 359/442, Training Loss: 0.9059
Epoch 1/10, Batch 360/442, Training Loss: 0.6413
Epoch 1/10, Batch 361/442, Training Loss: 1.0464
Epoch 1/10, Batch 362/442, Training Loss: 0.7732
Epoch 1/10, Batch 363/442, Training Loss: 0.8028
Epoch 1/10, Batch 364/442, Training Loss: 0.6050
Epoch 1/10, Batch 365/442, Training Loss: 0.7126
Epoch 1/10, Batch 366/442, Training Loss: 0.9334
Epoch 1/10, Batch 367/442, Training Loss: 0.7972
Epoch 1/10, Batch 368/442, Training Loss: 0.6904
Epoch 1/10, Batch 369/442, Training Loss: 0.9149
Epoch 1/10, Batch 370/442, Training Loss: 0.7112
Epoch 1/10, Batch 371/442, Training Loss: 0.9239
Epoch 1/10, Batch 372/442, Training Loss: 0.8495
Epoch 1/10, Batch 373/442, Training Loss: 0.8443
Epoch 1/10, Batch 374/442, Training Loss: 1.2212
Epoch 1/10, Batch 375/442, Training Loss: 1.0795
Epoch 1/10, Batch 376/442, Training Loss: 0.8893
Epoch 1/10, Batch 377/442, Training Loss: 0.8637
Epoch 1/10, Batch 378/442, Training Loss: 0.7956
Epoch 1/10, Batch 379/442, Training Loss: 0.8360
Epoch 1/10, Batch 380/442, Training Loss: 0.7838
Epoch 1/10, Batch 381/442, Training Loss: 0.8496
Epoch 1/10, Batch 382/442, Training Loss: 0.9984
Epoch 1/10, Batch 383/442, Training Loss: 0.9254
Epoch 1/10, Batch 384/442, Training Loss: 0.8249
Epoch 1/10, Batch 385/442, Training Loss: 0.8397
Epoch 1/10, Batch 386/442, Training Loss: 0.8076
Epoch 1/10, Batch 387/442, Training Loss: 0.7941
Epoch 1/10, Batch 388/442, Training Loss: 1.0047
Epoch 1/10, Batch 389/442, Training Loss: 0.8408
Epoch 1/10, Batch 390/442, Training Loss: 0.6914
Epoch 1/10, Batch 391/442, Training Loss: 0.7592
Epoch 1/10, Batch 392/442, Training Loss: 0.8793
Epoch 1/10, Batch 393/442, Training Loss: 0.7357
Epoch 1/10, Batch 394/442, Training Loss: 0.7842
Epoch 1/10, Batch 395/442, Training Loss: 0.7070
Epoch 1/10, Batch 396/442, Training Loss: 0.8922
Epoch 1/10, Batch 397/442, Training Loss: 0.9338
Epoch 1/10, Batch 398/442, Training Loss: 0.7705
Epoch 1/10, Batch 399/442, Training Loss: 1.1013
Epoch 1/10, Batch 400/442, Training Loss: 0.8369
Epoch 1/10, Batch 401/442, Training Loss: 1.1397
Epoch 1/10, Batch 402/442, Training Loss: 0.8993
Epoch 1/10, Batch 403/442, Training Loss: 0.6893
Epoch 1/10, Batch 404/442, Training Loss: 0.8672
Epoch 1/10, Batch 405/442, Training Loss: 0.7578
Epoch 1/10, Batch 406/442, Training Loss: 0.8377
Epoch 1/10, Batch 407/442, Training Loss: 1.1116
Epoch 1/10, Batch 408/442, Training Loss: 1.0100
Epoch 1/10, Batch 409/442, Training Loss: 1.0405
Epoch 1/10, Batch 410/442, Training Loss: 0.8009
Epoch 1/10, Batch 411/442, Training Loss: 0.6920
Epoch 1/10, Batch 412/442, Training Loss: 0.7652
Epoch 1/10, Batch 413/442, Training Loss: 0.7297
Epoch 1/10, Batch 414/442, Training Loss: 0.7933
Epoch 1/10, Batch 415/442, Training Loss: 0.6763
Epoch 1/10, Batch 416/442, Training Loss: 0.7297
Epoch 1/10, Batch 417/442, Training Loss: 0.7056
Epoch 1/10, Batch 418/442, Training Loss: 0.9086
Epoch 1/10, Batch 419/442, Training Loss: 0.8825
Epoch 1/10, Batch 420/442, Training Loss: 0.7834
Epoch 1/10, Batch 421/442, Training Loss: 0.8503
Epoch 1/10, Batch 422/442, Training Loss: 0.7315
Epoch 1/10, Batch 423/442, Training Loss: 0.8058
Epoch 1/10, Batch 424/442, Training Loss: 0.8695
Epoch 1/10, Batch 425/442, Training Loss: 1.1311
Epoch 1/10, Batch 426/442, Training Loss: 0.6683
Epoch 1/10, Batch 427/442, Training Loss: 0.8351
Epoch 1/10, Batch 428/442, Training Loss: 0.9033
Epoch 1/10, Batch 429/442, Training Loss: 0.7196
Epoch 1/10, Batch 430/442, Training Loss: 0.7840
Epoch 1/10, Batch 431/442, Training Loss: 0.7513
Epoch 1/10, Batch 432/442, Training Loss: 0.6958
Epoch 1/10, Batch 433/442, Training Loss: 0.9514
Epoch 1/10, Batch 434/442, Training Loss: 0.9757
Epoch 1/10, Batch 435/442, Training Loss: 0.7024
Epoch 1/10, Batch 436/442, Training Loss: 1.0181
Epoch 1/10, Batch 437/442, Training Loss: 0.6202
Epoch 1/10, Batch 438/442, Training Loss: 0.7650
Epoch 1/10, Batch 439/442, Training Loss: 0.8360
Epoch 1/10, Batch 440/442, Training Loss: 0.8520
Epoch 1/10, Batch 441/442, Training Loss: 0.9178
Epoch 1/10, Batch 442/442, Training Loss: 1.3174
Epoch 1/10, Training Loss: 0.9210, Validation Loss: 0.8317, Validation Accuracy: 0.5518
Epoch 2/10, Batch 1/442, Training Loss: 0.9133
Epoch 2/10, Batch 2/442, Training Loss: 0.7818
Epoch 2/10, Batch 3/442, Training Loss: 0.8304
Epoch 2/10, Batch 4/442, Training Loss: 0.9337
Epoch 2/10, Batch 5/442, Training Loss: 0.8344
Epoch 2/10, Batch 6/442, Training Loss: 0.9821
Epoch 2/10, Batch 7/442, Training Loss: 0.7633
Epoch 2/10, Batch 8/442, Training Loss: 1.0239
Epoch 2/10, Batch 9/442, Training Loss: 0.7443
Epoch 2/10, Batch 10/442, Training Loss: 0.8492
Epoch 2/10, Batch 11/442, Training Loss: 0.7284
Epoch 2/10, Batch 12/442, Training Loss: 0.6960
Epoch 2/10, Batch 13/442, Training Loss: 0.6820
Epoch 2/10, Batch 14/442, Training Loss: 0.9620
Epoch 2/10, Batch 15/442, Training Loss: 0.8949
Epoch 2/10, Batch 16/442, Training Loss: 0.9468
Epoch 2/10, Batch 17/442, Training Loss: 0.6371
Epoch 2/10, Batch 18/442, Training Loss: 0.7666
Epoch 2/10, Batch 19/442, Training Loss: 0.7520
Epoch 2/10, Batch 20/442, Training Loss: 0.7574
Epoch 2/10, Batch 21/442, Training Loss: 0.6912
Epoch 2/10, Batch 22/442, Training Loss: 0.7495
Epoch 2/10, Batch 23/442, Training Loss: 0.7843
Epoch 2/10, Batch 24/442, Training Loss: 0.6365
Epoch 2/10, Batch 25/442, Training Loss: 0.5333
Epoch 2/10, Batch 26/442, Training Loss: 1.0186
Epoch 2/10, Batch 27/442, Training Loss: 0.7244
Epoch 2/10, Batch 28/442, Training Loss: 0.7743
Epoch 2/10, Batch 29/442, Training Loss: 1.1262
Epoch 2/10, Batch 30/442, Training Loss: 0.7627
Epoch 2/10, Batch 31/442, Training Loss: 0.9262
Epoch 2/10, Batch 32/442, Training Loss: 1.1060
Epoch 2/10, Batch 33/442, Training Loss: 0.8529
Epoch 2/10, Batch 34/442, Training Loss: 0.7090
Epoch 2/10, Batch 35/442, Training Loss: 1.0524
Epoch 2/10, Batch 36/442, Training Loss: 0.7821
Epoch 2/10, Batch 37/442, Training Loss: 0.6832
Epoch 2/10, Batch 38/442, Training Loss: 0.9406
Epoch 2/10, Batch 39/442, Training Loss: 0.9073
Epoch 2/10, Batch 40/442, Training Loss: 0.7476
Epoch 2/10, Batch 41/442, Training Loss: 0.7295
Epoch 2/10, Batch 42/442, Training Loss: 0.8248
Epoch 2/10, Batch 43/442, Training Loss: 1.2464
Epoch 2/10, Batch 44/442, Training Loss: 0.7715
Epoch 2/10, Batch 45/442, Training Loss: 0.9199
Epoch 2/10, Batch 46/442, Training Loss: 0.7262
Epoch 2/10, Batch 47/442, Training Loss: 0.9499
Epoch 2/10, Batch 48/442, Training Loss: 0.8733
Epoch 2/10, Batch 49/442, Training Loss: 0.8816
Epoch 2/10, Batch 50/442, Training Loss: 0.8258
Epoch 2/10, Batch 51/442, Training Loss: 0.9522
Epoch 2/10, Batch 52/442, Training Loss: 1.0115
Epoch 2/10, Batch 53/442, Training Loss: 0.9351
Epoch 2/10, Batch 54/442, Training Loss: 0.6867
Epoch 2/10, Batch 55/442, Training Loss: 0.8836
Epoch 2/10, Batch 56/442, Training Loss: 0.8662
Epoch 2/10, Batch 57/442, Training Loss: 0.8199
Epoch 2/10, Batch 58/442, Training Loss: 0.6430
Epoch 2/10, Batch 59/442, Training Loss: 0.8036
Epoch 2/10, Batch 60/442, Training Loss: 0.7541
Epoch 2/10, Batch 61/442, Training Loss: 0.7675
Epoch 2/10, Batch 62/442, Training Loss: 0.9963
Epoch 2/10, Batch 63/442, Training Loss: 0.8852
Epoch 2/10, Batch 64/442, Training Loss: 0.8678
Epoch 2/10, Batch 65/442, Training Loss: 0.9426
Epoch 2/10, Batch 66/442, Training Loss: 0.8906
Epoch 2/10, Batch 67/442, Training Loss: 0.7075
Epoch 2/10, Batch 68/442, Training Loss: 0.7230
Epoch 2/10, Batch 69/442, Training Loss: 0.8533
Epoch 2/10, Batch 70/442, Training Loss: 1.0928
Epoch 2/10, Batch 71/442, Training Loss: 0.7850
Epoch 2/10, Batch 72/442, Training Loss: 0.8952
Epoch 2/10, Batch 73/442, Training Loss: 0.8585
Epoch 2/10, Batch 74/442, Training Loss: 0.7499
Epoch 2/10, Batch 75/442, Training Loss: 0.6966
Epoch 2/10, Batch 76/442, Training Loss: 0.9324
Epoch 2/10, Batch 77/442, Training Loss: 0.9369
Epoch 2/10, Batch 78/442, Training Loss: 0.8408
Epoch 2/10, Batch 79/442, Training Loss: 0.7979
Epoch 2/10, Batch 80/442, Training Loss: 0.7176
Epoch 2/10, Batch 81/442, Training Loss: 0.9951
Epoch 2/10, Batch 82/442, Training Loss: 0.8273
Epoch 2/10, Batch 83/442, Training Loss: 0.7124
Epoch 2/10, Batch 84/442, Training Loss: 0.7350
Epoch 2/10, Batch 85/442, Training Loss: 0.8148
Epoch 2/10, Batch 86/442, Training Loss: 0.7367
Epoch 2/10, Batch 87/442, Training Loss: 0.8893
Epoch 2/10, Batch 88/442, Training Loss: 0.7828
Epoch 2/10, Batch 89/442, Training Loss: 0.8723
Epoch 2/10, Batch 90/442, Training Loss: 0.8681
Epoch 2/10, Batch 91/442, Training Loss: 0.8336
Epoch 2/10, Batch 92/442, Training Loss: 0.9016
Epoch 2/10, Batch 93/442, Training Loss: 0.7749
Epoch 2/10, Batch 94/442, Training Loss: 0.8555
Epoch 2/10, Batch 95/442, Training Loss: 1.0815
Epoch 2/10, Batch 96/442, Training Loss: 0.8991
Epoch 2/10, Batch 97/442, Training Loss: 0.6622
Epoch 2/10, Batch 98/442, Training Loss: 0.6593
Epoch 2/10, Batch 99/442, Training Loss: 0.7920
Epoch 2/10, Batch 100/442, Training Loss: 0.7762
Epoch 2/10, Batch 101/442, Training Loss: 0.6940
Epoch 2/10, Batch 102/442, Training Loss: 0.8896
Epoch 2/10, Batch 103/442, Training Loss: 0.8789
Epoch 2/10, Batch 104/442, Training Loss: 0.7323
Epoch 2/10, Batch 105/442, Training Loss: 1.0989
Epoch 2/10, Batch 106/442, Training Loss: 0.6912
Epoch 2/10, Batch 107/442, Training Loss: 0.9629
Epoch 2/10, Batch 108/442, Training Loss: 0.9794
Epoch 2/10, Batch 109/442, Training Loss: 0.7698
Epoch 2/10, Batch 110/442, Training Loss: 0.7719
Epoch 2/10, Batch 111/442, Training Loss: 0.7569
Epoch 2/10, Batch 112/442, Training Loss: 0.8079
Epoch 2/10, Batch 113/442, Training Loss: 0.7179
Epoch 2/10, Batch 114/442, Training Loss: 0.7376
Epoch 2/10, Batch 115/442, Training Loss: 0.7754
Epoch 2/10, Batch 116/442, Training Loss: 0.7112
Epoch 2/10, Batch 117/442, Training Loss: 0.7408
Epoch 2/10, Batch 118/442, Training Loss: 0.9593
Epoch 2/10, Batch 119/442, Training Loss: 1.2291
Epoch 2/10, Batch 120/442, Training Loss: 0.8752
Epoch 2/10, Batch 121/442, Training Loss: 0.8528
Epoch 2/10, Batch 122/442, Training Loss: 0.7582
Epoch 2/10, Batch 123/442, Training Loss: 0.8490
Epoch 2/10, Batch 124/442, Training Loss: 0.7658
Epoch 2/10, Batch 125/442, Training Loss: 0.6393
Epoch 2/10, Batch 126/442, Training Loss: 0.5815
Epoch 2/10, Batch 127/442, Training Loss: 0.7668
Epoch 2/10, Batch 128/442, Training Loss: 0.6573
Epoch 2/10, Batch 129/442, Training Loss: 0.8465
Epoch 2/10, Batch 130/442, Training Loss: 0.8473
Epoch 2/10, Batch 131/442, Training Loss: 0.6509
Epoch 2/10, Batch 132/442, Training Loss: 0.6359
Epoch 2/10, Batch 133/442, Training Loss: 0.6321
Epoch 2/10, Batch 134/442, Training Loss: 0.8138
Epoch 2/10, Batch 135/442, Training Loss: 0.8544
Epoch 2/10, Batch 136/442, Training Loss: 0.9862
Epoch 2/10, Batch 137/442, Training Loss: 0.7991
Epoch 2/10, Batch 138/442, Training Loss: 0.6264
Epoch 2/10, Batch 139/442, Training Loss: 0.9546
Epoch 2/10, Batch 140/442, Training Loss: 0.6654
Epoch 2/10, Batch 141/442, Training Loss: 0.8419
Epoch 2/10, Batch 142/442, Training Loss: 0.8159
Epoch 2/10, Batch 143/442, Training Loss: 0.7023
Epoch 2/10, Batch 144/442, Training Loss: 0.6842
Epoch 2/10, Batch 145/442, Training Loss: 0.8413
Epoch 2/10, Batch 146/442, Training Loss: 0.7857
Epoch 2/10, Batch 147/442, Training Loss: 0.7393
Epoch 2/10, Batch 148/442, Training Loss: 0.7959
Epoch 2/10, Batch 149/442, Training Loss: 0.6970
Epoch 2/10, Batch 150/442, Training Loss: 0.8869
Epoch 2/10, Batch 151/442, Training Loss: 0.7765
Epoch 2/10, Batch 152/442, Training Loss: 0.6177
Epoch 2/10, Batch 153/442, Training Loss: 0.8100
Epoch 2/10, Batch 154/442, Training Loss: 0.8985
Epoch 2/10, Batch 155/442, Training Loss: 1.2621
Epoch 2/10, Batch 156/442, Training Loss: 0.6208
Epoch 2/10, Batch 157/442, Training Loss: 0.8954
Epoch 2/10, Batch 158/442, Training Loss: 0.7055
Epoch 2/10, Batch 159/442, Training Loss: 0.8635
Epoch 2/10, Batch 160/442, Training Loss: 0.7468
Epoch 2/10, Batch 161/442, Training Loss: 0.8320
Epoch 2/10, Batch 162/442, Training Loss: 0.8123
Epoch 2/10, Batch 163/442, Training Loss: 0.6074
Epoch 2/10, Batch 164/442, Training Loss: 0.8251
Epoch 2/10, Batch 165/442, Training Loss: 0.8273
Epoch 2/10, Batch 166/442, Training Loss: 0.9798
Epoch 2/10, Batch 167/442, Training Loss: 0.6609
Epoch 2/10, Batch 168/442, Training Loss: 0.6772
Epoch 2/10, Batch 169/442, Training Loss: 0.9718
Epoch 2/10, Batch 170/442, Training Loss: 0.8913
Epoch 2/10, Batch 171/442, Training Loss: 0.8331
Epoch 2/10, Batch 172/442, Training Loss: 0.7318
Epoch 2/10, Batch 173/442, Training Loss: 0.8910
Epoch 2/10, Batch 174/442, Training Loss: 0.7965
Epoch 2/10, Batch 175/442, Training Loss: 0.7584
Epoch 2/10, Batch 176/442, Training Loss: 0.6969
Epoch 2/10, Batch 177/442, Training Loss: 0.6842
Epoch 2/10, Batch 178/442, Training Loss: 0.6330
Epoch 2/10, Batch 179/442, Training Loss: 0.6983
Epoch 2/10, Batch 180/442, Training Loss: 0.7657
Epoch 2/10, Batch 181/442, Training Loss: 0.7586
Epoch 2/10, Batch 182/442, Training Loss: 0.7968
Epoch 2/10, Batch 183/442, Training Loss: 0.8881
Epoch 2/10, Batch 184/442, Training Loss: 0.6952
Epoch 2/10, Batch 185/442, Training Loss: 0.7049
Epoch 2/10, Batch 186/442, Training Loss: 0.8109
Epoch 2/10, Batch 187/442, Training Loss: 0.8481
Epoch 2/10, Batch 188/442, Training Loss: 0.7385
Epoch 2/10, Batch 189/442, Training Loss: 0.7324
Epoch 2/10, Batch 190/442, Training Loss: 0.7702
Epoch 2/10, Batch 191/442, Training Loss: 0.6424
Epoch 2/10, Batch 192/442, Training Loss: 0.6518
Epoch 2/10, Batch 193/442, Training Loss: 0.7178
Epoch 2/10, Batch 194/442, Training Loss: 0.7488
Epoch 2/10, Batch 195/442, Training Loss: 0.7311
Epoch 2/10, Batch 196/442, Training Loss: 0.7954
Epoch 2/10, Batch 197/442, Training Loss: 0.8057
Epoch 2/10, Batch 198/442, Training Loss: 0.6899
Epoch 2/10, Batch 199/442, Training Loss: 0.6824
Epoch 2/10, Batch 200/442, Training Loss: 0.8675
Epoch 2/10, Batch 201/442, Training Loss: 0.7137
Epoch 2/10, Batch 202/442, Training Loss: 0.6347
Epoch 2/10, Batch 203/442, Training Loss: 0.6307
Epoch 2/10, Batch 204/442, Training Loss: 1.1331
Epoch 2/10, Batch 205/442, Training Loss: 0.8470
Epoch 2/10, Batch 206/442, Training Loss: 0.7769
Epoch 2/10, Batch 207/442, Training Loss: 1.1680
Epoch 2/10, Batch 208/442, Training Loss: 0.7773
Epoch 2/10, Batch 209/442, Training Loss: 0.6247
Epoch 2/10, Batch 210/442, Training Loss: 0.5414
Epoch 2/10, Batch 211/442, Training Loss: 0.7185
Epoch 2/10, Batch 212/442, Training Loss: 0.8365
Epoch 2/10, Batch 213/442, Training Loss: 0.9998
Epoch 2/10, Batch 214/442, Training Loss: 0.6293
Epoch 2/10, Batch 215/442, Training Loss: 0.7347
Epoch 2/10, Batch 216/442, Training Loss: 0.7366
Epoch 2/10, Batch 217/442, Training Loss: 0.6919
Epoch 2/10, Batch 218/442, Training Loss: 0.7944
Epoch 2/10, Batch 219/442, Training Loss: 0.7387
Epoch 2/10, Batch 220/442, Training Loss: 0.7279
Epoch 2/10, Batch 221/442, Training Loss: 0.7438
Epoch 2/10, Batch 222/442, Training Loss: 0.7098
Epoch 2/10, Batch 223/442, Training Loss: 0.8273
Epoch 2/10, Batch 224/442, Training Loss: 0.6267
Epoch 2/10, Batch 225/442, Training Loss: 1.0414
Epoch 2/10, Batch 226/442, Training Loss: 0.8077
Epoch 2/10, Batch 227/442, Training Loss: 0.6625
Epoch 2/10, Batch 228/442, Training Loss: 0.9698
Epoch 2/10, Batch 229/442, Training Loss: 0.7618
Epoch 2/10, Batch 230/442, Training Loss: 0.7487
Epoch 2/10, Batch 231/442, Training Loss: 0.8115
Epoch 2/10, Batch 232/442, Training Loss: 0.8081
Epoch 2/10, Batch 233/442, Training Loss: 0.8504
Epoch 2/10, Batch 234/442, Training Loss: 0.8338
Epoch 2/10, Batch 235/442, Training Loss: 0.6366
Epoch 2/10, Batch 236/442, Training Loss: 0.7359
Epoch 2/10, Batch 237/442, Training Loss: 0.7340
Epoch 2/10, Batch 238/442, Training Loss: 0.7716
Epoch 2/10, Batch 239/442, Training Loss: 0.7678
Epoch 2/10, Batch 240/442, Training Loss: 0.8633
Epoch 2/10, Batch 241/442, Training Loss: 0.7414
Epoch 2/10, Batch 242/442, Training Loss: 0.7201
Epoch 2/10, Batch 243/442, Training Loss: 0.6982
Epoch 2/10, Batch 244/442, Training Loss: 0.8092
Epoch 2/10, Batch 245/442, Training Loss: 0.8951
Epoch 2/10, Batch 246/442, Training Loss: 0.7914
Epoch 2/10, Batch 247/442, Training Loss: 0.7508
Epoch 2/10, Batch 248/442, Training Loss: 0.7719
Epoch 2/10, Batch 249/442, Training Loss: 0.7901
Epoch 2/10, Batch 250/442, Training Loss: 0.9288
Epoch 2/10, Batch 251/442, Training Loss: 0.9282
Epoch 2/10, Batch 252/442, Training Loss: 0.6913
Epoch 2/10, Batch 253/442, Training Loss: 0.6960
Epoch 2/10, Batch 254/442, Training Loss: 0.7800
Epoch 2/10, Batch 255/442, Training Loss: 0.6134
Epoch 2/10, Batch 256/442, Training Loss: 0.8936
Epoch 2/10, Batch 257/442, Training Loss: 0.8941
Epoch 2/10, Batch 258/442, Training Loss: 0.8029
Epoch 2/10, Batch 259/442, Training Loss: 0.7989
Epoch 2/10, Batch 260/442, Training Loss: 1.0929
Epoch 2/10, Batch 261/442, Training Loss: 0.8217
Epoch 2/10, Batch 262/442, Training Loss: 0.6647
Epoch 2/10, Batch 263/442, Training Loss: 0.6503
Epoch 2/10, Batch 264/442, Training Loss: 1.0231
Epoch 2/10, Batch 265/442, Training Loss: 0.7331
Epoch 2/10, Batch 266/442, Training Loss: 0.8142
Epoch 2/10, Batch 267/442, Training Loss: 0.8886
Epoch 2/10, Batch 268/442, Training Loss: 0.9008
Epoch 2/10, Batch 269/442, Training Loss: 0.9072
Epoch 2/10, Batch 270/442, Training Loss: 0.8583
Epoch 2/10, Batch 271/442, Training Loss: 0.8664
Epoch 2/10, Batch 272/442, Training Loss: 0.6693
Epoch 2/10, Batch 273/442, Training Loss: 0.8713
Epoch 2/10, Batch 274/442, Training Loss: 0.7800
Epoch 2/10, Batch 275/442, Training Loss: 0.6848
Epoch 2/10, Batch 276/442, Training Loss: 0.7008
Epoch 2/10, Batch 277/442, Training Loss: 0.6930
Epoch 2/10, Batch 278/442, Training Loss: 1.0636
Epoch 2/10, Batch 279/442, Training Loss: 0.7693
Epoch 2/10, Batch 280/442, Training Loss: 0.7790
Epoch 2/10, Batch 281/442, Training Loss: 0.7561
Epoch 2/10, Batch 282/442, Training Loss: 0.7245
Epoch 2/10, Batch 283/442, Training Loss: 0.6161
Epoch 2/10, Batch 284/442, Training Loss: 0.6885
Epoch 2/10, Batch 285/442, Training Loss: 0.7208
Epoch 2/10, Batch 286/442, Training Loss: 0.7811
Epoch 2/10, Batch 287/442, Training Loss: 0.7697
Epoch 2/10, Batch 288/442, Training Loss: 0.7638
Epoch 2/10, Batch 289/442, Training Loss: 0.6427
Epoch 2/10, Batch 290/442, Training Loss: 0.6129
Epoch 2/10, Batch 291/442, Training Loss: 0.7398
Epoch 2/10, Batch 292/442, Training Loss: 0.9842
Epoch 2/10, Batch 293/442, Training Loss: 0.6637
Epoch 2/10, Batch 294/442, Training Loss: 0.6364
Epoch 2/10, Batch 295/442, Training Loss: 0.7661
Epoch 2/10, Batch 296/442, Training Loss: 0.8556
Epoch 2/10, Batch 297/442, Training Loss: 0.6267
Epoch 2/10, Batch 298/442, Training Loss: 0.7410
Epoch 2/10, Batch 299/442, Training Loss: 0.9640
Epoch 2/10, Batch 300/442, Training Loss: 1.0110
Epoch 2/10, Batch 301/442, Training Loss: 0.8477
Epoch 2/10, Batch 302/442, Training Loss: 0.6247
Epoch 2/10, Batch 303/442, Training Loss: 0.6060
Epoch 2/10, Batch 304/442, Training Loss: 0.9087
Epoch 2/10, Batch 305/442, Training Loss: 0.7992
Epoch 2/10, Batch 306/442, Training Loss: 0.8597
Epoch 2/10, Batch 307/442, Training Loss: 0.8002
Epoch 2/10, Batch 308/442, Training Loss: 0.7043
Epoch 2/10, Batch 309/442, Training Loss: 0.6828
Epoch 2/10, Batch 310/442, Training Loss: 0.8442
Epoch 2/10, Batch 311/442, Training Loss: 0.8354
Epoch 2/10, Batch 312/442, Training Loss: 0.7515
Epoch 2/10, Batch 313/442, Training Loss: 0.6224
Epoch 2/10, Batch 314/442, Training Loss: 0.7865
Epoch 2/10, Batch 315/442, Training Loss: 0.7520
Epoch 2/10, Batch 316/442, Training Loss: 0.8026
Epoch 2/10, Batch 317/442, Training Loss: 0.8331
Epoch 2/10, Batch 318/442, Training Loss: 0.8605
Epoch 2/10, Batch 319/442, Training Loss: 0.8922
Epoch 2/10, Batch 320/442, Training Loss: 0.8309
Epoch 2/10, Batch 321/442, Training Loss: 1.0973
Epoch 2/10, Batch 322/442, Training Loss: 0.7363
Epoch 2/10, Batch 323/442, Training Loss: 0.6506
Epoch 2/10, Batch 324/442, Training Loss: 0.5823
Epoch 2/10, Batch 325/442, Training Loss: 0.7836
Epoch 2/10, Batch 326/442, Training Loss: 0.8425
Epoch 2/10, Batch 327/442, Training Loss: 0.9142
Epoch 2/10, Batch 328/442, Training Loss: 0.7797
Epoch 2/10, Batch 329/442, Training Loss: 1.0497
Epoch 2/10, Batch 330/442, Training Loss: 0.5926
Epoch 2/10, Batch 331/442, Training Loss: 0.9629
Epoch 2/10, Batch 332/442, Training Loss: 1.0199
Epoch 2/10, Batch 333/442, Training Loss: 0.7282
Epoch 2/10, Batch 334/442, Training Loss: 0.7310
Epoch 2/10, Batch 335/442, Training Loss: 0.6686
Epoch 2/10, Batch 336/442, Training Loss: 0.8877
Epoch 2/10, Batch 337/442, Training Loss: 0.9668
Epoch 2/10, Batch 338/442, Training Loss: 0.7635
Epoch 2/10, Batch 339/442, Training Loss: 0.7443
Epoch 2/10, Batch 340/442, Training Loss: 0.9197
Epoch 2/10, Batch 341/442, Training Loss: 0.6984
Epoch 2/10, Batch 342/442, Training Loss: 0.8039
Epoch 2/10, Batch 343/442, Training Loss: 0.7736
Epoch 2/10, Batch 344/442, Training Loss: 0.7398
Epoch 2/10, Batch 345/442, Training Loss: 0.7405
Epoch 2/10, Batch 346/442, Training Loss: 0.7398
Epoch 2/10, Batch 347/442, Training Loss: 0.7574
Epoch 2/10, Batch 348/442, Training Loss: 0.9346
Epoch 2/10, Batch 349/442, Training Loss: 0.7445
Epoch 2/10, Batch 350/442, Training Loss: 0.7706
Epoch 2/10, Batch 351/442, Training Loss: 0.6312
Epoch 2/10, Batch 352/442, Training Loss: 0.7009
Epoch 2/10, Batch 353/442, Training Loss: 0.9198
Epoch 2/10, Batch 354/442, Training Loss: 0.6140
Epoch 2/10, Batch 355/442, Training Loss: 0.9084
Epoch 2/10, Batch 356/442, Training Loss: 0.7515
Epoch 2/10, Batch 357/442, Training Loss: 0.8872
Epoch 2/10, Batch 358/442, Training Loss: 0.8176
Epoch 2/10, Batch 359/442, Training Loss: 0.9831
Epoch 2/10, Batch 360/442, Training Loss: 0.8776
Epoch 2/10, Batch 361/442, Training Loss: 0.6764
Epoch 2/10, Batch 362/442, Training Loss: 0.8683
Epoch 2/10, Batch 363/442, Training Loss: 0.7663
Epoch 2/10, Batch 364/442, Training Loss: 1.0261
Epoch 2/10, Batch 365/442, Training Loss: 0.9379
Epoch 2/10, Batch 366/442, Training Loss: 0.7821
Epoch 2/10, Batch 367/442, Training Loss: 0.7530
Epoch 2/10, Batch 368/442, Training Loss: 0.7115
Epoch 2/10, Batch 369/442, Training Loss: 0.7492
Epoch 2/10, Batch 370/442, Training Loss: 0.7614
Epoch 2/10, Batch 371/442, Training Loss: 0.7830
Epoch 2/10, Batch 372/442, Training Loss: 0.6869
Epoch 2/10, Batch 373/442, Training Loss: 0.7788
Epoch 2/10, Batch 374/442, Training Loss: 0.8440
Epoch 2/10, Batch 375/442, Training Loss: 0.7428
Epoch 2/10, Batch 376/442, Training Loss: 0.7028
Epoch 2/10, Batch 377/442, Training Loss: 1.1939
Epoch 2/10, Batch 378/442, Training Loss: 0.8743
Epoch 2/10, Batch 379/442, Training Loss: 0.6171
Epoch 2/10, Batch 380/442, Training Loss: 0.4662
Epoch 2/10, Batch 381/442, Training Loss: 0.7460
Epoch 2/10, Batch 382/442, Training Loss: 0.6073
Epoch 2/10, Batch 383/442, Training Loss: 1.0070
Epoch 2/10, Batch 384/442, Training Loss: 0.7641
Epoch 2/10, Batch 385/442, Training Loss: 0.6476
Epoch 2/10, Batch 386/442, Training Loss: 0.7639
Epoch 2/10, Batch 387/442, Training Loss: 0.8647
Epoch 2/10, Batch 388/442, Training Loss: 0.6726
Epoch 2/10, Batch 389/442, Training Loss: 0.6911
Epoch 2/10, Batch 390/442, Training Loss: 0.8355
Epoch 2/10, Batch 391/442, Training Loss: 0.7925
Epoch 2/10, Batch 392/442, Training Loss: 0.8150
Epoch 2/10, Batch 393/442, Training Loss: 0.6848
Epoch 2/10, Batch 394/442, Training Loss: 0.7828
Epoch 2/10, Batch 395/442, Training Loss: 0.5941
Epoch 2/10, Batch 396/442, Training Loss: 0.9885
Epoch 2/10, Batch 397/442, Training Loss: 0.6403
Epoch 2/10, Batch 398/442, Training Loss: 0.7518
Epoch 2/10, Batch 399/442, Training Loss: 0.6913
Epoch 2/10, Batch 400/442, Training Loss: 0.8264
Epoch 2/10, Batch 401/442, Training Loss: 0.8667
Epoch 2/10, Batch 402/442, Training Loss: 0.6076
Epoch 2/10, Batch 403/442, Training Loss: 0.7418
Epoch 2/10, Batch 404/442, Training Loss: 0.6531
Epoch 2/10, Batch 405/442, Training Loss: 0.7061
Epoch 2/10, Batch 406/442, Training Loss: 0.6846
Epoch 2/10, Batch 407/442, Training Loss: 0.6137
Epoch 2/10, Batch 408/442, Training Loss: 0.5945
Epoch 2/10, Batch 409/442, Training Loss: 0.7248
Epoch 2/10, Batch 410/442, Training Loss: 0.7607
Epoch 2/10, Batch 411/442, Training Loss: 0.6495
Epoch 2/10, Batch 412/442, Training Loss: 0.7124
Epoch 2/10, Batch 413/442, Training Loss: 0.4752
Epoch 2/10, Batch 414/442, Training Loss: 0.8327
Epoch 2/10, Batch 415/442, Training Loss: 0.5972
Epoch 2/10, Batch 416/442, Training Loss: 0.7602
Epoch 2/10, Batch 417/442, Training Loss: 0.9128
Epoch 2/10, Batch 418/442, Training Loss: 0.7838
Epoch 2/10, Batch 419/442, Training Loss: 0.8562
Epoch 2/10, Batch 420/442, Training Loss: 0.9577
Epoch 2/10, Batch 421/442, Training Loss: 0.8617
Epoch 2/10, Batch 422/442, Training Loss: 0.9165
Epoch 2/10, Batch 423/442, Training Loss: 0.7704
Epoch 2/10, Batch 424/442, Training Loss: 0.7781
Epoch 2/10, Batch 425/442, Training Loss: 1.0820
Epoch 2/10, Batch 426/442, Training Loss: 1.1871
Epoch 2/10, Batch 427/442, Training Loss: 0.6953
Epoch 2/10, Batch 428/442, Training Loss: 0.7638
Epoch 2/10, Batch 429/442, Training Loss: 0.6249
Epoch 2/10, Batch 430/442, Training Loss: 0.7947
Epoch 2/10, Batch 431/442, Training Loss: 0.8329
Epoch 2/10, Batch 432/442, Training Loss: 0.7645
Epoch 2/10, Batch 433/442, Training Loss: 0.7287
Epoch 2/10, Batch 434/442, Training Loss: 0.7360
Epoch 2/10, Batch 435/442, Training Loss: 0.7155
Epoch 2/10, Batch 436/442, Training Loss: 0.7055
Epoch 2/10, Batch 437/442, Training Loss: 0.7060
Epoch 2/10, Batch 438/442, Training Loss: 0.7305
Epoch 2/10, Batch 439/442, Training Loss: 0.6326
Epoch 2/10, Batch 440/442, Training Loss: 0.7407
Epoch 2/10, Batch 441/442, Training Loss: 0.6195
Epoch 2/10, Batch 442/442, Training Loss: 0.6693
Epoch 2/10, Training Loss: 0.7938, Validation Loss: 0.8418, Validation Accuracy: 0.5928
Epoch 3/10, Batch 1/442, Training Loss: 0.6087
Epoch 3/10, Batch 2/442, Training Loss: 0.7386
Epoch 3/10, Batch 3/442, Training Loss: 0.9439
Epoch 3/10, Batch 4/442, Training Loss: 0.5941
Epoch 3/10, Batch 5/442, Training Loss: 0.8641
Epoch 3/10, Batch 6/442, Training Loss: 0.6638
Epoch 3/10, Batch 7/442, Training Loss: 0.6504
Epoch 3/10, Batch 8/442, Training Loss: 0.6212
Epoch 3/10, Batch 9/442, Training Loss: 0.6849
Epoch 3/10, Batch 10/442, Training Loss: 0.8329
Epoch 3/10, Batch 11/442, Training Loss: 0.9664
Epoch 3/10, Batch 12/442, Training Loss: 0.6791
Epoch 3/10, Batch 13/442, Training Loss: 0.7668
Epoch 3/10, Batch 14/442, Training Loss: 0.6900
Epoch 3/10, Batch 15/442, Training Loss: 0.6926
Epoch 3/10, Batch 16/442, Training Loss: 0.7789
Epoch 3/10, Batch 17/442, Training Loss: 0.6916
Epoch 3/10, Batch 18/442, Training Loss: 0.6402
Epoch 3/10, Batch 19/442, Training Loss: 0.6393
Epoch 3/10, Batch 20/442, Training Loss: 0.8088
Epoch 3/10, Batch 21/442, Training Loss: 0.6168
Epoch 3/10, Batch 22/442, Training Loss: 0.7513
Epoch 3/10, Batch 23/442, Training Loss: 0.6920
Epoch 3/10, Batch 24/442, Training Loss: 0.8803
Epoch 3/10, Batch 25/442, Training Loss: 0.5613
Epoch 3/10, Batch 26/442, Training Loss: 0.9292
Epoch 3/10, Batch 27/442, Training Loss: 0.7460
Epoch 3/10, Batch 28/442, Training Loss: 0.6413
Epoch 3/10, Batch 29/442, Training Loss: 0.5682
Epoch 3/10, Batch 30/442, Training Loss: 0.7147
Epoch 3/10, Batch 31/442, Training Loss: 0.8656
Epoch 3/10, Batch 32/442, Training Loss: 0.7650
Epoch 3/10, Batch 33/442, Training Loss: 0.6906
Epoch 3/10, Batch 34/442, Training Loss: 0.7564
Epoch 3/10, Batch 35/442, Training Loss: 0.7735
Epoch 3/10, Batch 36/442, Training Loss: 0.7306
Epoch 3/10, Batch 37/442, Training Loss: 1.1436
Epoch 3/10, Batch 38/442, Training Loss: 1.1473
Epoch 3/10, Batch 39/442, Training Loss: 0.9309
Epoch 3/10, Batch 40/442, Training Loss: 0.6039
Epoch 3/10, Batch 41/442, Training Loss: 0.6316
Epoch 3/10, Batch 42/442, Training Loss: 0.7670
Epoch 3/10, Batch 43/442, Training Loss: 0.8338
Epoch 3/10, Batch 44/442, Training Loss: 0.7045
Epoch 3/10, Batch 45/442, Training Loss: 0.7103
Epoch 3/10, Batch 46/442, Training Loss: 0.7153
Epoch 3/10, Batch 47/442, Training Loss: 0.8067
Epoch 3/10, Batch 48/442, Training Loss: 0.6844
Epoch 3/10, Batch 49/442, Training Loss: 0.8679
Epoch 3/10, Batch 50/442, Training Loss: 0.8337
Epoch 3/10, Batch 51/442, Training Loss: 0.7122
Epoch 3/10, Batch 52/442, Training Loss: 0.7178
Epoch 3/10, Batch 53/442, Training Loss: 1.1834
Epoch 3/10, Batch 54/442, Training Loss: 0.8707
Epoch 3/10, Batch 55/442, Training Loss: 0.7413
Epoch 3/10, Batch 56/442, Training Loss: 0.7748
Epoch 3/10, Batch 57/442, Training Loss: 0.7671
Epoch 3/10, Batch 58/442, Training Loss: 0.7985
Epoch 3/10, Batch 59/442, Training Loss: 0.8122
Epoch 3/10, Batch 60/442, Training Loss: 0.8448
Epoch 3/10, Batch 61/442, Training Loss: 0.6584
Epoch 3/10, Batch 62/442, Training Loss: 0.8284
Epoch 3/10, Batch 63/442, Training Loss: 0.7620
Epoch 3/10, Batch 64/442, Training Loss: 0.7394
Epoch 3/10, Batch 65/442, Training Loss: 0.7699
Epoch 3/10, Batch 66/442, Training Loss: 0.6575
Epoch 3/10, Batch 67/442, Training Loss: 0.7998
Epoch 3/10, Batch 68/442, Training Loss: 0.6463
Epoch 3/10, Batch 69/442, Training Loss: 0.7260
Epoch 3/10, Batch 70/442, Training Loss: 0.9352
Epoch 3/10, Batch 71/442, Training Loss: 0.7639
Epoch 3/10, Batch 72/442, Training Loss: 0.5737
Epoch 3/10, Batch 73/442, Training Loss: 0.8559
Epoch 3/10, Batch 74/442, Training Loss: 0.7620
Epoch 3/10, Batch 75/442, Training Loss: 0.6658
Epoch 3/10, Batch 76/442, Training Loss: 0.7229
Epoch 3/10, Batch 77/442, Training Loss: 0.6471
Epoch 3/10, Batch 78/442, Training Loss: 0.9821
Epoch 3/10, Batch 79/442, Training Loss: 0.8081
Epoch 3/10, Batch 80/442, Training Loss: 0.7509
Epoch 3/10, Batch 81/442, Training Loss: 0.6297
Epoch 3/10, Batch 82/442, Training Loss: 0.5720
Epoch 3/10, Batch 83/442, Training Loss: 0.8822
Epoch 3/10, Batch 84/442, Training Loss: 1.0378
Epoch 3/10, Batch 85/442, Training Loss: 0.6896
Epoch 3/10, Batch 86/442, Training Loss: 0.6367
Epoch 3/10, Batch 87/442, Training Loss: 0.6230
Epoch 3/10, Batch 88/442, Training Loss: 0.7229
Epoch 3/10, Batch 89/442, Training Loss: 0.9465
Epoch 3/10, Batch 90/442, Training Loss: 0.6948
Epoch 3/10, Batch 91/442, Training Loss: 0.8000
Epoch 3/10, Batch 92/442, Training Loss: 0.9123
Epoch 3/10, Batch 93/442, Training Loss: 0.9285
Epoch 3/10, Batch 94/442, Training Loss: 0.9416
Epoch 3/10, Batch 95/442, Training Loss: 0.7296
Epoch 3/10, Batch 96/442, Training Loss: 0.7035
Epoch 3/10, Batch 97/442, Training Loss: 0.6875
Epoch 3/10, Batch 98/442, Training Loss: 0.8493
Epoch 3/10, Batch 99/442, Training Loss: 0.7436
Epoch 3/10, Batch 100/442, Training Loss: 0.6275
Epoch 3/10, Batch 101/442, Training Loss: 0.8526
Epoch 3/10, Batch 102/442, Training Loss: 0.7593
Epoch 3/10, Batch 103/442, Training Loss: 0.8066
Epoch 3/10, Batch 104/442, Training Loss: 0.6981
Epoch 3/10, Batch 105/442, Training Loss: 0.5344
Epoch 3/10, Batch 106/442, Training Loss: 0.7560
Epoch 3/10, Batch 107/442, Training Loss: 1.0011
Epoch 3/10, Batch 108/442, Training Loss: 0.6794
Epoch 3/10, Batch 109/442, Training Loss: 0.9357
Epoch 3/10, Batch 110/442, Training Loss: 0.6706
Epoch 3/10, Batch 111/442, Training Loss: 0.9968
Epoch 3/10, Batch 112/442, Training Loss: 0.8075
Epoch 3/10, Batch 113/442, Training Loss: 0.7952
Epoch 3/10, Batch 114/442, Training Loss: 0.7475
Epoch 3/10, Batch 115/442, Training Loss: 0.4829
Epoch 3/10, Batch 116/442, Training Loss: 0.5942
Epoch 3/10, Batch 117/442, Training Loss: 0.7215
Epoch 3/10, Batch 118/442, Training Loss: 0.8540
Epoch 3/10, Batch 119/442, Training Loss: 0.8804
Epoch 3/10, Batch 120/442, Training Loss: 0.5864
Epoch 3/10, Batch 121/442, Training Loss: 0.6991
Epoch 3/10, Batch 122/442, Training Loss: 0.7366
Epoch 3/10, Batch 123/442, Training Loss: 0.7243
Epoch 3/10, Batch 124/442, Training Loss: 0.7204
Epoch 3/10, Batch 125/442, Training Loss: 0.7354
Epoch 3/10, Batch 126/442, Training Loss: 0.6971
Epoch 3/10, Batch 127/442, Training Loss: 0.6584
Epoch 3/10, Batch 128/442, Training Loss: 0.8484
Epoch 3/10, Batch 129/442, Training Loss: 0.8448
Epoch 3/10, Batch 130/442, Training Loss: 0.5965
Epoch 3/10, Batch 131/442, Training Loss: 0.6194
Epoch 3/10, Batch 132/442, Training Loss: 0.6429
Epoch 3/10, Batch 133/442, Training Loss: 0.9156
Epoch 3/10, Batch 134/442, Training Loss: 0.8078
Epoch 3/10, Batch 135/442, Training Loss: 0.6372
Epoch 3/10, Batch 136/442, Training Loss: 0.7003
Epoch 3/10, Batch 137/442, Training Loss: 0.8565
Epoch 3/10, Batch 138/442, Training Loss: 0.7934
Epoch 3/10, Batch 139/442, Training Loss: 0.9947
Epoch 3/10, Batch 140/442, Training Loss: 0.9106
Epoch 3/10, Batch 141/442, Training Loss: 0.7775
Epoch 3/10, Batch 142/442, Training Loss: 0.8967
Epoch 3/10, Batch 143/442, Training Loss: 0.6253
Epoch 3/10, Batch 144/442, Training Loss: 0.5647
Epoch 3/10, Batch 145/442, Training Loss: 0.9120
Epoch 3/10, Batch 146/442, Training Loss: 0.7283
Epoch 3/10, Batch 147/442, Training Loss: 0.6270
Epoch 3/10, Batch 148/442, Training Loss: 0.6443
Epoch 3/10, Batch 149/442, Training Loss: 0.6581
Epoch 3/10, Batch 150/442, Training Loss: 0.7538
Epoch 3/10, Batch 151/442, Training Loss: 0.8441
Epoch 3/10, Batch 152/442, Training Loss: 0.6631
Epoch 3/10, Batch 153/442, Training Loss: 0.5120
Epoch 3/10, Batch 154/442, Training Loss: 0.6396
Epoch 3/10, Batch 155/442, Training Loss: 0.8028
Epoch 3/10, Batch 156/442, Training Loss: 0.7097
Epoch 3/10, Batch 157/442, Training Loss: 0.6967
Epoch 3/10, Batch 158/442, Training Loss: 0.6583
Epoch 3/10, Batch 159/442, Training Loss: 0.8855
Epoch 3/10, Batch 160/442, Training Loss: 0.6684
Epoch 3/10, Batch 161/442, Training Loss: 0.7838
Epoch 3/10, Batch 162/442, Training Loss: 0.6175
Epoch 3/10, Batch 163/442, Training Loss: 0.6757
Epoch 3/10, Batch 164/442, Training Loss: 0.7292
Epoch 3/10, Batch 165/442, Training Loss: 0.6049
Epoch 3/10, Batch 166/442, Training Loss: 0.9969
Epoch 3/10, Batch 167/442, Training Loss: 0.8174
Epoch 3/10, Batch 168/442, Training Loss: 0.8211
Epoch 3/10, Batch 169/442, Training Loss: 0.7628
Epoch 3/10, Batch 170/442, Training Loss: 0.6233
Epoch 3/10, Batch 171/442, Training Loss: 0.7034
Epoch 3/10, Batch 172/442, Training Loss: 0.7318
Epoch 3/10, Batch 173/442, Training Loss: 0.7944
Epoch 3/10, Batch 174/442, Training Loss: 0.6382
Epoch 3/10, Batch 175/442, Training Loss: 0.6870
Epoch 3/10, Batch 176/442, Training Loss: 0.7114
Epoch 3/10, Batch 177/442, Training Loss: 0.7443
Epoch 3/10, Batch 178/442, Training Loss: 0.6237
Epoch 3/10, Batch 179/442, Training Loss: 0.7131
Epoch 3/10, Batch 180/442, Training Loss: 0.7745
Epoch 3/10, Batch 181/442, Training Loss: 0.6451
Epoch 3/10, Batch 182/442, Training Loss: 0.6971
Epoch 3/10, Batch 183/442, Training Loss: 0.8763
Epoch 3/10, Batch 184/442, Training Loss: 0.8060
Epoch 3/10, Batch 185/442, Training Loss: 0.5609
Epoch 3/10, Batch 186/442, Training Loss: 0.8951
Epoch 3/10, Batch 187/442, Training Loss: 0.6633
Epoch 3/10, Batch 188/442, Training Loss: 0.6966
Epoch 3/10, Batch 189/442, Training Loss: 0.6453
Epoch 3/10, Batch 190/442, Training Loss: 0.6734
Epoch 3/10, Batch 191/442, Training Loss: 0.5755
Epoch 3/10, Batch 192/442, Training Loss: 0.6670
Epoch 3/10, Batch 193/442, Training Loss: 0.7825
Epoch 3/10, Batch 194/442, Training Loss: 0.9132
Epoch 3/10, Batch 195/442, Training Loss: 0.8252
Epoch 3/10, Batch 196/442, Training Loss: 0.6243
Epoch 3/10, Batch 197/442, Training Loss: 0.5783
Epoch 3/10, Batch 198/442, Training Loss: 0.7315
Epoch 3/10, Batch 199/442, Training Loss: 0.9327
Epoch 3/10, Batch 200/442, Training Loss: 0.6373
Epoch 3/10, Batch 201/442, Training Loss: 0.6016
Epoch 3/10, Batch 202/442, Training Loss: 0.9971
Epoch 3/10, Batch 203/442, Training Loss: 0.6810
Epoch 3/10, Batch 204/442, Training Loss: 0.7982
Epoch 3/10, Batch 205/442, Training Loss: 0.6157
Epoch 3/10, Batch 206/442, Training Loss: 0.5196
Epoch 3/10, Batch 207/442, Training Loss: 0.7784
Epoch 3/10, Batch 208/442, Training Loss: 0.7327
Epoch 3/10, Batch 209/442, Training Loss: 0.7032
Epoch 3/10, Batch 210/442, Training Loss: 0.6469
Epoch 3/10, Batch 211/442, Training Loss: 0.7180
Epoch 3/10, Batch 212/442, Training Loss: 0.5969
Epoch 3/10, Batch 213/442, Training Loss: 0.5295
Epoch 3/10, Batch 214/442, Training Loss: 0.6933
Epoch 3/10, Batch 215/442, Training Loss: 0.8433
Epoch 3/10, Batch 216/442, Training Loss: 0.9274
Epoch 3/10, Batch 217/442, Training Loss: 0.9487
Epoch 3/10, Batch 218/442, Training Loss: 0.7358
Epoch 3/10, Batch 219/442, Training Loss: 0.8772
Epoch 3/10, Batch 220/442, Training Loss: 0.8425
Epoch 3/10, Batch 221/442, Training Loss: 0.7614
Epoch 3/10, Batch 222/442, Training Loss: 0.6710
Epoch 3/10, Batch 223/442, Training Loss: 0.5632
Epoch 3/10, Batch 224/442, Training Loss: 0.5614
Epoch 3/10, Batch 225/442, Training Loss: 0.6883
Epoch 3/10, Batch 226/442, Training Loss: 0.6966
Epoch 3/10, Batch 227/442, Training Loss: 0.7674
Epoch 3/10, Batch 228/442, Training Loss: 0.6127
Epoch 3/10, Batch 229/442, Training Loss: 0.8263
Epoch 3/10, Batch 230/442, Training Loss: 0.7569
Epoch 3/10, Batch 231/442, Training Loss: 0.8754
Epoch 3/10, Batch 232/442, Training Loss: 0.7413
Epoch 3/10, Batch 233/442, Training Loss: 0.9177
Epoch 3/10, Batch 234/442, Training Loss: 0.6619
Epoch 3/10, Batch 235/442, Training Loss: 0.7246
Epoch 3/10, Batch 236/442, Training Loss: 0.7309
Epoch 3/10, Batch 237/442, Training Loss: 0.7550
Epoch 3/10, Batch 238/442, Training Loss: 0.6416
Epoch 3/10, Batch 239/442, Training Loss: 0.5763
Epoch 3/10, Batch 240/442, Training Loss: 0.6597
Epoch 3/10, Batch 241/442, Training Loss: 0.6281
Epoch 3/10, Batch 242/442, Training Loss: 0.7121
Epoch 3/10, Batch 243/442, Training Loss: 0.9987
Epoch 3/10, Batch 244/442, Training Loss: 0.7227
Epoch 3/10, Batch 245/442, Training Loss: 0.6117
Epoch 3/10, Batch 246/442, Training Loss: 0.7042
Epoch 3/10, Batch 247/442, Training Loss: 0.5950
Epoch 3/10, Batch 248/442, Training Loss: 0.6500
Epoch 3/10, Batch 249/442, Training Loss: 0.5078
Epoch 3/10, Batch 250/442, Training Loss: 0.7885
Epoch 3/10, Batch 251/442, Training Loss: 0.5955
Epoch 3/10, Batch 252/442, Training Loss: 0.8195
Epoch 3/10, Batch 253/442, Training Loss: 0.8605
Epoch 3/10, Batch 254/442, Training Loss: 0.8051
Epoch 3/10, Batch 255/442, Training Loss: 0.7235
Epoch 3/10, Batch 256/442, Training Loss: 0.7214
Epoch 3/10, Batch 257/442, Training Loss: 0.8110
Epoch 3/10, Batch 258/442, Training Loss: 1.0230
Epoch 3/10, Batch 259/442, Training Loss: 0.7456
Epoch 3/10, Batch 260/442, Training Loss: 0.5868
Epoch 3/10, Batch 261/442, Training Loss: 0.6649
Epoch 3/10, Batch 262/442, Training Loss: 0.6075
Epoch 3/10, Batch 263/442, Training Loss: 0.5700
Epoch 3/10, Batch 264/442, Training Loss: 0.5805
Epoch 3/10, Batch 265/442, Training Loss: 0.5893
Epoch 3/10, Batch 266/442, Training Loss: 0.6150
Epoch 3/10, Batch 267/442, Training Loss: 0.6967
Epoch 3/10, Batch 268/442, Training Loss: 0.7184
Epoch 3/10, Batch 269/442, Training Loss: 0.7863
Epoch 3/10, Batch 270/442, Training Loss: 0.6689
Epoch 3/10, Batch 271/442, Training Loss: 0.8196
Epoch 3/10, Batch 272/442, Training Loss: 0.8102
Epoch 3/10, Batch 273/442, Training Loss: 0.6103
Epoch 3/10, Batch 274/442, Training Loss: 0.8313
Epoch 3/10, Batch 275/442, Training Loss: 0.5931
Epoch 3/10, Batch 276/442, Training Loss: 0.8261
Epoch 3/10, Batch 277/442, Training Loss: 0.6971
Epoch 3/10, Batch 278/442, Training Loss: 0.5143
Epoch 3/10, Batch 279/442, Training Loss: 0.6069
Epoch 3/10, Batch 280/442, Training Loss: 0.7482
Epoch 3/10, Batch 281/442, Training Loss: 0.7349
Epoch 3/10, Batch 282/442, Training Loss: 0.6577
Epoch 3/10, Batch 283/442, Training Loss: 0.6662
Epoch 3/10, Batch 284/442, Training Loss: 0.7075
Epoch 3/10, Batch 285/442, Training Loss: 0.7803
Epoch 3/10, Batch 286/442, Training Loss: 0.7525
Epoch 3/10, Batch 287/442, Training Loss: 0.6990
Epoch 3/10, Batch 288/442, Training Loss: 0.6787
Epoch 3/10, Batch 289/442, Training Loss: 0.5926
Epoch 3/10, Batch 290/442, Training Loss: 0.6712
Epoch 3/10, Batch 291/442, Training Loss: 0.4991
Epoch 3/10, Batch 292/442, Training Loss: 0.7731
Epoch 3/10, Batch 293/442, Training Loss: 0.4393
Epoch 3/10, Batch 294/442, Training Loss: 0.7607
Epoch 3/10, Batch 295/442, Training Loss: 0.7593
Epoch 3/10, Batch 296/442, Training Loss: 0.7684
Epoch 3/10, Batch 297/442, Training Loss: 0.6866
Epoch 3/10, Batch 298/442, Training Loss: 0.9611
Epoch 3/10, Batch 299/442, Training Loss: 0.7122
Epoch 3/10, Batch 300/442, Training Loss: 0.7939
Epoch 3/10, Batch 301/442, Training Loss: 0.7826
Epoch 3/10, Batch 302/442, Training Loss: 0.7022
Epoch 3/10, Batch 303/442, Training Loss: 0.6717
Epoch 3/10, Batch 304/442, Training Loss: 0.7152
Epoch 3/10, Batch 305/442, Training Loss: 0.5268
Epoch 3/10, Batch 306/442, Training Loss: 0.6635
Epoch 3/10, Batch 307/442, Training Loss: 0.7456
Epoch 3/10, Batch 308/442, Training Loss: 0.7673
Epoch 3/10, Batch 309/442, Training Loss: 0.7128
Epoch 3/10, Batch 310/442, Training Loss: 0.6121
Epoch 3/10, Batch 311/442, Training Loss: 0.8237
Epoch 3/10, Batch 312/442, Training Loss: 0.6740
Epoch 3/10, Batch 313/442, Training Loss: 0.7942
Epoch 3/10, Batch 314/442, Training Loss: 0.8468
Epoch 3/10, Batch 315/442, Training Loss: 0.6566
Epoch 3/10, Batch 316/442, Training Loss: 0.8730
Epoch 3/10, Batch 317/442, Training Loss: 0.8917
Epoch 3/10, Batch 318/442, Training Loss: 0.6597
Epoch 3/10, Batch 319/442, Training Loss: 0.7840
Epoch 3/10, Batch 320/442, Training Loss: 0.5241
Epoch 3/10, Batch 321/442, Training Loss: 0.5123
Epoch 3/10, Batch 322/442, Training Loss: 0.7439
Epoch 3/10, Batch 323/442, Training Loss: 0.7045
Epoch 3/10, Batch 324/442, Training Loss: 0.8606
Epoch 3/10, Batch 325/442, Training Loss: 0.7258
Epoch 3/10, Batch 326/442, Training Loss: 0.6918
Epoch 3/10, Batch 327/442, Training Loss: 0.8292
Epoch 3/10, Batch 328/442, Training Loss: 1.0255
Epoch 3/10, Batch 329/442, Training Loss: 0.6804
Epoch 3/10, Batch 330/442, Training Loss: 0.7545
Epoch 3/10, Batch 331/442, Training Loss: 0.6406
Epoch 3/10, Batch 332/442, Training Loss: 0.6633
Epoch 3/10, Batch 333/442, Training Loss: 0.7925
Epoch 3/10, Batch 334/442, Training Loss: 0.8474
Epoch 3/10, Batch 335/442, Training Loss: 0.6335
Epoch 3/10, Batch 336/442, Training Loss: 0.9522
Epoch 3/10, Batch 337/442, Training Loss: 0.6754
Epoch 3/10, Batch 338/442, Training Loss: 0.5225
Epoch 3/10, Batch 339/442, Training Loss: 0.7629
Epoch 3/10, Batch 340/442, Training Loss: 0.6060
Epoch 3/10, Batch 341/442, Training Loss: 0.8017
Epoch 3/10, Batch 342/442, Training Loss: 0.6282
Epoch 3/10, Batch 343/442, Training Loss: 0.6831
Epoch 3/10, Batch 344/442, Training Loss: 0.6916
Epoch 3/10, Batch 345/442, Training Loss: 0.8840
Epoch 3/10, Batch 346/442, Training Loss: 0.6964
Epoch 3/10, Batch 347/442, Training Loss: 0.7014
Epoch 3/10, Batch 348/442, Training Loss: 0.6334
Epoch 3/10, Batch 349/442, Training Loss: 0.7340
Epoch 3/10, Batch 350/442, Training Loss: 0.7111
Epoch 3/10, Batch 351/442, Training Loss: 0.7937
Epoch 3/10, Batch 352/442, Training Loss: 0.8234
Epoch 3/10, Batch 353/442, Training Loss: 0.9198
Epoch 3/10, Batch 354/442, Training Loss: 0.7769
Epoch 3/10, Batch 355/442, Training Loss: 0.6257
Epoch 3/10, Batch 356/442, Training Loss: 0.7625
Epoch 3/10, Batch 357/442, Training Loss: 0.7007
Epoch 3/10, Batch 358/442, Training Loss: 0.8877
Epoch 3/10, Batch 359/442, Training Loss: 0.7405
Epoch 3/10, Batch 360/442, Training Loss: 0.6280
Epoch 3/10, Batch 361/442, Training Loss: 0.6632
Epoch 3/10, Batch 362/442, Training Loss: 0.8876
Epoch 3/10, Batch 363/442, Training Loss: 0.8324
Epoch 3/10, Batch 364/442, Training Loss: 0.6694
Epoch 3/10, Batch 365/442, Training Loss: 0.6559
Epoch 3/10, Batch 366/442, Training Loss: 0.6577
Epoch 3/10, Batch 367/442, Training Loss: 0.6374
Epoch 3/10, Batch 368/442, Training Loss: 0.9726
Epoch 3/10, Batch 369/442, Training Loss: 0.6348
Epoch 3/10, Batch 370/442, Training Loss: 0.7614
Epoch 3/10, Batch 371/442, Training Loss: 0.6754
Epoch 3/10, Batch 372/442, Training Loss: 0.7251
Epoch 3/10, Batch 373/442, Training Loss: 0.7375
Epoch 3/10, Batch 374/442, Training Loss: 0.8284
Epoch 3/10, Batch 375/442, Training Loss: 0.4556
Epoch 3/10, Batch 376/442, Training Loss: 0.8521
Epoch 3/10, Batch 377/442, Training Loss: 0.8635
Epoch 3/10, Batch 378/442, Training Loss: 0.5924
Epoch 3/10, Batch 379/442, Training Loss: 0.6503
Epoch 3/10, Batch 380/442, Training Loss: 0.8422
Epoch 3/10, Batch 381/442, Training Loss: 0.7324
Epoch 3/10, Batch 382/442, Training Loss: 0.6448
Epoch 3/10, Batch 383/442, Training Loss: 0.7111
Epoch 3/10, Batch 384/442, Training Loss: 0.6775
Epoch 3/10, Batch 385/442, Training Loss: 0.7500
Epoch 3/10, Batch 386/442, Training Loss: 0.6073
Epoch 3/10, Batch 387/442, Training Loss: 0.6432
Epoch 3/10, Batch 388/442, Training Loss: 0.8330
Epoch 3/10, Batch 389/442, Training Loss: 0.7669
Epoch 3/10, Batch 390/442, Training Loss: 0.7137
Epoch 3/10, Batch 391/442, Training Loss: 0.5712
Epoch 3/10, Batch 392/442, Training Loss: 0.7529
Epoch 3/10, Batch 393/442, Training Loss: 0.8317
Epoch 3/10, Batch 394/442, Training Loss: 0.6186
Epoch 3/10, Batch 395/442, Training Loss: 0.6726
Epoch 3/10, Batch 396/442, Training Loss: 0.8455
Epoch 3/10, Batch 397/442, Training Loss: 0.5421
Epoch 3/10, Batch 398/442, Training Loss: 0.6396
Epoch 3/10, Batch 399/442, Training Loss: 0.8170
Epoch 3/10, Batch 400/442, Training Loss: 0.8473
Epoch 3/10, Batch 401/442, Training Loss: 0.7285
Epoch 3/10, Batch 402/442, Training Loss: 0.7434
Epoch 3/10, Batch 403/442, Training Loss: 0.7444
Epoch 3/10, Batch 404/442, Training Loss: 0.8602
Epoch 3/10, Batch 405/442, Training Loss: 0.5719
Epoch 3/10, Batch 406/442, Training Loss: 0.7391
Epoch 3/10, Batch 407/442, Training Loss: 0.6653
Epoch 3/10, Batch 408/442, Training Loss: 0.7439
Epoch 3/10, Batch 409/442, Training Loss: 0.5487
Epoch 3/10, Batch 410/442, Training Loss: 0.7073
Epoch 3/10, Batch 411/442, Training Loss: 0.5290
Epoch 3/10, Batch 412/442, Training Loss: 0.7185
Epoch 3/10, Batch 413/442, Training Loss: 0.7738
Epoch 3/10, Batch 414/442, Training Loss: 0.8158
Epoch 3/10, Batch 415/442, Training Loss: 0.7975
Epoch 3/10, Batch 416/442, Training Loss: 0.6982
Epoch 3/10, Batch 417/442, Training Loss: 0.7806
Epoch 3/10, Batch 418/442, Training Loss: 0.7323
Epoch 3/10, Batch 419/442, Training Loss: 0.7438
Epoch 3/10, Batch 420/442, Training Loss: 0.7935
Epoch 3/10, Batch 421/442, Training Loss: 0.7709
Epoch 3/10, Batch 422/442, Training Loss: 0.6928
Epoch 3/10, Batch 423/442, Training Loss: 1.1493
Epoch 3/10, Batch 424/442, Training Loss: 1.0323
Epoch 3/10, Batch 425/442, Training Loss: 0.6644
Epoch 3/10, Batch 426/442, Training Loss: 0.6997
Epoch 3/10, Batch 427/442, Training Loss: 0.6833
Epoch 3/10, Batch 428/442, Training Loss: 0.5396
Epoch 3/10, Batch 429/442, Training Loss: 0.7106
Epoch 3/10, Batch 430/442, Training Loss: 0.5772
Epoch 3/10, Batch 431/442, Training Loss: 0.7249
Epoch 3/10, Batch 432/442, Training Loss: 0.8079
Epoch 3/10, Batch 433/442, Training Loss: 0.7842
Epoch 3/10, Batch 434/442, Training Loss: 0.8383
Epoch 3/10, Batch 435/442, Training Loss: 0.7279
Epoch 3/10, Batch 436/442, Training Loss: 0.6207
Epoch 3/10, Batch 437/442, Training Loss: 0.5485
Epoch 3/10, Batch 438/442, Training Loss: 0.6911
Epoch 3/10, Batch 439/442, Training Loss: 0.7318
Epoch 3/10, Batch 440/442, Training Loss: 0.8606
Epoch 3/10, Batch 441/442, Training Loss: 0.9629
Epoch 3/10, Batch 442/442, Training Loss: 1.0131
Epoch 3/10, Training Loss: 0.7353, Validation Loss: 0.7573, Validation Accuracy: 0.6447
Epoch 4/10, Batch 1/442, Training Loss: 0.4356
Epoch 4/10, Batch 2/442, Training Loss: 0.9002
Epoch 4/10, Batch 3/442, Training Loss: 0.6130
Epoch 4/10, Batch 4/442, Training Loss: 0.8967
Epoch 4/10, Batch 5/442, Training Loss: 0.6382
Epoch 4/10, Batch 6/442, Training Loss: 0.8418
Epoch 4/10, Batch 7/442, Training Loss: 0.7666
Epoch 4/10, Batch 8/442, Training Loss: 0.7183
Epoch 4/10, Batch 9/442, Training Loss: 0.7469
Epoch 4/10, Batch 10/442, Training Loss: 0.8393
Epoch 4/10, Batch 11/442, Training Loss: 0.6679
Epoch 4/10, Batch 12/442, Training Loss: 0.5354
Epoch 4/10, Batch 13/442, Training Loss: 0.6791
Epoch 4/10, Batch 14/442, Training Loss: 0.6659
Epoch 4/10, Batch 15/442, Training Loss: 0.5347
Epoch 4/10, Batch 16/442, Training Loss: 0.7083
Epoch 4/10, Batch 17/442, Training Loss: 0.7762
Epoch 4/10, Batch 18/442, Training Loss: 0.5838
Epoch 4/10, Batch 19/442, Training Loss: 0.6205
Epoch 4/10, Batch 20/442, Training Loss: 0.6693
Epoch 4/10, Batch 21/442, Training Loss: 0.7689
Epoch 4/10, Batch 22/442, Training Loss: 0.7315
Epoch 4/10, Batch 23/442, Training Loss: 0.9370
Epoch 4/10, Batch 24/442, Training Loss: 0.6679
Epoch 4/10, Batch 25/442, Training Loss: 0.9463
Epoch 4/10, Batch 26/442, Training Loss: 0.7334
Epoch 4/10, Batch 27/442, Training Loss: 0.6804
Epoch 4/10, Batch 28/442, Training Loss: 0.8357
Epoch 4/10, Batch 29/442, Training Loss: 0.6388
Epoch 4/10, Batch 30/442, Training Loss: 0.6237
Epoch 4/10, Batch 31/442, Training Loss: 0.8877
Epoch 4/10, Batch 32/442, Training Loss: 0.5586
Epoch 4/10, Batch 33/442, Training Loss: 0.6037
Epoch 4/10, Batch 34/442, Training Loss: 0.6828
Epoch 4/10, Batch 35/442, Training Loss: 0.7772
Epoch 4/10, Batch 36/442, Training Loss: 0.6686
Epoch 4/10, Batch 37/442, Training Loss: 0.5795
Epoch 4/10, Batch 38/442, Training Loss: 0.5940
Epoch 4/10, Batch 39/442, Training Loss: 0.7909
Epoch 4/10, Batch 40/442, Training Loss: 0.6382
Epoch 4/10, Batch 41/442, Training Loss: 0.7420
Epoch 4/10, Batch 42/442, Training Loss: 0.7695
Epoch 4/10, Batch 43/442, Training Loss: 1.0078
Epoch 4/10, Batch 44/442, Training Loss: 0.5074
Epoch 4/10, Batch 45/442, Training Loss: 0.6596
Epoch 4/10, Batch 46/442, Training Loss: 0.9707
Epoch 4/10, Batch 47/442, Training Loss: 0.6318
Epoch 4/10, Batch 48/442, Training Loss: 0.9870
Epoch 4/10, Batch 49/442, Training Loss: 0.8221
Epoch 4/10, Batch 50/442, Training Loss: 0.7362
Epoch 4/10, Batch 51/442, Training Loss: 0.7077
Epoch 4/10, Batch 52/442, Training Loss: 0.8810
Epoch 4/10, Batch 53/442, Training Loss: 0.6748
Epoch 4/10, Batch 54/442, Training Loss: 0.7590
Epoch 4/10, Batch 55/442, Training Loss: 0.6308
Epoch 4/10, Batch 56/442, Training Loss: 0.8616
Epoch 4/10, Batch 57/442, Training Loss: 0.7273
Epoch 4/10, Batch 58/442, Training Loss: 0.7638
Epoch 4/10, Batch 59/442, Training Loss: 0.5813
Epoch 4/10, Batch 60/442, Training Loss: 0.6793
Epoch 4/10, Batch 61/442, Training Loss: 0.9557
Epoch 4/10, Batch 62/442, Training Loss: 0.6826
Epoch 4/10, Batch 63/442, Training Loss: 0.4609
Epoch 4/10, Batch 64/442, Training Loss: 0.5850
Epoch 4/10, Batch 65/442, Training Loss: 0.5764
Epoch 4/10, Batch 66/442, Training Loss: 0.5503
Epoch 4/10, Batch 67/442, Training Loss: 0.6657
Epoch 4/10, Batch 68/442, Training Loss: 0.9678
Epoch 4/10, Batch 69/442, Training Loss: 0.5067
Epoch 4/10, Batch 70/442, Training Loss: 0.6949
Epoch 4/10, Batch 71/442, Training Loss: 0.8099
Epoch 4/10, Batch 72/442, Training Loss: 0.6022
Epoch 4/10, Batch 73/442, Training Loss: 0.7190
Epoch 4/10, Batch 74/442, Training Loss: 0.8216
Epoch 4/10, Batch 75/442, Training Loss: 0.9130
Epoch 4/10, Batch 76/442, Training Loss: 0.6784
Epoch 4/10, Batch 77/442, Training Loss: 0.6725
Epoch 4/10, Batch 78/442, Training Loss: 0.6723
Epoch 4/10, Batch 79/442, Training Loss: 0.7504
Epoch 4/10, Batch 80/442, Training Loss: 0.7339
Epoch 4/10, Batch 81/442, Training Loss: 0.6662
Epoch 4/10, Batch 82/442, Training Loss: 0.9215
Epoch 4/10, Batch 83/442, Training Loss: 0.8332
Epoch 4/10, Batch 84/442, Training Loss: 0.7855
Epoch 4/10, Batch 85/442, Training Loss: 0.6693
Epoch 4/10, Batch 86/442, Training Loss: 0.7468
Epoch 4/10, Batch 87/442, Training Loss: 0.6914
Epoch 4/10, Batch 88/442, Training Loss: 0.9594
Epoch 4/10, Batch 89/442, Training Loss: 0.6682
Epoch 4/10, Batch 90/442, Training Loss: 0.5096
Epoch 4/10, Batch 91/442, Training Loss: 0.6048
Epoch 4/10, Batch 92/442, Training Loss: 0.8229
Epoch 4/10, Batch 93/442, Training Loss: 0.6405
Epoch 4/10, Batch 94/442, Training Loss: 0.6927
Epoch 4/10, Batch 95/442, Training Loss: 0.5809
Epoch 4/10, Batch 96/442, Training Loss: 0.6156
Epoch 4/10, Batch 97/442, Training Loss: 0.5384
Epoch 4/10, Batch 98/442, Training Loss: 0.9686
Epoch 4/10, Batch 99/442, Training Loss: 0.6162
Epoch 4/10, Batch 100/442, Training Loss: 1.1946
Epoch 4/10, Batch 101/442, Training Loss: 0.6030
Epoch 4/10, Batch 102/442, Training Loss: 0.7160
Epoch 4/10, Batch 103/442, Training Loss: 0.6588
Epoch 4/10, Batch 104/442, Training Loss: 0.6943
Epoch 4/10, Batch 105/442, Training Loss: 0.7801
Epoch 4/10, Batch 106/442, Training Loss: 0.7301
Epoch 4/10, Batch 107/442, Training Loss: 0.6093
Epoch 4/10, Batch 108/442, Training Loss: 0.5826
Epoch 4/10, Batch 109/442, Training Loss: 0.6134
Epoch 4/10, Batch 110/442, Training Loss: 0.5319
Epoch 4/10, Batch 111/442, Training Loss: 0.7067
Epoch 4/10, Batch 112/442, Training Loss: 0.5121
Epoch 4/10, Batch 113/442, Training Loss: 0.5558
Epoch 4/10, Batch 114/442, Training Loss: 0.7262
Epoch 4/10, Batch 115/442, Training Loss: 0.9366
Epoch 4/10, Batch 116/442, Training Loss: 0.6293
Epoch 4/10, Batch 117/442, Training Loss: 0.5073
Epoch 4/10, Batch 118/442, Training Loss: 0.4959
Epoch 4/10, Batch 119/442, Training Loss: 0.6452
Epoch 4/10, Batch 120/442, Training Loss: 0.5700
Epoch 4/10, Batch 121/442, Training Loss: 0.8120
Epoch 4/10, Batch 122/442, Training Loss: 1.1154
Epoch 4/10, Batch 123/442, Training Loss: 0.4822
Epoch 4/10, Batch 124/442, Training Loss: 0.8020
Epoch 4/10, Batch 125/442, Training Loss: 0.5933
Epoch 4/10, Batch 126/442, Training Loss: 0.8694
Epoch 4/10, Batch 127/442, Training Loss: 0.6556
Epoch 4/10, Batch 128/442, Training Loss: 0.6961
Epoch 4/10, Batch 129/442, Training Loss: 0.7049
Epoch 4/10, Batch 130/442, Training Loss: 0.6732
Epoch 4/10, Batch 131/442, Training Loss: 0.7328
Epoch 4/10, Batch 132/442, Training Loss: 0.8456
Epoch 4/10, Batch 133/442, Training Loss: 0.6034
Epoch 4/10, Batch 134/442, Training Loss: 0.6066
Epoch 4/10, Batch 135/442, Training Loss: 0.7988
Epoch 4/10, Batch 136/442, Training Loss: 0.7807
Epoch 4/10, Batch 137/442, Training Loss: 0.7364
Epoch 4/10, Batch 138/442, Training Loss: 0.7120
Epoch 4/10, Batch 139/442, Training Loss: 0.6868
Epoch 4/10, Batch 140/442, Training Loss: 0.7025
Epoch 4/10, Batch 141/442, Training Loss: 1.2092
Epoch 4/10, Batch 142/442, Training Loss: 0.7386
Epoch 4/10, Batch 143/442, Training Loss: 0.5818
Epoch 4/10, Batch 144/442, Training Loss: 0.8305
Epoch 4/10, Batch 145/442, Training Loss: 0.7635
Epoch 4/10, Batch 146/442, Training Loss: 0.6839
Epoch 4/10, Batch 147/442, Training Loss: 0.6514
Epoch 4/10, Batch 148/442, Training Loss: 0.7123
Epoch 4/10, Batch 149/442, Training Loss: 0.5599
Epoch 4/10, Batch 150/442, Training Loss: 0.8329
Epoch 4/10, Batch 151/442, Training Loss: 0.6037
Epoch 4/10, Batch 152/442, Training Loss: 0.7382
Epoch 4/10, Batch 153/442, Training Loss: 0.6138
Epoch 4/10, Batch 154/442, Training Loss: 0.5958
Epoch 4/10, Batch 155/442, Training Loss: 0.5538
Epoch 4/10, Batch 156/442, Training Loss: 0.8221
Epoch 4/10, Batch 157/442, Training Loss: 0.7321
Epoch 4/10, Batch 158/442, Training Loss: 0.7596
Epoch 4/10, Batch 159/442, Training Loss: 0.7392
Epoch 4/10, Batch 160/442, Training Loss: 0.7440
Epoch 4/10, Batch 161/442, Training Loss: 0.6790
Epoch 4/10, Batch 162/442, Training Loss: 0.6857
Epoch 4/10, Batch 163/442, Training Loss: 0.8069
Epoch 4/10, Batch 164/442, Training Loss: 0.7819
Epoch 4/10, Batch 165/442, Training Loss: 0.6653
Epoch 4/10, Batch 166/442, Training Loss: 0.9207
Epoch 4/10, Batch 167/442, Training Loss: 0.6806
Epoch 4/10, Batch 168/442, Training Loss: 0.5822
Epoch 4/10, Batch 169/442, Training Loss: 0.8048
Epoch 4/10, Batch 170/442, Training Loss: 0.4748
Epoch 4/10, Batch 171/442, Training Loss: 0.8103
Epoch 4/10, Batch 172/442, Training Loss: 0.6468
Epoch 4/10, Batch 173/442, Training Loss: 0.8240
Epoch 4/10, Batch 174/442, Training Loss: 0.5794
Epoch 4/10, Batch 175/442, Training Loss: 0.8076
Epoch 4/10, Batch 176/442, Training Loss: 0.7342
Epoch 4/10, Batch 177/442, Training Loss: 0.5597
Epoch 4/10, Batch 178/442, Training Loss: 0.5991
Epoch 4/10, Batch 179/442, Training Loss: 0.6720
Epoch 4/10, Batch 180/442, Training Loss: 0.6637
Epoch 4/10, Batch 181/442, Training Loss: 0.7794
Epoch 4/10, Batch 182/442, Training Loss: 0.6518
Epoch 4/10, Batch 183/442, Training Loss: 0.6706
Epoch 4/10, Batch 184/442, Training Loss: 0.6599
Epoch 4/10, Batch 185/442, Training Loss: 0.5681
Epoch 4/10, Batch 186/442, Training Loss: 0.8937
Epoch 4/10, Batch 187/442, Training Loss: 0.6592
Epoch 4/10, Batch 188/442, Training Loss: 0.6816
Epoch 4/10, Batch 189/442, Training Loss: 0.7286
Epoch 4/10, Batch 190/442, Training Loss: 0.7131
Epoch 4/10, Batch 191/442, Training Loss: 0.5904
Epoch 4/10, Batch 192/442, Training Loss: 0.6742
Epoch 4/10, Batch 193/442, Training Loss: 0.5485
Epoch 4/10, Batch 194/442, Training Loss: 0.7448
Epoch 4/10, Batch 195/442, Training Loss: 0.8190
Epoch 4/10, Batch 196/442, Training Loss: 0.6229
Epoch 4/10, Batch 197/442, Training Loss: 0.5829
Epoch 4/10, Batch 198/442, Training Loss: 0.5712
Epoch 4/10, Batch 199/442, Training Loss: 0.8548
Epoch 4/10, Batch 200/442, Training Loss: 0.6446
Epoch 4/10, Batch 201/442, Training Loss: 0.6757
Epoch 4/10, Batch 202/442, Training Loss: 0.5960
Epoch 4/10, Batch 203/442, Training Loss: 0.6131
Epoch 4/10, Batch 204/442, Training Loss: 0.7732
Epoch 4/10, Batch 205/442, Training Loss: 0.6742
Epoch 4/10, Batch 206/442, Training Loss: 0.6425
Epoch 4/10, Batch 207/442, Training Loss: 1.0813
Epoch 4/10, Batch 208/442, Training Loss: 0.6659
Epoch 4/10, Batch 209/442, Training Loss: 0.6513
Epoch 4/10, Batch 210/442, Training Loss: 0.7051
Epoch 4/10, Batch 211/442, Training Loss: 0.6460
Epoch 4/10, Batch 212/442, Training Loss: 0.5176
Epoch 4/10, Batch 213/442, Training Loss: 0.9595
Epoch 4/10, Batch 214/442, Training Loss: 0.7294
Epoch 4/10, Batch 215/442, Training Loss: 0.5762
Epoch 4/10, Batch 216/442, Training Loss: 0.5963
Epoch 4/10, Batch 217/442, Training Loss: 0.8482
Epoch 4/10, Batch 218/442, Training Loss: 0.7046
Epoch 4/10, Batch 219/442, Training Loss: 0.5821
Epoch 4/10, Batch 220/442, Training Loss: 0.5828
Epoch 4/10, Batch 221/442, Training Loss: 0.5841
Epoch 4/10, Batch 222/442, Training Loss: 0.8939
Epoch 4/10, Batch 223/442, Training Loss: 0.6145
Epoch 4/10, Batch 224/442, Training Loss: 0.4189
Epoch 4/10, Batch 225/442, Training Loss: 0.9395
Epoch 4/10, Batch 226/442, Training Loss: 0.7207
Epoch 4/10, Batch 227/442, Training Loss: 0.6261
Epoch 4/10, Batch 228/442, Training Loss: 1.0175
Epoch 4/10, Batch 229/442, Training Loss: 0.7198
Epoch 4/10, Batch 230/442, Training Loss: 0.5829
Epoch 4/10, Batch 231/442, Training Loss: 0.7841
Epoch 4/10, Batch 232/442, Training Loss: 0.6699
Epoch 4/10, Batch 233/442, Training Loss: 0.6758
Epoch 4/10, Batch 234/442, Training Loss: 0.5280
Epoch 4/10, Batch 235/442, Training Loss: 0.7058
Epoch 4/10, Batch 236/442, Training Loss: 0.5375
Epoch 4/10, Batch 237/442, Training Loss: 0.6114
Epoch 4/10, Batch 238/442, Training Loss: 0.6803
Epoch 4/10, Batch 239/442, Training Loss: 0.6825
Epoch 4/10, Batch 240/442, Training Loss: 0.5644
Epoch 4/10, Batch 241/442, Training Loss: 0.7427
Epoch 4/10, Batch 242/442, Training Loss: 0.8121
Epoch 4/10, Batch 243/442, Training Loss: 0.6310
Epoch 4/10, Batch 244/442, Training Loss: 0.6503
Epoch 4/10, Batch 245/442, Training Loss: 0.5097
Epoch 4/10, Batch 246/442, Training Loss: 0.5953
Epoch 4/10, Batch 247/442, Training Loss: 0.9551
Epoch 4/10, Batch 248/442, Training Loss: 0.7686
Epoch 4/10, Batch 249/442, Training Loss: 0.5192
Epoch 4/10, Batch 250/442, Training Loss: 0.5211
Epoch 4/10, Batch 251/442, Training Loss: 0.6101
Epoch 4/10, Batch 252/442, Training Loss: 0.6463
Epoch 4/10, Batch 253/442, Training Loss: 0.7401
Epoch 4/10, Batch 254/442, Training Loss: 0.9636
Epoch 4/10, Batch 255/442, Training Loss: 0.7486
Epoch 4/10, Batch 256/442, Training Loss: 0.6508
Epoch 4/10, Batch 257/442, Training Loss: 0.7914
Epoch 4/10, Batch 258/442, Training Loss: 0.7313
Epoch 4/10, Batch 259/442, Training Loss: 0.5829
Epoch 4/10, Batch 260/442, Training Loss: 0.6910
Epoch 4/10, Batch 261/442, Training Loss: 0.7168
Epoch 4/10, Batch 262/442, Training Loss: 0.5710
Epoch 4/10, Batch 263/442, Training Loss: 0.7117
Epoch 4/10, Batch 264/442, Training Loss: 0.8223
Epoch 4/10, Batch 265/442, Training Loss: 0.5401
Epoch 4/10, Batch 266/442, Training Loss: 0.5998
Epoch 4/10, Batch 267/442, Training Loss: 0.7506
Epoch 4/10, Batch 268/442, Training Loss: 0.6520
Epoch 4/10, Batch 269/442, Training Loss: 0.7186
Epoch 4/10, Batch 270/442, Training Loss: 0.5895
Epoch 4/10, Batch 271/442, Training Loss: 0.7983
Epoch 4/10, Batch 272/442, Training Loss: 0.7590
Epoch 4/10, Batch 273/442, Training Loss: 0.6867
Epoch 4/10, Batch 274/442, Training Loss: 0.6955
Epoch 4/10, Batch 275/442, Training Loss: 0.7547
Epoch 4/10, Batch 276/442, Training Loss: 0.6449
Epoch 4/10, Batch 277/442, Training Loss: 0.7399
Epoch 4/10, Batch 278/442, Training Loss: 0.8155
Epoch 4/10, Batch 279/442, Training Loss: 0.5748
Epoch 4/10, Batch 280/442, Training Loss: 0.6687
Epoch 4/10, Batch 281/442, Training Loss: 0.7440
Epoch 4/10, Batch 282/442, Training Loss: 0.6873
Epoch 4/10, Batch 283/442, Training Loss: 0.6839
Epoch 4/10, Batch 284/442, Training Loss: 0.6708
Epoch 4/10, Batch 285/442, Training Loss: 0.9503
Epoch 4/10, Batch 286/442, Training Loss: 0.8266
Epoch 4/10, Batch 287/442, Training Loss: 0.6087
Epoch 4/10, Batch 288/442, Training Loss: 0.5948
Epoch 4/10, Batch 289/442, Training Loss: 0.6964
Epoch 4/10, Batch 290/442, Training Loss: 0.6146
Epoch 4/10, Batch 291/442, Training Loss: 0.6252
Epoch 4/10, Batch 292/442, Training Loss: 0.6319
Epoch 4/10, Batch 293/442, Training Loss: 0.6141
Epoch 4/10, Batch 294/442, Training Loss: 0.5574
Epoch 4/10, Batch 295/442, Training Loss: 0.6705
Epoch 4/10, Batch 296/442, Training Loss: 0.6396
Epoch 4/10, Batch 297/442, Training Loss: 0.6212
Epoch 4/10, Batch 298/442, Training Loss: 0.6834
Epoch 4/10, Batch 299/442, Training Loss: 0.5916
Epoch 4/10, Batch 300/442, Training Loss: 0.8654
Epoch 4/10, Batch 301/442, Training Loss: 0.5654
Epoch 4/10, Batch 302/442, Training Loss: 0.5945
Epoch 4/10, Batch 303/442, Training Loss: 0.6954
Epoch 4/10, Batch 304/442, Training Loss: 0.6946
Epoch 4/10, Batch 305/442, Training Loss: 0.6826
Epoch 4/10, Batch 306/442, Training Loss: 0.9691
Epoch 4/10, Batch 307/442, Training Loss: 0.5783
Epoch 4/10, Batch 308/442, Training Loss: 0.7201
Epoch 4/10, Batch 309/442, Training Loss: 0.8251
Epoch 4/10, Batch 310/442, Training Loss: 0.7886
Epoch 4/10, Batch 311/442, Training Loss: 0.6643
Epoch 4/10, Batch 312/442, Training Loss: 0.5238
Epoch 4/10, Batch 313/442, Training Loss: 0.5417
Epoch 4/10, Batch 314/442, Training Loss: 0.5864
Epoch 4/10, Batch 315/442, Training Loss: 0.7520
Epoch 4/10, Batch 316/442, Training Loss: 0.7256
Epoch 4/10, Batch 317/442, Training Loss: 0.6001
Epoch 4/10, Batch 318/442, Training Loss: 0.6373
Epoch 4/10, Batch 319/442, Training Loss: 0.5707
Epoch 4/10, Batch 320/442, Training Loss: 0.6141
Epoch 4/10, Batch 321/442, Training Loss: 0.4175
Epoch 4/10, Batch 322/442, Training Loss: 0.6357
Epoch 4/10, Batch 323/442, Training Loss: 0.5815
Epoch 4/10, Batch 324/442, Training Loss: 0.5575
Epoch 4/10, Batch 325/442, Training Loss: 0.7839
Epoch 4/10, Batch 326/442, Training Loss: 0.7374
Epoch 4/10, Batch 327/442, Training Loss: 0.5411
Epoch 4/10, Batch 328/442, Training Loss: 0.5483
Epoch 4/10, Batch 329/442, Training Loss: 0.7858
Epoch 4/10, Batch 330/442, Training Loss: 1.0626
Epoch 4/10, Batch 331/442, Training Loss: 0.6969
Epoch 4/10, Batch 332/442, Training Loss: 0.5921
Epoch 4/10, Batch 333/442, Training Loss: 0.7901
Epoch 4/10, Batch 334/442, Training Loss: 0.6998
Epoch 4/10, Batch 335/442, Training Loss: 0.7621
Epoch 4/10, Batch 336/442, Training Loss: 0.6480
Epoch 4/10, Batch 337/442, Training Loss: 0.6940
Epoch 4/10, Batch 338/442, Training Loss: 0.7087
Epoch 4/10, Batch 339/442, Training Loss: 0.6533
Epoch 4/10, Batch 340/442, Training Loss: 0.5966
Epoch 4/10, Batch 341/442, Training Loss: 0.8261
Epoch 4/10, Batch 342/442, Training Loss: 0.6429
Epoch 4/10, Batch 343/442, Training Loss: 0.6014
Epoch 4/10, Batch 344/442, Training Loss: 0.6796
Epoch 4/10, Batch 345/442, Training Loss: 0.7772
Epoch 4/10, Batch 346/442, Training Loss: 0.7214
Epoch 4/10, Batch 347/442, Training Loss: 0.7961
Epoch 4/10, Batch 348/442, Training Loss: 0.4985
Epoch 4/10, Batch 349/442, Training Loss: 0.8013
Epoch 4/10, Batch 350/442, Training Loss: 0.6972
Epoch 4/10, Batch 351/442, Training Loss: 0.7704
Epoch 4/10, Batch 352/442, Training Loss: 0.5998
Epoch 4/10, Batch 353/442, Training Loss: 0.5501
Epoch 4/10, Batch 354/442, Training Loss: 0.5982
Epoch 4/10, Batch 355/442, Training Loss: 0.6393
Epoch 4/10, Batch 356/442, Training Loss: 0.5585
Epoch 4/10, Batch 357/442, Training Loss: 0.9020
Epoch 4/10, Batch 358/442, Training Loss: 0.7860
Epoch 4/10, Batch 359/442, Training Loss: 0.6727
Epoch 4/10, Batch 360/442, Training Loss: 0.5531
Epoch 4/10, Batch 361/442, Training Loss: 0.5678
Epoch 4/10, Batch 362/442, Training Loss: 0.8065
Epoch 4/10, Batch 363/442, Training Loss: 0.5839
Epoch 4/10, Batch 364/442, Training Loss: 0.6627
Epoch 4/10, Batch 365/442, Training Loss: 0.5227
Epoch 4/10, Batch 366/442, Training Loss: 0.8229
Epoch 4/10, Batch 367/442, Training Loss: 0.7436
Epoch 4/10, Batch 368/442, Training Loss: 0.5987
Epoch 4/10, Batch 369/442, Training Loss: 0.6848
Epoch 4/10, Batch 370/442, Training Loss: 0.7751
Epoch 4/10, Batch 371/442, Training Loss: 0.7741
Epoch 4/10, Batch 372/442, Training Loss: 0.7997
Epoch 4/10, Batch 373/442, Training Loss: 0.5800
Epoch 4/10, Batch 374/442, Training Loss: 0.7213
Epoch 4/10, Batch 375/442, Training Loss: 0.5825
Epoch 4/10, Batch 376/442, Training Loss: 0.5694
Epoch 4/10, Batch 377/442, Training Loss: 0.6951
Epoch 4/10, Batch 378/442, Training Loss: 0.6833
Epoch 4/10, Batch 379/442, Training Loss: 0.5155
Epoch 4/10, Batch 380/442, Training Loss: 0.8879
Epoch 4/10, Batch 381/442, Training Loss: 0.5838
Epoch 4/10, Batch 382/442, Training Loss: 0.6681
Epoch 4/10, Batch 383/442, Training Loss: 0.8591
Epoch 4/10, Batch 384/442, Training Loss: 0.5994
Epoch 4/10, Batch 385/442, Training Loss: 0.8142
Epoch 4/10, Batch 386/442, Training Loss: 0.5674
Epoch 4/10, Batch 387/442, Training Loss: 0.5278
Epoch 4/10, Batch 388/442, Training Loss: 0.5868
Epoch 4/10, Batch 389/442, Training Loss: 0.6924
Epoch 4/10, Batch 390/442, Training Loss: 0.6034
Epoch 4/10, Batch 391/442, Training Loss: 0.6191
Epoch 4/10, Batch 392/442, Training Loss: 0.5932
Epoch 4/10, Batch 393/442, Training Loss: 0.6948
Epoch 4/10, Batch 394/442, Training Loss: 0.7386
Epoch 4/10, Batch 395/442, Training Loss: 0.6976
Epoch 4/10, Batch 396/442, Training Loss: 0.6677
Epoch 4/10, Batch 397/442, Training Loss: 0.5994
Epoch 4/10, Batch 398/442, Training Loss: 0.6482
Epoch 4/10, Batch 399/442, Training Loss: 0.6840
Epoch 4/10, Batch 400/442, Training Loss: 0.5928
Epoch 4/10, Batch 401/442, Training Loss: 0.5693
Epoch 4/10, Batch 402/442, Training Loss: 0.5585
Epoch 4/10, Batch 403/442, Training Loss: 0.4995
Epoch 4/10, Batch 404/442, Training Loss: 0.7727
Epoch 4/10, Batch 405/442, Training Loss: 0.6626
Epoch 4/10, Batch 406/442, Training Loss: 0.6542
Epoch 4/10, Batch 407/442, Training Loss: 0.5887
Epoch 4/10, Batch 408/442, Training Loss: 0.6146
Epoch 4/10, Batch 409/442, Training Loss: 0.6111
Epoch 4/10, Batch 410/442, Training Loss: 0.6087
Epoch 4/10, Batch 411/442, Training Loss: 0.6325
Epoch 4/10, Batch 412/442, Training Loss: 0.5167
Epoch 4/10, Batch 413/442, Training Loss: 0.5588
Epoch 4/10, Batch 414/442, Training Loss: 0.7271
Epoch 4/10, Batch 415/442, Training Loss: 0.5150
Epoch 4/10, Batch 416/442, Training Loss: 0.4314
Epoch 4/10, Batch 417/442, Training Loss: 0.6878
Epoch 4/10, Batch 418/442, Training Loss: 0.7191
Epoch 4/10, Batch 419/442, Training Loss: 0.5706
Epoch 4/10, Batch 420/442, Training Loss: 0.6599
Epoch 4/10, Batch 421/442, Training Loss: 1.0421
Epoch 4/10, Batch 422/442, Training Loss: 0.7097
Epoch 4/10, Batch 423/442, Training Loss: 0.8607
Epoch 4/10, Batch 424/442, Training Loss: 0.8437
Epoch 4/10, Batch 425/442, Training Loss: 0.7433
Epoch 4/10, Batch 426/442, Training Loss: 0.6845
Epoch 4/10, Batch 427/442, Training Loss: 0.6375
Epoch 4/10, Batch 428/442, Training Loss: 0.6048
Epoch 4/10, Batch 429/442, Training Loss: 0.5892
Epoch 4/10, Batch 430/442, Training Loss: 0.7857
Epoch 4/10, Batch 431/442, Training Loss: 0.6341
Epoch 4/10, Batch 432/442, Training Loss: 0.6475
Epoch 4/10, Batch 433/442, Training Loss: 0.6078
Epoch 4/10, Batch 434/442, Training Loss: 0.5990
Epoch 4/10, Batch 435/442, Training Loss: 0.8045
Epoch 4/10, Batch 436/442, Training Loss: 0.6384
Epoch 4/10, Batch 437/442, Training Loss: 0.5419
Epoch 4/10, Batch 438/442, Training Loss: 0.7314
Epoch 4/10, Batch 439/442, Training Loss: 0.4718
Epoch 4/10, Batch 440/442, Training Loss: 0.4057
Epoch 4/10, Batch 441/442, Training Loss: 0.5688
Epoch 4/10, Batch 442/442, Training Loss: 0.6331
Epoch 4/10, Training Loss: 0.6883, Validation Loss: 0.7196, Validation Accuracy: 0.6596
Epoch 5/10, Batch 1/442, Training Loss: 0.7063
Epoch 5/10, Batch 2/442, Training Loss: 0.7076
Epoch 5/10, Batch 3/442, Training Loss: 1.0392
Epoch 5/10, Batch 4/442, Training Loss: 0.6561
Epoch 5/10, Batch 5/442, Training Loss: 0.7248
Epoch 5/10, Batch 6/442, Training Loss: 0.6067
Epoch 5/10, Batch 7/442, Training Loss: 0.8377
Epoch 5/10, Batch 8/442, Training Loss: 0.6490
Epoch 5/10, Batch 9/442, Training Loss: 0.6755
Epoch 5/10, Batch 10/442, Training Loss: 0.6928
Epoch 5/10, Batch 11/442, Training Loss: 0.5296
Epoch 5/10, Batch 12/442, Training Loss: 0.6305
Epoch 5/10, Batch 13/442, Training Loss: 0.5074
Epoch 5/10, Batch 14/442, Training Loss: 0.5712
Epoch 5/10, Batch 15/442, Training Loss: 0.7052
Epoch 5/10, Batch 16/442, Training Loss: 0.5793
Epoch 5/10, Batch 17/442, Training Loss: 0.6201
Epoch 5/10, Batch 18/442, Training Loss: 0.6191
Epoch 5/10, Batch 19/442, Training Loss: 0.6233
Epoch 5/10, Batch 20/442, Training Loss: 0.5739
Epoch 5/10, Batch 21/442, Training Loss: 0.7316
Epoch 5/10, Batch 22/442, Training Loss: 0.6129
Epoch 5/10, Batch 23/442, Training Loss: 0.4978
Epoch 5/10, Batch 24/442, Training Loss: 0.7726
Epoch 5/10, Batch 25/442, Training Loss: 0.9008
Epoch 5/10, Batch 26/442, Training Loss: 0.7420
Epoch 5/10, Batch 27/442, Training Loss: 0.7238
Epoch 5/10, Batch 28/442, Training Loss: 0.7049
Epoch 5/10, Batch 29/442, Training Loss: 0.6910
Epoch 5/10, Batch 30/442, Training Loss: 0.4836
Epoch 5/10, Batch 31/442, Training Loss: 0.5983
Epoch 5/10, Batch 32/442, Training Loss: 0.4588
Epoch 5/10, Batch 33/442, Training Loss: 0.5978
Epoch 5/10, Batch 34/442, Training Loss: 0.9106
Epoch 5/10, Batch 35/442, Training Loss: 0.5866
Epoch 5/10, Batch 36/442, Training Loss: 0.5414
Epoch 5/10, Batch 37/442, Training Loss: 0.7490
Epoch 5/10, Batch 38/442, Training Loss: 0.9882
Epoch 5/10, Batch 39/442, Training Loss: 0.4759
Epoch 5/10, Batch 40/442, Training Loss: 0.3681
Epoch 5/10, Batch 41/442, Training Loss: 0.5319
Epoch 5/10, Batch 42/442, Training Loss: 0.4885
Epoch 5/10, Batch 43/442, Training Loss: 0.6099
Epoch 5/10, Batch 44/442, Training Loss: 0.7167
Epoch 5/10, Batch 45/442, Training Loss: 0.6539
Epoch 5/10, Batch 46/442, Training Loss: 0.5102
Epoch 5/10, Batch 47/442, Training Loss: 0.6011
Epoch 5/10, Batch 48/442, Training Loss: 0.5910
Epoch 5/10, Batch 49/442, Training Loss: 0.5814
Epoch 5/10, Batch 50/442, Training Loss: 0.6961
Epoch 5/10, Batch 51/442, Training Loss: 0.7413
Epoch 5/10, Batch 52/442, Training Loss: 0.7400
Epoch 5/10, Batch 53/442, Training Loss: 0.6901
Epoch 5/10, Batch 54/442, Training Loss: 0.5583
Epoch 5/10, Batch 55/442, Training Loss: 0.7800
Epoch 5/10, Batch 56/442, Training Loss: 0.5762
Epoch 5/10, Batch 57/442, Training Loss: 0.6465
Epoch 5/10, Batch 58/442, Training Loss: 0.5298
Epoch 5/10, Batch 59/442, Training Loss: 0.5336
Epoch 5/10, Batch 60/442, Training Loss: 0.5248
Epoch 5/10, Batch 61/442, Training Loss: 0.5557
Epoch 5/10, Batch 62/442, Training Loss: 0.5731
Epoch 5/10, Batch 63/442, Training Loss: 0.6507
Epoch 5/10, Batch 64/442, Training Loss: 0.6139
Epoch 5/10, Batch 65/442, Training Loss: 0.6348
Epoch 5/10, Batch 66/442, Training Loss: 0.6236
Epoch 5/10, Batch 67/442, Training Loss: 0.5747
Epoch 5/10, Batch 68/442, Training Loss: 0.4891
Epoch 5/10, Batch 69/442, Training Loss: 0.6233
Epoch 5/10, Batch 70/442, Training Loss: 0.6713
Epoch 5/10, Batch 71/442, Training Loss: 0.5064
Epoch 5/10, Batch 72/442, Training Loss: 0.6218
Epoch 5/10, Batch 73/442, Training Loss: 0.6824
Epoch 5/10, Batch 74/442, Training Loss: 0.6132
Epoch 5/10, Batch 75/442, Training Loss: 0.8095
Epoch 5/10, Batch 76/442, Training Loss: 0.6306
Epoch 5/10, Batch 77/442, Training Loss: 0.6914
Epoch 5/10, Batch 78/442, Training Loss: 0.8991
Epoch 5/10, Batch 79/442, Training Loss: 0.4455
Epoch 5/10, Batch 80/442, Training Loss: 0.6308
Epoch 5/10, Batch 81/442, Training Loss: 0.6476
Epoch 5/10, Batch 82/442, Training Loss: 0.5882
Epoch 5/10, Batch 83/442, Training Loss: 0.6515
Epoch 5/10, Batch 84/442, Training Loss: 0.4625
Epoch 5/10, Batch 85/442, Training Loss: 0.5503
Epoch 5/10, Batch 86/442, Training Loss: 0.6348
Epoch 5/10, Batch 87/442, Training Loss: 0.4615
Epoch 5/10, Batch 88/442, Training Loss: 0.6753
Epoch 5/10, Batch 89/442, Training Loss: 0.7849
Epoch 5/10, Batch 90/442, Training Loss: 0.7888
Epoch 5/10, Batch 91/442, Training Loss: 0.5402
Epoch 5/10, Batch 92/442, Training Loss: 0.6925
Epoch 5/10, Batch 93/442, Training Loss: 0.9460
Epoch 5/10, Batch 94/442, Training Loss: 1.1690
Epoch 5/10, Batch 95/442, Training Loss: 0.4892
Epoch 5/10, Batch 96/442, Training Loss: 0.7269
Epoch 5/10, Batch 97/442, Training Loss: 0.4637
Epoch 5/10, Batch 98/442, Training Loss: 0.5365
Epoch 5/10, Batch 99/442, Training Loss: 0.5828
Epoch 5/10, Batch 100/442, Training Loss: 0.5806
Epoch 5/10, Batch 101/442, Training Loss: 0.4628
Epoch 5/10, Batch 102/442, Training Loss: 0.6234
Epoch 5/10, Batch 103/442, Training Loss: 0.6007
Epoch 5/10, Batch 104/442, Training Loss: 0.7378
Epoch 5/10, Batch 105/442, Training Loss: 0.6924
Epoch 5/10, Batch 106/442, Training Loss: 0.9728
Epoch 5/10, Batch 107/442, Training Loss: 0.6081
Epoch 5/10, Batch 108/442, Training Loss: 0.5861
Epoch 5/10, Batch 109/442, Training Loss: 0.5452
Epoch 5/10, Batch 110/442, Training Loss: 0.7641
Epoch 5/10, Batch 111/442, Training Loss: 0.6268
Epoch 5/10, Batch 112/442, Training Loss: 0.6992
Epoch 5/10, Batch 113/442, Training Loss: 0.6914
Epoch 5/10, Batch 114/442, Training Loss: 0.5277
Epoch 5/10, Batch 115/442, Training Loss: 0.6961
Epoch 5/10, Batch 116/442, Training Loss: 0.6827
Epoch 5/10, Batch 117/442, Training Loss: 0.6658
Epoch 5/10, Batch 118/442, Training Loss: 0.5939
Epoch 5/10, Batch 119/442, Training Loss: 0.4775
Epoch 5/10, Batch 120/442, Training Loss: 0.5936
Epoch 5/10, Batch 121/442, Training Loss: 0.8646
Epoch 5/10, Batch 122/442, Training Loss: 0.5770
Epoch 5/10, Batch 123/442, Training Loss: 0.7862
Epoch 5/10, Batch 124/442, Training Loss: 0.7284
Epoch 5/10, Batch 125/442, Training Loss: 0.4866
Epoch 5/10, Batch 126/442, Training Loss: 0.7163
Epoch 5/10, Batch 127/442, Training Loss: 0.7556
Epoch 5/10, Batch 128/442, Training Loss: 0.6881
Epoch 5/10, Batch 129/442, Training Loss: 0.6617
Epoch 5/10, Batch 130/442, Training Loss: 0.5928
Epoch 5/10, Batch 131/442, Training Loss: 0.6651
Epoch 5/10, Batch 132/442, Training Loss: 0.5355
Epoch 5/10, Batch 133/442, Training Loss: 0.6021
Epoch 5/10, Batch 134/442, Training Loss: 0.5712
Epoch 5/10, Batch 135/442, Training Loss: 0.4699
Epoch 5/10, Batch 136/442, Training Loss: 0.7387
Epoch 5/10, Batch 137/442, Training Loss: 0.7933
Epoch 5/10, Batch 138/442, Training Loss: 0.6657
Epoch 5/10, Batch 139/442, Training Loss: 0.6119
Epoch 5/10, Batch 140/442, Training Loss: 0.5419
Epoch 5/10, Batch 141/442, Training Loss: 0.6004
Epoch 5/10, Batch 142/442, Training Loss: 0.4782
Epoch 5/10, Batch 143/442, Training Loss: 0.7160
Epoch 5/10, Batch 144/442, Training Loss: 0.8858
Epoch 5/10, Batch 145/442, Training Loss: 0.9923
Epoch 5/10, Batch 146/442, Training Loss: 0.8957
Epoch 5/10, Batch 147/442, Training Loss: 0.9226
Epoch 5/10, Batch 148/442, Training Loss: 0.5354
Epoch 5/10, Batch 149/442, Training Loss: 0.7784
Epoch 5/10, Batch 150/442, Training Loss: 0.5653
Epoch 5/10, Batch 151/442, Training Loss: 0.7015
Epoch 5/10, Batch 152/442, Training Loss: 0.5990
Epoch 5/10, Batch 153/442, Training Loss: 0.6092
Epoch 5/10, Batch 154/442, Training Loss: 0.7066
Epoch 5/10, Batch 155/442, Training Loss: 0.7039
Epoch 5/10, Batch 156/442, Training Loss: 0.6961
Epoch 5/10, Batch 157/442, Training Loss: 0.7465
Epoch 5/10, Batch 158/442, Training Loss: 0.5897
Epoch 5/10, Batch 159/442, Training Loss: 0.7712
Epoch 5/10, Batch 160/442, Training Loss: 0.6623
Epoch 5/10, Batch 161/442, Training Loss: 0.6608
Epoch 5/10, Batch 162/442, Training Loss: 0.7408
Epoch 5/10, Batch 163/442, Training Loss: 0.5999
Epoch 5/10, Batch 164/442, Training Loss: 0.5472
Epoch 5/10, Batch 165/442, Training Loss: 0.5687
Epoch 5/10, Batch 166/442, Training Loss: 0.6906
Epoch 5/10, Batch 167/442, Training Loss: 0.5902
Epoch 5/10, Batch 168/442, Training Loss: 0.6354
Epoch 5/10, Batch 169/442, Training Loss: 0.5417
Epoch 5/10, Batch 170/442, Training Loss: 0.6366
Epoch 5/10, Batch 171/442, Training Loss: 0.6657
Epoch 5/10, Batch 172/442, Training Loss: 0.4935
Epoch 5/10, Batch 173/442, Training Loss: 0.5241
Epoch 5/10, Batch 174/442, Training Loss: 0.6730
Epoch 5/10, Batch 175/442, Training Loss: 0.7508
Epoch 5/10, Batch 176/442, Training Loss: 0.6237
Epoch 5/10, Batch 177/442, Training Loss: 0.6346
Epoch 5/10, Batch 178/442, Training Loss: 0.4361
Epoch 5/10, Batch 179/442, Training Loss: 0.5865
Epoch 5/10, Batch 180/442, Training Loss: 0.6188
Epoch 5/10, Batch 181/442, Training Loss: 0.4911
Epoch 5/10, Batch 182/442, Training Loss: 0.6116
Epoch 5/10, Batch 183/442, Training Loss: 0.6635
Epoch 5/10, Batch 184/442, Training Loss: 0.5677
Epoch 5/10, Batch 185/442, Training Loss: 0.4142
Epoch 5/10, Batch 186/442, Training Loss: 0.5327
Epoch 5/10, Batch 187/442, Training Loss: 0.6616
Epoch 5/10, Batch 188/442, Training Loss: 0.5407
Epoch 5/10, Batch 189/442, Training Loss: 0.5391
Epoch 5/10, Batch 190/442, Training Loss: 0.6793
Epoch 5/10, Batch 191/442, Training Loss: 0.6361
Epoch 5/10, Batch 192/442, Training Loss: 0.4601
Epoch 5/10, Batch 193/442, Training Loss: 0.5957
Epoch 5/10, Batch 194/442, Training Loss: 0.5285
Epoch 5/10, Batch 195/442, Training Loss: 0.7009
Epoch 5/10, Batch 196/442, Training Loss: 0.4488
Epoch 5/10, Batch 197/442, Training Loss: 0.5038
Epoch 5/10, Batch 198/442, Training Loss: 0.5150
Epoch 5/10, Batch 199/442, Training Loss: 0.6446
Epoch 5/10, Batch 200/442, Training Loss: 0.5915
Epoch 5/10, Batch 201/442, Training Loss: 0.6743
Epoch 5/10, Batch 202/442, Training Loss: 0.6400
Epoch 5/10, Batch 203/442, Training Loss: 0.4989
Epoch 5/10, Batch 204/442, Training Loss: 0.6581
Epoch 5/10, Batch 205/442, Training Loss: 0.4835
Epoch 5/10, Batch 206/442, Training Loss: 0.4311
Epoch 5/10, Batch 207/442, Training Loss: 0.5023
Epoch 5/10, Batch 208/442, Training Loss: 0.6493
Epoch 5/10, Batch 209/442, Training Loss: 0.4123
Epoch 5/10, Batch 210/442, Training Loss: 0.8686
Epoch 5/10, Batch 211/442, Training Loss: 0.6187
Epoch 5/10, Batch 212/442, Training Loss: 0.7960
Epoch 5/10, Batch 213/442, Training Loss: 0.5641
Epoch 5/10, Batch 214/442, Training Loss: 0.6618
Epoch 5/10, Batch 215/442, Training Loss: 0.5236
Epoch 5/10, Batch 216/442, Training Loss: 0.8167
Epoch 5/10, Batch 217/442, Training Loss: 0.5883
Epoch 5/10, Batch 218/442, Training Loss: 0.5473
Epoch 5/10, Batch 219/442, Training Loss: 0.4775
Epoch 5/10, Batch 220/442, Training Loss: 0.5737
Epoch 5/10, Batch 221/442, Training Loss: 0.5255
Epoch 5/10, Batch 222/442, Training Loss: 0.4778
Epoch 5/10, Batch 223/442, Training Loss: 0.8411
Epoch 5/10, Batch 224/442, Training Loss: 0.5970
Epoch 5/10, Batch 225/442, Training Loss: 0.5962
Epoch 5/10, Batch 226/442, Training Loss: 0.4827
Epoch 5/10, Batch 227/442, Training Loss: 0.8710
Epoch 5/10, Batch 228/442, Training Loss: 0.6231
Epoch 5/10, Batch 229/442, Training Loss: 0.6060
Epoch 5/10, Batch 230/442, Training Loss: 0.5566
Epoch 5/10, Batch 231/442, Training Loss: 0.5694
Epoch 5/10, Batch 232/442, Training Loss: 0.5187
Epoch 5/10, Batch 233/442, Training Loss: 0.5782
Epoch 5/10, Batch 234/442, Training Loss: 0.7323
Epoch 5/10, Batch 235/442, Training Loss: 0.7864
Epoch 5/10, Batch 236/442, Training Loss: 0.6188
Epoch 5/10, Batch 237/442, Training Loss: 0.6299
Epoch 5/10, Batch 238/442, Training Loss: 0.6136
Epoch 5/10, Batch 239/442, Training Loss: 0.4860
Epoch 5/10, Batch 240/442, Training Loss: 0.8592
Epoch 5/10, Batch 241/442, Training Loss: 0.6557
Epoch 5/10, Batch 242/442, Training Loss: 0.5896
Epoch 5/10, Batch 243/442, Training Loss: 0.6996
Epoch 5/10, Batch 244/442, Training Loss: 0.7104
Epoch 5/10, Batch 245/442, Training Loss: 0.6468
Epoch 5/10, Batch 246/442, Training Loss: 0.5437
Epoch 5/10, Batch 247/442, Training Loss: 0.4760
Epoch 5/10, Batch 248/442, Training Loss: 0.9424
Epoch 5/10, Batch 249/442, Training Loss: 0.8765
Epoch 5/10, Batch 250/442, Training Loss: 0.7266
Epoch 5/10, Batch 251/442, Training Loss: 0.5931
Epoch 5/10, Batch 252/442, Training Loss: 0.6958
Epoch 5/10, Batch 253/442, Training Loss: 0.6026
Epoch 5/10, Batch 254/442, Training Loss: 0.7968
Epoch 5/10, Batch 255/442, Training Loss: 0.6123
Epoch 5/10, Batch 256/442, Training Loss: 0.6387
Epoch 5/10, Batch 257/442, Training Loss: 0.4921
Epoch 5/10, Batch 258/442, Training Loss: 0.5683
Epoch 5/10, Batch 259/442, Training Loss: 0.7128
Epoch 5/10, Batch 260/442, Training Loss: 0.5277
Epoch 5/10, Batch 261/442, Training Loss: 0.5825
Epoch 5/10, Batch 262/442, Training Loss: 0.5694
Epoch 5/10, Batch 263/442, Training Loss: 0.6218
Epoch 5/10, Batch 264/442, Training Loss: 0.6898
Epoch 5/10, Batch 265/442, Training Loss: 0.6515
Epoch 5/10, Batch 266/442, Training Loss: 0.8283
Epoch 5/10, Batch 267/442, Training Loss: 0.5200
Epoch 5/10, Batch 268/442, Training Loss: 0.4039
Epoch 5/10, Batch 269/442, Training Loss: 0.8512
Epoch 5/10, Batch 270/442, Training Loss: 0.5368
Epoch 5/10, Batch 271/442, Training Loss: 0.6680
Epoch 5/10, Batch 272/442, Training Loss: 0.7272
Epoch 5/10, Batch 273/442, Training Loss: 0.6933
Epoch 5/10, Batch 274/442, Training Loss: 0.5757
Epoch 5/10, Batch 275/442, Training Loss: 0.6145
Epoch 5/10, Batch 276/442, Training Loss: 0.5860
Epoch 5/10, Batch 277/442, Training Loss: 0.6431
Epoch 5/10, Batch 278/442, Training Loss: 0.8427
Epoch 5/10, Batch 279/442, Training Loss: 0.6066
Epoch 5/10, Batch 280/442, Training Loss: 0.4494
Epoch 5/10, Batch 281/442, Training Loss: 0.5569
Epoch 5/10, Batch 282/442, Training Loss: 0.6861
Epoch 5/10, Batch 283/442, Training Loss: 0.7366
Epoch 5/10, Batch 284/442, Training Loss: 0.7277
Epoch 5/10, Batch 285/442, Training Loss: 0.7667
Epoch 5/10, Batch 286/442, Training Loss: 0.7961
Epoch 5/10, Batch 287/442, Training Loss: 0.5120
Epoch 5/10, Batch 288/442, Training Loss: 0.6529
Epoch 5/10, Batch 289/442, Training Loss: 0.5158
Epoch 5/10, Batch 290/442, Training Loss: 0.5827
Epoch 5/10, Batch 291/442, Training Loss: 0.4645
Epoch 5/10, Batch 292/442, Training Loss: 0.5653
Epoch 5/10, Batch 293/442, Training Loss: 0.5554
Epoch 5/10, Batch 294/442, Training Loss: 0.5861
Epoch 5/10, Batch 295/442, Training Loss: 0.7023
Epoch 5/10, Batch 296/442, Training Loss: 0.5239
Epoch 5/10, Batch 297/442, Training Loss: 0.5771
Epoch 5/10, Batch 298/442, Training Loss: 0.7239
Epoch 5/10, Batch 299/442, Training Loss: 0.5870
Epoch 5/10, Batch 300/442, Training Loss: 0.4690
Epoch 5/10, Batch 301/442, Training Loss: 0.7381
Epoch 5/10, Batch 302/442, Training Loss: 0.5754
Epoch 5/10, Batch 303/442, Training Loss: 0.5770
Epoch 5/10, Batch 304/442, Training Loss: 0.5357
Epoch 5/10, Batch 305/442, Training Loss: 0.5017
Epoch 5/10, Batch 306/442, Training Loss: 0.6291
Epoch 5/10, Batch 307/442, Training Loss: 0.6999
Epoch 5/10, Batch 308/442, Training Loss: 0.4878
Epoch 5/10, Batch 309/442, Training Loss: 0.6441
Epoch 5/10, Batch 310/442, Training Loss: 0.5607
Epoch 5/10, Batch 311/442, Training Loss: 0.4676
Epoch 5/10, Batch 312/442, Training Loss: 0.7201
Epoch 5/10, Batch 313/442, Training Loss: 0.5262
Epoch 5/10, Batch 314/442, Training Loss: 0.4542
Epoch 5/10, Batch 315/442, Training Loss: 0.6573
Epoch 5/10, Batch 316/442, Training Loss: 0.6756
Epoch 5/10, Batch 317/442, Training Loss: 0.6153
Epoch 5/10, Batch 318/442, Training Loss: 0.5114
Epoch 5/10, Batch 319/442, Training Loss: 0.5154
Epoch 5/10, Batch 320/442, Training Loss: 0.5961
Epoch 5/10, Batch 321/442, Training Loss: 0.4577
Epoch 5/10, Batch 322/442, Training Loss: 0.5541
Epoch 5/10, Batch 323/442, Training Loss: 0.6352
Epoch 5/10, Batch 324/442, Training Loss: 0.5079
Epoch 5/10, Batch 325/442, Training Loss: 0.3717
Epoch 5/10, Batch 326/442, Training Loss: 0.3497
Epoch 5/10, Batch 327/442, Training Loss: 0.5648
Epoch 5/10, Batch 328/442, Training Loss: 0.7920
Epoch 5/10, Batch 329/442, Training Loss: 0.5100
Epoch 5/10, Batch 330/442, Training Loss: 0.7313
Epoch 5/10, Batch 331/442, Training Loss: 0.5763
Epoch 5/10, Batch 332/442, Training Loss: 0.6852
Epoch 5/10, Batch 333/442, Training Loss: 0.6057
Epoch 5/10, Batch 334/442, Training Loss: 0.3824
Epoch 5/10, Batch 335/442, Training Loss: 0.6230
Epoch 5/10, Batch 336/442, Training Loss: 0.7406
Epoch 5/10, Batch 337/442, Training Loss: 0.6057
Epoch 5/10, Batch 338/442, Training Loss: 0.8199
Epoch 5/10, Batch 339/442, Training Loss: 0.7372
Epoch 5/10, Batch 340/442, Training Loss: 0.8218
Epoch 5/10, Batch 341/442, Training Loss: 0.4117
Epoch 5/10, Batch 342/442, Training Loss: 0.9458
Epoch 5/10, Batch 343/442, Training Loss: 0.4232
Epoch 5/10, Batch 344/442, Training Loss: 0.5880
Epoch 5/10, Batch 345/442, Training Loss: 0.6071
Epoch 5/10, Batch 346/442, Training Loss: 0.5183
Epoch 5/10, Batch 347/442, Training Loss: 0.3518
Epoch 5/10, Batch 348/442, Training Loss: 0.8627
Epoch 5/10, Batch 349/442, Training Loss: 0.6715
Epoch 5/10, Batch 350/442, Training Loss: 0.5151
Epoch 5/10, Batch 351/442, Training Loss: 0.8514
Epoch 5/10, Batch 352/442, Training Loss: 0.5603
Epoch 5/10, Batch 353/442, Training Loss: 0.5203
Epoch 5/10, Batch 354/442, Training Loss: 0.5924
Epoch 5/10, Batch 355/442, Training Loss: 0.5511
Epoch 5/10, Batch 356/442, Training Loss: 0.5300
Epoch 5/10, Batch 357/442, Training Loss: 0.5866
Epoch 5/10, Batch 358/442, Training Loss: 0.3935
Epoch 5/10, Batch 359/442, Training Loss: 0.6994
Epoch 5/10, Batch 360/442, Training Loss: 0.6408
Epoch 5/10, Batch 361/442, Training Loss: 0.5543
Epoch 5/10, Batch 362/442, Training Loss: 0.4413
Epoch 5/10, Batch 363/442, Training Loss: 0.6841
Epoch 5/10, Batch 364/442, Training Loss: 0.4269
Epoch 5/10, Batch 365/442, Training Loss: 0.5594
Epoch 5/10, Batch 366/442, Training Loss: 0.7100
Epoch 5/10, Batch 367/442, Training Loss: 0.5702
Epoch 5/10, Batch 368/442, Training Loss: 0.5977
Epoch 5/10, Batch 369/442, Training Loss: 0.4821
Epoch 5/10, Batch 370/442, Training Loss: 0.4879
Epoch 5/10, Batch 371/442, Training Loss: 0.5993
Epoch 5/10, Batch 372/442, Training Loss: 0.8578
Epoch 5/10, Batch 373/442, Training Loss: 0.6433
Epoch 5/10, Batch 374/442, Training Loss: 0.4777
Epoch 5/10, Batch 375/442, Training Loss: 0.6764
Epoch 5/10, Batch 376/442, Training Loss: 0.6531
Epoch 5/10, Batch 377/442, Training Loss: 0.6514
Epoch 5/10, Batch 378/442, Training Loss: 0.7310
Epoch 5/10, Batch 379/442, Training Loss: 0.7184
Epoch 5/10, Batch 380/442, Training Loss: 0.5594
Epoch 5/10, Batch 381/442, Training Loss: 0.5976
Epoch 5/10, Batch 382/442, Training Loss: 0.6447
Epoch 5/10, Batch 383/442, Training Loss: 0.5842
Epoch 5/10, Batch 384/442, Training Loss: 0.6485
Epoch 5/10, Batch 385/442, Training Loss: 0.7044
Epoch 5/10, Batch 386/442, Training Loss: 0.6474
Epoch 5/10, Batch 387/442, Training Loss: 0.7551
Epoch 5/10, Batch 388/442, Training Loss: 0.5185
Epoch 5/10, Batch 389/442, Training Loss: 0.5095
Epoch 5/10, Batch 390/442, Training Loss: 0.7054
Epoch 5/10, Batch 391/442, Training Loss: 0.5193
Epoch 5/10, Batch 392/442, Training Loss: 0.5884
Epoch 5/10, Batch 393/442, Training Loss: 0.5908
Epoch 5/10, Batch 394/442, Training Loss: 0.7081
Epoch 5/10, Batch 395/442, Training Loss: 0.6079
Epoch 5/10, Batch 396/442, Training Loss: 0.4947
Epoch 5/10, Batch 397/442, Training Loss: 0.5538
Epoch 5/10, Batch 398/442, Training Loss: 0.7183
Epoch 5/10, Batch 399/442, Training Loss: 0.4104
Epoch 5/10, Batch 400/442, Training Loss: 0.6104
Epoch 5/10, Batch 401/442, Training Loss: 0.4676
Epoch 5/10, Batch 402/442, Training Loss: 0.8279
Epoch 5/10, Batch 403/442, Training Loss: 0.4827
Epoch 5/10, Batch 404/442, Training Loss: 0.4834
Epoch 5/10, Batch 405/442, Training Loss: 0.6849
Epoch 5/10, Batch 406/442, Training Loss: 0.2949
Epoch 5/10, Batch 407/442, Training Loss: 0.5233
Epoch 5/10, Batch 408/442, Training Loss: 0.4772
Epoch 5/10, Batch 409/442, Training Loss: 0.6148
Epoch 5/10, Batch 410/442, Training Loss: 0.3449
Epoch 5/10, Batch 411/442, Training Loss: 0.4823
Epoch 5/10, Batch 412/442, Training Loss: 0.6498
Epoch 5/10, Batch 413/442, Training Loss: 0.6052
Epoch 5/10, Batch 414/442, Training Loss: 0.7498
Epoch 5/10, Batch 415/442, Training Loss: 0.4899
Epoch 5/10, Batch 416/442, Training Loss: 0.8384
Epoch 5/10, Batch 417/442, Training Loss: 0.5726
Epoch 5/10, Batch 418/442, Training Loss: 0.6290
Epoch 5/10, Batch 419/442, Training Loss: 0.4879
Epoch 5/10, Batch 420/442, Training Loss: 0.8348
Epoch 5/10, Batch 421/442, Training Loss: 0.8176
Epoch 5/10, Batch 422/442, Training Loss: 0.5173
Epoch 5/10, Batch 423/442, Training Loss: 0.7516
Epoch 5/10, Batch 424/442, Training Loss: 0.5573
Epoch 5/10, Batch 425/442, Training Loss: 0.6560
Epoch 5/10, Batch 426/442, Training Loss: 0.6304
Epoch 5/10, Batch 427/442, Training Loss: 0.5260
Epoch 5/10, Batch 428/442, Training Loss: 0.5015
Epoch 5/10, Batch 429/442, Training Loss: 0.7449
Epoch 5/10, Batch 430/442, Training Loss: 0.6453
Epoch 5/10, Batch 431/442, Training Loss: 0.7354
Epoch 5/10, Batch 432/442, Training Loss: 0.8573
Epoch 5/10, Batch 433/442, Training Loss: 0.7463
Epoch 5/10, Batch 434/442, Training Loss: 0.6702
Epoch 5/10, Batch 435/442, Training Loss: 0.7379
Epoch 5/10, Batch 436/442, Training Loss: 0.5584
Epoch 5/10, Batch 437/442, Training Loss: 0.7427
Epoch 5/10, Batch 438/442, Training Loss: 0.7265
Epoch 5/10, Batch 439/442, Training Loss: 0.5717
Epoch 5/10, Batch 440/442, Training Loss: 0.5264
Epoch 5/10, Batch 441/442, Training Loss: 0.5913
Epoch 5/10, Batch 442/442, Training Loss: 0.6981
Epoch 5/10, Training Loss: 0.6250, Validation Loss: 0.6956, Validation Accuracy: 0.6638
Epoch 6/10, Batch 1/442, Training Loss: 0.5216
Epoch 6/10, Batch 2/442, Training Loss: 0.6591
Epoch 6/10, Batch 3/442, Training Loss: 0.6672
Epoch 6/10, Batch 4/442, Training Loss: 0.5730
Epoch 6/10, Batch 5/442, Training Loss: 0.7852
Epoch 6/10, Batch 6/442, Training Loss: 0.6387
Epoch 6/10, Batch 7/442, Training Loss: 0.7110
Epoch 6/10, Batch 8/442, Training Loss: 0.5460
Epoch 6/10, Batch 9/442, Training Loss: 0.6345
Epoch 6/10, Batch 10/442, Training Loss: 0.5789
Epoch 6/10, Batch 11/442, Training Loss: 0.4786
Epoch 6/10, Batch 12/442, Training Loss: 0.5137
Epoch 6/10, Batch 13/442, Training Loss: 0.5594
Epoch 6/10, Batch 14/442, Training Loss: 0.7924
Epoch 6/10, Batch 15/442, Training Loss: 0.5636
Epoch 6/10, Batch 16/442, Training Loss: 0.4655
Epoch 6/10, Batch 17/442, Training Loss: 0.5109
Epoch 6/10, Batch 18/442, Training Loss: 0.5451
Epoch 6/10, Batch 19/442, Training Loss: 0.6451
Epoch 6/10, Batch 20/442, Training Loss: 0.7148
Epoch 6/10, Batch 21/442, Training Loss: 0.4955
Epoch 6/10, Batch 22/442, Training Loss: 0.6874
Epoch 6/10, Batch 23/442, Training Loss: 0.5445
Epoch 6/10, Batch 24/442, Training Loss: 0.6814
Epoch 6/10, Batch 25/442, Training Loss: 0.4662
Epoch 6/10, Batch 26/442, Training Loss: 0.5263
Epoch 6/10, Batch 27/442, Training Loss: 0.4416
Epoch 6/10, Batch 28/442, Training Loss: 0.4800
Epoch 6/10, Batch 29/442, Training Loss: 0.6635
Epoch 6/10, Batch 30/442, Training Loss: 0.5621
Epoch 6/10, Batch 31/442, Training Loss: 0.3883
Epoch 6/10, Batch 32/442, Training Loss: 0.4022
Epoch 6/10, Batch 33/442, Training Loss: 0.7358
Epoch 6/10, Batch 34/442, Training Loss: 0.6729
Epoch 6/10, Batch 35/442, Training Loss: 0.3387
Epoch 6/10, Batch 36/442, Training Loss: 0.8091
Epoch 6/10, Batch 37/442, Training Loss: 0.8408
Epoch 6/10, Batch 38/442, Training Loss: 0.6116
Epoch 6/10, Batch 39/442, Training Loss: 0.5952
Epoch 6/10, Batch 40/442, Training Loss: 0.4320
Epoch 6/10, Batch 41/442, Training Loss: 0.4715
Epoch 6/10, Batch 42/442, Training Loss: 0.6356
Epoch 6/10, Batch 43/442, Training Loss: 0.4584
Epoch 6/10, Batch 44/442, Training Loss: 0.5452
Epoch 6/10, Batch 45/442, Training Loss: 0.4445
Epoch 6/10, Batch 46/442, Training Loss: 0.5412
Epoch 6/10, Batch 47/442, Training Loss: 0.6594
Epoch 6/10, Batch 48/442, Training Loss: 0.5176
Epoch 6/10, Batch 49/442, Training Loss: 0.8587
Epoch 6/10, Batch 50/442, Training Loss: 0.5651
Epoch 6/10, Batch 51/442, Training Loss: 0.8634
Epoch 6/10, Batch 52/442, Training Loss: 0.6339
Epoch 6/10, Batch 53/442, Training Loss: 0.6344
Epoch 6/10, Batch 54/442, Training Loss: 0.4329
Epoch 6/10, Batch 55/442, Training Loss: 0.5387
Epoch 6/10, Batch 56/442, Training Loss: 0.3844
Epoch 6/10, Batch 57/442, Training Loss: 0.3794
Epoch 6/10, Batch 58/442, Training Loss: 0.4057
Epoch 6/10, Batch 59/442, Training Loss: 0.4424
Epoch 6/10, Batch 60/442, Training Loss: 0.3979
Epoch 6/10, Batch 61/442, Training Loss: 0.6575
Epoch 6/10, Batch 62/442, Training Loss: 0.7187
Epoch 6/10, Batch 63/442, Training Loss: 0.6945
Epoch 6/10, Batch 64/442, Training Loss: 0.7689
Epoch 6/10, Batch 65/442, Training Loss: 0.4118
Epoch 6/10, Batch 66/442, Training Loss: 0.5898
Epoch 6/10, Batch 67/442, Training Loss: 0.5366
Epoch 6/10, Batch 68/442, Training Loss: 0.7078
Epoch 6/10, Batch 69/442, Training Loss: 0.6021
Epoch 6/10, Batch 70/442, Training Loss: 0.5452
Epoch 6/10, Batch 71/442, Training Loss: 0.6069
Epoch 6/10, Batch 72/442, Training Loss: 0.6812
Epoch 6/10, Batch 73/442, Training Loss: 0.8514
Epoch 6/10, Batch 74/442, Training Loss: 0.6152
Epoch 6/10, Batch 75/442, Training Loss: 0.4670
Epoch 6/10, Batch 76/442, Training Loss: 0.5739
Epoch 6/10, Batch 77/442, Training Loss: 0.3553
Epoch 6/10, Batch 78/442, Training Loss: 0.6712
Epoch 6/10, Batch 79/442, Training Loss: 0.4580
Epoch 6/10, Batch 80/442, Training Loss: 0.5520
Epoch 6/10, Batch 81/442, Training Loss: 0.7885
Epoch 6/10, Batch 82/442, Training Loss: 0.4596
Epoch 6/10, Batch 83/442, Training Loss: 0.6772
Epoch 6/10, Batch 84/442, Training Loss: 0.5546
Epoch 6/10, Batch 85/442, Training Loss: 0.6193
Epoch 6/10, Batch 86/442, Training Loss: 0.7091
Epoch 6/10, Batch 87/442, Training Loss: 0.5284
Epoch 6/10, Batch 88/442, Training Loss: 0.6669
Epoch 6/10, Batch 89/442, Training Loss: 0.8225
Epoch 6/10, Batch 90/442, Training Loss: 0.5011
Epoch 6/10, Batch 91/442, Training Loss: 0.7573
Epoch 6/10, Batch 92/442, Training Loss: 0.8135
Epoch 6/10, Batch 93/442, Training Loss: 0.8184
Epoch 6/10, Batch 94/442, Training Loss: 0.4184
Epoch 6/10, Batch 95/442, Training Loss: 0.5705
Epoch 6/10, Batch 96/442, Training Loss: 0.5164
Epoch 6/10, Batch 97/442, Training Loss: 0.6060
Epoch 6/10, Batch 98/442, Training Loss: 0.7076
Epoch 6/10, Batch 99/442, Training Loss: 0.4682
Epoch 6/10, Batch 100/442, Training Loss: 0.6182
Epoch 6/10, Batch 101/442, Training Loss: 0.4978
Epoch 6/10, Batch 102/442, Training Loss: 0.3636
Epoch 6/10, Batch 103/442, Training Loss: 0.6135
Epoch 6/10, Batch 104/442, Training Loss: 0.6217
Epoch 6/10, Batch 105/442, Training Loss: 0.5334
Epoch 6/10, Batch 106/442, Training Loss: 0.4558
Epoch 6/10, Batch 107/442, Training Loss: 0.6033
Epoch 6/10, Batch 108/442, Training Loss: 0.5899
Epoch 6/10, Batch 109/442, Training Loss: 0.5165
Epoch 6/10, Batch 110/442, Training Loss: 0.4378
Epoch 6/10, Batch 111/442, Training Loss: 0.5816
Epoch 6/10, Batch 112/442, Training Loss: 0.4306
Epoch 6/10, Batch 113/442, Training Loss: 0.2760
Epoch 6/10, Batch 114/442, Training Loss: 0.5039
Epoch 6/10, Batch 115/442, Training Loss: 0.5279
Epoch 6/10, Batch 116/442, Training Loss: 0.5086
Epoch 6/10, Batch 117/442, Training Loss: 0.6895
Epoch 6/10, Batch 118/442, Training Loss: 0.5680
Epoch 6/10, Batch 119/442, Training Loss: 1.0625
Epoch 6/10, Batch 120/442, Training Loss: 0.4301
Epoch 6/10, Batch 121/442, Training Loss: 0.7167
Epoch 6/10, Batch 122/442, Training Loss: 0.7257
Epoch 6/10, Batch 123/442, Training Loss: 0.6446
Epoch 6/10, Batch 124/442, Training Loss: 0.6353
Epoch 6/10, Batch 125/442, Training Loss: 0.4622
Epoch 6/10, Batch 126/442, Training Loss: 0.7445
Epoch 6/10, Batch 127/442, Training Loss: 0.7124
Epoch 6/10, Batch 128/442, Training Loss: 0.5938
Epoch 6/10, Batch 129/442, Training Loss: 0.3894
Epoch 6/10, Batch 130/442, Training Loss: 0.5889
Epoch 6/10, Batch 131/442, Training Loss: 0.7034
Epoch 6/10, Batch 132/442, Training Loss: 0.5272
Epoch 6/10, Batch 133/442, Training Loss: 0.5143
Epoch 6/10, Batch 134/442, Training Loss: 0.6404
Epoch 6/10, Batch 135/442, Training Loss: 0.5511
Epoch 6/10, Batch 136/442, Training Loss: 0.3606
Epoch 6/10, Batch 137/442, Training Loss: 0.7160
Epoch 6/10, Batch 138/442, Training Loss: 0.5075
Epoch 6/10, Batch 139/442, Training Loss: 0.5177
Epoch 6/10, Batch 140/442, Training Loss: 0.5567
Epoch 6/10, Batch 141/442, Training Loss: 0.4887
Epoch 6/10, Batch 142/442, Training Loss: 0.5544
Epoch 6/10, Batch 143/442, Training Loss: 0.7092
Epoch 6/10, Batch 144/442, Training Loss: 0.6160
Epoch 6/10, Batch 145/442, Training Loss: 0.3805
Epoch 6/10, Batch 146/442, Training Loss: 0.7311
Epoch 6/10, Batch 147/442, Training Loss: 0.5302
Epoch 6/10, Batch 148/442, Training Loss: 0.5474
Epoch 6/10, Batch 149/442, Training Loss: 0.9497
Epoch 6/10, Batch 150/442, Training Loss: 0.6692
Epoch 6/10, Batch 151/442, Training Loss: 0.7280
Epoch 6/10, Batch 152/442, Training Loss: 0.7218
Epoch 6/10, Batch 153/442, Training Loss: 0.6925
Epoch 6/10, Batch 154/442, Training Loss: 0.4852
Epoch 6/10, Batch 155/442, Training Loss: 0.7617
Epoch 6/10, Batch 156/442, Training Loss: 0.5990
Epoch 6/10, Batch 157/442, Training Loss: 0.6350
Epoch 6/10, Batch 158/442, Training Loss: 0.5728
Epoch 6/10, Batch 159/442, Training Loss: 0.4407
Epoch 6/10, Batch 160/442, Training Loss: 0.4633
Epoch 6/10, Batch 161/442, Training Loss: 0.6181
Epoch 6/10, Batch 162/442, Training Loss: 0.7202
Epoch 6/10, Batch 163/442, Training Loss: 0.5182
Epoch 6/10, Batch 164/442, Training Loss: 0.4902
Epoch 6/10, Batch 165/442, Training Loss: 0.7092
Epoch 6/10, Batch 166/442, Training Loss: 0.6050
Epoch 6/10, Batch 167/442, Training Loss: 0.3941
Epoch 6/10, Batch 168/442, Training Loss: 0.6477
Epoch 6/10, Batch 169/442, Training Loss: 0.4004
Epoch 6/10, Batch 170/442, Training Loss: 0.6961
Epoch 6/10, Batch 171/442, Training Loss: 0.4771
Epoch 6/10, Batch 172/442, Training Loss: 0.7274
Epoch 6/10, Batch 173/442, Training Loss: 0.4419
Epoch 6/10, Batch 174/442, Training Loss: 0.8354
Epoch 6/10, Batch 175/442, Training Loss: 0.6210
Epoch 6/10, Batch 176/442, Training Loss: 0.5076
Epoch 6/10, Batch 177/442, Training Loss: 0.4670
Epoch 6/10, Batch 178/442, Training Loss: 0.9440
Epoch 6/10, Batch 179/442, Training Loss: 0.3445
Epoch 6/10, Batch 180/442, Training Loss: 0.3864
Epoch 6/10, Batch 181/442, Training Loss: 0.5890
Epoch 6/10, Batch 182/442, Training Loss: 0.6791
Epoch 6/10, Batch 183/442, Training Loss: 0.6041
Epoch 6/10, Batch 184/442, Training Loss: 0.6296
Epoch 6/10, Batch 185/442, Training Loss: 0.6166
Epoch 6/10, Batch 186/442, Training Loss: 0.7452
Epoch 6/10, Batch 187/442, Training Loss: 0.7182
Epoch 6/10, Batch 188/442, Training Loss: 0.5048
Epoch 6/10, Batch 189/442, Training Loss: 0.6870
Epoch 6/10, Batch 190/442, Training Loss: 0.4729
Epoch 6/10, Batch 191/442, Training Loss: 0.6226
Epoch 6/10, Batch 192/442, Training Loss: 0.6009
Epoch 6/10, Batch 193/442, Training Loss: 0.6933
Epoch 6/10, Batch 194/442, Training Loss: 0.6440
Epoch 6/10, Batch 195/442, Training Loss: 0.4808
Epoch 6/10, Batch 196/442, Training Loss: 0.5491
Epoch 6/10, Batch 197/442, Training Loss: 0.5886
Epoch 6/10, Batch 198/442, Training Loss: 0.3810
Epoch 6/10, Batch 199/442, Training Loss: 0.6112
Epoch 6/10, Batch 200/442, Training Loss: 0.6748
Epoch 6/10, Batch 201/442, Training Loss: 1.0364
Epoch 6/10, Batch 202/442, Training Loss: 0.7160
Epoch 6/10, Batch 203/442, Training Loss: 0.5801
Epoch 6/10, Batch 204/442, Training Loss: 0.8123
Epoch 6/10, Batch 205/442, Training Loss: 0.6713
Epoch 6/10, Batch 206/442, Training Loss: 0.4637
Epoch 6/10, Batch 207/442, Training Loss: 0.6273
Epoch 6/10, Batch 208/442, Training Loss: 0.4483
Epoch 6/10, Batch 209/442, Training Loss: 0.5053
Epoch 6/10, Batch 210/442, Training Loss: 0.6967
Epoch 6/10, Batch 211/442, Training Loss: 0.7054
Epoch 6/10, Batch 212/442, Training Loss: 0.6294
Epoch 6/10, Batch 213/442, Training Loss: 0.5601
Epoch 6/10, Batch 214/442, Training Loss: 0.5443
Epoch 6/10, Batch 215/442, Training Loss: 0.4561
Epoch 6/10, Batch 216/442, Training Loss: 0.6629
Epoch 6/10, Batch 217/442, Training Loss: 0.5589
Epoch 6/10, Batch 218/442, Training Loss: 0.4948
Epoch 6/10, Batch 219/442, Training Loss: 0.6562
Epoch 6/10, Batch 220/442, Training Loss: 0.6056
Epoch 6/10, Batch 221/442, Training Loss: 0.6174
Epoch 6/10, Batch 222/442, Training Loss: 0.6421
Epoch 6/10, Batch 223/442, Training Loss: 0.3890
Epoch 6/10, Batch 224/442, Training Loss: 0.3587
Epoch 6/10, Batch 225/442, Training Loss: 0.4722
Epoch 6/10, Batch 226/442, Training Loss: 0.6552
Epoch 6/10, Batch 227/442, Training Loss: 0.5982
Epoch 6/10, Batch 228/442, Training Loss: 0.6300
Epoch 6/10, Batch 229/442, Training Loss: 0.5597
Epoch 6/10, Batch 230/442, Training Loss: 0.6020
Epoch 6/10, Batch 231/442, Training Loss: 0.5934
Epoch 6/10, Batch 232/442, Training Loss: 0.6852
Epoch 6/10, Batch 233/442, Training Loss: 0.7446
Epoch 6/10, Batch 234/442, Training Loss: 0.4630
Epoch 6/10, Batch 235/442, Training Loss: 0.3272
Epoch 6/10, Batch 236/442, Training Loss: 0.7856
Epoch 6/10, Batch 237/442, Training Loss: 0.7903
Epoch 6/10, Batch 238/442, Training Loss: 0.4980
Epoch 6/10, Batch 239/442, Training Loss: 0.4822
Epoch 6/10, Batch 240/442, Training Loss: 0.5860
Epoch 6/10, Batch 241/442, Training Loss: 0.5495
Epoch 6/10, Batch 242/442, Training Loss: 0.5138
Epoch 6/10, Batch 243/442, Training Loss: 0.5706
Epoch 6/10, Batch 244/442, Training Loss: 0.3242
Epoch 6/10, Batch 245/442, Training Loss: 0.7674
Epoch 6/10, Batch 246/442, Training Loss: 0.3680
Epoch 6/10, Batch 247/442, Training Loss: 0.5414
Epoch 6/10, Batch 248/442, Training Loss: 0.8296
Epoch 6/10, Batch 249/442, Training Loss: 0.6416
Epoch 6/10, Batch 250/442, Training Loss: 0.6963
Epoch 6/10, Batch 251/442, Training Loss: 0.6986
Epoch 6/10, Batch 252/442, Training Loss: 0.4774
Epoch 6/10, Batch 253/442, Training Loss: 0.6736
Epoch 6/10, Batch 254/442, Training Loss: 0.6550
Epoch 6/10, Batch 255/442, Training Loss: 0.6301
Epoch 6/10, Batch 256/442, Training Loss: 0.5618
Epoch 6/10, Batch 257/442, Training Loss: 0.5513
Epoch 6/10, Batch 258/442, Training Loss: 0.6035
Epoch 6/10, Batch 259/442, Training Loss: 0.6945
Epoch 6/10, Batch 260/442, Training Loss: 0.5348
Epoch 6/10, Batch 261/442, Training Loss: 0.7790
Epoch 6/10, Batch 262/442, Training Loss: 0.4995
Epoch 6/10, Batch 263/442, Training Loss: 0.4102
Epoch 6/10, Batch 264/442, Training Loss: 0.7359
Epoch 6/10, Batch 265/442, Training Loss: 0.5820
Epoch 6/10, Batch 266/442, Training Loss: 0.6063
Epoch 6/10, Batch 267/442, Training Loss: 0.6884
Epoch 6/10, Batch 268/442, Training Loss: 0.5529
Epoch 6/10, Batch 269/442, Training Loss: 0.4079
Epoch 6/10, Batch 270/442, Training Loss: 0.5673
Epoch 6/10, Batch 271/442, Training Loss: 0.5565
Epoch 6/10, Batch 272/442, Training Loss: 0.4828
Epoch 6/10, Batch 273/442, Training Loss: 0.4261
Epoch 6/10, Batch 274/442, Training Loss: 0.4505
Epoch 6/10, Batch 275/442, Training Loss: 0.4007
Epoch 6/10, Batch 276/442, Training Loss: 0.5710
Epoch 6/10, Batch 277/442, Training Loss: 0.7036
Epoch 6/10, Batch 278/442, Training Loss: 0.4727
Epoch 6/10, Batch 279/442, Training Loss: 0.8257
Epoch 6/10, Batch 280/442, Training Loss: 0.2994
Epoch 6/10, Batch 281/442, Training Loss: 0.4810
Epoch 6/10, Batch 282/442, Training Loss: 0.5697
Epoch 6/10, Batch 283/442, Training Loss: 0.5409
Epoch 6/10, Batch 284/442, Training Loss: 0.4404
Epoch 6/10, Batch 285/442, Training Loss: 0.5599
Epoch 6/10, Batch 286/442, Training Loss: 0.5145
Epoch 6/10, Batch 287/442, Training Loss: 0.6747
Epoch 6/10, Batch 288/442, Training Loss: 0.4258
Epoch 6/10, Batch 289/442, Training Loss: 0.8192
Epoch 6/10, Batch 290/442, Training Loss: 0.4939
Epoch 6/10, Batch 291/442, Training Loss: 0.5943
Epoch 6/10, Batch 292/442, Training Loss: 0.7943
Epoch 6/10, Batch 293/442, Training Loss: 0.6076
Epoch 6/10, Batch 294/442, Training Loss: 0.4438
Epoch 6/10, Batch 295/442, Training Loss: 0.4784
Epoch 6/10, Batch 296/442, Training Loss: 0.5228
Epoch 6/10, Batch 297/442, Training Loss: 0.5756
Epoch 6/10, Batch 298/442, Training Loss: 0.3812
Epoch 6/10, Batch 299/442, Training Loss: 0.5073
Epoch 6/10, Batch 300/442, Training Loss: 0.5582
Epoch 6/10, Batch 301/442, Training Loss: 0.5954
Epoch 6/10, Batch 302/442, Training Loss: 0.7763
Epoch 6/10, Batch 303/442, Training Loss: 0.5152
Epoch 6/10, Batch 304/442, Training Loss: 0.4134
Epoch 6/10, Batch 305/442, Training Loss: 0.6537
Epoch 6/10, Batch 306/442, Training Loss: 0.4406
Epoch 6/10, Batch 307/442, Training Loss: 0.4608
Epoch 6/10, Batch 308/442, Training Loss: 0.5082
Epoch 6/10, Batch 309/442, Training Loss: 0.7160
Epoch 6/10, Batch 310/442, Training Loss: 0.4834
Epoch 6/10, Batch 311/442, Training Loss: 0.4900
Epoch 6/10, Batch 312/442, Training Loss: 0.6324
Epoch 6/10, Batch 313/442, Training Loss: 0.5641
Epoch 6/10, Batch 314/442, Training Loss: 0.6568
Epoch 6/10, Batch 315/442, Training Loss: 0.6495
Epoch 6/10, Batch 316/442, Training Loss: 0.5948
Epoch 6/10, Batch 317/442, Training Loss: 0.6286
Epoch 6/10, Batch 318/442, Training Loss: 0.4828
Epoch 6/10, Batch 319/442, Training Loss: 0.6903
Epoch 6/10, Batch 320/442, Training Loss: 0.6336
Epoch 6/10, Batch 321/442, Training Loss: 0.3897
Epoch 6/10, Batch 322/442, Training Loss: 0.4402
Epoch 6/10, Batch 323/442, Training Loss: 0.5828
Epoch 6/10, Batch 324/442, Training Loss: 0.5538
Epoch 6/10, Batch 325/442, Training Loss: 0.5228
Epoch 6/10, Batch 326/442, Training Loss: 0.5306
Epoch 6/10, Batch 327/442, Training Loss: 0.6592
Epoch 6/10, Batch 328/442, Training Loss: 0.5346
Epoch 6/10, Batch 329/442, Training Loss: 0.4929
Epoch 6/10, Batch 330/442, Training Loss: 0.5862
Epoch 6/10, Batch 331/442, Training Loss: 0.4974
Epoch 6/10, Batch 332/442, Training Loss: 0.3886
Epoch 6/10, Batch 333/442, Training Loss: 0.3087
Epoch 6/10, Batch 334/442, Training Loss: 0.3502
Epoch 6/10, Batch 335/442, Training Loss: 0.6374
Epoch 6/10, Batch 336/442, Training Loss: 0.3669
Epoch 6/10, Batch 337/442, Training Loss: 0.4446
Epoch 6/10, Batch 338/442, Training Loss: 0.4635
Epoch 6/10, Batch 339/442, Training Loss: 0.6306
Epoch 6/10, Batch 340/442, Training Loss: 0.5529
Epoch 6/10, Batch 341/442, Training Loss: 0.3565
Epoch 6/10, Batch 342/442, Training Loss: 0.5615
Epoch 6/10, Batch 343/442, Training Loss: 0.3784
Epoch 6/10, Batch 344/442, Training Loss: 0.4088
Epoch 6/10, Batch 345/442, Training Loss: 0.6162
Epoch 6/10, Batch 346/442, Training Loss: 0.7467
Epoch 6/10, Batch 347/442, Training Loss: 0.3689
Epoch 6/10, Batch 348/442, Training Loss: 0.6513
Epoch 6/10, Batch 349/442, Training Loss: 0.5376
Epoch 6/10, Batch 350/442, Training Loss: 0.3766
Epoch 6/10, Batch 351/442, Training Loss: 0.5453
Epoch 6/10, Batch 352/442, Training Loss: 0.4508
Epoch 6/10, Batch 353/442, Training Loss: 0.8444
Epoch 6/10, Batch 354/442, Training Loss: 0.5906
Epoch 6/10, Batch 355/442, Training Loss: 0.7668
Epoch 6/10, Batch 356/442, Training Loss: 0.5539
Epoch 6/10, Batch 357/442, Training Loss: 0.5992
Epoch 6/10, Batch 358/442, Training Loss: 0.6313
Epoch 6/10, Batch 359/442, Training Loss: 0.7954
Epoch 6/10, Batch 360/442, Training Loss: 0.5501
Epoch 6/10, Batch 361/442, Training Loss: 0.7519
Epoch 6/10, Batch 362/442, Training Loss: 0.4191
Epoch 6/10, Batch 363/442, Training Loss: 0.3753
Epoch 6/10, Batch 364/442, Training Loss: 0.5153
Epoch 6/10, Batch 365/442, Training Loss: 0.4292
Epoch 6/10, Batch 366/442, Training Loss: 0.4966
Epoch 6/10, Batch 367/442, Training Loss: 0.7833
Epoch 6/10, Batch 368/442, Training Loss: 0.7271
Epoch 6/10, Batch 369/442, Training Loss: 0.5794
Epoch 6/10, Batch 370/442, Training Loss: 0.5832
Epoch 6/10, Batch 371/442, Training Loss: 0.5660
Epoch 6/10, Batch 372/442, Training Loss: 0.4490
Epoch 6/10, Batch 373/442, Training Loss: 0.5500
Epoch 6/10, Batch 374/442, Training Loss: 0.7413
Epoch 6/10, Batch 375/442, Training Loss: 0.7437
Epoch 6/10, Batch 376/442, Training Loss: 0.6126
Epoch 6/10, Batch 377/442, Training Loss: 0.5765
Epoch 6/10, Batch 378/442, Training Loss: 0.4777
Epoch 6/10, Batch 379/442, Training Loss: 0.7088
Epoch 6/10, Batch 380/442, Training Loss: 0.6214
Epoch 6/10, Batch 381/442, Training Loss: 0.4414
Epoch 6/10, Batch 382/442, Training Loss: 0.6007
Epoch 6/10, Batch 383/442, Training Loss: 0.6273
Epoch 6/10, Batch 384/442, Training Loss: 0.5336
Epoch 6/10, Batch 385/442, Training Loss: 0.6005
Epoch 6/10, Batch 386/442, Training Loss: 0.5883
Epoch 6/10, Batch 387/442, Training Loss: 0.4064
Epoch 6/10, Batch 388/442, Training Loss: 0.4170
Epoch 6/10, Batch 389/442, Training Loss: 0.4782
Epoch 6/10, Batch 390/442, Training Loss: 0.5433
Epoch 6/10, Batch 391/442, Training Loss: 0.5632
Epoch 6/10, Batch 392/442, Training Loss: 0.4057
Epoch 6/10, Batch 393/442, Training Loss: 0.5252
Epoch 6/10, Batch 394/442, Training Loss: 0.5632
Epoch 6/10, Batch 395/442, Training Loss: 0.7543
Epoch 6/10, Batch 396/442, Training Loss: 0.7973
Epoch 6/10, Batch 397/442, Training Loss: 0.3802
Epoch 6/10, Batch 398/442, Training Loss: 0.6758
Epoch 6/10, Batch 399/442, Training Loss: 0.4509
Epoch 6/10, Batch 400/442, Training Loss: 0.4254
Epoch 6/10, Batch 401/442, Training Loss: 0.6748
Epoch 6/10, Batch 402/442, Training Loss: 0.6786
Epoch 6/10, Batch 403/442, Training Loss: 0.3224
Epoch 6/10, Batch 404/442, Training Loss: 0.4057
Epoch 6/10, Batch 405/442, Training Loss: 0.6149
Epoch 6/10, Batch 406/442, Training Loss: 0.4653
Epoch 6/10, Batch 407/442, Training Loss: 0.5898
Epoch 6/10, Batch 408/442, Training Loss: 0.5764
Epoch 6/10, Batch 409/442, Training Loss: 0.6299
Epoch 6/10, Batch 410/442, Training Loss: 0.4687
Epoch 6/10, Batch 411/442, Training Loss: 0.4339
Epoch 6/10, Batch 412/442, Training Loss: 0.7177
Epoch 6/10, Batch 413/442, Training Loss: 0.4381
Epoch 6/10, Batch 414/442, Training Loss: 0.7240
Epoch 6/10, Batch 415/442, Training Loss: 0.5399
Epoch 6/10, Batch 416/442, Training Loss: 0.5859
Epoch 6/10, Batch 417/442, Training Loss: 0.3695
Epoch 6/10, Batch 418/442, Training Loss: 0.5265
Epoch 6/10, Batch 419/442, Training Loss: 0.3920
Epoch 6/10, Batch 420/442, Training Loss: 0.5056
Epoch 6/10, Batch 421/442, Training Loss: 0.7239
Epoch 6/10, Batch 422/442, Training Loss: 0.5407
Epoch 6/10, Batch 423/442, Training Loss: 0.6770
Epoch 6/10, Batch 424/442, Training Loss: 0.6085
Epoch 6/10, Batch 425/442, Training Loss: 0.4328
Epoch 6/10, Batch 426/442, Training Loss: 0.5919
Epoch 6/10, Batch 427/442, Training Loss: 0.5351
Epoch 6/10, Batch 428/442, Training Loss: 0.5230
Epoch 6/10, Batch 429/442, Training Loss: 0.5287
Epoch 6/10, Batch 430/442, Training Loss: 0.6220
Epoch 6/10, Batch 431/442, Training Loss: 0.4566
Epoch 6/10, Batch 432/442, Training Loss: 0.7026
Epoch 6/10, Batch 433/442, Training Loss: 1.0149
Epoch 6/10, Batch 434/442, Training Loss: 0.5606
Epoch 6/10, Batch 435/442, Training Loss: 0.5931
Epoch 6/10, Batch 436/442, Training Loss: 0.8306
Epoch 6/10, Batch 437/442, Training Loss: 0.5811
Epoch 6/10, Batch 438/442, Training Loss: 0.4881
Epoch 6/10, Batch 439/442, Training Loss: 0.7887
Epoch 6/10, Batch 440/442, Training Loss: 0.7546
Epoch 6/10, Batch 441/442, Training Loss: 0.6700
Epoch 6/10, Batch 442/442, Training Loss: 0.6351
Epoch 6/10, Training Loss: 0.5773, Validation Loss: 0.7068, Validation Accuracy: 0.6667
Epoch 7/10, Batch 1/442, Training Loss: 0.5462
Epoch 7/10, Batch 2/442, Training Loss: 0.6153
Epoch 7/10, Batch 3/442, Training Loss: 0.5825
Epoch 7/10, Batch 4/442, Training Loss: 0.5077
Epoch 7/10, Batch 5/442, Training Loss: 0.3920
Epoch 7/10, Batch 6/442, Training Loss: 0.4457
Epoch 7/10, Batch 7/442, Training Loss: 0.7257
Epoch 7/10, Batch 8/442, Training Loss: 0.4980
Epoch 7/10, Batch 9/442, Training Loss: 0.8305
Epoch 7/10, Batch 10/442, Training Loss: 0.6720
Epoch 7/10, Batch 11/442, Training Loss: 0.5179
Epoch 7/10, Batch 12/442, Training Loss: 0.5499
Epoch 7/10, Batch 13/442, Training Loss: 0.6077
Epoch 7/10, Batch 14/442, Training Loss: 0.5152
Epoch 7/10, Batch 15/442, Training Loss: 0.5290
Epoch 7/10, Batch 16/442, Training Loss: 0.5417
Epoch 7/10, Batch 17/442, Training Loss: 0.5210
Epoch 7/10, Batch 18/442, Training Loss: 0.6078
Epoch 7/10, Batch 19/442, Training Loss: 0.4913
Epoch 7/10, Batch 20/442, Training Loss: 0.6286
Epoch 7/10, Batch 21/442, Training Loss: 0.7939
Epoch 7/10, Batch 22/442, Training Loss: 0.3922
Epoch 7/10, Batch 23/442, Training Loss: 0.7137
Epoch 7/10, Batch 24/442, Training Loss: 0.6865
Epoch 7/10, Batch 25/442, Training Loss: 0.5697
Epoch 7/10, Batch 26/442, Training Loss: 0.5184
Epoch 7/10, Batch 27/442, Training Loss: 0.3858
Epoch 7/10, Batch 28/442, Training Loss: 0.4743
Epoch 7/10, Batch 29/442, Training Loss: 0.4172
Epoch 7/10, Batch 30/442, Training Loss: 0.5269
Epoch 7/10, Batch 31/442, Training Loss: 0.5719
Epoch 7/10, Batch 32/442, Training Loss: 0.4706
Epoch 7/10, Batch 33/442, Training Loss: 0.6701
Epoch 7/10, Batch 34/442, Training Loss: 0.4710
Epoch 7/10, Batch 35/442, Training Loss: 0.3407
Epoch 7/10, Batch 36/442, Training Loss: 0.4921
Epoch 7/10, Batch 37/442, Training Loss: 0.5579
Epoch 7/10, Batch 38/442, Training Loss: 0.5048
Epoch 7/10, Batch 39/442, Training Loss: 0.3627
Epoch 7/10, Batch 40/442, Training Loss: 0.4851
Epoch 7/10, Batch 41/442, Training Loss: 0.6486
Epoch 7/10, Batch 42/442, Training Loss: 0.5052
Epoch 7/10, Batch 43/442, Training Loss: 0.4030
Epoch 7/10, Batch 44/442, Training Loss: 0.6419
Epoch 7/10, Batch 45/442, Training Loss: 0.7753
Epoch 7/10, Batch 46/442, Training Loss: 0.6358
Epoch 7/10, Batch 47/442, Training Loss: 0.5247
Epoch 7/10, Batch 48/442, Training Loss: 0.6753
Epoch 7/10, Batch 49/442, Training Loss: 0.4155
Epoch 7/10, Batch 50/442, Training Loss: 0.4046
Epoch 7/10, Batch 51/442, Training Loss: 0.4137
Epoch 7/10, Batch 52/442, Training Loss: 0.4958
Epoch 7/10, Batch 53/442, Training Loss: 0.3703
Epoch 7/10, Batch 54/442, Training Loss: 0.2533
Epoch 7/10, Batch 55/442, Training Loss: 0.4444
Epoch 7/10, Batch 56/442, Training Loss: 0.7387
Epoch 7/10, Batch 57/442, Training Loss: 0.5807
Epoch 7/10, Batch 58/442, Training Loss: 0.6185
Epoch 7/10, Batch 59/442, Training Loss: 0.3174
Epoch 7/10, Batch 60/442, Training Loss: 0.5042
Epoch 7/10, Batch 61/442, Training Loss: 0.4318
Epoch 7/10, Batch 62/442, Training Loss: 0.4622
Epoch 7/10, Batch 63/442, Training Loss: 0.4002
Epoch 7/10, Batch 64/442, Training Loss: 0.6931
Epoch 7/10, Batch 65/442, Training Loss: 0.3752
Epoch 7/10, Batch 66/442, Training Loss: 0.4680
Epoch 7/10, Batch 67/442, Training Loss: 0.6155
Epoch 7/10, Batch 68/442, Training Loss: 0.6324
Epoch 7/10, Batch 69/442, Training Loss: 0.4167
Epoch 7/10, Batch 70/442, Training Loss: 0.4409
Epoch 7/10, Batch 71/442, Training Loss: 0.5229
Epoch 7/10, Batch 72/442, Training Loss: 0.4925
Epoch 7/10, Batch 73/442, Training Loss: 0.4664
Epoch 7/10, Batch 74/442, Training Loss: 0.4904
Epoch 7/10, Batch 75/442, Training Loss: 0.4295
Epoch 7/10, Batch 76/442, Training Loss: 0.5574
Epoch 7/10, Batch 77/442, Training Loss: 0.5348
Epoch 7/10, Batch 78/442, Training Loss: 0.6427
Epoch 7/10, Batch 79/442, Training Loss: 0.4423
Epoch 7/10, Batch 80/442, Training Loss: 0.4570
Epoch 7/10, Batch 81/442, Training Loss: 0.4392
Epoch 7/10, Batch 82/442, Training Loss: 0.6014
Epoch 7/10, Batch 83/442, Training Loss: 0.7691
Epoch 7/10, Batch 84/442, Training Loss: 0.4286
Epoch 7/10, Batch 85/442, Training Loss: 0.6976
Epoch 7/10, Batch 86/442, Training Loss: 0.5176
Epoch 7/10, Batch 87/442, Training Loss: 0.3931
Epoch 7/10, Batch 88/442, Training Loss: 0.6912
Epoch 7/10, Batch 89/442, Training Loss: 0.3658
Epoch 7/10, Batch 90/442, Training Loss: 0.6166
Epoch 7/10, Batch 91/442, Training Loss: 0.5501
Epoch 7/10, Batch 92/442, Training Loss: 0.7088
Epoch 7/10, Batch 93/442, Training Loss: 0.6233
Epoch 7/10, Batch 94/442, Training Loss: 0.5700
Epoch 7/10, Batch 95/442, Training Loss: 0.4416
Epoch 7/10, Batch 96/442, Training Loss: 0.5817
Epoch 7/10, Batch 97/442, Training Loss: 0.4448
Epoch 7/10, Batch 98/442, Training Loss: 0.4846
Epoch 7/10, Batch 99/442, Training Loss: 0.4343
Epoch 7/10, Batch 100/442, Training Loss: 0.5328
Epoch 7/10, Batch 101/442, Training Loss: 0.5670
Epoch 7/10, Batch 102/442, Training Loss: 0.4542
Epoch 7/10, Batch 103/442, Training Loss: 0.4958
Epoch 7/10, Batch 104/442, Training Loss: 0.4260
Epoch 7/10, Batch 105/442, Training Loss: 0.4690
Epoch 7/10, Batch 106/442, Training Loss: 0.4084
Epoch 7/10, Batch 107/442, Training Loss: 0.3935
Epoch 7/10, Batch 108/442, Training Loss: 0.5823
Epoch 7/10, Batch 109/442, Training Loss: 0.5429
Epoch 7/10, Batch 110/442, Training Loss: 0.5534
Epoch 7/10, Batch 111/442, Training Loss: 0.3296
Epoch 7/10, Batch 112/442, Training Loss: 0.6860
Epoch 7/10, Batch 113/442, Training Loss: 0.4524
Epoch 7/10, Batch 114/442, Training Loss: 0.5879
Epoch 7/10, Batch 115/442, Training Loss: 0.7942
Epoch 7/10, Batch 116/442, Training Loss: 0.4624
Epoch 7/10, Batch 117/442, Training Loss: 0.5560
Epoch 7/10, Batch 118/442, Training Loss: 0.5028
Epoch 7/10, Batch 119/442, Training Loss: 0.4111
Epoch 7/10, Batch 120/442, Training Loss: 0.4346
Epoch 7/10, Batch 121/442, Training Loss: 0.5519
Epoch 7/10, Batch 122/442, Training Loss: 0.5741
Epoch 7/10, Batch 123/442, Training Loss: 0.7335
Epoch 7/10, Batch 124/442, Training Loss: 0.5251
Epoch 7/10, Batch 125/442, Training Loss: 0.5734
Epoch 7/10, Batch 126/442, Training Loss: 0.3632
Epoch 7/10, Batch 127/442, Training Loss: 0.5215
Epoch 7/10, Batch 128/442, Training Loss: 0.4695
Epoch 7/10, Batch 129/442, Training Loss: 0.6315
Epoch 7/10, Batch 130/442, Training Loss: 0.6764
Epoch 7/10, Batch 131/442, Training Loss: 0.4141
Epoch 7/10, Batch 132/442, Training Loss: 0.6083
Epoch 7/10, Batch 133/442, Training Loss: 0.5888
Epoch 7/10, Batch 134/442, Training Loss: 0.7921
Epoch 7/10, Batch 135/442, Training Loss: 0.7073
Epoch 7/10, Batch 136/442, Training Loss: 0.7123
Epoch 7/10, Batch 137/442, Training Loss: 0.4560
Epoch 7/10, Batch 138/442, Training Loss: 0.3644
Epoch 7/10, Batch 139/442, Training Loss: 0.6174
Epoch 7/10, Batch 140/442, Training Loss: 0.4534
Epoch 7/10, Batch 141/442, Training Loss: 0.4888
Epoch 7/10, Batch 142/442, Training Loss: 0.4862
Epoch 7/10, Batch 143/442, Training Loss: 0.4288
Epoch 7/10, Batch 144/442, Training Loss: 0.5176
Epoch 7/10, Batch 145/442, Training Loss: 0.3811
Epoch 7/10, Batch 146/442, Training Loss: 0.5222
Epoch 7/10, Batch 147/442, Training Loss: 0.5359
Epoch 7/10, Batch 148/442, Training Loss: 0.5963
Epoch 7/10, Batch 149/442, Training Loss: 0.3525
Epoch 7/10, Batch 150/442, Training Loss: 0.6369
Epoch 7/10, Batch 151/442, Training Loss: 0.5735
Epoch 7/10, Batch 152/442, Training Loss: 0.6098
Epoch 7/10, Batch 153/442, Training Loss: 0.6645
Epoch 7/10, Batch 154/442, Training Loss: 0.6243
Epoch 7/10, Batch 155/442, Training Loss: 0.4670
Epoch 7/10, Batch 156/442, Training Loss: 0.6600
Epoch 7/10, Batch 157/442, Training Loss: 0.4871
Epoch 7/10, Batch 158/442, Training Loss: 0.9250
Epoch 7/10, Batch 159/442, Training Loss: 0.7394
Epoch 7/10, Batch 160/442, Training Loss: 0.7113
Epoch 7/10, Batch 161/442, Training Loss: 0.5083
Epoch 7/10, Batch 162/442, Training Loss: 0.5413
Epoch 7/10, Batch 163/442, Training Loss: 0.6068
Epoch 7/10, Batch 164/442, Training Loss: 0.5608
Epoch 7/10, Batch 165/442, Training Loss: 0.5503
Epoch 7/10, Batch 166/442, Training Loss: 0.6303
Epoch 7/10, Batch 167/442, Training Loss: 0.4695
Epoch 7/10, Batch 168/442, Training Loss: 0.6849
Epoch 7/10, Batch 169/442, Training Loss: 0.3829
Epoch 7/10, Batch 170/442, Training Loss: 0.5670
Epoch 7/10, Batch 171/442, Training Loss: 0.5577
Epoch 7/10, Batch 172/442, Training Loss: 0.5261
Epoch 7/10, Batch 173/442, Training Loss: 0.4126
Epoch 7/10, Batch 174/442, Training Loss: 0.4627
Epoch 7/10, Batch 175/442, Training Loss: 0.3728
Epoch 7/10, Batch 176/442, Training Loss: 0.5348
Epoch 7/10, Batch 177/442, Training Loss: 0.3454
Epoch 7/10, Batch 178/442, Training Loss: 0.4979
Epoch 7/10, Batch 179/442, Training Loss: 0.4730
Epoch 7/10, Batch 180/442, Training Loss: 0.4085
Epoch 7/10, Batch 181/442, Training Loss: 0.5340
Epoch 7/10, Batch 182/442, Training Loss: 0.5875
Epoch 7/10, Batch 183/442, Training Loss: 0.4324
Epoch 7/10, Batch 184/442, Training Loss: 0.4000
Epoch 7/10, Batch 185/442, Training Loss: 0.5068
Epoch 7/10, Batch 186/442, Training Loss: 0.7757
Epoch 7/10, Batch 187/442, Training Loss: 0.4637
Epoch 7/10, Batch 188/442, Training Loss: 0.4480
Epoch 7/10, Batch 189/442, Training Loss: 0.5957
Epoch 7/10, Batch 190/442, Training Loss: 0.4177
Epoch 7/10, Batch 191/442, Training Loss: 0.3733
Epoch 7/10, Batch 192/442, Training Loss: 0.3462
Epoch 7/10, Batch 193/442, Training Loss: 0.3993
Epoch 7/10, Batch 194/442, Training Loss: 0.4964
Epoch 7/10, Batch 195/442, Training Loss: 0.3429
Epoch 7/10, Batch 196/442, Training Loss: 0.4377
Epoch 7/10, Batch 197/442, Training Loss: 0.4753
Epoch 7/10, Batch 198/442, Training Loss: 0.5128
Epoch 7/10, Batch 199/442, Training Loss: 0.5278
Epoch 7/10, Batch 200/442, Training Loss: 0.6068
Epoch 7/10, Batch 201/442, Training Loss: 0.3610
Epoch 7/10, Batch 202/442, Training Loss: 0.5331
Epoch 7/10, Batch 203/442, Training Loss: 0.6045
Epoch 7/10, Batch 204/442, Training Loss: 0.4282
Epoch 7/10, Batch 205/442, Training Loss: 0.4078
Epoch 7/10, Batch 206/442, Training Loss: 0.2273
Epoch 7/10, Batch 207/442, Training Loss: 0.4445
Epoch 7/10, Batch 208/442, Training Loss: 0.5278
Epoch 7/10, Batch 209/442, Training Loss: 0.4653
Epoch 7/10, Batch 210/442, Training Loss: 0.3967
Epoch 7/10, Batch 211/442, Training Loss: 0.5385
Epoch 7/10, Batch 212/442, Training Loss: 0.6674
Epoch 7/10, Batch 213/442, Training Loss: 0.7683
Epoch 7/10, Batch 214/442, Training Loss: 0.2871
Epoch 7/10, Batch 215/442, Training Loss: 0.6704
Epoch 7/10, Batch 216/442, Training Loss: 0.8730
Epoch 7/10, Batch 217/442, Training Loss: 0.4222
Epoch 7/10, Batch 218/442, Training Loss: 0.6476
Epoch 7/10, Batch 219/442, Training Loss: 0.2801
Epoch 7/10, Batch 220/442, Training Loss: 0.3729
Epoch 7/10, Batch 221/442, Training Loss: 0.4907
Epoch 7/10, Batch 222/442, Training Loss: 0.5461
Epoch 7/10, Batch 223/442, Training Loss: 0.4843
Epoch 7/10, Batch 224/442, Training Loss: 0.4663
Epoch 7/10, Batch 225/442, Training Loss: 0.3647
Epoch 7/10, Batch 226/442, Training Loss: 0.5329
Epoch 7/10, Batch 227/442, Training Loss: 0.6378
Epoch 7/10, Batch 228/442, Training Loss: 0.5860
Epoch 7/10, Batch 229/442, Training Loss: 0.4272
Epoch 7/10, Batch 230/442, Training Loss: 0.4689
Epoch 7/10, Batch 231/442, Training Loss: 0.4850
Epoch 7/10, Batch 232/442, Training Loss: 0.5432
Epoch 7/10, Batch 233/442, Training Loss: 0.5956
Epoch 7/10, Batch 234/442, Training Loss: 0.6275
Epoch 7/10, Batch 235/442, Training Loss: 0.3859
Epoch 7/10, Batch 236/442, Training Loss: 0.7721
Epoch 7/10, Batch 237/442, Training Loss: 0.7296
Epoch 7/10, Batch 238/442, Training Loss: 0.7393
Epoch 7/10, Batch 239/442, Training Loss: 0.3893
Epoch 7/10, Batch 240/442, Training Loss: 0.5081
Epoch 7/10, Batch 241/442, Training Loss: 0.5507
Epoch 7/10, Batch 242/442, Training Loss: 0.3709
Epoch 7/10, Batch 243/442, Training Loss: 0.4793
Epoch 7/10, Batch 244/442, Training Loss: 0.5553
Epoch 7/10, Batch 245/442, Training Loss: 0.5266
Epoch 7/10, Batch 246/442, Training Loss: 0.6808
Epoch 7/10, Batch 247/442, Training Loss: 0.5292
Epoch 7/10, Batch 248/442, Training Loss: 0.4310
Epoch 7/10, Batch 249/442, Training Loss: 0.4141
Epoch 7/10, Batch 250/442, Training Loss: 0.6995
Epoch 7/10, Batch 251/442, Training Loss: 0.5399
Epoch 7/10, Batch 252/442, Training Loss: 0.5341
Epoch 7/10, Batch 253/442, Training Loss: 0.3793
Epoch 7/10, Batch 254/442, Training Loss: 0.6436
Epoch 7/10, Batch 255/442, Training Loss: 0.4093
Epoch 7/10, Batch 256/442, Training Loss: 0.6279
Epoch 7/10, Batch 257/442, Training Loss: 0.5504
Epoch 7/10, Batch 258/442, Training Loss: 0.6747
Epoch 7/10, Batch 259/442, Training Loss: 0.5000
Epoch 7/10, Batch 260/442, Training Loss: 0.5217
Epoch 7/10, Batch 261/442, Training Loss: 0.3688
Epoch 7/10, Batch 262/442, Training Loss: 0.5663
Epoch 7/10, Batch 263/442, Training Loss: 0.6120
Epoch 7/10, Batch 264/442, Training Loss: 0.9058
Epoch 7/10, Batch 265/442, Training Loss: 0.4737
Epoch 7/10, Batch 266/442, Training Loss: 0.4216
Epoch 7/10, Batch 267/442, Training Loss: 0.5615
Epoch 7/10, Batch 268/442, Training Loss: 0.5336
Epoch 7/10, Batch 269/442, Training Loss: 0.5568
Epoch 7/10, Batch 270/442, Training Loss: 0.4147
Epoch 7/10, Batch 271/442, Training Loss: 0.4978
Epoch 7/10, Batch 272/442, Training Loss: 0.7207
Epoch 7/10, Batch 273/442, Training Loss: 0.5600
Epoch 7/10, Batch 274/442, Training Loss: 0.3488
Epoch 7/10, Batch 275/442, Training Loss: 0.4361
Epoch 7/10, Batch 276/442, Training Loss: 0.3999
Epoch 7/10, Batch 277/442, Training Loss: 0.5564
Epoch 7/10, Batch 278/442, Training Loss: 0.4026
Epoch 7/10, Batch 279/442, Training Loss: 0.8681
Epoch 7/10, Batch 280/442, Training Loss: 0.3533
Epoch 7/10, Batch 281/442, Training Loss: 0.7860
Epoch 7/10, Batch 282/442, Training Loss: 0.4831
Epoch 7/10, Batch 283/442, Training Loss: 0.6933
Epoch 7/10, Batch 284/442, Training Loss: 0.5272
Epoch 7/10, Batch 285/442, Training Loss: 0.6779
Epoch 7/10, Batch 286/442, Training Loss: 0.4219
Epoch 7/10, Batch 287/442, Training Loss: 0.3963
Epoch 7/10, Batch 288/442, Training Loss: 0.9369
Epoch 7/10, Batch 289/442, Training Loss: 0.6493
Epoch 7/10, Batch 290/442, Training Loss: 0.3819
Epoch 7/10, Batch 291/442, Training Loss: 0.7928
Epoch 7/10, Batch 292/442, Training Loss: 0.5814
Epoch 7/10, Batch 293/442, Training Loss: 0.5737
Epoch 7/10, Batch 294/442, Training Loss: 0.5057
Epoch 7/10, Batch 295/442, Training Loss: 0.4959
Epoch 7/10, Batch 296/442, Training Loss: 0.4776
Epoch 7/10, Batch 297/442, Training Loss: 0.4574
Epoch 7/10, Batch 298/442, Training Loss: 0.4944
Epoch 7/10, Batch 299/442, Training Loss: 0.6581
Epoch 7/10, Batch 300/442, Training Loss: 0.5111
Epoch 7/10, Batch 301/442, Training Loss: 0.4296
Epoch 7/10, Batch 302/442, Training Loss: 0.7052
Epoch 7/10, Batch 303/442, Training Loss: 0.5385
Epoch 7/10, Batch 304/442, Training Loss: 0.6736
Epoch 7/10, Batch 305/442, Training Loss: 0.3651
Epoch 7/10, Batch 306/442, Training Loss: 0.5347
Epoch 7/10, Batch 307/442, Training Loss: 0.3794
Epoch 7/10, Batch 308/442, Training Loss: 0.5385
Epoch 7/10, Batch 309/442, Training Loss: 0.5972
Epoch 7/10, Batch 310/442, Training Loss: 0.4606
Epoch 7/10, Batch 311/442, Training Loss: 0.3711
Epoch 7/10, Batch 312/442, Training Loss: 0.5274
Epoch 7/10, Batch 313/442, Training Loss: 0.6860
Epoch 7/10, Batch 314/442, Training Loss: 0.7331
Epoch 7/10, Batch 315/442, Training Loss: 0.5558
Epoch 7/10, Batch 316/442, Training Loss: 0.4604
Epoch 7/10, Batch 317/442, Training Loss: 0.6473
Epoch 7/10, Batch 318/442, Training Loss: 0.6489
Epoch 7/10, Batch 319/442, Training Loss: 0.3812
Epoch 7/10, Batch 320/442, Training Loss: 0.4049
Epoch 7/10, Batch 321/442, Training Loss: 0.3936
Epoch 7/10, Batch 322/442, Training Loss: 0.5099
Epoch 7/10, Batch 323/442, Training Loss: 0.7003
Epoch 7/10, Batch 324/442, Training Loss: 0.4230
Epoch 7/10, Batch 325/442, Training Loss: 0.5328
Epoch 7/10, Batch 326/442, Training Loss: 0.5745
Epoch 7/10, Batch 327/442, Training Loss: 0.3784
Epoch 7/10, Batch 328/442, Training Loss: 0.4384
Epoch 7/10, Batch 329/442, Training Loss: 0.6875
Epoch 7/10, Batch 330/442, Training Loss: 0.4777
Epoch 7/10, Batch 331/442, Training Loss: 0.6131
Epoch 7/10, Batch 332/442, Training Loss: 0.5904
Epoch 7/10, Batch 333/442, Training Loss: 0.6649
Epoch 7/10, Batch 334/442, Training Loss: 0.6739
Epoch 7/10, Batch 335/442, Training Loss: 0.5238
Epoch 7/10, Batch 336/442, Training Loss: 0.4367
Epoch 7/10, Batch 337/442, Training Loss: 0.3763
Epoch 7/10, Batch 338/442, Training Loss: 0.5237
Epoch 7/10, Batch 339/442, Training Loss: 0.6665
Epoch 7/10, Batch 340/442, Training Loss: 0.5182
Epoch 7/10, Batch 341/442, Training Loss: 0.5700
Epoch 7/10, Batch 342/442, Training Loss: 0.3639
Epoch 7/10, Batch 343/442, Training Loss: 0.5703
Epoch 7/10, Batch 344/442, Training Loss: 0.6289
Epoch 7/10, Batch 345/442, Training Loss: 0.5103
Epoch 7/10, Batch 346/442, Training Loss: 0.5906
Epoch 7/10, Batch 347/442, Training Loss: 0.5031
Epoch 7/10, Batch 348/442, Training Loss: 0.6021
Epoch 7/10, Batch 349/442, Training Loss: 0.5762
Epoch 7/10, Batch 350/442, Training Loss: 0.5805
Epoch 7/10, Batch 351/442, Training Loss: 0.5376
Epoch 7/10, Batch 352/442, Training Loss: 0.5215
Epoch 7/10, Batch 353/442, Training Loss: 0.4878
Epoch 7/10, Batch 354/442, Training Loss: 0.7578
Epoch 7/10, Batch 355/442, Training Loss: 0.6820
Epoch 7/10, Batch 356/442, Training Loss: 0.3697
Epoch 7/10, Batch 357/442, Training Loss: 0.6099
Epoch 7/10, Batch 358/442, Training Loss: 0.3846
Epoch 7/10, Batch 359/442, Training Loss: 0.4468
Epoch 7/10, Batch 360/442, Training Loss: 0.4851
Epoch 7/10, Batch 361/442, Training Loss: 0.5600
Epoch 7/10, Batch 362/442, Training Loss: 0.3538
Epoch 7/10, Batch 363/442, Training Loss: 0.3877
Epoch 7/10, Batch 364/442, Training Loss: 0.4447
Epoch 7/10, Batch 365/442, Training Loss: 0.5093
Epoch 7/10, Batch 366/442, Training Loss: 0.4173
Epoch 7/10, Batch 367/442, Training Loss: 0.6029
Epoch 7/10, Batch 368/442, Training Loss: 0.5013
Epoch 7/10, Batch 369/442, Training Loss: 0.5700
Epoch 7/10, Batch 370/442, Training Loss: 0.4910
Epoch 7/10, Batch 371/442, Training Loss: 0.8416
Epoch 7/10, Batch 372/442, Training Loss: 0.4254
Epoch 7/10, Batch 373/442, Training Loss: 0.4592
Epoch 7/10, Batch 374/442, Training Loss: 0.6673
Epoch 7/10, Batch 375/442, Training Loss: 0.5539
Epoch 7/10, Batch 376/442, Training Loss: 0.4347
Epoch 7/10, Batch 377/442, Training Loss: 0.3717
Epoch 7/10, Batch 378/442, Training Loss: 0.5542
Epoch 7/10, Batch 379/442, Training Loss: 0.4754
Epoch 7/10, Batch 380/442, Training Loss: 0.3789
Epoch 7/10, Batch 381/442, Training Loss: 0.4570
Epoch 7/10, Batch 382/442, Training Loss: 0.6088
Epoch 7/10, Batch 383/442, Training Loss: 0.3842
Epoch 7/10, Batch 384/442, Training Loss: 0.3753
Epoch 7/10, Batch 385/442, Training Loss: 0.6831
Epoch 7/10, Batch 386/442, Training Loss: 0.2935
Epoch 7/10, Batch 387/442, Training Loss: 0.4177
Epoch 7/10, Batch 388/442, Training Loss: 0.3512
Epoch 7/10, Batch 389/442, Training Loss: 0.5884
Epoch 7/10, Batch 390/442, Training Loss: 0.6636
Epoch 7/10, Batch 391/442, Training Loss: 0.4691
Epoch 7/10, Batch 392/442, Training Loss: 0.3125
Epoch 7/10, Batch 393/442, Training Loss: 0.6352
Epoch 7/10, Batch 394/442, Training Loss: 0.4907
Epoch 7/10, Batch 395/442, Training Loss: 0.4932
Epoch 7/10, Batch 396/442, Training Loss: 0.3626
Epoch 7/10, Batch 397/442, Training Loss: 0.4878
Epoch 7/10, Batch 398/442, Training Loss: 0.5103
Epoch 7/10, Batch 399/442, Training Loss: 0.4215
Epoch 7/10, Batch 400/442, Training Loss: 0.5490
Epoch 7/10, Batch 401/442, Training Loss: 0.4360
Epoch 7/10, Batch 402/442, Training Loss: 0.5541
Epoch 7/10, Batch 403/442, Training Loss: 0.4545
Epoch 7/10, Batch 404/442, Training Loss: 0.3610
Epoch 7/10, Batch 405/442, Training Loss: 0.3814
Epoch 7/10, Batch 406/442, Training Loss: 0.3411
Epoch 7/10, Batch 407/442, Training Loss: 0.4588
Epoch 7/10, Batch 408/442, Training Loss: 0.3747
Epoch 7/10, Batch 409/442, Training Loss: 0.4552
Epoch 7/10, Batch 410/442, Training Loss: 0.5039
Epoch 7/10, Batch 411/442, Training Loss: 0.5462
Epoch 7/10, Batch 412/442, Training Loss: 0.5019
Epoch 7/10, Batch 413/442, Training Loss: 0.8029
Epoch 7/10, Batch 414/442, Training Loss: 0.6016
Epoch 7/10, Batch 415/442, Training Loss: 0.4095
Epoch 7/10, Batch 416/442, Training Loss: 0.4344
Epoch 7/10, Batch 417/442, Training Loss: 0.5429
Epoch 7/10, Batch 418/442, Training Loss: 0.5374
Epoch 7/10, Batch 419/442, Training Loss: 0.4117
Epoch 7/10, Batch 420/442, Training Loss: 0.4154
Epoch 7/10, Batch 421/442, Training Loss: 0.2529
Epoch 7/10, Batch 422/442, Training Loss: 0.2776
Epoch 7/10, Batch 423/442, Training Loss: 0.5137
Epoch 7/10, Batch 424/442, Training Loss: 0.7942
Epoch 7/10, Batch 425/442, Training Loss: 0.5747
Epoch 7/10, Batch 426/442, Training Loss: 0.6739
Epoch 7/10, Batch 427/442, Training Loss: 0.4873
Epoch 7/10, Batch 428/442, Training Loss: 0.5463
Epoch 7/10, Batch 429/442, Training Loss: 0.5355
Epoch 7/10, Batch 430/442, Training Loss: 0.7453
Epoch 7/10, Batch 431/442, Training Loss: 0.4525
Epoch 7/10, Batch 432/442, Training Loss: 0.3224
Epoch 7/10, Batch 433/442, Training Loss: 0.3990
Epoch 7/10, Batch 434/442, Training Loss: 0.4620
Epoch 7/10, Batch 435/442, Training Loss: 0.4145
Epoch 7/10, Batch 436/442, Training Loss: 0.6230
Epoch 7/10, Batch 437/442, Training Loss: 0.5414
Epoch 7/10, Batch 438/442, Training Loss: 0.4646
Epoch 7/10, Batch 439/442, Training Loss: 0.3615
Epoch 7/10, Batch 440/442, Training Loss: 0.7776
Epoch 7/10, Batch 441/442, Training Loss: 0.4691
Epoch 7/10, Batch 442/442, Training Loss: 0.2351
Epoch 7/10, Training Loss: 0.5228, Validation Loss: 0.6343, Validation Accuracy: 0.7470
Epoch 8/10, Batch 1/442, Training Loss: 0.4173
Epoch 8/10, Batch 2/442, Training Loss: 0.4739
Epoch 8/10, Batch 3/442, Training Loss: 0.4697
Epoch 8/10, Batch 4/442, Training Loss: 0.4977
Epoch 8/10, Batch 5/442, Training Loss: 0.6582
Epoch 8/10, Batch 6/442, Training Loss: 0.4649
Epoch 8/10, Batch 7/442, Training Loss: 0.5799
Epoch 8/10, Batch 8/442, Training Loss: 0.3912
Epoch 8/10, Batch 9/442, Training Loss: 0.4677
Epoch 8/10, Batch 10/442, Training Loss: 0.5648
Epoch 8/10, Batch 11/442, Training Loss: 0.2337
Epoch 8/10, Batch 12/442, Training Loss: 0.5566
Epoch 8/10, Batch 13/442, Training Loss: 0.6491
Epoch 8/10, Batch 14/442, Training Loss: 0.2795
Epoch 8/10, Batch 15/442, Training Loss: 0.4301
Epoch 8/10, Batch 16/442, Training Loss: 0.5767
Epoch 8/10, Batch 17/442, Training Loss: 0.4930
Epoch 8/10, Batch 18/442, Training Loss: 0.4402
Epoch 8/10, Batch 19/442, Training Loss: 0.5010
Epoch 8/10, Batch 20/442, Training Loss: 0.4913
Epoch 8/10, Batch 21/442, Training Loss: 0.5018
Epoch 8/10, Batch 22/442, Training Loss: 0.7729
Epoch 8/10, Batch 23/442, Training Loss: 0.7384
Epoch 8/10, Batch 24/442, Training Loss: 0.2767
Epoch 8/10, Batch 25/442, Training Loss: 0.4083
Epoch 8/10, Batch 26/442, Training Loss: 0.4851
Epoch 8/10, Batch 27/442, Training Loss: 0.6342
Epoch 8/10, Batch 28/442, Training Loss: 0.6235
Epoch 8/10, Batch 29/442, Training Loss: 0.2329
Epoch 8/10, Batch 30/442, Training Loss: 0.5237
Epoch 8/10, Batch 31/442, Training Loss: 0.4921
Epoch 8/10, Batch 32/442, Training Loss: 0.4707
Epoch 8/10, Batch 33/442, Training Loss: 0.4205
Epoch 8/10, Batch 34/442, Training Loss: 0.4510
Epoch 8/10, Batch 35/442, Training Loss: 0.4385
Epoch 8/10, Batch 36/442, Training Loss: 0.4465
Epoch 8/10, Batch 37/442, Training Loss: 0.5017
Epoch 8/10, Batch 38/442, Training Loss: 0.5239
Epoch 8/10, Batch 39/442, Training Loss: 0.5027
Epoch 8/10, Batch 40/442, Training Loss: 0.5001
Epoch 8/10, Batch 41/442, Training Loss: 0.6029
Epoch 8/10, Batch 42/442, Training Loss: 0.3549
Epoch 8/10, Batch 43/442, Training Loss: 0.5223
Epoch 8/10, Batch 44/442, Training Loss: 0.4627
Epoch 8/10, Batch 45/442, Training Loss: 0.4619
Epoch 8/10, Batch 46/442, Training Loss: 0.4873
Epoch 8/10, Batch 47/442, Training Loss: 0.4756
Epoch 8/10, Batch 48/442, Training Loss: 0.4787
Epoch 8/10, Batch 49/442, Training Loss: 0.8156
Epoch 8/10, Batch 50/442, Training Loss: 0.3189
Epoch 8/10, Batch 51/442, Training Loss: 0.6134
Epoch 8/10, Batch 52/442, Training Loss: 0.4355
Epoch 8/10, Batch 53/442, Training Loss: 0.5374
Epoch 8/10, Batch 54/442, Training Loss: 0.4063
Epoch 8/10, Batch 55/442, Training Loss: 0.4733
Epoch 8/10, Batch 56/442, Training Loss: 0.6786
Epoch 8/10, Batch 57/442, Training Loss: 0.3779
Epoch 8/10, Batch 58/442, Training Loss: 0.4113
Epoch 8/10, Batch 59/442, Training Loss: 0.3923
Epoch 8/10, Batch 60/442, Training Loss: 0.3502
Epoch 8/10, Batch 61/442, Training Loss: 0.4274
Epoch 8/10, Batch 62/442, Training Loss: 0.4853
Epoch 8/10, Batch 63/442, Training Loss: 0.5495
Epoch 8/10, Batch 64/442, Training Loss: 0.4220
Epoch 8/10, Batch 65/442, Training Loss: 0.5906
Epoch 8/10, Batch 66/442, Training Loss: 0.7098
Epoch 8/10, Batch 67/442, Training Loss: 0.4683
Epoch 8/10, Batch 68/442, Training Loss: 0.4699
Epoch 8/10, Batch 69/442, Training Loss: 0.3883
Epoch 8/10, Batch 70/442, Training Loss: 0.5066
Epoch 8/10, Batch 71/442, Training Loss: 0.6362
Epoch 8/10, Batch 72/442, Training Loss: 0.8141
Epoch 8/10, Batch 73/442, Training Loss: 0.5116
Epoch 8/10, Batch 74/442, Training Loss: 0.4465
Epoch 8/10, Batch 75/442, Training Loss: 0.7372
Epoch 8/10, Batch 76/442, Training Loss: 0.4854
Epoch 8/10, Batch 77/442, Training Loss: 0.4182
Epoch 8/10, Batch 78/442, Training Loss: 0.4311
Epoch 8/10, Batch 79/442, Training Loss: 0.3001
Epoch 8/10, Batch 80/442, Training Loss: 0.5424
Epoch 8/10, Batch 81/442, Training Loss: 0.3553
Epoch 8/10, Batch 82/442, Training Loss: 0.4019
Epoch 8/10, Batch 83/442, Training Loss: 0.4600
Epoch 8/10, Batch 84/442, Training Loss: 0.4107
Epoch 8/10, Batch 85/442, Training Loss: 0.6620
Epoch 8/10, Batch 86/442, Training Loss: 0.4281
Epoch 8/10, Batch 87/442, Training Loss: 0.2419
Epoch 8/10, Batch 88/442, Training Loss: 0.3754
Epoch 8/10, Batch 89/442, Training Loss: 0.7664
Epoch 8/10, Batch 90/442, Training Loss: 0.7085
Epoch 8/10, Batch 91/442, Training Loss: 0.5332
Epoch 8/10, Batch 92/442, Training Loss: 0.4437
Epoch 8/10, Batch 93/442, Training Loss: 0.2752
Epoch 8/10, Batch 94/442, Training Loss: 0.6143
Epoch 8/10, Batch 95/442, Training Loss: 0.4954
Epoch 8/10, Batch 96/442, Training Loss: 0.4102
Epoch 8/10, Batch 97/442, Training Loss: 0.4094
Epoch 8/10, Batch 98/442, Training Loss: 0.5423
Epoch 8/10, Batch 99/442, Training Loss: 0.3470
Epoch 8/10, Batch 100/442, Training Loss: 0.3156
Epoch 8/10, Batch 101/442, Training Loss: 0.5240
Epoch 8/10, Batch 102/442, Training Loss: 0.3923
Epoch 8/10, Batch 103/442, Training Loss: 0.4719
Epoch 8/10, Batch 104/442, Training Loss: 0.2936
Epoch 8/10, Batch 105/442, Training Loss: 0.3042
Epoch 8/10, Batch 106/442, Training Loss: 0.2939
Epoch 8/10, Batch 107/442, Training Loss: 0.3966
Epoch 8/10, Batch 108/442, Training Loss: 0.6801
Epoch 8/10, Batch 109/442, Training Loss: 0.3325
Epoch 8/10, Batch 110/442, Training Loss: 0.3828
Epoch 8/10, Batch 111/442, Training Loss: 0.5223
Epoch 8/10, Batch 112/442, Training Loss: 0.4416
Epoch 8/10, Batch 113/442, Training Loss: 0.2946
Epoch 8/10, Batch 114/442, Training Loss: 0.5050
Epoch 8/10, Batch 115/442, Training Loss: 0.4573
Epoch 8/10, Batch 116/442, Training Loss: 0.7682
Epoch 8/10, Batch 117/442, Training Loss: 0.5628
Epoch 8/10, Batch 118/442, Training Loss: 0.2707
Epoch 8/10, Batch 119/442, Training Loss: 0.4577
Epoch 8/10, Batch 120/442, Training Loss: 0.4724
Epoch 8/10, Batch 121/442, Training Loss: 0.4484
Epoch 8/10, Batch 122/442, Training Loss: 0.6996
Epoch 8/10, Batch 123/442, Training Loss: 0.4222
Epoch 8/10, Batch 124/442, Training Loss: 0.4884
Epoch 8/10, Batch 125/442, Training Loss: 0.4672
Epoch 8/10, Batch 126/442, Training Loss: 0.4124
Epoch 8/10, Batch 127/442, Training Loss: 0.4021
Epoch 8/10, Batch 128/442, Training Loss: 0.4264
Epoch 8/10, Batch 129/442, Training Loss: 0.4177
Epoch 8/10, Batch 130/442, Training Loss: 0.4767
Epoch 8/10, Batch 131/442, Training Loss: 0.3891
Epoch 8/10, Batch 132/442, Training Loss: 0.4312
Epoch 8/10, Batch 133/442, Training Loss: 0.3920
Epoch 8/10, Batch 134/442, Training Loss: 0.3557
Epoch 8/10, Batch 135/442, Training Loss: 0.4592
Epoch 8/10, Batch 136/442, Training Loss: 0.4525
Epoch 8/10, Batch 137/442, Training Loss: 0.4216
Epoch 8/10, Batch 138/442, Training Loss: 0.3612
Epoch 8/10, Batch 139/442, Training Loss: 0.8493
Epoch 8/10, Batch 140/442, Training Loss: 0.6109
Epoch 8/10, Batch 141/442, Training Loss: 0.2776
Epoch 8/10, Batch 142/442, Training Loss: 0.4138
Epoch 8/10, Batch 143/442, Training Loss: 0.5979
Epoch 8/10, Batch 144/442, Training Loss: 0.4353
Epoch 8/10, Batch 145/442, Training Loss: 0.4488
Epoch 8/10, Batch 146/442, Training Loss: 0.5457
Epoch 8/10, Batch 147/442, Training Loss: 0.3645
Epoch 8/10, Batch 148/442, Training Loss: 0.5878
Epoch 8/10, Batch 149/442, Training Loss: 0.4329
Epoch 8/10, Batch 150/442, Training Loss: 0.4954
Epoch 8/10, Batch 151/442, Training Loss: 0.4362
Epoch 8/10, Batch 152/442, Training Loss: 0.4128
Epoch 8/10, Batch 153/442, Training Loss: 0.3603
Epoch 8/10, Batch 154/442, Training Loss: 0.3382
Epoch 8/10, Batch 155/442, Training Loss: 0.3925
Epoch 8/10, Batch 156/442, Training Loss: 0.3644
Epoch 8/10, Batch 157/442, Training Loss: 0.5131
Epoch 8/10, Batch 158/442, Training Loss: 0.2702
Epoch 8/10, Batch 159/442, Training Loss: 0.3827
Epoch 8/10, Batch 160/442, Training Loss: 0.6435
Epoch 8/10, Batch 161/442, Training Loss: 0.7006
Epoch 8/10, Batch 162/442, Training Loss: 0.5733
Epoch 8/10, Batch 163/442, Training Loss: 0.3689
Epoch 8/10, Batch 164/442, Training Loss: 0.6012
Epoch 8/10, Batch 165/442, Training Loss: 0.2345
Epoch 8/10, Batch 166/442, Training Loss: 0.8785
Epoch 8/10, Batch 167/442, Training Loss: 0.3817
Epoch 8/10, Batch 168/442, Training Loss: 0.5059
Epoch 8/10, Batch 169/442, Training Loss: 0.4644
Epoch 8/10, Batch 170/442, Training Loss: 0.5488
Epoch 8/10, Batch 171/442, Training Loss: 0.5408
Epoch 8/10, Batch 172/442, Training Loss: 0.5916
Epoch 8/10, Batch 173/442, Training Loss: 0.6551
Epoch 8/10, Batch 174/442, Training Loss: 0.4060
Epoch 8/10, Batch 175/442, Training Loss: 0.3608
Epoch 8/10, Batch 176/442, Training Loss: 0.7236
Epoch 8/10, Batch 177/442, Training Loss: 0.5886
Epoch 8/10, Batch 178/442, Training Loss: 0.7715
Epoch 8/10, Batch 179/442, Training Loss: 0.4636
Epoch 8/10, Batch 180/442, Training Loss: 0.2682
Epoch 8/10, Batch 181/442, Training Loss: 0.5078
Epoch 8/10, Batch 182/442, Training Loss: 0.4464
Epoch 8/10, Batch 183/442, Training Loss: 0.4418
Epoch 8/10, Batch 184/442, Training Loss: 0.5307
Epoch 8/10, Batch 185/442, Training Loss: 0.3654
Epoch 8/10, Batch 186/442, Training Loss: 0.4245
Epoch 8/10, Batch 187/442, Training Loss: 0.3162
Epoch 8/10, Batch 188/442, Training Loss: 0.4761
Epoch 8/10, Batch 189/442, Training Loss: 0.3878
Epoch 8/10, Batch 190/442, Training Loss: 0.3884
Epoch 8/10, Batch 191/442, Training Loss: 0.4113
Epoch 8/10, Batch 192/442, Training Loss: 0.5168
Epoch 8/10, Batch 193/442, Training Loss: 0.3924
Epoch 8/10, Batch 194/442, Training Loss: 0.5232
Epoch 8/10, Batch 195/442, Training Loss: 0.3687
Epoch 8/10, Batch 196/442, Training Loss: 0.6259
Epoch 8/10, Batch 197/442, Training Loss: 0.4220
Epoch 8/10, Batch 198/442, Training Loss: 0.4776
Epoch 8/10, Batch 199/442, Training Loss: 0.5784
Epoch 8/10, Batch 200/442, Training Loss: 0.7064
Epoch 8/10, Batch 201/442, Training Loss: 0.3623
Epoch 8/10, Batch 202/442, Training Loss: 0.4192
Epoch 8/10, Batch 203/442, Training Loss: 0.4867
Epoch 8/10, Batch 204/442, Training Loss: 1.0327
Epoch 8/10, Batch 205/442, Training Loss: 0.4606
Epoch 8/10, Batch 206/442, Training Loss: 0.3355
Epoch 8/10, Batch 207/442, Training Loss: 0.3242
Epoch 8/10, Batch 208/442, Training Loss: 0.5454
Epoch 8/10, Batch 209/442, Training Loss: 0.2160
Epoch 8/10, Batch 210/442, Training Loss: 0.5625
Epoch 8/10, Batch 211/442, Training Loss: 0.3548
Epoch 8/10, Batch 212/442, Training Loss: 0.4570
Epoch 8/10, Batch 213/442, Training Loss: 0.4666
Epoch 8/10, Batch 214/442, Training Loss: 0.4553
Epoch 8/10, Batch 215/442, Training Loss: 0.6129
Epoch 8/10, Batch 216/442, Training Loss: 0.3607
Epoch 8/10, Batch 217/442, Training Loss: 0.4280
Epoch 8/10, Batch 218/442, Training Loss: 0.6121
Epoch 8/10, Batch 219/442, Training Loss: 0.3796
Epoch 8/10, Batch 220/442, Training Loss: 0.6115
Epoch 8/10, Batch 221/442, Training Loss: 0.9147
Epoch 8/10, Batch 222/442, Training Loss: 0.5246
Epoch 8/10, Batch 223/442, Training Loss: 0.6222
Epoch 8/10, Batch 224/442, Training Loss: 0.3542
Epoch 8/10, Batch 225/442, Training Loss: 0.5212
Epoch 8/10, Batch 226/442, Training Loss: 0.5490
Epoch 8/10, Batch 227/442, Training Loss: 0.4238
Epoch 8/10, Batch 228/442, Training Loss: 0.4643
Epoch 8/10, Batch 229/442, Training Loss: 0.6247
Epoch 8/10, Batch 230/442, Training Loss: 0.6445
Epoch 8/10, Batch 231/442, Training Loss: 0.4972
Epoch 8/10, Batch 232/442, Training Loss: 0.6320
Epoch 8/10, Batch 233/442, Training Loss: 0.6443
Epoch 8/10, Batch 234/442, Training Loss: 0.4377
Epoch 8/10, Batch 235/442, Training Loss: 0.4363
Epoch 8/10, Batch 236/442, Training Loss: 0.5299
Epoch 8/10, Batch 237/442, Training Loss: 0.6166
Epoch 8/10, Batch 238/442, Training Loss: 0.4727
Epoch 8/10, Batch 239/442, Training Loss: 0.4054
Epoch 8/10, Batch 240/442, Training Loss: 0.5623
Epoch 8/10, Batch 241/442, Training Loss: 0.7100
Epoch 8/10, Batch 242/442, Training Loss: 0.3675
Epoch 8/10, Batch 243/442, Training Loss: 0.4377
Epoch 8/10, Batch 244/442, Training Loss: 0.3624
Epoch 8/10, Batch 245/442, Training Loss: 0.4870
Epoch 8/10, Batch 246/442, Training Loss: 0.6999
Epoch 8/10, Batch 247/442, Training Loss: 0.3467
Epoch 8/10, Batch 248/442, Training Loss: 0.4012
Epoch 8/10, Batch 249/442, Training Loss: 0.6562
Epoch 8/10, Batch 250/442, Training Loss: 0.4489
Epoch 8/10, Batch 251/442, Training Loss: 0.6261
Epoch 8/10, Batch 252/442, Training Loss: 0.4536
Epoch 8/10, Batch 253/442, Training Loss: 0.3368
Epoch 8/10, Batch 254/442, Training Loss: 0.8896
Epoch 8/10, Batch 255/442, Training Loss: 0.6456
Epoch 8/10, Batch 256/442, Training Loss: 0.5103
Epoch 8/10, Batch 257/442, Training Loss: 0.3339
Epoch 8/10, Batch 258/442, Training Loss: 0.6001
Epoch 8/10, Batch 259/442, Training Loss: 0.3731
Epoch 8/10, Batch 260/442, Training Loss: 0.2843
Epoch 8/10, Batch 261/442, Training Loss: 0.7454
Epoch 8/10, Batch 262/442, Training Loss: 0.2897
Epoch 8/10, Batch 263/442, Training Loss: 0.6435
Epoch 8/10, Batch 264/442, Training Loss: 0.5836
Epoch 8/10, Batch 265/442, Training Loss: 0.2661
Epoch 8/10, Batch 266/442, Training Loss: 0.6360
Epoch 8/10, Batch 267/442, Training Loss: 0.4601
Epoch 8/10, Batch 268/442, Training Loss: 0.3845
Epoch 8/10, Batch 269/442, Training Loss: 0.4681
Epoch 8/10, Batch 270/442, Training Loss: 0.5344
Epoch 8/10, Batch 271/442, Training Loss: 0.3100
Epoch 8/10, Batch 272/442, Training Loss: 0.6345
Epoch 8/10, Batch 273/442, Training Loss: 0.5403
Epoch 8/10, Batch 274/442, Training Loss: 0.4703
Epoch 8/10, Batch 275/442, Training Loss: 0.3875
Epoch 8/10, Batch 276/442, Training Loss: 0.6639
Epoch 8/10, Batch 277/442, Training Loss: 0.3729
Epoch 8/10, Batch 278/442, Training Loss: 0.4816
Epoch 8/10, Batch 279/442, Training Loss: 0.4409
Epoch 8/10, Batch 280/442, Training Loss: 0.3039
Epoch 8/10, Batch 281/442, Training Loss: 0.6677
Epoch 8/10, Batch 282/442, Training Loss: 0.6761
Epoch 8/10, Batch 283/442, Training Loss: 0.4878
Epoch 8/10, Batch 284/442, Training Loss: 0.3910
Epoch 8/10, Batch 285/442, Training Loss: 0.7429
Epoch 8/10, Batch 286/442, Training Loss: 0.7128
Epoch 8/10, Batch 287/442, Training Loss: 0.7734
Epoch 8/10, Batch 288/442, Training Loss: 0.5859
Epoch 8/10, Batch 289/442, Training Loss: 0.3480
Epoch 8/10, Batch 290/442, Training Loss: 0.6106
Epoch 8/10, Batch 291/442, Training Loss: 0.3049
Epoch 8/10, Batch 292/442, Training Loss: 0.4121
Epoch 8/10, Batch 293/442, Training Loss: 0.5861
Epoch 8/10, Batch 294/442, Training Loss: 0.3502
Epoch 8/10, Batch 295/442, Training Loss: 0.4731
Epoch 8/10, Batch 296/442, Training Loss: 0.3802
Epoch 8/10, Batch 297/442, Training Loss: 0.5960
Epoch 8/10, Batch 298/442, Training Loss: 0.5159
Epoch 8/10, Batch 299/442, Training Loss: 0.4394
Epoch 8/10, Batch 300/442, Training Loss: 0.3750
Epoch 8/10, Batch 301/442, Training Loss: 0.4364
Epoch 8/10, Batch 302/442, Training Loss: 0.4579
Epoch 8/10, Batch 303/442, Training Loss: 0.5164
Epoch 8/10, Batch 304/442, Training Loss: 0.4295
Epoch 8/10, Batch 305/442, Training Loss: 0.5913
Epoch 8/10, Batch 306/442, Training Loss: 0.5218
Epoch 8/10, Batch 307/442, Training Loss: 0.3510
Epoch 8/10, Batch 308/442, Training Loss: 0.2550
Epoch 8/10, Batch 309/442, Training Loss: 0.2702
Epoch 8/10, Batch 310/442, Training Loss: 0.4345
Epoch 8/10, Batch 311/442, Training Loss: 0.3255
Epoch 8/10, Batch 312/442, Training Loss: 0.2967
Epoch 8/10, Batch 313/442, Training Loss: 0.4678
Epoch 8/10, Batch 314/442, Training Loss: 0.3220
Epoch 8/10, Batch 315/442, Training Loss: 0.4417
Epoch 8/10, Batch 316/442, Training Loss: 0.6311
Epoch 8/10, Batch 317/442, Training Loss: 0.2963
Epoch 8/10, Batch 318/442, Training Loss: 0.6560
Epoch 8/10, Batch 319/442, Training Loss: 0.5145
Epoch 8/10, Batch 320/442, Training Loss: 0.5471
Epoch 8/10, Batch 321/442, Training Loss: 0.4872
Epoch 8/10, Batch 322/442, Training Loss: 0.4721
Epoch 8/10, Batch 323/442, Training Loss: 0.5455
Epoch 8/10, Batch 324/442, Training Loss: 0.4480
Epoch 8/10, Batch 325/442, Training Loss: 0.5042
Epoch 8/10, Batch 326/442, Training Loss: 0.6246
Epoch 8/10, Batch 327/442, Training Loss: 0.3762
Epoch 8/10, Batch 328/442, Training Loss: 0.4730
Epoch 8/10, Batch 329/442, Training Loss: 0.3078
Epoch 8/10, Batch 330/442, Training Loss: 0.4847
Epoch 8/10, Batch 331/442, Training Loss: 0.7586
Epoch 8/10, Batch 332/442, Training Loss: 0.5193
Epoch 8/10, Batch 333/442, Training Loss: 0.4983
Epoch 8/10, Batch 334/442, Training Loss: 0.3014
Epoch 8/10, Batch 335/442, Training Loss: 0.3749
Epoch 8/10, Batch 336/442, Training Loss: 0.5344
Epoch 8/10, Batch 337/442, Training Loss: 0.4232
Epoch 8/10, Batch 338/442, Training Loss: 0.4266
Epoch 8/10, Batch 339/442, Training Loss: 0.6502
Epoch 8/10, Batch 340/442, Training Loss: 0.4400
Epoch 8/10, Batch 341/442, Training Loss: 0.5452
Epoch 8/10, Batch 342/442, Training Loss: 0.5092
Epoch 8/10, Batch 343/442, Training Loss: 0.7807
Epoch 8/10, Batch 344/442, Training Loss: 0.2742
Epoch 8/10, Batch 345/442, Training Loss: 0.6214
Epoch 8/10, Batch 346/442, Training Loss: 0.5462
Epoch 8/10, Batch 347/442, Training Loss: 0.6506
Epoch 8/10, Batch 348/442, Training Loss: 0.5246
Epoch 8/10, Batch 349/442, Training Loss: 0.9547
Epoch 8/10, Batch 350/442, Training Loss: 0.5319
Epoch 8/10, Batch 351/442, Training Loss: 0.7102
Epoch 8/10, Batch 352/442, Training Loss: 0.4883
Epoch 8/10, Batch 353/442, Training Loss: 0.6546
Epoch 8/10, Batch 354/442, Training Loss: 0.4211
Epoch 8/10, Batch 355/442, Training Loss: 0.5081
Epoch 8/10, Batch 356/442, Training Loss: 0.3710
Epoch 8/10, Batch 357/442, Training Loss: 0.3785
Epoch 8/10, Batch 358/442, Training Loss: 0.6127
Epoch 8/10, Batch 359/442, Training Loss: 0.3712
Epoch 8/10, Batch 360/442, Training Loss: 0.5386
Epoch 8/10, Batch 361/442, Training Loss: 0.4530
Epoch 8/10, Batch 362/442, Training Loss: 0.3751
Epoch 8/10, Batch 363/442, Training Loss: 0.4973
Epoch 8/10, Batch 364/442, Training Loss: 0.6288
Epoch 8/10, Batch 365/442, Training Loss: 0.3556
Epoch 8/10, Batch 366/442, Training Loss: 0.5019
Epoch 8/10, Batch 367/442, Training Loss: 0.2940
Epoch 8/10, Batch 368/442, Training Loss: 0.4792
Epoch 8/10, Batch 369/442, Training Loss: 0.4180
Epoch 8/10, Batch 370/442, Training Loss: 0.4670
Epoch 8/10, Batch 371/442, Training Loss: 0.4399
Epoch 8/10, Batch 372/442, Training Loss: 0.3739
Epoch 8/10, Batch 373/442, Training Loss: 0.3110
Epoch 8/10, Batch 374/442, Training Loss: 0.3405
Epoch 8/10, Batch 375/442, Training Loss: 0.3299
Epoch 8/10, Batch 376/442, Training Loss: 0.2813
Epoch 8/10, Batch 377/442, Training Loss: 0.5635
Epoch 8/10, Batch 378/442, Training Loss: 0.3589
Epoch 8/10, Batch 379/442, Training Loss: 0.5856
Epoch 8/10, Batch 380/442, Training Loss: 0.4027
Epoch 8/10, Batch 381/442, Training Loss: 0.7333
Epoch 8/10, Batch 382/442, Training Loss: 0.3210
Epoch 8/10, Batch 383/442, Training Loss: 0.4931
Epoch 8/10, Batch 384/442, Training Loss: 0.4632
Epoch 8/10, Batch 385/442, Training Loss: 0.4464
Epoch 8/10, Batch 386/442, Training Loss: 0.5617
Epoch 8/10, Batch 387/442, Training Loss: 0.3535
Epoch 8/10, Batch 388/442, Training Loss: 0.4272
Epoch 8/10, Batch 389/442, Training Loss: 0.4198
Epoch 8/10, Batch 390/442, Training Loss: 0.4685
Epoch 8/10, Batch 391/442, Training Loss: 0.5360
Epoch 8/10, Batch 392/442, Training Loss: 0.2607
Epoch 8/10, Batch 393/442, Training Loss: 0.8457
Epoch 8/10, Batch 394/442, Training Loss: 0.5166
Epoch 8/10, Batch 395/442, Training Loss: 0.4361
Epoch 8/10, Batch 396/442, Training Loss: 0.5032
Epoch 8/10, Batch 397/442, Training Loss: 0.6569
Epoch 8/10, Batch 398/442, Training Loss: 0.5699
Epoch 8/10, Batch 399/442, Training Loss: 0.4931
Epoch 8/10, Batch 400/442, Training Loss: 0.4261
Epoch 8/10, Batch 401/442, Training Loss: 0.4123
Epoch 8/10, Batch 402/442, Training Loss: 0.5201
Epoch 8/10, Batch 403/442, Training Loss: 0.5426
Epoch 8/10, Batch 404/442, Training Loss: 0.5104
Epoch 8/10, Batch 405/442, Training Loss: 0.5694
Epoch 8/10, Batch 406/442, Training Loss: 0.4642
Epoch 8/10, Batch 407/442, Training Loss: 0.3789
Epoch 8/10, Batch 408/442, Training Loss: 0.5061
Epoch 8/10, Batch 409/442, Training Loss: 0.3662
Epoch 8/10, Batch 410/442, Training Loss: 0.3481
Epoch 8/10, Batch 411/442, Training Loss: 0.5311
Epoch 8/10, Batch 412/442, Training Loss: 0.4259
Epoch 8/10, Batch 413/442, Training Loss: 0.6286
Epoch 8/10, Batch 414/442, Training Loss: 0.4178
Epoch 8/10, Batch 415/442, Training Loss: 0.4305
Epoch 8/10, Batch 416/442, Training Loss: 0.4726
Epoch 8/10, Batch 417/442, Training Loss: 0.3305
Epoch 8/10, Batch 418/442, Training Loss: 0.5516
Epoch 8/10, Batch 419/442, Training Loss: 0.4704
Epoch 8/10, Batch 420/442, Training Loss: 0.3545
Epoch 8/10, Batch 421/442, Training Loss: 0.4969
Epoch 8/10, Batch 422/442, Training Loss: 0.3586
Epoch 8/10, Batch 423/442, Training Loss: 0.4574
Epoch 8/10, Batch 424/442, Training Loss: 0.5617
Epoch 8/10, Batch 425/442, Training Loss: 0.3449
Epoch 8/10, Batch 426/442, Training Loss: 0.4340
Epoch 8/10, Batch 427/442, Training Loss: 0.4786
Epoch 8/10, Batch 428/442, Training Loss: 0.6258
Epoch 8/10, Batch 429/442, Training Loss: 0.3336
Epoch 8/10, Batch 430/442, Training Loss: 0.3346
Epoch 8/10, Batch 431/442, Training Loss: 0.6620
Epoch 8/10, Batch 432/442, Training Loss: 0.5180
Epoch 8/10, Batch 433/442, Training Loss: 0.5830
Epoch 8/10, Batch 434/442, Training Loss: 0.5554
Epoch 8/10, Batch 435/442, Training Loss: 0.5016
Epoch 8/10, Batch 436/442, Training Loss: 0.3280
Epoch 8/10, Batch 437/442, Training Loss: 0.2604
Epoch 8/10, Batch 438/442, Training Loss: 0.4760
Epoch 8/10, Batch 439/442, Training Loss: 0.7903
Epoch 8/10, Batch 440/442, Training Loss: 0.5045
Epoch 8/10, Batch 441/442, Training Loss: 0.3474
Epoch 8/10, Batch 442/442, Training Loss: 0.4592
Epoch 8/10, Training Loss: 0.4829, Validation Loss: 0.5885, Validation Accuracy: 0.7470
Epoch 9/10, Batch 1/442, Training Loss: 0.3877
Epoch 9/10, Batch 2/442, Training Loss: 0.3772
Epoch 9/10, Batch 3/442, Training Loss: 0.5141
Epoch 9/10, Batch 4/442, Training Loss: 0.6644
Epoch 9/10, Batch 5/442, Training Loss: 0.4748
Epoch 9/10, Batch 6/442, Training Loss: 0.2057
Epoch 9/10, Batch 7/442, Training Loss: 0.2883
Epoch 9/10, Batch 8/442, Training Loss: 0.5599
Epoch 9/10, Batch 9/442, Training Loss: 0.3628
Epoch 9/10, Batch 10/442, Training Loss: 0.3028
Epoch 9/10, Batch 11/442, Training Loss: 0.4739
Epoch 9/10, Batch 12/442, Training Loss: 0.5078
Epoch 9/10, Batch 13/442, Training Loss: 0.5429
Epoch 9/10, Batch 14/442, Training Loss: 0.6755
Epoch 9/10, Batch 15/442, Training Loss: 0.3548
Epoch 9/10, Batch 16/442, Training Loss: 0.4091
Epoch 9/10, Batch 17/442, Training Loss: 0.4944
Epoch 9/10, Batch 18/442, Training Loss: 0.4814
Epoch 9/10, Batch 19/442, Training Loss: 0.4658
Epoch 9/10, Batch 20/442, Training Loss: 0.3125
Epoch 9/10, Batch 21/442, Training Loss: 0.5735
Epoch 9/10, Batch 22/442, Training Loss: 0.4756
Epoch 9/10, Batch 23/442, Training Loss: 0.4122
Epoch 9/10, Batch 24/442, Training Loss: 0.5354
Epoch 9/10, Batch 25/442, Training Loss: 0.5185
Epoch 9/10, Batch 26/442, Training Loss: 0.4225
Epoch 9/10, Batch 27/442, Training Loss: 0.4438
Epoch 9/10, Batch 28/442, Training Loss: 0.5667
Epoch 9/10, Batch 29/442, Training Loss: 0.6117
Epoch 9/10, Batch 30/442, Training Loss: 0.4985
Epoch 9/10, Batch 31/442, Training Loss: 0.3065
Epoch 9/10, Batch 32/442, Training Loss: 0.5853
Epoch 9/10, Batch 33/442, Training Loss: 0.4372
Epoch 9/10, Batch 34/442, Training Loss: 0.3082
Epoch 9/10, Batch 35/442, Training Loss: 0.4632
Epoch 9/10, Batch 36/442, Training Loss: 0.4316
Epoch 9/10, Batch 37/442, Training Loss: 0.4711
Epoch 9/10, Batch 38/442, Training Loss: 0.3246
Epoch 9/10, Batch 39/442, Training Loss: 0.3112
Epoch 9/10, Batch 40/442, Training Loss: 0.4227
Epoch 9/10, Batch 41/442, Training Loss: 0.3150
Epoch 9/10, Batch 42/442, Training Loss: 0.5161
Epoch 9/10, Batch 43/442, Training Loss: 0.4911
Epoch 9/10, Batch 44/442, Training Loss: 0.2761
Epoch 9/10, Batch 45/442, Training Loss: 0.3912
Epoch 9/10, Batch 46/442, Training Loss: 0.4179
Epoch 9/10, Batch 47/442, Training Loss: 0.2971
Epoch 9/10, Batch 48/442, Training Loss: 0.4368
Epoch 9/10, Batch 49/442, Training Loss: 0.4175
Epoch 9/10, Batch 50/442, Training Loss: 0.3099
Epoch 9/10, Batch 51/442, Training Loss: 0.3659
Epoch 9/10, Batch 52/442, Training Loss: 0.3893
Epoch 9/10, Batch 53/442, Training Loss: 0.5347
Epoch 9/10, Batch 54/442, Training Loss: 0.4816
Epoch 9/10, Batch 55/442, Training Loss: 0.3377
Epoch 9/10, Batch 56/442, Training Loss: 0.4069
Epoch 9/10, Batch 57/442, Training Loss: 0.5782
Epoch 9/10, Batch 58/442, Training Loss: 0.2900
Epoch 9/10, Batch 59/442, Training Loss: 0.3816
Epoch 9/10, Batch 60/442, Training Loss: 0.3548
Epoch 9/10, Batch 61/442, Training Loss: 0.4828
Epoch 9/10, Batch 62/442, Training Loss: 0.6151
Epoch 9/10, Batch 63/442, Training Loss: 0.5448
Epoch 9/10, Batch 64/442, Training Loss: 0.5747
Epoch 9/10, Batch 65/442, Training Loss: 0.6376
Epoch 9/10, Batch 66/442, Training Loss: 0.3008
Epoch 9/10, Batch 67/442, Training Loss: 0.5636
Epoch 9/10, Batch 68/442, Training Loss: 0.5078
Epoch 9/10, Batch 69/442, Training Loss: 0.5858
Epoch 9/10, Batch 70/442, Training Loss: 0.4997
Epoch 9/10, Batch 71/442, Training Loss: 0.6166
Epoch 9/10, Batch 72/442, Training Loss: 0.4833
Epoch 9/10, Batch 73/442, Training Loss: 0.4346
Epoch 9/10, Batch 74/442, Training Loss: 0.3340
Epoch 9/10, Batch 75/442, Training Loss: 0.4529
Epoch 9/10, Batch 76/442, Training Loss: 0.4371
Epoch 9/10, Batch 77/442, Training Loss: 0.7206
Epoch 9/10, Batch 78/442, Training Loss: 0.7799
Epoch 9/10, Batch 79/442, Training Loss: 0.4503
Epoch 9/10, Batch 80/442, Training Loss: 0.4409
Epoch 9/10, Batch 81/442, Training Loss: 0.3913
Epoch 9/10, Batch 82/442, Training Loss: 0.5606
Epoch 9/10, Batch 83/442, Training Loss: 0.3428
Epoch 9/10, Batch 84/442, Training Loss: 0.4614
Epoch 9/10, Batch 85/442, Training Loss: 0.5087
Epoch 9/10, Batch 86/442, Training Loss: 0.5874
Epoch 9/10, Batch 87/442, Training Loss: 0.3624
Epoch 9/10, Batch 88/442, Training Loss: 0.5811
Epoch 9/10, Batch 89/442, Training Loss: 0.4237
Epoch 9/10, Batch 90/442, Training Loss: 0.3694
Epoch 9/10, Batch 91/442, Training Loss: 0.3173
Epoch 9/10, Batch 92/442, Training Loss: 0.4574
Epoch 9/10, Batch 93/442, Training Loss: 0.5048
Epoch 9/10, Batch 94/442, Training Loss: 0.6425
Epoch 9/10, Batch 95/442, Training Loss: 0.4404
Epoch 9/10, Batch 96/442, Training Loss: 0.5089
Epoch 9/10, Batch 97/442, Training Loss: 0.4069
Epoch 9/10, Batch 98/442, Training Loss: 0.6420
Epoch 9/10, Batch 99/442, Training Loss: 0.5500
Epoch 9/10, Batch 100/442, Training Loss: 0.3574
Epoch 9/10, Batch 101/442, Training Loss: 0.5124
Epoch 9/10, Batch 102/442, Training Loss: 0.5636
Epoch 9/10, Batch 103/442, Training Loss: 0.6551
Epoch 9/10, Batch 104/442, Training Loss: 0.4059
Epoch 9/10, Batch 105/442, Training Loss: 0.4810
Epoch 9/10, Batch 106/442, Training Loss: 0.2956
Epoch 9/10, Batch 107/442, Training Loss: 0.4040
Epoch 9/10, Batch 108/442, Training Loss: 0.3588
Epoch 9/10, Batch 109/442, Training Loss: 0.3690
Epoch 9/10, Batch 110/442, Training Loss: 0.6226
Epoch 9/10, Batch 111/442, Training Loss: 0.4500
Epoch 9/10, Batch 112/442, Training Loss: 0.5259
Epoch 9/10, Batch 113/442, Training Loss: 0.3749
Epoch 9/10, Batch 114/442, Training Loss: 0.4058
Epoch 9/10, Batch 115/442, Training Loss: 0.3825
Epoch 9/10, Batch 116/442, Training Loss: 0.5738
Epoch 9/10, Batch 117/442, Training Loss: 0.2217
Epoch 9/10, Batch 118/442, Training Loss: 0.4328
Epoch 9/10, Batch 119/442, Training Loss: 0.4475
Epoch 9/10, Batch 120/442, Training Loss: 0.3885
Epoch 9/10, Batch 121/442, Training Loss: 0.4289
Epoch 9/10, Batch 122/442, Training Loss: 0.4439
Epoch 9/10, Batch 123/442, Training Loss: 0.4906
Epoch 9/10, Batch 124/442, Training Loss: 0.3306
Epoch 9/10, Batch 125/442, Training Loss: 0.5229
Epoch 9/10, Batch 126/442, Training Loss: 0.5100
Epoch 9/10, Batch 127/442, Training Loss: 0.2766
Epoch 9/10, Batch 128/442, Training Loss: 0.4223
Epoch 9/10, Batch 129/442, Training Loss: 0.4042
Epoch 9/10, Batch 130/442, Training Loss: 0.4852
Epoch 9/10, Batch 131/442, Training Loss: 0.3434
Epoch 9/10, Batch 132/442, Training Loss: 0.5600
Epoch 9/10, Batch 133/442, Training Loss: 0.3477
Epoch 9/10, Batch 134/442, Training Loss: 0.4294
Epoch 9/10, Batch 135/442, Training Loss: 0.4884
Epoch 9/10, Batch 136/442, Training Loss: 0.3828
Epoch 9/10, Batch 137/442, Training Loss: 0.6611
Epoch 9/10, Batch 138/442, Training Loss: 0.4371
Epoch 9/10, Batch 139/442, Training Loss: 0.4376
Epoch 9/10, Batch 140/442, Training Loss: 0.5878
Epoch 9/10, Batch 141/442, Training Loss: 0.6021
Epoch 9/10, Batch 142/442, Training Loss: 0.3371
Epoch 9/10, Batch 143/442, Training Loss: 0.3636
Epoch 9/10, Batch 144/442, Training Loss: 0.6437
Epoch 9/10, Batch 145/442, Training Loss: 0.4519
Epoch 9/10, Batch 146/442, Training Loss: 0.6439
Epoch 9/10, Batch 147/442, Training Loss: 0.4246
Epoch 9/10, Batch 148/442, Training Loss: 0.4485
Epoch 9/10, Batch 149/442, Training Loss: 0.2639
Epoch 9/10, Batch 150/442, Training Loss: 0.6192
Epoch 9/10, Batch 151/442, Training Loss: 0.4300
Epoch 9/10, Batch 152/442, Training Loss: 0.6341
Epoch 9/10, Batch 153/442, Training Loss: 0.4807
Epoch 9/10, Batch 154/442, Training Loss: 0.4430
Epoch 9/10, Batch 155/442, Training Loss: 0.4289
Epoch 9/10, Batch 156/442, Training Loss: 0.6612
Epoch 9/10, Batch 157/442, Training Loss: 0.8796
Epoch 9/10, Batch 158/442, Training Loss: 0.4802
Epoch 9/10, Batch 159/442, Training Loss: 0.4025
Epoch 9/10, Batch 160/442, Training Loss: 0.5452
Epoch 9/10, Batch 161/442, Training Loss: 0.6692
Epoch 9/10, Batch 162/442, Training Loss: 0.4440
Epoch 9/10, Batch 163/442, Training Loss: 0.4682
Epoch 9/10, Batch 164/442, Training Loss: 0.4223
Epoch 9/10, Batch 165/442, Training Loss: 0.4616
Epoch 9/10, Batch 166/442, Training Loss: 0.3988
Epoch 9/10, Batch 167/442, Training Loss: 0.4946
Epoch 9/10, Batch 168/442, Training Loss: 0.4869
Epoch 9/10, Batch 169/442, Training Loss: 0.4858
Epoch 9/10, Batch 170/442, Training Loss: 0.4033
Epoch 9/10, Batch 171/442, Training Loss: 0.4063
Epoch 9/10, Batch 172/442, Training Loss: 0.4978
Epoch 9/10, Batch 173/442, Training Loss: 0.3388
Epoch 9/10, Batch 174/442, Training Loss: 0.5505
Epoch 9/10, Batch 175/442, Training Loss: 0.3860
Epoch 9/10, Batch 176/442, Training Loss: 0.5904
Epoch 9/10, Batch 177/442, Training Loss: 0.2575
Epoch 9/10, Batch 178/442, Training Loss: 0.6679
Epoch 9/10, Batch 179/442, Training Loss: 0.4763
Epoch 9/10, Batch 180/442, Training Loss: 0.4869
Epoch 9/10, Batch 181/442, Training Loss: 0.6949
Epoch 9/10, Batch 182/442, Training Loss: 0.3258
Epoch 9/10, Batch 183/442, Training Loss: 0.5357
Epoch 9/10, Batch 184/442, Training Loss: 0.4576
Epoch 9/10, Batch 185/442, Training Loss: 0.5234
Epoch 9/10, Batch 186/442, Training Loss: 0.4891
Epoch 9/10, Batch 187/442, Training Loss: 0.6906
Epoch 9/10, Batch 188/442, Training Loss: 0.4320
Epoch 9/10, Batch 189/442, Training Loss: 0.5787
Epoch 9/10, Batch 190/442, Training Loss: 0.4272
Epoch 9/10, Batch 191/442, Training Loss: 0.5043
Epoch 9/10, Batch 192/442, Training Loss: 0.5504
Epoch 9/10, Batch 193/442, Training Loss: 0.4773
Epoch 9/10, Batch 194/442, Training Loss: 0.3628
Epoch 9/10, Batch 195/442, Training Loss: 0.3967
Epoch 9/10, Batch 196/442, Training Loss: 0.4782
Epoch 9/10, Batch 197/442, Training Loss: 0.3835
Epoch 9/10, Batch 198/442, Training Loss: 0.3586
Epoch 9/10, Batch 199/442, Training Loss: 0.5075
Epoch 9/10, Batch 200/442, Training Loss: 0.5202
Epoch 9/10, Batch 201/442, Training Loss: 0.6055
Epoch 9/10, Batch 202/442, Training Loss: 0.6602
Epoch 9/10, Batch 203/442, Training Loss: 0.5546
Epoch 9/10, Batch 204/442, Training Loss: 0.2921
Epoch 9/10, Batch 205/442, Training Loss: 0.4320
Epoch 9/10, Batch 206/442, Training Loss: 0.4253
Epoch 9/10, Batch 207/442, Training Loss: 0.4135
Epoch 9/10, Batch 208/442, Training Loss: 0.2978
Epoch 9/10, Batch 209/442, Training Loss: 0.4072
Epoch 9/10, Batch 210/442, Training Loss: 0.5998
Epoch 9/10, Batch 211/442, Training Loss: 0.4880
Epoch 9/10, Batch 212/442, Training Loss: 0.2647
Epoch 9/10, Batch 213/442, Training Loss: 0.5785
Epoch 9/10, Batch 214/442, Training Loss: 0.2656
Epoch 9/10, Batch 215/442, Training Loss: 0.5817
Epoch 9/10, Batch 216/442, Training Loss: 0.3786
Epoch 9/10, Batch 217/442, Training Loss: 0.3618
Epoch 9/10, Batch 218/442, Training Loss: 0.3434
Epoch 9/10, Batch 219/442, Training Loss: 0.3131
Epoch 9/10, Batch 220/442, Training Loss: 0.2994
Epoch 9/10, Batch 221/442, Training Loss: 0.4681
Epoch 9/10, Batch 222/442, Training Loss: 0.3793
Epoch 9/10, Batch 223/442, Training Loss: 0.5046
Epoch 9/10, Batch 224/442, Training Loss: 0.3632
Epoch 9/10, Batch 225/442, Training Loss: 0.3911
Epoch 9/10, Batch 226/442, Training Loss: 0.4884
Epoch 9/10, Batch 227/442, Training Loss: 0.5461
Epoch 9/10, Batch 228/442, Training Loss: 0.5314
Epoch 9/10, Batch 229/442, Training Loss: 0.4159
Epoch 9/10, Batch 230/442, Training Loss: 0.3326
Epoch 9/10, Batch 231/442, Training Loss: 0.3850
Epoch 9/10, Batch 232/442, Training Loss: 0.2801
Epoch 9/10, Batch 233/442, Training Loss: 0.3796
Epoch 9/10, Batch 234/442, Training Loss: 0.4854
Epoch 9/10, Batch 235/442, Training Loss: 0.7194
Epoch 9/10, Batch 236/442, Training Loss: 0.2495
Epoch 9/10, Batch 237/442, Training Loss: 0.4443
Epoch 9/10, Batch 238/442, Training Loss: 0.4688
Epoch 9/10, Batch 239/442, Training Loss: 0.3105
Epoch 9/10, Batch 240/442, Training Loss: 0.4043
Epoch 9/10, Batch 241/442, Training Loss: 0.5074
Epoch 9/10, Batch 242/442, Training Loss: 0.3264
Epoch 9/10, Batch 243/442, Training Loss: 0.3864
Epoch 9/10, Batch 244/442, Training Loss: 0.5648
Epoch 9/10, Batch 245/442, Training Loss: 0.5185
Epoch 9/10, Batch 246/442, Training Loss: 0.5346
Epoch 9/10, Batch 247/442, Training Loss: 0.3327
Epoch 9/10, Batch 248/442, Training Loss: 0.4090
Epoch 9/10, Batch 249/442, Training Loss: 0.4045
Epoch 9/10, Batch 250/442, Training Loss: 0.8479
Epoch 9/10, Batch 251/442, Training Loss: 0.2298
Epoch 9/10, Batch 252/442, Training Loss: 0.6107
Epoch 9/10, Batch 253/442, Training Loss: 0.2206
Epoch 9/10, Batch 254/442, Training Loss: 0.4288
Epoch 9/10, Batch 255/442, Training Loss: 0.4234
Epoch 9/10, Batch 256/442, Training Loss: 0.4085
Epoch 9/10, Batch 257/442, Training Loss: 0.2831
Epoch 9/10, Batch 258/442, Training Loss: 0.4080
Epoch 9/10, Batch 259/442, Training Loss: 0.3511
Epoch 9/10, Batch 260/442, Training Loss: 0.6080
Epoch 9/10, Batch 261/442, Training Loss: 0.3800
Epoch 9/10, Batch 262/442, Training Loss: 0.3874
Epoch 9/10, Batch 263/442, Training Loss: 0.3604
Epoch 9/10, Batch 264/442, Training Loss: 0.2649
Epoch 9/10, Batch 265/442, Training Loss: 0.3755
Epoch 9/10, Batch 266/442, Training Loss: 0.3466
Epoch 9/10, Batch 267/442, Training Loss: 0.4108
Epoch 9/10, Batch 268/442, Training Loss: 0.5702
Epoch 9/10, Batch 269/442, Training Loss: 0.5327
Epoch 9/10, Batch 270/442, Training Loss: 0.4669
Epoch 9/10, Batch 271/442, Training Loss: 0.5579
Epoch 9/10, Batch 272/442, Training Loss: 0.4443
Epoch 9/10, Batch 273/442, Training Loss: 0.4033
Epoch 9/10, Batch 274/442, Training Loss: 0.3376
Epoch 9/10, Batch 275/442, Training Loss: 0.4227
Epoch 9/10, Batch 276/442, Training Loss: 0.4321
Epoch 9/10, Batch 277/442, Training Loss: 0.3603
Epoch 9/10, Batch 278/442, Training Loss: 0.3834
Epoch 9/10, Batch 279/442, Training Loss: 0.3702
Epoch 9/10, Batch 280/442, Training Loss: 0.5266
Epoch 9/10, Batch 281/442, Training Loss: 0.4912
Epoch 9/10, Batch 282/442, Training Loss: 0.6504
Epoch 9/10, Batch 283/442, Training Loss: 0.6146
Epoch 9/10, Batch 284/442, Training Loss: 0.5808
Epoch 9/10, Batch 285/442, Training Loss: 0.3142
Epoch 9/10, Batch 286/442, Training Loss: 0.2636
Epoch 9/10, Batch 287/442, Training Loss: 0.4866
Epoch 9/10, Batch 288/442, Training Loss: 0.4060
Epoch 9/10, Batch 289/442, Training Loss: 0.3970
Epoch 9/10, Batch 290/442, Training Loss: 0.5089
Epoch 9/10, Batch 291/442, Training Loss: 0.6330
Epoch 9/10, Batch 292/442, Training Loss: 0.6662
Epoch 9/10, Batch 293/442, Training Loss: 0.4976
Epoch 9/10, Batch 294/442, Training Loss: 0.4747
Epoch 9/10, Batch 295/442, Training Loss: 0.4145
Epoch 9/10, Batch 296/442, Training Loss: 0.2880
Epoch 9/10, Batch 297/442, Training Loss: 0.3184
Epoch 9/10, Batch 298/442, Training Loss: 0.3260
Epoch 9/10, Batch 299/442, Training Loss: 0.5712
Epoch 9/10, Batch 300/442, Training Loss: 0.6327
Epoch 9/10, Batch 301/442, Training Loss: 0.4804
Epoch 9/10, Batch 302/442, Training Loss: 0.4839
Epoch 9/10, Batch 303/442, Training Loss: 0.5149
Epoch 9/10, Batch 304/442, Training Loss: 0.5768
Epoch 9/10, Batch 305/442, Training Loss: 0.2645
Epoch 9/10, Batch 306/442, Training Loss: 0.4531
Epoch 9/10, Batch 307/442, Training Loss: 0.4259
Epoch 9/10, Batch 308/442, Training Loss: 0.5949
Epoch 9/10, Batch 309/442, Training Loss: 0.4731
Epoch 9/10, Batch 310/442, Training Loss: 0.5447
Epoch 9/10, Batch 311/442, Training Loss: 0.2560
Epoch 9/10, Batch 312/442, Training Loss: 0.4996
Epoch 9/10, Batch 313/442, Training Loss: 0.3144
Epoch 9/10, Batch 314/442, Training Loss: 0.4080
Epoch 9/10, Batch 315/442, Training Loss: 0.4501
Epoch 9/10, Batch 316/442, Training Loss: 0.4098
Epoch 9/10, Batch 317/442, Training Loss: 0.3379
Epoch 9/10, Batch 318/442, Training Loss: 0.4442
Epoch 9/10, Batch 319/442, Training Loss: 0.5360
Epoch 9/10, Batch 320/442, Training Loss: 0.4091
Epoch 9/10, Batch 321/442, Training Loss: 0.7361
Epoch 9/10, Batch 322/442, Training Loss: 0.5990
Epoch 9/10, Batch 323/442, Training Loss: 0.6625
Epoch 9/10, Batch 324/442, Training Loss: 0.3032
Epoch 9/10, Batch 325/442, Training Loss: 0.3915
Epoch 9/10, Batch 326/442, Training Loss: 0.4307
Epoch 9/10, Batch 327/442, Training Loss: 0.5557
Epoch 9/10, Batch 328/442, Training Loss: 0.2924
Epoch 9/10, Batch 329/442, Training Loss: 0.5831
Epoch 9/10, Batch 330/442, Training Loss: 0.6709
Epoch 9/10, Batch 331/442, Training Loss: 0.4023
Epoch 9/10, Batch 332/442, Training Loss: 0.5452
Epoch 9/10, Batch 333/442, Training Loss: 0.4394
Epoch 9/10, Batch 334/442, Training Loss: 0.2220
Epoch 9/10, Batch 335/442, Training Loss: 0.4109
Epoch 9/10, Batch 336/442, Training Loss: 0.6186
Epoch 9/10, Batch 337/442, Training Loss: 0.4448
Epoch 9/10, Batch 338/442, Training Loss: 0.4022
Epoch 9/10, Batch 339/442, Training Loss: 0.2871
Epoch 9/10, Batch 340/442, Training Loss: 0.3420
Epoch 9/10, Batch 341/442, Training Loss: 0.3766
Epoch 9/10, Batch 342/442, Training Loss: 0.3595
Epoch 9/10, Batch 343/442, Training Loss: 0.3541
Epoch 9/10, Batch 344/442, Training Loss: 0.5655
Epoch 9/10, Batch 345/442, Training Loss: 0.5270
Epoch 9/10, Batch 346/442, Training Loss: 0.4615
Epoch 9/10, Batch 347/442, Training Loss: 0.5689
Epoch 9/10, Batch 348/442, Training Loss: 0.6452
Epoch 9/10, Batch 349/442, Training Loss: 0.3411
Epoch 9/10, Batch 350/442, Training Loss: 0.3575
Epoch 9/10, Batch 351/442, Training Loss: 0.4947
Epoch 9/10, Batch 352/442, Training Loss: 0.6888
Epoch 9/10, Batch 353/442, Training Loss: 0.4266
Epoch 9/10, Batch 354/442, Training Loss: 0.8844
Epoch 9/10, Batch 355/442, Training Loss: 0.4291
Epoch 9/10, Batch 356/442, Training Loss: 0.1898
Epoch 9/10, Batch 357/442, Training Loss: 0.4335
Epoch 9/10, Batch 358/442, Training Loss: 0.4391
Epoch 9/10, Batch 359/442, Training Loss: 0.3396
Epoch 9/10, Batch 360/442, Training Loss: 0.2959
Epoch 9/10, Batch 361/442, Training Loss: 0.3923
Epoch 9/10, Batch 362/442, Training Loss: 0.3306
Epoch 9/10, Batch 363/442, Training Loss: 0.4030
Epoch 9/10, Batch 364/442, Training Loss: 0.3925
Epoch 9/10, Batch 365/442, Training Loss: 0.6042
Epoch 9/10, Batch 366/442, Training Loss: 0.5541
Epoch 9/10, Batch 367/442, Training Loss: 0.3129
Epoch 9/10, Batch 368/442, Training Loss: 0.4699
Epoch 9/10, Batch 369/442, Training Loss: 0.3829
Epoch 9/10, Batch 370/442, Training Loss: 0.5026
Epoch 9/10, Batch 371/442, Training Loss: 0.5392
Epoch 9/10, Batch 372/442, Training Loss: 0.4223
Epoch 9/10, Batch 373/442, Training Loss: 0.7657
Epoch 9/10, Batch 374/442, Training Loss: 0.2109
Epoch 9/10, Batch 375/442, Training Loss: 0.5461
Epoch 9/10, Batch 376/442, Training Loss: 0.5111
Epoch 9/10, Batch 377/442, Training Loss: 0.3736
Epoch 9/10, Batch 378/442, Training Loss: 0.4685
Epoch 9/10, Batch 379/442, Training Loss: 0.2771
Epoch 9/10, Batch 380/442, Training Loss: 0.5209
Epoch 9/10, Batch 381/442, Training Loss: 0.3478
Epoch 9/10, Batch 382/442, Training Loss: 0.5020
Epoch 9/10, Batch 383/442, Training Loss: 0.5057
Epoch 9/10, Batch 384/442, Training Loss: 0.5315
Epoch 9/10, Batch 385/442, Training Loss: 0.4972
Epoch 9/10, Batch 386/442, Training Loss: 0.6253
Epoch 9/10, Batch 387/442, Training Loss: 0.6817
Epoch 9/10, Batch 388/442, Training Loss: 0.3330
Epoch 9/10, Batch 389/442, Training Loss: 0.4574
Epoch 9/10, Batch 390/442, Training Loss: 0.3992
Epoch 9/10, Batch 391/442, Training Loss: 0.5423
Epoch 9/10, Batch 392/442, Training Loss: 0.4548
Epoch 9/10, Batch 393/442, Training Loss: 0.4881
Epoch 9/10, Batch 394/442, Training Loss: 0.3442
Epoch 9/10, Batch 395/442, Training Loss: 0.3231
Epoch 9/10, Batch 396/442, Training Loss: 0.4181
Epoch 9/10, Batch 397/442, Training Loss: 0.3797
Epoch 9/10, Batch 398/442, Training Loss: 0.3438
Epoch 9/10, Batch 399/442, Training Loss: 0.5923
Epoch 9/10, Batch 400/442, Training Loss: 0.5196
Epoch 9/10, Batch 401/442, Training Loss: 0.3955
Epoch 9/10, Batch 402/442, Training Loss: 0.2579
Epoch 9/10, Batch 403/442, Training Loss: 0.3702
Epoch 9/10, Batch 404/442, Training Loss: 0.4157
Epoch 9/10, Batch 405/442, Training Loss: 0.4338
Epoch 9/10, Batch 406/442, Training Loss: 0.6395
Epoch 9/10, Batch 407/442, Training Loss: 0.4884
Epoch 9/10, Batch 408/442, Training Loss: 0.5260
Epoch 9/10, Batch 409/442, Training Loss: 0.3089
Epoch 9/10, Batch 410/442, Training Loss: 0.3137
Epoch 9/10, Batch 411/442, Training Loss: 0.6010
Epoch 9/10, Batch 412/442, Training Loss: 0.4265
Epoch 9/10, Batch 413/442, Training Loss: 0.4140
Epoch 9/10, Batch 414/442, Training Loss: 0.3317
Epoch 9/10, Batch 415/442, Training Loss: 0.4647
Epoch 9/10, Batch 416/442, Training Loss: 0.3164
Epoch 9/10, Batch 417/442, Training Loss: 0.3309
Epoch 9/10, Batch 418/442, Training Loss: 0.6008
Epoch 9/10, Batch 419/442, Training Loss: 0.6793
Epoch 9/10, Batch 420/442, Training Loss: 0.4528
Epoch 9/10, Batch 421/442, Training Loss: 0.6237
Epoch 9/10, Batch 422/442, Training Loss: 0.6093
Epoch 9/10, Batch 423/442, Training Loss: 0.6546
Epoch 9/10, Batch 424/442, Training Loss: 0.4260
Epoch 9/10, Batch 425/442, Training Loss: 0.5807
Epoch 9/10, Batch 426/442, Training Loss: 0.4610
Epoch 9/10, Batch 427/442, Training Loss: 0.5013
Epoch 9/10, Batch 428/442, Training Loss: 0.3390
Epoch 9/10, Batch 429/442, Training Loss: 0.6367
Epoch 9/10, Batch 430/442, Training Loss: 0.7050
Epoch 9/10, Batch 431/442, Training Loss: 0.3252
Epoch 9/10, Batch 432/442, Training Loss: 0.4876
Epoch 9/10, Batch 433/442, Training Loss: 0.5385
Epoch 9/10, Batch 434/442, Training Loss: 0.3242
Epoch 9/10, Batch 435/442, Training Loss: 0.4914
Epoch 9/10, Batch 436/442, Training Loss: 0.3868
Epoch 9/10, Batch 437/442, Training Loss: 0.4467
Epoch 9/10, Batch 438/442, Training Loss: 0.2424
Epoch 9/10, Batch 439/442, Training Loss: 0.3837
Epoch 9/10, Batch 440/442, Training Loss: 0.4499
Epoch 9/10, Batch 441/442, Training Loss: 0.7487
Epoch 9/10, Batch 442/442, Training Loss: 0.7358
Epoch 9/10, Training Loss: 0.4577, Validation Loss: 0.4465, Validation Accuracy: 0.8196
Epoch 10/10, Batch 1/442, Training Loss: 0.4028
Epoch 10/10, Batch 2/442, Training Loss: 0.3949
Epoch 10/10, Batch 3/442, Training Loss: 0.3066
Epoch 10/10, Batch 4/442, Training Loss: 0.4386
Epoch 10/10, Batch 5/442, Training Loss: 0.5235
Epoch 10/10, Batch 6/442, Training Loss: 0.2369
Epoch 10/10, Batch 7/442, Training Loss: 0.4087
Epoch 10/10, Batch 8/442, Training Loss: 0.4048
Epoch 10/10, Batch 9/442, Training Loss: 0.3909
Epoch 10/10, Batch 10/442, Training Loss: 0.2900
Epoch 10/10, Batch 11/442, Training Loss: 0.4854
Epoch 10/10, Batch 12/442, Training Loss: 0.2838
Epoch 10/10, Batch 13/442, Training Loss: 0.3412
Epoch 10/10, Batch 14/442, Training Loss: 0.3417
Epoch 10/10, Batch 15/442, Training Loss: 0.4099
Epoch 10/10, Batch 16/442, Training Loss: 0.3441
Epoch 10/10, Batch 17/442, Training Loss: 0.2559
Epoch 10/10, Batch 18/442, Training Loss: 0.1460
Epoch 10/10, Batch 19/442, Training Loss: 0.5336
Epoch 10/10, Batch 20/442, Training Loss: 0.4717
Epoch 10/10, Batch 21/442, Training Loss: 0.6887
Epoch 10/10, Batch 22/442, Training Loss: 0.4314
Epoch 10/10, Batch 23/442, Training Loss: 0.6094
Epoch 10/10, Batch 24/442, Training Loss: 0.3122
Epoch 10/10, Batch 25/442, Training Loss: 0.4565
Epoch 10/10, Batch 26/442, Training Loss: 0.2534
Epoch 10/10, Batch 27/442, Training Loss: 0.4199
Epoch 10/10, Batch 28/442, Training Loss: 0.3176
Epoch 10/10, Batch 29/442, Training Loss: 0.3240
Epoch 10/10, Batch 30/442, Training Loss: 0.2401
Epoch 10/10, Batch 31/442, Training Loss: 0.7977
Epoch 10/10, Batch 32/442, Training Loss: 0.3461
Epoch 10/10, Batch 33/442, Training Loss: 0.3841
Epoch 10/10, Batch 34/442, Training Loss: 0.3807
Epoch 10/10, Batch 35/442, Training Loss: 0.4265
Epoch 10/10, Batch 36/442, Training Loss: 0.4920
Epoch 10/10, Batch 37/442, Training Loss: 0.3666
Epoch 10/10, Batch 38/442, Training Loss: 0.4512
Epoch 10/10, Batch 39/442, Training Loss: 0.3937
Epoch 10/10, Batch 40/442, Training Loss: 0.2513
Epoch 10/10, Batch 41/442, Training Loss: 0.3689
Epoch 10/10, Batch 42/442, Training Loss: 0.3032
Epoch 10/10, Batch 43/442, Training Loss: 0.3840
Epoch 10/10, Batch 44/442, Training Loss: 0.5007
Epoch 10/10, Batch 45/442, Training Loss: 0.3194
Epoch 10/10, Batch 46/442, Training Loss: 0.2326
Epoch 10/10, Batch 47/442, Training Loss: 0.4619
Epoch 10/10, Batch 48/442, Training Loss: 0.3014
Epoch 10/10, Batch 49/442, Training Loss: 0.7032
Epoch 10/10, Batch 50/442, Training Loss: 0.4134
Epoch 10/10, Batch 51/442, Training Loss: 0.5201
Epoch 10/10, Batch 52/442, Training Loss: 0.4549
Epoch 10/10, Batch 53/442, Training Loss: 0.2471
Epoch 10/10, Batch 54/442, Training Loss: 0.3818
Epoch 10/10, Batch 55/442, Training Loss: 0.6586
Epoch 10/10, Batch 56/442, Training Loss: 0.3480
Epoch 10/10, Batch 57/442, Training Loss: 0.5109
Epoch 10/10, Batch 58/442, Training Loss: 0.3558
Epoch 10/10, Batch 59/442, Training Loss: 0.2438
Epoch 10/10, Batch 60/442, Training Loss: 0.3156
Epoch 10/10, Batch 61/442, Training Loss: 0.3900
Epoch 10/10, Batch 62/442, Training Loss: 0.4875
Epoch 10/10, Batch 63/442, Training Loss: 0.4615
Epoch 10/10, Batch 64/442, Training Loss: 0.4868
Epoch 10/10, Batch 65/442, Training Loss: 0.4816
Epoch 10/10, Batch 66/442, Training Loss: 0.6276
Epoch 10/10, Batch 67/442, Training Loss: 0.7339
Epoch 10/10, Batch 68/442, Training Loss: 0.3489
Epoch 10/10, Batch 69/442, Training Loss: 0.3948
Epoch 10/10, Batch 70/442, Training Loss: 0.2539
Epoch 10/10, Batch 71/442, Training Loss: 0.3541
Epoch 10/10, Batch 72/442, Training Loss: 0.4980
Epoch 10/10, Batch 73/442, Training Loss: 0.2795
Epoch 10/10, Batch 74/442, Training Loss: 0.4147
Epoch 10/10, Batch 75/442, Training Loss: 0.4066
Epoch 10/10, Batch 76/442, Training Loss: 0.3036
Epoch 10/10, Batch 77/442, Training Loss: 0.2539
Epoch 10/10, Batch 78/442, Training Loss: 0.4445
Epoch 10/10, Batch 79/442, Training Loss: 0.2171
Epoch 10/10, Batch 80/442, Training Loss: 0.4568
Epoch 10/10, Batch 81/442, Training Loss: 0.2029
Epoch 10/10, Batch 82/442, Training Loss: 0.3414
Epoch 10/10, Batch 83/442, Training Loss: 0.3251
Epoch 10/10, Batch 84/442, Training Loss: 0.4557
Epoch 10/10, Batch 85/442, Training Loss: 0.4047
Epoch 10/10, Batch 86/442, Training Loss: 0.5666
Epoch 10/10, Batch 87/442, Training Loss: 0.4750
Epoch 10/10, Batch 88/442, Training Loss: 0.1823
Epoch 10/10, Batch 89/442, Training Loss: 0.7144
Epoch 10/10, Batch 90/442, Training Loss: 0.5307
Epoch 10/10, Batch 91/442, Training Loss: 0.6990
Epoch 10/10, Batch 92/442, Training Loss: 0.3378
Epoch 10/10, Batch 93/442, Training Loss: 0.4038
Epoch 10/10, Batch 94/442, Training Loss: 0.2823
Epoch 10/10, Batch 95/442, Training Loss: 0.6089
Epoch 10/10, Batch 96/442, Training Loss: 0.4456
Epoch 10/10, Batch 97/442, Training Loss: 0.3668
Epoch 10/10, Batch 98/442, Training Loss: 0.5100
Epoch 10/10, Batch 99/442, Training Loss: 0.6328
Epoch 10/10, Batch 100/442, Training Loss: 0.4151
Epoch 10/10, Batch 101/442, Training Loss: 0.3323
Epoch 10/10, Batch 102/442, Training Loss: 0.3962
Epoch 10/10, Batch 103/442, Training Loss: 0.5588
Epoch 10/10, Batch 104/442, Training Loss: 0.3084
Epoch 10/10, Batch 105/442, Training Loss: 0.5307
Epoch 10/10, Batch 106/442, Training Loss: 0.5289
Epoch 10/10, Batch 107/442, Training Loss: 0.2841
Epoch 10/10, Batch 108/442, Training Loss: 0.3619
Epoch 10/10, Batch 109/442, Training Loss: 0.3463
Epoch 10/10, Batch 110/442, Training Loss: 0.3490
Epoch 10/10, Batch 111/442, Training Loss: 0.3256
Epoch 10/10, Batch 112/442, Training Loss: 0.4070
Epoch 10/10, Batch 113/442, Training Loss: 0.4793
Epoch 10/10, Batch 114/442, Training Loss: 0.5778
Epoch 10/10, Batch 115/442, Training Loss: 0.6755
Epoch 10/10, Batch 116/442, Training Loss: 0.4204
Epoch 10/10, Batch 117/442, Training Loss: 0.4881
Epoch 10/10, Batch 118/442, Training Loss: 0.5613
Epoch 10/10, Batch 119/442, Training Loss: 0.5359
Epoch 10/10, Batch 120/442, Training Loss: 0.2981
Epoch 10/10, Batch 121/442, Training Loss: 0.4219
Epoch 10/10, Batch 122/442, Training Loss: 0.4848
Epoch 10/10, Batch 123/442, Training Loss: 0.5126
Epoch 10/10, Batch 124/442, Training Loss: 0.4700
Epoch 10/10, Batch 125/442, Training Loss: 0.4256
Epoch 10/10, Batch 126/442, Training Loss: 0.4155
Epoch 10/10, Batch 127/442, Training Loss: 0.5571
Epoch 10/10, Batch 128/442, Training Loss: 0.4540
Epoch 10/10, Batch 129/442, Training Loss: 0.4400
Epoch 10/10, Batch 130/442, Training Loss: 0.4489
Epoch 10/10, Batch 131/442, Training Loss: 0.5725
Epoch 10/10, Batch 132/442, Training Loss: 0.4675
Epoch 10/10, Batch 133/442, Training Loss: 0.5320
Epoch 10/10, Batch 134/442, Training Loss: 0.3827
Epoch 10/10, Batch 135/442, Training Loss: 0.4413
Epoch 10/10, Batch 136/442, Training Loss: 0.4813
Epoch 10/10, Batch 137/442, Training Loss: 0.3074
Epoch 10/10, Batch 138/442, Training Loss: 0.3078
Epoch 10/10, Batch 139/442, Training Loss: 0.1800
Epoch 10/10, Batch 140/442, Training Loss: 0.4185
Epoch 10/10, Batch 141/442, Training Loss: 0.4830
Epoch 10/10, Batch 142/442, Training Loss: 0.3435
Epoch 10/10, Batch 143/442, Training Loss: 0.4524
Epoch 10/10, Batch 144/442, Training Loss: 0.3157
Epoch 10/10, Batch 145/442, Training Loss: 0.4051
Epoch 10/10, Batch 146/442, Training Loss: 0.3718
Epoch 10/10, Batch 147/442, Training Loss: 0.4011
Epoch 10/10, Batch 148/442, Training Loss: 0.3231
Epoch 10/10, Batch 149/442, Training Loss: 0.2919
Epoch 10/10, Batch 150/442, Training Loss: 0.3848
Epoch 10/10, Batch 151/442, Training Loss: 0.3516
Epoch 10/10, Batch 152/442, Training Loss: 0.6560
Epoch 10/10, Batch 153/442, Training Loss: 0.4451
Epoch 10/10, Batch 154/442, Training Loss: 0.2178
Epoch 10/10, Batch 155/442, Training Loss: 0.2963
Epoch 10/10, Batch 156/442, Training Loss: 0.3344
Epoch 10/10, Batch 157/442, Training Loss: 0.4985
Epoch 10/10, Batch 158/442, Training Loss: 0.4193
Epoch 10/10, Batch 159/442, Training Loss: 0.5255
Epoch 10/10, Batch 160/442, Training Loss: 0.4185
Epoch 10/10, Batch 161/442, Training Loss: 0.2910
Epoch 10/10, Batch 162/442, Training Loss: 0.3494
Epoch 10/10, Batch 163/442, Training Loss: 0.3236
Epoch 10/10, Batch 164/442, Training Loss: 0.2809
Epoch 10/10, Batch 165/442, Training Loss: 0.5998
Epoch 10/10, Batch 166/442, Training Loss: 0.4911
Epoch 10/10, Batch 167/442, Training Loss: 0.7414
Epoch 10/10, Batch 168/442, Training Loss: 0.3604
Epoch 10/10, Batch 169/442, Training Loss: 0.1551
Epoch 10/10, Batch 170/442, Training Loss: 0.5236
Epoch 10/10, Batch 171/442, Training Loss: 0.3144
Epoch 10/10, Batch 172/442, Training Loss: 0.5053
Epoch 10/10, Batch 173/442, Training Loss: 0.2969
Epoch 10/10, Batch 174/442, Training Loss: 0.3711
Epoch 10/10, Batch 175/442, Training Loss: 0.2512
Epoch 10/10, Batch 176/442, Training Loss: 0.3814
Epoch 10/10, Batch 177/442, Training Loss: 0.3081
Epoch 10/10, Batch 178/442, Training Loss: 0.5319
Epoch 10/10, Batch 179/442, Training Loss: 0.4459
Epoch 10/10, Batch 180/442, Training Loss: 0.7556
Epoch 10/10, Batch 181/442, Training Loss: 0.3236
Epoch 10/10, Batch 182/442, Training Loss: 0.3986
Epoch 10/10, Batch 183/442, Training Loss: 0.2956
Epoch 10/10, Batch 184/442, Training Loss: 0.4247
Epoch 10/10, Batch 185/442, Training Loss: 0.3678
Epoch 10/10, Batch 186/442, Training Loss: 0.3864
Epoch 10/10, Batch 187/442, Training Loss: 0.3283
Epoch 10/10, Batch 188/442, Training Loss: 0.5300
Epoch 10/10, Batch 189/442, Training Loss: 0.2335
Epoch 10/10, Batch 190/442, Training Loss: 0.3736
Epoch 10/10, Batch 191/442, Training Loss: 0.3768
Epoch 10/10, Batch 192/442, Training Loss: 0.4404
Epoch 10/10, Batch 193/442, Training Loss: 0.5160
Epoch 10/10, Batch 194/442, Training Loss: 0.2131
Epoch 10/10, Batch 195/442, Training Loss: 0.3592
Epoch 10/10, Batch 196/442, Training Loss: 0.4884
Epoch 10/10, Batch 197/442, Training Loss: 0.4423
Epoch 10/10, Batch 198/442, Training Loss: 0.2552
Epoch 10/10, Batch 199/442, Training Loss: 0.3931
Epoch 10/10, Batch 200/442, Training Loss: 0.2657
Epoch 10/10, Batch 201/442, Training Loss: 0.2526
Epoch 10/10, Batch 202/442, Training Loss: 0.4666
Epoch 10/10, Batch 203/442, Training Loss: 0.4451
Epoch 10/10, Batch 204/442, Training Loss: 0.1794
Epoch 10/10, Batch 205/442, Training Loss: 0.3282
Epoch 10/10, Batch 206/442, Training Loss: 0.4589
Epoch 10/10, Batch 207/442, Training Loss: 0.5946
Epoch 10/10, Batch 208/442, Training Loss: 0.5905
Epoch 10/10, Batch 209/442, Training Loss: 0.5958
Epoch 10/10, Batch 210/442, Training Loss: 0.3159
Epoch 10/10, Batch 211/442, Training Loss: 0.3145
Epoch 10/10, Batch 212/442, Training Loss: 0.4704
Epoch 10/10, Batch 213/442, Training Loss: 0.6199
Epoch 10/10, Batch 214/442, Training Loss: 0.4514
Epoch 10/10, Batch 215/442, Training Loss: 0.3673
Epoch 10/10, Batch 216/442, Training Loss: 0.5486
Epoch 10/10, Batch 217/442, Training Loss: 0.6474
Epoch 10/10, Batch 218/442, Training Loss: 0.4853
Epoch 10/10, Batch 219/442, Training Loss: 0.5775
Epoch 10/10, Batch 220/442, Training Loss: 0.6474
Epoch 10/10, Batch 221/442, Training Loss: 0.4205
Epoch 10/10, Batch 222/442, Training Loss: 0.3877
Epoch 10/10, Batch 223/442, Training Loss: 0.5609
Epoch 10/10, Batch 224/442, Training Loss: 0.3411
Epoch 10/10, Batch 225/442, Training Loss: 0.6221
Epoch 10/10, Batch 226/442, Training Loss: 0.4517
Epoch 10/10, Batch 227/442, Training Loss: 0.3605
Epoch 10/10, Batch 228/442, Training Loss: 0.5767
Epoch 10/10, Batch 229/442, Training Loss: 0.5712
Epoch 10/10, Batch 230/442, Training Loss: 0.3812
Epoch 10/10, Batch 231/442, Training Loss: 0.2920
Epoch 10/10, Batch 232/442, Training Loss: 0.4116
Epoch 10/10, Batch 233/442, Training Loss: 0.5410
Epoch 10/10, Batch 234/442, Training Loss: 0.3750
Epoch 10/10, Batch 235/442, Training Loss: 0.6047
Epoch 10/10, Batch 236/442, Training Loss: 0.3855
Epoch 10/10, Batch 237/442, Training Loss: 0.3682
Epoch 10/10, Batch 238/442, Training Loss: 0.5322
Epoch 10/10, Batch 239/442, Training Loss: 0.6246
Epoch 10/10, Batch 240/442, Training Loss: 0.3983
Epoch 10/10, Batch 241/442, Training Loss: 0.3118
Epoch 10/10, Batch 242/442, Training Loss: 0.4650
Epoch 10/10, Batch 243/442, Training Loss: 0.3602
Epoch 10/10, Batch 244/442, Training Loss: 0.5929
Epoch 10/10, Batch 245/442, Training Loss: 0.5208
Epoch 10/10, Batch 246/442, Training Loss: 0.2659
Epoch 10/10, Batch 247/442, Training Loss: 0.4340
Epoch 10/10, Batch 248/442, Training Loss: 0.3872
Epoch 10/10, Batch 249/442, Training Loss: 0.3648
Epoch 10/10, Batch 250/442, Training Loss: 0.2759
Epoch 10/10, Batch 251/442, Training Loss: 0.2936
Epoch 10/10, Batch 252/442, Training Loss: 0.2162
Epoch 10/10, Batch 253/442, Training Loss: 0.3379
Epoch 10/10, Batch 254/442, Training Loss: 0.5373
Epoch 10/10, Batch 255/442, Training Loss: 0.4201
Epoch 10/10, Batch 256/442, Training Loss: 0.2930
Epoch 10/10, Batch 257/442, Training Loss: 0.3030
Epoch 10/10, Batch 258/442, Training Loss: 0.4593
Epoch 10/10, Batch 259/442, Training Loss: 0.2270
Epoch 10/10, Batch 260/442, Training Loss: 0.2687
Epoch 10/10, Batch 261/442, Training Loss: 0.6008
Epoch 10/10, Batch 262/442, Training Loss: 0.4422
Epoch 10/10, Batch 263/442, Training Loss: 0.4787
Epoch 10/10, Batch 264/442, Training Loss: 0.3756
Epoch 10/10, Batch 265/442, Training Loss: 0.2811
Epoch 10/10, Batch 266/442, Training Loss: 0.4282
Epoch 10/10, Batch 267/442, Training Loss: 0.4103
Epoch 10/10, Batch 268/442, Training Loss: 0.2905
Epoch 10/10, Batch 269/442, Training Loss: 0.4654
Epoch 10/10, Batch 270/442, Training Loss: 0.3761
Epoch 10/10, Batch 271/442, Training Loss: 0.3946
Epoch 10/10, Batch 272/442, Training Loss: 0.4856
Epoch 10/10, Batch 273/442, Training Loss: 0.3750
Epoch 10/10, Batch 274/442, Training Loss: 0.2553
Epoch 10/10, Batch 275/442, Training Loss: 0.4727
Epoch 10/10, Batch 276/442, Training Loss: 0.3691
Epoch 10/10, Batch 277/442, Training Loss: 0.6858
Epoch 10/10, Batch 278/442, Training Loss: 0.4656
Epoch 10/10, Batch 279/442, Training Loss: 0.6725
Epoch 10/10, Batch 280/442, Training Loss: 0.4248
Epoch 10/10, Batch 281/442, Training Loss: 0.2852
Epoch 10/10, Batch 282/442, Training Loss: 0.5998
Epoch 10/10, Batch 283/442, Training Loss: 0.4110
Epoch 10/10, Batch 284/442, Training Loss: 0.4348
Epoch 10/10, Batch 285/442, Training Loss: 0.2354
Epoch 10/10, Batch 286/442, Training Loss: 0.2873
Epoch 10/10, Batch 287/442, Training Loss: 0.4338
Epoch 10/10, Batch 288/442, Training Loss: 0.4624
Epoch 10/10, Batch 289/442, Training Loss: 0.2835
Epoch 10/10, Batch 290/442, Training Loss: 0.4166
Epoch 10/10, Batch 291/442, Training Loss: 0.3842
Epoch 10/10, Batch 292/442, Training Loss: 0.3474
Epoch 10/10, Batch 293/442, Training Loss: 0.2181
Epoch 10/10, Batch 294/442, Training Loss: 0.6593
Epoch 10/10, Batch 295/442, Training Loss: 0.4191
Epoch 10/10, Batch 296/442, Training Loss: 0.3232
Epoch 10/10, Batch 297/442, Training Loss: 0.5402
Epoch 10/10, Batch 298/442, Training Loss: 0.3304
Epoch 10/10, Batch 299/442, Training Loss: 0.4482
Epoch 10/10, Batch 300/442, Training Loss: 0.3420
Epoch 10/10, Batch 301/442, Training Loss: 0.4237
Epoch 10/10, Batch 302/442, Training Loss: 0.2735
Epoch 10/10, Batch 303/442, Training Loss: 0.4614
Epoch 10/10, Batch 304/442, Training Loss: 0.5813
Epoch 10/10, Batch 305/442, Training Loss: 0.4548
Epoch 10/10, Batch 306/442, Training Loss: 0.4452
Epoch 10/10, Batch 307/442, Training Loss: 0.4430
Epoch 10/10, Batch 308/442, Training Loss: 0.4397
Epoch 10/10, Batch 309/442, Training Loss: 0.6647
Epoch 10/10, Batch 310/442, Training Loss: 0.8057
Epoch 10/10, Batch 311/442, Training Loss: 0.2621
Epoch 10/10, Batch 312/442, Training Loss: 0.4466
Epoch 10/10, Batch 313/442, Training Loss: 0.4732
Epoch 10/10, Batch 314/442, Training Loss: 0.5558
Epoch 10/10, Batch 315/442, Training Loss: 0.6044
Epoch 10/10, Batch 316/442, Training Loss: 0.3911
Epoch 10/10, Batch 317/442, Training Loss: 0.4479
Epoch 10/10, Batch 318/442, Training Loss: 0.4585
Epoch 10/10, Batch 319/442, Training Loss: 0.5545
Epoch 10/10, Batch 320/442, Training Loss: 0.3313
Epoch 10/10, Batch 321/442, Training Loss: 0.3384
Epoch 10/10, Batch 322/442, Training Loss: 0.3564
Epoch 10/10, Batch 323/442, Training Loss: 0.3533
Epoch 10/10, Batch 324/442, Training Loss: 0.3036
Epoch 10/10, Batch 325/442, Training Loss: 0.4380
Epoch 10/10, Batch 326/442, Training Loss: 0.4067
Epoch 10/10, Batch 327/442, Training Loss: 0.7197
Epoch 10/10, Batch 328/442, Training Loss: 0.6403
Epoch 10/10, Batch 329/442, Training Loss: 0.5627
Epoch 10/10, Batch 330/442, Training Loss: 0.5190
Epoch 10/10, Batch 331/442, Training Loss: 0.5505
Epoch 10/10, Batch 332/442, Training Loss: 0.4165
Epoch 10/10, Batch 333/442, Training Loss: 0.5605
Epoch 10/10, Batch 334/442, Training Loss: 0.3246
Epoch 10/10, Batch 335/442, Training Loss: 0.6262
Epoch 10/10, Batch 336/442, Training Loss: 0.4124
Epoch 10/10, Batch 337/442, Training Loss: 0.2422
Epoch 10/10, Batch 338/442, Training Loss: 0.4709
Epoch 10/10, Batch 339/442, Training Loss: 0.3754
Epoch 10/10, Batch 340/442, Training Loss: 0.2946
Epoch 10/10, Batch 341/442, Training Loss: 0.3986
Epoch 10/10, Batch 342/442, Training Loss: 0.3486
Epoch 10/10, Batch 343/442, Training Loss: 0.5952
Epoch 10/10, Batch 344/442, Training Loss: 0.3589
Epoch 10/10, Batch 345/442, Training Loss: 0.2752
Epoch 10/10, Batch 346/442, Training Loss: 0.4425
Epoch 10/10, Batch 347/442, Training Loss: 0.3419
Epoch 10/10, Batch 348/442, Training Loss: 0.4825
Epoch 10/10, Batch 349/442, Training Loss: 0.3623
Epoch 10/10, Batch 350/442, Training Loss: 0.3669
Epoch 10/10, Batch 351/442, Training Loss: 0.4093
Epoch 10/10, Batch 352/442, Training Loss: 0.4597
Epoch 10/10, Batch 353/442, Training Loss: 0.3587
Epoch 10/10, Batch 354/442, Training Loss: 0.5360
Epoch 10/10, Batch 355/442, Training Loss: 0.5117
Epoch 10/10, Batch 356/442, Training Loss: 0.3842
Epoch 10/10, Batch 357/442, Training Loss: 0.4560
Epoch 10/10, Batch 358/442, Training Loss: 0.2586
Epoch 10/10, Batch 359/442, Training Loss: 0.3215
Epoch 10/10, Batch 360/442, Training Loss: 0.5674
Epoch 10/10, Batch 361/442, Training Loss: 0.5728
Epoch 10/10, Batch 362/442, Training Loss: 0.5161
Epoch 10/10, Batch 363/442, Training Loss: 0.4225
Epoch 10/10, Batch 364/442, Training Loss: 0.4205
Epoch 10/10, Batch 365/442, Training Loss: 0.3437
Epoch 10/10, Batch 366/442, Training Loss: 0.4508
Epoch 10/10, Batch 367/442, Training Loss: 0.4134
Epoch 10/10, Batch 368/442, Training Loss: 0.3098
Epoch 10/10, Batch 369/442, Training Loss: 0.3437
Epoch 10/10, Batch 370/442, Training Loss: 0.2924
Epoch 10/10, Batch 371/442, Training Loss: 0.6741
Epoch 10/10, Batch 372/442, Training Loss: 0.3013
Epoch 10/10, Batch 373/442, Training Loss: 0.2554
Epoch 10/10, Batch 374/442, Training Loss: 0.3478
Epoch 10/10, Batch 375/442, Training Loss: 0.3444
Epoch 10/10, Batch 376/442, Training Loss: 0.4042
Epoch 10/10, Batch 377/442, Training Loss: 0.5070
Epoch 10/10, Batch 378/442, Training Loss: 0.7407
Epoch 10/10, Batch 379/442, Training Loss: 0.5518
Epoch 10/10, Batch 380/442, Training Loss: 0.4188
Epoch 10/10, Batch 381/442, Training Loss: 0.5852
Epoch 10/10, Batch 382/442, Training Loss: 0.4125
Epoch 10/10, Batch 383/442, Training Loss: 0.3310
Epoch 10/10, Batch 384/442, Training Loss: 0.3729
Epoch 10/10, Batch 385/442, Training Loss: 0.5204
Epoch 10/10, Batch 386/442, Training Loss: 0.4948
Epoch 10/10, Batch 387/442, Training Loss: 0.4671
Epoch 10/10, Batch 388/442, Training Loss: 0.3660
Epoch 10/10, Batch 389/442, Training Loss: 0.4295
Epoch 10/10, Batch 390/442, Training Loss: 0.3082
Epoch 10/10, Batch 391/442, Training Loss: 0.3881
Epoch 10/10, Batch 392/442, Training Loss: 0.6211
Epoch 10/10, Batch 393/442, Training Loss: 0.4534
Epoch 10/10, Batch 394/442, Training Loss: 0.3163
Epoch 10/10, Batch 395/442, Training Loss: 0.2596
Epoch 10/10, Batch 396/442, Training Loss: 0.4429
Epoch 10/10, Batch 397/442, Training Loss: 0.5086
Epoch 10/10, Batch 398/442, Training Loss: 0.3597
Epoch 10/10, Batch 399/442, Training Loss: 0.4897
Epoch 10/10, Batch 400/442, Training Loss: 0.5767
Epoch 10/10, Batch 401/442, Training Loss: 0.3028
Epoch 10/10, Batch 402/442, Training Loss: 0.2908
Epoch 10/10, Batch 403/442, Training Loss: 0.4917
Epoch 10/10, Batch 404/442, Training Loss: 0.4441
Epoch 10/10, Batch 405/442, Training Loss: 0.5146
Epoch 10/10, Batch 406/442, Training Loss: 0.6903
Epoch 10/10, Batch 407/442, Training Loss: 0.3967
Epoch 10/10, Batch 408/442, Training Loss: 0.5101
Epoch 10/10, Batch 409/442, Training Loss: 0.2798
Epoch 10/10, Batch 410/442, Training Loss: 0.1762
Epoch 10/10, Batch 411/442, Training Loss: 0.4717
Epoch 10/10, Batch 412/442, Training Loss: 0.3116
Epoch 10/10, Batch 413/442, Training Loss: 0.4271
Epoch 10/10, Batch 414/442, Training Loss: 0.3412
Epoch 10/10, Batch 415/442, Training Loss: 0.4140
Epoch 10/10, Batch 416/442, Training Loss: 0.4534
Epoch 10/10, Batch 417/442, Training Loss: 0.4820
Epoch 10/10, Batch 418/442, Training Loss: 0.5552
Epoch 10/10, Batch 419/442, Training Loss: 0.3865
Epoch 10/10, Batch 420/442, Training Loss: 0.3473
Epoch 10/10, Batch 421/442, Training Loss: 0.3211
Epoch 10/10, Batch 422/442, Training Loss: 0.5189
Epoch 10/10, Batch 423/442, Training Loss: 0.5042
Epoch 10/10, Batch 424/442, Training Loss: 0.1945
Epoch 10/10, Batch 425/442, Training Loss: 0.3313
Epoch 10/10, Batch 426/442, Training Loss: 0.3528
Epoch 10/10, Batch 427/442, Training Loss: 0.2418
Epoch 10/10, Batch 428/442, Training Loss: 0.1807
Epoch 10/10, Batch 429/442, Training Loss: 0.3813
Epoch 10/10, Batch 430/442, Training Loss: 0.3841
Epoch 10/10, Batch 431/442, Training Loss: 0.5222
Epoch 10/10, Batch 432/442, Training Loss: 0.5201
Epoch 10/10, Batch 433/442, Training Loss: 0.2757
Epoch 10/10, Batch 434/442, Training Loss: 0.3815
Epoch 10/10, Batch 435/442, Training Loss: 0.3542
Epoch 10/10, Batch 436/442, Training Loss: 0.4288
Epoch 10/10, Batch 437/442, Training Loss: 0.4822
Epoch 10/10, Batch 438/442, Training Loss: 0.3293
Epoch 10/10, Batch 439/442, Training Loss: 0.5705
Epoch 10/10, Batch 440/442, Training Loss: 0.3585
Epoch 10/10, Batch 441/442, Training Loss: 0.8481
Epoch 10/10, Batch 442/442, Training Loss: 0.5798
Epoch 10/10, Training Loss: 0.4207, Validation Loss: 0.4887, Validation Accuracy: 0.8070
Test Loss: 0.4738, Test Accuracy: 0.8086
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-04-27 20:23:59,560] Trial 2 finished with value: 0.446455329368563 and parameters: {'batch_size': 32, 'learning_rate': 0.0036333389999519293, 'weight_decay': 1.2022950123015283e-06}. Best is trial 0 with value: 0.2196991708036512.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/10, Batch 1/883, Training Loss: 1.3854
Epoch 1/10, Batch 2/883, Training Loss: 3.8887
Epoch 1/10, Batch 3/883, Training Loss: 2.9946
Epoch 1/10, Batch 4/883, Training Loss: 2.3365
Epoch 1/10, Batch 5/883, Training Loss: 1.9075
Epoch 1/10, Batch 6/883, Training Loss: 2.8220
Epoch 1/10, Batch 7/883, Training Loss: 3.1399
Epoch 1/10, Batch 8/883, Training Loss: 2.9855
Epoch 1/10, Batch 9/883, Training Loss: 1.9982
Epoch 1/10, Batch 10/883, Training Loss: 2.0543
Epoch 1/10, Batch 11/883, Training Loss: 1.2037
Epoch 1/10, Batch 12/883, Training Loss: 1.0874
Epoch 1/10, Batch 13/883, Training Loss: 2.0908
Epoch 1/10, Batch 14/883, Training Loss: 1.5815
Epoch 1/10, Batch 15/883, Training Loss: 0.6989
Epoch 1/10, Batch 16/883, Training Loss: 2.0420
Epoch 1/10, Batch 17/883, Training Loss: 2.1085
Epoch 1/10, Batch 18/883, Training Loss: 1.3202
Epoch 1/10, Batch 19/883, Training Loss: 1.1209
Epoch 1/10, Batch 20/883, Training Loss: 1.3163
Epoch 1/10, Batch 21/883, Training Loss: 1.4990
Epoch 1/10, Batch 22/883, Training Loss: 0.9683
Epoch 1/10, Batch 23/883, Training Loss: 1.2375
Epoch 1/10, Batch 24/883, Training Loss: 1.1721
Epoch 1/10, Batch 25/883, Training Loss: 1.2217
Epoch 1/10, Batch 26/883, Training Loss: 1.0801
Epoch 1/10, Batch 27/883, Training Loss: 2.1956
Epoch 1/10, Batch 28/883, Training Loss: 1.5211
Epoch 1/10, Batch 29/883, Training Loss: 1.4138
Epoch 1/10, Batch 30/883, Training Loss: 1.6589
Epoch 1/10, Batch 31/883, Training Loss: 1.2373
Epoch 1/10, Batch 32/883, Training Loss: 0.9551
Epoch 1/10, Batch 33/883, Training Loss: 1.0162
Epoch 1/10, Batch 34/883, Training Loss: 0.8445
Epoch 1/10, Batch 35/883, Training Loss: 1.1227
Epoch 1/10, Batch 36/883, Training Loss: 0.9767
Epoch 1/10, Batch 37/883, Training Loss: 0.8519
Epoch 1/10, Batch 38/883, Training Loss: 1.3821
Epoch 1/10, Batch 39/883, Training Loss: 1.0912
Epoch 1/10, Batch 40/883, Training Loss: 1.3806
Epoch 1/10, Batch 41/883, Training Loss: 1.1074
Epoch 1/10, Batch 42/883, Training Loss: 1.6646
Epoch 1/10, Batch 43/883, Training Loss: 1.0806
Epoch 1/10, Batch 44/883, Training Loss: 0.9647
Epoch 1/10, Batch 45/883, Training Loss: 1.1154
Epoch 1/10, Batch 46/883, Training Loss: 1.2812
Epoch 1/10, Batch 47/883, Training Loss: 0.9647
Epoch 1/10, Batch 48/883, Training Loss: 0.9189
Epoch 1/10, Batch 49/883, Training Loss: 1.2579
Epoch 1/10, Batch 50/883, Training Loss: 0.9443
Epoch 1/10, Batch 51/883, Training Loss: 1.1672
Epoch 1/10, Batch 52/883, Training Loss: 1.1883
Epoch 1/10, Batch 53/883, Training Loss: 1.2061
Epoch 1/10, Batch 54/883, Training Loss: 0.9487
Epoch 1/10, Batch 55/883, Training Loss: 1.0810
Epoch 1/10, Batch 56/883, Training Loss: 1.2428
Epoch 1/10, Batch 57/883, Training Loss: 1.4192
Epoch 1/10, Batch 58/883, Training Loss: 1.2216
Epoch 1/10, Batch 59/883, Training Loss: 1.2378
Epoch 1/10, Batch 60/883, Training Loss: 1.0956
Epoch 1/10, Batch 61/883, Training Loss: 0.9453
Epoch 1/10, Batch 62/883, Training Loss: 1.2370
Epoch 1/10, Batch 63/883, Training Loss: 1.6047
Epoch 1/10, Batch 64/883, Training Loss: 1.1715
Epoch 1/10, Batch 65/883, Training Loss: 1.4222
Epoch 1/10, Batch 66/883, Training Loss: 0.8882
Epoch 1/10, Batch 67/883, Training Loss: 1.2498
Epoch 1/10, Batch 68/883, Training Loss: 1.1752
Epoch 1/10, Batch 69/883, Training Loss: 1.1441
Epoch 1/10, Batch 70/883, Training Loss: 1.1775
Epoch 1/10, Batch 71/883, Training Loss: 1.3512
Epoch 1/10, Batch 72/883, Training Loss: 1.3796
Epoch 1/10, Batch 73/883, Training Loss: 1.5224
Epoch 1/10, Batch 74/883, Training Loss: 1.4801
Epoch 1/10, Batch 75/883, Training Loss: 1.0589
Epoch 1/10, Batch 76/883, Training Loss: 1.1999
Epoch 1/10, Batch 77/883, Training Loss: 1.0319
Epoch 1/10, Batch 78/883, Training Loss: 1.0607
Epoch 1/10, Batch 79/883, Training Loss: 0.8726
Epoch 1/10, Batch 80/883, Training Loss: 0.9557
Epoch 1/10, Batch 81/883, Training Loss: 1.3339
Epoch 1/10, Batch 82/883, Training Loss: 0.8506
Epoch 1/10, Batch 83/883, Training Loss: 1.0370
Epoch 1/10, Batch 84/883, Training Loss: 0.9614
Epoch 1/10, Batch 85/883, Training Loss: 0.8999
Epoch 1/10, Batch 86/883, Training Loss: 1.4111
Epoch 1/10, Batch 87/883, Training Loss: 1.2371
Epoch 1/10, Batch 88/883, Training Loss: 0.9149
Epoch 1/10, Batch 89/883, Training Loss: 1.3310
Epoch 1/10, Batch 90/883, Training Loss: 1.1100
Epoch 1/10, Batch 91/883, Training Loss: 0.7918
Epoch 1/10, Batch 92/883, Training Loss: 1.0686
Epoch 1/10, Batch 93/883, Training Loss: 0.8757
Epoch 1/10, Batch 94/883, Training Loss: 1.2796
Epoch 1/10, Batch 95/883, Training Loss: 1.3406
Epoch 1/10, Batch 96/883, Training Loss: 1.1409
Epoch 1/10, Batch 97/883, Training Loss: 0.9392
Epoch 1/10, Batch 98/883, Training Loss: 1.4560
Epoch 1/10, Batch 99/883, Training Loss: 0.8313
Epoch 1/10, Batch 100/883, Training Loss: 1.5333
Epoch 1/10, Batch 101/883, Training Loss: 0.6890
Epoch 1/10, Batch 102/883, Training Loss: 0.6753
Epoch 1/10, Batch 103/883, Training Loss: 1.0252
Epoch 1/10, Batch 104/883, Training Loss: 1.0913
Epoch 1/10, Batch 105/883, Training Loss: 1.1885
Epoch 1/10, Batch 106/883, Training Loss: 0.7750
Epoch 1/10, Batch 107/883, Training Loss: 1.4102
Epoch 1/10, Batch 108/883, Training Loss: 0.9434
Epoch 1/10, Batch 109/883, Training Loss: 1.1479
Epoch 1/10, Batch 110/883, Training Loss: 1.4942
Epoch 1/10, Batch 111/883, Training Loss: 0.7844
Epoch 1/10, Batch 112/883, Training Loss: 0.9690
Epoch 1/10, Batch 113/883, Training Loss: 0.9619
Epoch 1/10, Batch 114/883, Training Loss: 0.9904
Epoch 1/10, Batch 115/883, Training Loss: 1.1761
Epoch 1/10, Batch 116/883, Training Loss: 1.0265
Epoch 1/10, Batch 117/883, Training Loss: 1.1174
Epoch 1/10, Batch 118/883, Training Loss: 0.9814
Epoch 1/10, Batch 119/883, Training Loss: 0.9641
Epoch 1/10, Batch 120/883, Training Loss: 0.9813
Epoch 1/10, Batch 121/883, Training Loss: 0.9319
Epoch 1/10, Batch 122/883, Training Loss: 1.3254
Epoch 1/10, Batch 123/883, Training Loss: 1.4217
Epoch 1/10, Batch 124/883, Training Loss: 1.2426
Epoch 1/10, Batch 125/883, Training Loss: 1.0835
Epoch 1/10, Batch 126/883, Training Loss: 1.3772
Epoch 1/10, Batch 127/883, Training Loss: 1.1764
Epoch 1/10, Batch 128/883, Training Loss: 0.8667
Epoch 1/10, Batch 129/883, Training Loss: 0.9126
Epoch 1/10, Batch 130/883, Training Loss: 1.0458
Epoch 1/10, Batch 131/883, Training Loss: 0.8594
Epoch 1/10, Batch 132/883, Training Loss: 1.1547
Epoch 1/10, Batch 133/883, Training Loss: 1.5096
Epoch 1/10, Batch 134/883, Training Loss: 1.1535
Epoch 1/10, Batch 135/883, Training Loss: 0.8201
Epoch 1/10, Batch 136/883, Training Loss: 1.1609
Epoch 1/10, Batch 137/883, Training Loss: 1.1602
Epoch 1/10, Batch 138/883, Training Loss: 1.8307
Epoch 1/10, Batch 139/883, Training Loss: 1.5346
Epoch 1/10, Batch 140/883, Training Loss: 0.7438
Epoch 1/10, Batch 141/883, Training Loss: 1.0651
Epoch 1/10, Batch 142/883, Training Loss: 0.9898
Epoch 1/10, Batch 143/883, Training Loss: 0.9855
Epoch 1/10, Batch 144/883, Training Loss: 0.9207
Epoch 1/10, Batch 145/883, Training Loss: 0.9424
Epoch 1/10, Batch 146/883, Training Loss: 1.1878
Epoch 1/10, Batch 147/883, Training Loss: 1.1050
Epoch 1/10, Batch 148/883, Training Loss: 1.0099
Epoch 1/10, Batch 149/883, Training Loss: 1.1428
Epoch 1/10, Batch 150/883, Training Loss: 0.9756
Epoch 1/10, Batch 151/883, Training Loss: 1.0733
Epoch 1/10, Batch 152/883, Training Loss: 0.7910
Epoch 1/10, Batch 153/883, Training Loss: 1.1379
Epoch 1/10, Batch 154/883, Training Loss: 1.0401
Epoch 1/10, Batch 155/883, Training Loss: 0.8734
Epoch 1/10, Batch 156/883, Training Loss: 0.9739
Epoch 1/10, Batch 157/883, Training Loss: 1.1668
Epoch 1/10, Batch 158/883, Training Loss: 0.6453
Epoch 1/10, Batch 159/883, Training Loss: 1.6293
Epoch 1/10, Batch 160/883, Training Loss: 0.7983
Epoch 1/10, Batch 161/883, Training Loss: 1.4385
Epoch 1/10, Batch 162/883, Training Loss: 1.0285
Epoch 1/10, Batch 163/883, Training Loss: 0.9018
Epoch 1/10, Batch 164/883, Training Loss: 1.0945
Epoch 1/10, Batch 165/883, Training Loss: 0.8261
Epoch 1/10, Batch 166/883, Training Loss: 0.8280
Epoch 1/10, Batch 167/883, Training Loss: 1.2565
Epoch 1/10, Batch 168/883, Training Loss: 0.9600
Epoch 1/10, Batch 169/883, Training Loss: 1.1717
Epoch 1/10, Batch 170/883, Training Loss: 0.9255
Epoch 1/10, Batch 171/883, Training Loss: 0.9892
Epoch 1/10, Batch 172/883, Training Loss: 0.9174
Epoch 1/10, Batch 173/883, Training Loss: 0.9741
Epoch 1/10, Batch 174/883, Training Loss: 0.8498
Epoch 1/10, Batch 175/883, Training Loss: 0.9036
Epoch 1/10, Batch 176/883, Training Loss: 0.8311
Epoch 1/10, Batch 177/883, Training Loss: 1.0118
Epoch 1/10, Batch 178/883, Training Loss: 0.6553
Epoch 1/10, Batch 179/883, Training Loss: 1.3241
Epoch 1/10, Batch 180/883, Training Loss: 0.9817
Epoch 1/10, Batch 181/883, Training Loss: 1.1547
Epoch 1/10, Batch 182/883, Training Loss: 0.8848
Epoch 1/10, Batch 183/883, Training Loss: 0.9348
Epoch 1/10, Batch 184/883, Training Loss: 0.8831
Epoch 1/10, Batch 185/883, Training Loss: 1.2209
Epoch 1/10, Batch 186/883, Training Loss: 1.3632
Epoch 1/10, Batch 187/883, Training Loss: 1.0538
Epoch 1/10, Batch 188/883, Training Loss: 1.0249
Epoch 1/10, Batch 189/883, Training Loss: 1.1366
Epoch 1/10, Batch 190/883, Training Loss: 0.7694
Epoch 1/10, Batch 191/883, Training Loss: 0.8232
Epoch 1/10, Batch 192/883, Training Loss: 1.1725
Epoch 1/10, Batch 193/883, Training Loss: 0.9156
Epoch 1/10, Batch 194/883, Training Loss: 0.9285
Epoch 1/10, Batch 195/883, Training Loss: 1.0460
Epoch 1/10, Batch 196/883, Training Loss: 0.8200
Epoch 1/10, Batch 197/883, Training Loss: 1.0538
Epoch 1/10, Batch 198/883, Training Loss: 0.9969
Epoch 1/10, Batch 199/883, Training Loss: 1.1513
Epoch 1/10, Batch 200/883, Training Loss: 0.9477
Epoch 1/10, Batch 201/883, Training Loss: 0.9989
Epoch 1/10, Batch 202/883, Training Loss: 1.1574
Epoch 1/10, Batch 203/883, Training Loss: 1.3261
Epoch 1/10, Batch 204/883, Training Loss: 0.9350
Epoch 1/10, Batch 205/883, Training Loss: 1.0733
Epoch 1/10, Batch 206/883, Training Loss: 0.8004
Epoch 1/10, Batch 207/883, Training Loss: 1.0413
Epoch 1/10, Batch 208/883, Training Loss: 0.8606
Epoch 1/10, Batch 209/883, Training Loss: 1.0656
Epoch 1/10, Batch 210/883, Training Loss: 0.9109
Epoch 1/10, Batch 211/883, Training Loss: 1.1598
Epoch 1/10, Batch 212/883, Training Loss: 0.9658
Epoch 1/10, Batch 213/883, Training Loss: 0.9847
Epoch 1/10, Batch 214/883, Training Loss: 0.8882
Epoch 1/10, Batch 215/883, Training Loss: 0.8945
Epoch 1/10, Batch 216/883, Training Loss: 0.8752
Epoch 1/10, Batch 217/883, Training Loss: 1.2205
Epoch 1/10, Batch 218/883, Training Loss: 0.6670
Epoch 1/10, Batch 219/883, Training Loss: 1.1303
Epoch 1/10, Batch 220/883, Training Loss: 1.4481
Epoch 1/10, Batch 221/883, Training Loss: 1.1525
Epoch 1/10, Batch 222/883, Training Loss: 1.0507
Epoch 1/10, Batch 223/883, Training Loss: 1.0432
Epoch 1/10, Batch 224/883, Training Loss: 1.0546
Epoch 1/10, Batch 225/883, Training Loss: 0.9321
Epoch 1/10, Batch 226/883, Training Loss: 1.0777
Epoch 1/10, Batch 227/883, Training Loss: 0.9955
Epoch 1/10, Batch 228/883, Training Loss: 0.9813
Epoch 1/10, Batch 229/883, Training Loss: 0.9524
Epoch 1/10, Batch 230/883, Training Loss: 0.9596
Epoch 1/10, Batch 231/883, Training Loss: 0.8462
Epoch 1/10, Batch 232/883, Training Loss: 0.8638
Epoch 1/10, Batch 233/883, Training Loss: 0.9767
Epoch 1/10, Batch 234/883, Training Loss: 1.0016
Epoch 1/10, Batch 235/883, Training Loss: 0.8720
Epoch 1/10, Batch 236/883, Training Loss: 0.8847
Epoch 1/10, Batch 237/883, Training Loss: 1.1082
Epoch 1/10, Batch 238/883, Training Loss: 1.0958
Epoch 1/10, Batch 239/883, Training Loss: 1.0971
Epoch 1/10, Batch 240/883, Training Loss: 0.9457
Epoch 1/10, Batch 241/883, Training Loss: 1.0398
Epoch 1/10, Batch 242/883, Training Loss: 0.8939
Epoch 1/10, Batch 243/883, Training Loss: 0.9933
Epoch 1/10, Batch 244/883, Training Loss: 1.3344
Epoch 1/10, Batch 245/883, Training Loss: 0.8548
Epoch 1/10, Batch 246/883, Training Loss: 0.9214
Epoch 1/10, Batch 247/883, Training Loss: 0.9170
Epoch 1/10, Batch 248/883, Training Loss: 0.6655
Epoch 1/10, Batch 249/883, Training Loss: 1.0662
Epoch 1/10, Batch 250/883, Training Loss: 0.8816
Epoch 1/10, Batch 251/883, Training Loss: 0.8598
Epoch 1/10, Batch 252/883, Training Loss: 1.0550
Epoch 1/10, Batch 253/883, Training Loss: 1.0921
Epoch 1/10, Batch 254/883, Training Loss: 1.2046
Epoch 1/10, Batch 255/883, Training Loss: 1.0076
Epoch 1/10, Batch 256/883, Training Loss: 0.8762
Epoch 1/10, Batch 257/883, Training Loss: 0.8564
Epoch 1/10, Batch 258/883, Training Loss: 0.7941
Epoch 1/10, Batch 259/883, Training Loss: 0.8451
Epoch 1/10, Batch 260/883, Training Loss: 1.0367
Epoch 1/10, Batch 261/883, Training Loss: 0.8894
Epoch 1/10, Batch 262/883, Training Loss: 1.0119
Epoch 1/10, Batch 263/883, Training Loss: 0.9235
Epoch 1/10, Batch 264/883, Training Loss: 1.5405
Epoch 1/10, Batch 265/883, Training Loss: 0.8670
Epoch 1/10, Batch 266/883, Training Loss: 1.0620
Epoch 1/10, Batch 267/883, Training Loss: 0.7206
Epoch 1/10, Batch 268/883, Training Loss: 1.0281
Epoch 1/10, Batch 269/883, Training Loss: 1.0127
Epoch 1/10, Batch 270/883, Training Loss: 0.8589
Epoch 1/10, Batch 271/883, Training Loss: 0.7702
Epoch 1/10, Batch 272/883, Training Loss: 1.0124
Epoch 1/10, Batch 273/883, Training Loss: 1.0121
Epoch 1/10, Batch 274/883, Training Loss: 0.9488
Epoch 1/10, Batch 275/883, Training Loss: 0.9334
Epoch 1/10, Batch 276/883, Training Loss: 0.9217
Epoch 1/10, Batch 277/883, Training Loss: 0.8372
Epoch 1/10, Batch 278/883, Training Loss: 0.7591
Epoch 1/10, Batch 279/883, Training Loss: 0.6920
Epoch 1/10, Batch 280/883, Training Loss: 0.8025
Epoch 1/10, Batch 281/883, Training Loss: 0.8205
Epoch 1/10, Batch 282/883, Training Loss: 0.7270
Epoch 1/10, Batch 283/883, Training Loss: 1.0182
Epoch 1/10, Batch 284/883, Training Loss: 0.8327
Epoch 1/10, Batch 285/883, Training Loss: 0.8230
Epoch 1/10, Batch 286/883, Training Loss: 0.5922
Epoch 1/10, Batch 287/883, Training Loss: 0.6235
Epoch 1/10, Batch 288/883, Training Loss: 1.2140
Epoch 1/10, Batch 289/883, Training Loss: 1.3490
Epoch 1/10, Batch 290/883, Training Loss: 0.7924
Epoch 1/10, Batch 291/883, Training Loss: 1.0443
Epoch 1/10, Batch 292/883, Training Loss: 0.8903
Epoch 1/10, Batch 293/883, Training Loss: 0.9960
Epoch 1/10, Batch 294/883, Training Loss: 1.2878
Epoch 1/10, Batch 295/883, Training Loss: 0.7159
Epoch 1/10, Batch 296/883, Training Loss: 0.7874
Epoch 1/10, Batch 297/883, Training Loss: 0.9549
Epoch 1/10, Batch 298/883, Training Loss: 0.9495
Epoch 1/10, Batch 299/883, Training Loss: 1.1088
Epoch 1/10, Batch 300/883, Training Loss: 0.7807
Epoch 1/10, Batch 301/883, Training Loss: 1.0246
Epoch 1/10, Batch 302/883, Training Loss: 0.8909
Epoch 1/10, Batch 303/883, Training Loss: 0.9153
Epoch 1/10, Batch 304/883, Training Loss: 1.0977
Epoch 1/10, Batch 305/883, Training Loss: 0.7639
Epoch 1/10, Batch 306/883, Training Loss: 1.0207
Epoch 1/10, Batch 307/883, Training Loss: 1.0100
Epoch 1/10, Batch 308/883, Training Loss: 1.1271
Epoch 1/10, Batch 309/883, Training Loss: 0.8963
Epoch 1/10, Batch 310/883, Training Loss: 0.6111
Epoch 1/10, Batch 311/883, Training Loss: 0.8785
Epoch 1/10, Batch 312/883, Training Loss: 0.6157
Epoch 1/10, Batch 313/883, Training Loss: 1.2559
Epoch 1/10, Batch 314/883, Training Loss: 0.7892
Epoch 1/10, Batch 315/883, Training Loss: 1.0010
Epoch 1/10, Batch 316/883, Training Loss: 0.8856
Epoch 1/10, Batch 317/883, Training Loss: 0.9650
Epoch 1/10, Batch 318/883, Training Loss: 0.8763
Epoch 1/10, Batch 319/883, Training Loss: 0.8184
Epoch 1/10, Batch 320/883, Training Loss: 0.7716
Epoch 1/10, Batch 321/883, Training Loss: 0.8792
Epoch 1/10, Batch 322/883, Training Loss: 0.8710
Epoch 1/10, Batch 323/883, Training Loss: 0.8101
Epoch 1/10, Batch 324/883, Training Loss: 0.8166
Epoch 1/10, Batch 325/883, Training Loss: 0.7893
Epoch 1/10, Batch 326/883, Training Loss: 0.8988
Epoch 1/10, Batch 327/883, Training Loss: 1.5385
Epoch 1/10, Batch 328/883, Training Loss: 0.9558
Epoch 1/10, Batch 329/883, Training Loss: 1.2438
Epoch 1/10, Batch 330/883, Training Loss: 0.8192
Epoch 1/10, Batch 331/883, Training Loss: 0.9149
Epoch 1/10, Batch 332/883, Training Loss: 0.9673
Epoch 1/10, Batch 333/883, Training Loss: 0.7958
Epoch 1/10, Batch 334/883, Training Loss: 0.8518
Epoch 1/10, Batch 335/883, Training Loss: 0.8397
Epoch 1/10, Batch 336/883, Training Loss: 0.8920
Epoch 1/10, Batch 337/883, Training Loss: 0.8577
Epoch 1/10, Batch 338/883, Training Loss: 1.0853
Epoch 1/10, Batch 339/883, Training Loss: 0.7103
Epoch 1/10, Batch 340/883, Training Loss: 0.9487
Epoch 1/10, Batch 341/883, Training Loss: 1.0225
Epoch 1/10, Batch 342/883, Training Loss: 0.7889
Epoch 1/10, Batch 343/883, Training Loss: 1.0211
Epoch 1/10, Batch 344/883, Training Loss: 0.9495
Epoch 1/10, Batch 345/883, Training Loss: 0.9647
Epoch 1/10, Batch 346/883, Training Loss: 1.0765
Epoch 1/10, Batch 347/883, Training Loss: 1.0372
Epoch 1/10, Batch 348/883, Training Loss: 0.9854
Epoch 1/10, Batch 349/883, Training Loss: 0.7866
Epoch 1/10, Batch 350/883, Training Loss: 0.8907
Epoch 1/10, Batch 351/883, Training Loss: 1.1544
Epoch 1/10, Batch 352/883, Training Loss: 0.8154
Epoch 1/10, Batch 353/883, Training Loss: 0.8958
Epoch 1/10, Batch 354/883, Training Loss: 0.9909
Epoch 1/10, Batch 355/883, Training Loss: 0.9147
Epoch 1/10, Batch 356/883, Training Loss: 0.9888
Epoch 1/10, Batch 357/883, Training Loss: 0.8659
Epoch 1/10, Batch 358/883, Training Loss: 1.0445
Epoch 1/10, Batch 359/883, Training Loss: 0.9408
Epoch 1/10, Batch 360/883, Training Loss: 1.0009
Epoch 1/10, Batch 361/883, Training Loss: 0.9582
Epoch 1/10, Batch 362/883, Training Loss: 0.9242
Epoch 1/10, Batch 363/883, Training Loss: 1.1916
Epoch 1/10, Batch 364/883, Training Loss: 0.9644
Epoch 1/10, Batch 365/883, Training Loss: 0.8495
Epoch 1/10, Batch 366/883, Training Loss: 0.9940
Epoch 1/10, Batch 367/883, Training Loss: 0.7013
Epoch 1/10, Batch 368/883, Training Loss: 0.8295
Epoch 1/10, Batch 369/883, Training Loss: 1.1053
Epoch 1/10, Batch 370/883, Training Loss: 0.8172
Epoch 1/10, Batch 371/883, Training Loss: 0.9667
Epoch 1/10, Batch 372/883, Training Loss: 1.0725
Epoch 1/10, Batch 373/883, Training Loss: 0.7997
Epoch 1/10, Batch 374/883, Training Loss: 1.0875
Epoch 1/10, Batch 375/883, Training Loss: 0.8939
Epoch 1/10, Batch 376/883, Training Loss: 0.7454
Epoch 1/10, Batch 377/883, Training Loss: 0.8090
Epoch 1/10, Batch 378/883, Training Loss: 1.1087
Epoch 1/10, Batch 379/883, Training Loss: 0.7963
Epoch 1/10, Batch 380/883, Training Loss: 0.8418
Epoch 1/10, Batch 381/883, Training Loss: 1.1088
Epoch 1/10, Batch 382/883, Training Loss: 1.0253
Epoch 1/10, Batch 383/883, Training Loss: 0.9080
Epoch 1/10, Batch 384/883, Training Loss: 0.8791
Epoch 1/10, Batch 385/883, Training Loss: 0.8785
Epoch 1/10, Batch 386/883, Training Loss: 1.0382
Epoch 1/10, Batch 387/883, Training Loss: 0.8788
Epoch 1/10, Batch 388/883, Training Loss: 0.7880
Epoch 1/10, Batch 389/883, Training Loss: 0.8670
Epoch 1/10, Batch 390/883, Training Loss: 1.8754
Epoch 1/10, Batch 391/883, Training Loss: 0.7846
Epoch 1/10, Batch 392/883, Training Loss: 0.8369
Epoch 1/10, Batch 393/883, Training Loss: 0.9027
Epoch 1/10, Batch 394/883, Training Loss: 0.9329
Epoch 1/10, Batch 395/883, Training Loss: 0.9790
Epoch 1/10, Batch 396/883, Training Loss: 0.8371
Epoch 1/10, Batch 397/883, Training Loss: 0.7702
Epoch 1/10, Batch 398/883, Training Loss: 0.8735
Epoch 1/10, Batch 399/883, Training Loss: 1.0768
Epoch 1/10, Batch 400/883, Training Loss: 0.8456
Epoch 1/10, Batch 401/883, Training Loss: 0.9499
Epoch 1/10, Batch 402/883, Training Loss: 1.0495
Epoch 1/10, Batch 403/883, Training Loss: 0.9380
Epoch 1/10, Batch 404/883, Training Loss: 0.8382
Epoch 1/10, Batch 405/883, Training Loss: 1.0231
Epoch 1/10, Batch 406/883, Training Loss: 1.1980
Epoch 1/10, Batch 407/883, Training Loss: 0.9312
Epoch 1/10, Batch 408/883, Training Loss: 0.7868
Epoch 1/10, Batch 409/883, Training Loss: 1.0349
Epoch 1/10, Batch 410/883, Training Loss: 0.7616
Epoch 1/10, Batch 411/883, Training Loss: 0.9429
Epoch 1/10, Batch 412/883, Training Loss: 0.8713
Epoch 1/10, Batch 413/883, Training Loss: 0.8186
Epoch 1/10, Batch 414/883, Training Loss: 1.0004
Epoch 1/10, Batch 415/883, Training Loss: 0.9630
Epoch 1/10, Batch 416/883, Training Loss: 0.8859
Epoch 1/10, Batch 417/883, Training Loss: 0.8325
Epoch 1/10, Batch 418/883, Training Loss: 1.0299
Epoch 1/10, Batch 419/883, Training Loss: 1.0346
Epoch 1/10, Batch 420/883, Training Loss: 1.0382
Epoch 1/10, Batch 421/883, Training Loss: 0.7877
Epoch 1/10, Batch 422/883, Training Loss: 0.8712
Epoch 1/10, Batch 423/883, Training Loss: 0.9925
Epoch 1/10, Batch 424/883, Training Loss: 1.1443
Epoch 1/10, Batch 425/883, Training Loss: 0.9553
Epoch 1/10, Batch 426/883, Training Loss: 0.8840
Epoch 1/10, Batch 427/883, Training Loss: 1.2376
Epoch 1/10, Batch 428/883, Training Loss: 0.8394
Epoch 1/10, Batch 429/883, Training Loss: 0.9086
Epoch 1/10, Batch 430/883, Training Loss: 1.0365
Epoch 1/10, Batch 431/883, Training Loss: 1.0608
Epoch 1/10, Batch 432/883, Training Loss: 0.9922
Epoch 1/10, Batch 433/883, Training Loss: 1.1763
Epoch 1/10, Batch 434/883, Training Loss: 0.8528
Epoch 1/10, Batch 435/883, Training Loss: 1.0156
Epoch 1/10, Batch 436/883, Training Loss: 0.8337
Epoch 1/10, Batch 437/883, Training Loss: 0.9283
Epoch 1/10, Batch 438/883, Training Loss: 0.9040
Epoch 1/10, Batch 439/883, Training Loss: 0.9741
Epoch 1/10, Batch 440/883, Training Loss: 0.9340
Epoch 1/10, Batch 441/883, Training Loss: 0.9128
Epoch 1/10, Batch 442/883, Training Loss: 0.9319
Epoch 1/10, Batch 443/883, Training Loss: 0.9745
Epoch 1/10, Batch 444/883, Training Loss: 0.7910
Epoch 1/10, Batch 445/883, Training Loss: 0.8007
Epoch 1/10, Batch 446/883, Training Loss: 0.9409
Epoch 1/10, Batch 447/883, Training Loss: 0.8585
Epoch 1/10, Batch 448/883, Training Loss: 0.9285
Epoch 1/10, Batch 449/883, Training Loss: 0.8812
Epoch 1/10, Batch 450/883, Training Loss: 0.9233
Epoch 1/10, Batch 451/883, Training Loss: 0.8207
Epoch 1/10, Batch 452/883, Training Loss: 1.4504
Epoch 1/10, Batch 453/883, Training Loss: 0.6762
Epoch 1/10, Batch 454/883, Training Loss: 0.7668
Epoch 1/10, Batch 455/883, Training Loss: 0.9404
Epoch 1/10, Batch 456/883, Training Loss: 0.8606
Epoch 1/10, Batch 457/883, Training Loss: 0.8897
Epoch 1/10, Batch 458/883, Training Loss: 1.1741
Epoch 1/10, Batch 459/883, Training Loss: 0.7478
Epoch 1/10, Batch 460/883, Training Loss: 0.8957
Epoch 1/10, Batch 461/883, Training Loss: 1.0144
Epoch 1/10, Batch 462/883, Training Loss: 0.9100
Epoch 1/10, Batch 463/883, Training Loss: 0.8266
Epoch 1/10, Batch 464/883, Training Loss: 1.0049
Epoch 1/10, Batch 465/883, Training Loss: 0.8850
Epoch 1/10, Batch 466/883, Training Loss: 0.7202
Epoch 1/10, Batch 467/883, Training Loss: 0.6521
Epoch 1/10, Batch 468/883, Training Loss: 0.8985
Epoch 1/10, Batch 469/883, Training Loss: 0.8644
Epoch 1/10, Batch 470/883, Training Loss: 0.8715
Epoch 1/10, Batch 471/883, Training Loss: 0.9186
Epoch 1/10, Batch 472/883, Training Loss: 0.8762
Epoch 1/10, Batch 473/883, Training Loss: 1.2472
Epoch 1/10, Batch 474/883, Training Loss: 0.8471
Epoch 1/10, Batch 475/883, Training Loss: 0.5826
Epoch 1/10, Batch 476/883, Training Loss: 0.7647
Epoch 1/10, Batch 477/883, Training Loss: 0.8708
Epoch 1/10, Batch 478/883, Training Loss: 0.6620
Epoch 1/10, Batch 479/883, Training Loss: 0.8780
Epoch 1/10, Batch 480/883, Training Loss: 0.7524
Epoch 1/10, Batch 481/883, Training Loss: 0.9238
Epoch 1/10, Batch 482/883, Training Loss: 0.9663
Epoch 1/10, Batch 483/883, Training Loss: 0.7829
Epoch 1/10, Batch 484/883, Training Loss: 0.9668
Epoch 1/10, Batch 485/883, Training Loss: 0.7875
Epoch 1/10, Batch 486/883, Training Loss: 0.6390
Epoch 1/10, Batch 487/883, Training Loss: 0.8260
Epoch 1/10, Batch 488/883, Training Loss: 0.9274
Epoch 1/10, Batch 489/883, Training Loss: 0.9412
Epoch 1/10, Batch 490/883, Training Loss: 1.1748
Epoch 1/10, Batch 491/883, Training Loss: 1.1135
Epoch 1/10, Batch 492/883, Training Loss: 0.9996
Epoch 1/10, Batch 493/883, Training Loss: 0.7275
Epoch 1/10, Batch 494/883, Training Loss: 1.0675
Epoch 1/10, Batch 495/883, Training Loss: 0.6799
Epoch 1/10, Batch 496/883, Training Loss: 0.9130
Epoch 1/10, Batch 497/883, Training Loss: 0.7040
Epoch 1/10, Batch 498/883, Training Loss: 0.8860
Epoch 1/10, Batch 499/883, Training Loss: 0.7913
Epoch 1/10, Batch 500/883, Training Loss: 1.4089
Epoch 1/10, Batch 501/883, Training Loss: 1.0058
Epoch 1/10, Batch 502/883, Training Loss: 0.8130
Epoch 1/10, Batch 503/883, Training Loss: 0.7053
Epoch 1/10, Batch 504/883, Training Loss: 0.8063
Epoch 1/10, Batch 505/883, Training Loss: 1.0178
Epoch 1/10, Batch 506/883, Training Loss: 1.0165
Epoch 1/10, Batch 507/883, Training Loss: 0.9614
Epoch 1/10, Batch 508/883, Training Loss: 0.9232
Epoch 1/10, Batch 509/883, Training Loss: 0.7554
Epoch 1/10, Batch 510/883, Training Loss: 0.7816
Epoch 1/10, Batch 511/883, Training Loss: 0.8263
Epoch 1/10, Batch 512/883, Training Loss: 0.8880
Epoch 1/10, Batch 513/883, Training Loss: 1.1057
Epoch 1/10, Batch 514/883, Training Loss: 0.9834
Epoch 1/10, Batch 515/883, Training Loss: 0.9608
Epoch 1/10, Batch 516/883, Training Loss: 1.2623
Epoch 1/10, Batch 517/883, Training Loss: 0.8639
Epoch 1/10, Batch 518/883, Training Loss: 1.0133
Epoch 1/10, Batch 519/883, Training Loss: 0.9637
Epoch 1/10, Batch 520/883, Training Loss: 0.9020
Epoch 1/10, Batch 521/883, Training Loss: 0.7559
Epoch 1/10, Batch 522/883, Training Loss: 1.1921
Epoch 1/10, Batch 523/883, Training Loss: 1.0680
Epoch 1/10, Batch 524/883, Training Loss: 0.9939
Epoch 1/10, Batch 525/883, Training Loss: 0.7834
Epoch 1/10, Batch 526/883, Training Loss: 0.8807
Epoch 1/10, Batch 527/883, Training Loss: 1.2533
Epoch 1/10, Batch 528/883, Training Loss: 0.8975
Epoch 1/10, Batch 529/883, Training Loss: 0.8793
Epoch 1/10, Batch 530/883, Training Loss: 0.9790
Epoch 1/10, Batch 531/883, Training Loss: 0.7745
Epoch 1/10, Batch 532/883, Training Loss: 0.8794
Epoch 1/10, Batch 533/883, Training Loss: 0.8494
Epoch 1/10, Batch 534/883, Training Loss: 0.8834
Epoch 1/10, Batch 535/883, Training Loss: 0.9343
Epoch 1/10, Batch 536/883, Training Loss: 0.7440
Epoch 1/10, Batch 537/883, Training Loss: 1.0434
Epoch 1/10, Batch 538/883, Training Loss: 0.9066
Epoch 1/10, Batch 539/883, Training Loss: 0.8090
Epoch 1/10, Batch 540/883, Training Loss: 0.9219
Epoch 1/10, Batch 541/883, Training Loss: 0.9339
Epoch 1/10, Batch 542/883, Training Loss: 0.9712
Epoch 1/10, Batch 543/883, Training Loss: 0.8405
Epoch 1/10, Batch 544/883, Training Loss: 0.7086
Epoch 1/10, Batch 545/883, Training Loss: 0.9008
Epoch 1/10, Batch 546/883, Training Loss: 1.1838
Epoch 1/10, Batch 547/883, Training Loss: 0.8372
Epoch 1/10, Batch 548/883, Training Loss: 1.1467
Epoch 1/10, Batch 549/883, Training Loss: 1.4027
Epoch 1/10, Batch 550/883, Training Loss: 0.8021
Epoch 1/10, Batch 551/883, Training Loss: 1.0177
Epoch 1/10, Batch 552/883, Training Loss: 1.0959
Epoch 1/10, Batch 553/883, Training Loss: 0.9514
Epoch 1/10, Batch 554/883, Training Loss: 0.8752
Epoch 1/10, Batch 555/883, Training Loss: 0.7592
Epoch 1/10, Batch 556/883, Training Loss: 1.5800
Epoch 1/10, Batch 557/883, Training Loss: 0.8161
Epoch 1/10, Batch 558/883, Training Loss: 0.9557
Epoch 1/10, Batch 559/883, Training Loss: 0.8735
Epoch 1/10, Batch 560/883, Training Loss: 0.9746
Epoch 1/10, Batch 561/883, Training Loss: 0.9310
Epoch 1/10, Batch 562/883, Training Loss: 0.9064
Epoch 1/10, Batch 563/883, Training Loss: 0.7775
Epoch 1/10, Batch 564/883, Training Loss: 0.8224
Epoch 1/10, Batch 565/883, Training Loss: 0.8437
Epoch 1/10, Batch 566/883, Training Loss: 0.8895
Epoch 1/10, Batch 567/883, Training Loss: 1.0142
Epoch 1/10, Batch 568/883, Training Loss: 0.8048
Epoch 1/10, Batch 569/883, Training Loss: 0.8995
Epoch 1/10, Batch 570/883, Training Loss: 0.9009
Epoch 1/10, Batch 571/883, Training Loss: 0.7169
Epoch 1/10, Batch 572/883, Training Loss: 0.8434
Epoch 1/10, Batch 573/883, Training Loss: 1.2574
Epoch 1/10, Batch 574/883, Training Loss: 1.0119
Epoch 1/10, Batch 575/883, Training Loss: 1.0566
Epoch 1/10, Batch 576/883, Training Loss: 0.5956
Epoch 1/10, Batch 577/883, Training Loss: 0.8389
Epoch 1/10, Batch 578/883, Training Loss: 0.9411
Epoch 1/10, Batch 579/883, Training Loss: 0.7734
Epoch 1/10, Batch 580/883, Training Loss: 0.8434
Epoch 1/10, Batch 581/883, Training Loss: 0.7896
Epoch 1/10, Batch 582/883, Training Loss: 0.8927
Epoch 1/10, Batch 583/883, Training Loss: 1.0253
Epoch 1/10, Batch 584/883, Training Loss: 0.6992
Epoch 1/10, Batch 585/883, Training Loss: 0.9057
Epoch 1/10, Batch 586/883, Training Loss: 1.0855
Epoch 1/10, Batch 587/883, Training Loss: 0.8759
Epoch 1/10, Batch 588/883, Training Loss: 0.8726
Epoch 1/10, Batch 589/883, Training Loss: 0.9001
Epoch 1/10, Batch 590/883, Training Loss: 0.9698
Epoch 1/10, Batch 591/883, Training Loss: 0.7047
Epoch 1/10, Batch 592/883, Training Loss: 0.6420
Epoch 1/10, Batch 593/883, Training Loss: 0.7334
Epoch 1/10, Batch 594/883, Training Loss: 0.8554
Epoch 1/10, Batch 595/883, Training Loss: 0.8035
Epoch 1/10, Batch 596/883, Training Loss: 1.1854
Epoch 1/10, Batch 597/883, Training Loss: 0.9312
Epoch 1/10, Batch 598/883, Training Loss: 0.8099
Epoch 1/10, Batch 599/883, Training Loss: 0.9273
Epoch 1/10, Batch 600/883, Training Loss: 0.9503
Epoch 1/10, Batch 601/883, Training Loss: 0.7896
Epoch 1/10, Batch 602/883, Training Loss: 0.8399
Epoch 1/10, Batch 603/883, Training Loss: 0.7811
Epoch 1/10, Batch 604/883, Training Loss: 0.7957
Epoch 1/10, Batch 605/883, Training Loss: 0.8507
Epoch 1/10, Batch 606/883, Training Loss: 0.7623
Epoch 1/10, Batch 607/883, Training Loss: 0.8341
Epoch 1/10, Batch 608/883, Training Loss: 0.7938
Epoch 1/10, Batch 609/883, Training Loss: 0.7555
Epoch 1/10, Batch 610/883, Training Loss: 0.8073
Epoch 1/10, Batch 611/883, Training Loss: 0.8435
Epoch 1/10, Batch 612/883, Training Loss: 0.9354
Epoch 1/10, Batch 613/883, Training Loss: 1.0401
Epoch 1/10, Batch 614/883, Training Loss: 0.9344
Epoch 1/10, Batch 615/883, Training Loss: 0.7004
Epoch 1/10, Batch 616/883, Training Loss: 0.9412
Epoch 1/10, Batch 617/883, Training Loss: 0.9466
Epoch 1/10, Batch 618/883, Training Loss: 1.1218
Epoch 1/10, Batch 619/883, Training Loss: 0.7370
Epoch 1/10, Batch 620/883, Training Loss: 0.9567
Epoch 1/10, Batch 621/883, Training Loss: 0.7017
Epoch 1/10, Batch 622/883, Training Loss: 0.8142
Epoch 1/10, Batch 623/883, Training Loss: 0.7763
Epoch 1/10, Batch 624/883, Training Loss: 1.1142
Epoch 1/10, Batch 625/883, Training Loss: 0.8498
Epoch 1/10, Batch 626/883, Training Loss: 0.8328
Epoch 1/10, Batch 627/883, Training Loss: 1.0101
Epoch 1/10, Batch 628/883, Training Loss: 0.9350
Epoch 1/10, Batch 629/883, Training Loss: 0.8058
Epoch 1/10, Batch 630/883, Training Loss: 0.8156
Epoch 1/10, Batch 631/883, Training Loss: 0.8333
Epoch 1/10, Batch 632/883, Training Loss: 0.7863
Epoch 1/10, Batch 633/883, Training Loss: 1.0450
Epoch 1/10, Batch 634/883, Training Loss: 0.7612
Epoch 1/10, Batch 635/883, Training Loss: 0.7403
Epoch 1/10, Batch 636/883, Training Loss: 0.9991
Epoch 1/10, Batch 637/883, Training Loss: 0.9540
Epoch 1/10, Batch 638/883, Training Loss: 0.9166
Epoch 1/10, Batch 639/883, Training Loss: 1.0804
Epoch 1/10, Batch 640/883, Training Loss: 0.9986
Epoch 1/10, Batch 641/883, Training Loss: 0.9302
Epoch 1/10, Batch 642/883, Training Loss: 0.9893
Epoch 1/10, Batch 643/883, Training Loss: 0.6529
Epoch 1/10, Batch 644/883, Training Loss: 0.9426
Epoch 1/10, Batch 645/883, Training Loss: 0.6642
Epoch 1/10, Batch 646/883, Training Loss: 0.9951
Epoch 1/10, Batch 647/883, Training Loss: 1.0416
Epoch 1/10, Batch 648/883, Training Loss: 0.9403
Epoch 1/10, Batch 649/883, Training Loss: 0.6924
Epoch 1/10, Batch 650/883, Training Loss: 0.8067
Epoch 1/10, Batch 651/883, Training Loss: 0.8995
Epoch 1/10, Batch 652/883, Training Loss: 1.0635
Epoch 1/10, Batch 653/883, Training Loss: 0.8491
Epoch 1/10, Batch 654/883, Training Loss: 0.8925
Epoch 1/10, Batch 655/883, Training Loss: 0.7723
Epoch 1/10, Batch 656/883, Training Loss: 0.7877
Epoch 1/10, Batch 657/883, Training Loss: 0.9423
Epoch 1/10, Batch 658/883, Training Loss: 0.8704
Epoch 1/10, Batch 659/883, Training Loss: 0.8407
Epoch 1/10, Batch 660/883, Training Loss: 0.8701
Epoch 1/10, Batch 661/883, Training Loss: 1.0322
Epoch 1/10, Batch 662/883, Training Loss: 0.8430
Epoch 1/10, Batch 663/883, Training Loss: 0.8220
Epoch 1/10, Batch 664/883, Training Loss: 1.0401
Epoch 1/10, Batch 665/883, Training Loss: 0.8923
Epoch 1/10, Batch 666/883, Training Loss: 1.1216
Epoch 1/10, Batch 667/883, Training Loss: 0.9640
Epoch 1/10, Batch 668/883, Training Loss: 0.9391
Epoch 1/10, Batch 669/883, Training Loss: 0.8906
Epoch 1/10, Batch 670/883, Training Loss: 0.9526
Epoch 1/10, Batch 671/883, Training Loss: 1.0273
Epoch 1/10, Batch 672/883, Training Loss: 0.8400
Epoch 1/10, Batch 673/883, Training Loss: 1.0521
Epoch 1/10, Batch 674/883, Training Loss: 0.8921
Epoch 1/10, Batch 675/883, Training Loss: 0.8508
Epoch 1/10, Batch 676/883, Training Loss: 0.8673
Epoch 1/10, Batch 677/883, Training Loss: 0.8507
Epoch 1/10, Batch 678/883, Training Loss: 0.8287
Epoch 1/10, Batch 679/883, Training Loss: 0.9655
Epoch 1/10, Batch 680/883, Training Loss: 1.2059
Epoch 1/10, Batch 681/883, Training Loss: 0.7506
Epoch 1/10, Batch 682/883, Training Loss: 1.0154
Epoch 1/10, Batch 683/883, Training Loss: 0.8521
Epoch 1/10, Batch 684/883, Training Loss: 1.0779
Epoch 1/10, Batch 685/883, Training Loss: 0.7394
Epoch 1/10, Batch 686/883, Training Loss: 0.7641
Epoch 1/10, Batch 687/883, Training Loss: 0.9514
Epoch 1/10, Batch 688/883, Training Loss: 0.9055
Epoch 1/10, Batch 689/883, Training Loss: 0.8092
Epoch 1/10, Batch 690/883, Training Loss: 0.7694
Epoch 1/10, Batch 691/883, Training Loss: 0.9805
Epoch 1/10, Batch 692/883, Training Loss: 0.6858
Epoch 1/10, Batch 693/883, Training Loss: 0.8442
Epoch 1/10, Batch 694/883, Training Loss: 0.9256
Epoch 1/10, Batch 695/883, Training Loss: 0.8455
Epoch 1/10, Batch 696/883, Training Loss: 0.7607
Epoch 1/10, Batch 697/883, Training Loss: 0.9027
Epoch 1/10, Batch 698/883, Training Loss: 0.8702
Epoch 1/10, Batch 699/883, Training Loss: 0.7507
Epoch 1/10, Batch 700/883, Training Loss: 0.7198
Epoch 1/10, Batch 701/883, Training Loss: 0.7629
Epoch 1/10, Batch 702/883, Training Loss: 0.8856
Epoch 1/10, Batch 703/883, Training Loss: 1.7300
Epoch 1/10, Batch 704/883, Training Loss: 0.9332
Epoch 1/10, Batch 705/883, Training Loss: 0.9210
Epoch 1/10, Batch 706/883, Training Loss: 0.7617
Epoch 1/10, Batch 707/883, Training Loss: 0.7688
Epoch 1/10, Batch 708/883, Training Loss: 0.6903
Epoch 1/10, Batch 709/883, Training Loss: 0.8556
Epoch 1/10, Batch 710/883, Training Loss: 0.7804
Epoch 1/10, Batch 711/883, Training Loss: 0.8462
Epoch 1/10, Batch 712/883, Training Loss: 0.6385
Epoch 1/10, Batch 713/883, Training Loss: 0.6850
Epoch 1/10, Batch 714/883, Training Loss: 0.9614
Epoch 1/10, Batch 715/883, Training Loss: 0.9882
Epoch 1/10, Batch 716/883, Training Loss: 0.8701
Epoch 1/10, Batch 717/883, Training Loss: 0.7682
Epoch 1/10, Batch 718/883, Training Loss: 0.7635
Epoch 1/10, Batch 719/883, Training Loss: 0.8963
Epoch 1/10, Batch 720/883, Training Loss: 0.7051
Epoch 1/10, Batch 721/883, Training Loss: 0.7634
Epoch 1/10, Batch 722/883, Training Loss: 0.8650
Epoch 1/10, Batch 723/883, Training Loss: 0.6531
Epoch 1/10, Batch 724/883, Training Loss: 1.2431
Epoch 1/10, Batch 725/883, Training Loss: 0.7150
Epoch 1/10, Batch 726/883, Training Loss: 0.6119
Epoch 1/10, Batch 727/883, Training Loss: 0.8987
Epoch 1/10, Batch 728/883, Training Loss: 0.8280
Epoch 1/10, Batch 729/883, Training Loss: 1.3152
Epoch 1/10, Batch 730/883, Training Loss: 0.8343
Epoch 1/10, Batch 731/883, Training Loss: 0.8255
Epoch 1/10, Batch 732/883, Training Loss: 0.8231
Epoch 1/10, Batch 733/883, Training Loss: 1.2749
Epoch 1/10, Batch 734/883, Training Loss: 0.9194
Epoch 1/10, Batch 735/883, Training Loss: 0.7042
Epoch 1/10, Batch 736/883, Training Loss: 0.8902
Epoch 1/10, Batch 737/883, Training Loss: 0.9020
Epoch 1/10, Batch 738/883, Training Loss: 0.8633
Epoch 1/10, Batch 739/883, Training Loss: 1.1252
Epoch 1/10, Batch 740/883, Training Loss: 0.7862
Epoch 1/10, Batch 741/883, Training Loss: 0.9228
Epoch 1/10, Batch 742/883, Training Loss: 0.7640
Epoch 1/10, Batch 743/883, Training Loss: 0.8041
Epoch 1/10, Batch 744/883, Training Loss: 1.0380
Epoch 1/10, Batch 745/883, Training Loss: 0.7111
Epoch 1/10, Batch 746/883, Training Loss: 0.8535
Epoch 1/10, Batch 747/883, Training Loss: 0.6522
Epoch 1/10, Batch 748/883, Training Loss: 0.9216
Epoch 1/10, Batch 749/883, Training Loss: 1.0209
Epoch 1/10, Batch 750/883, Training Loss: 1.0466
Epoch 1/10, Batch 751/883, Training Loss: 0.9902
Epoch 1/10, Batch 752/883, Training Loss: 1.1377
Epoch 1/10, Batch 753/883, Training Loss: 1.0047
Epoch 1/10, Batch 754/883, Training Loss: 0.8028
Epoch 1/10, Batch 755/883, Training Loss: 0.8202
Epoch 1/10, Batch 756/883, Training Loss: 0.7891
Epoch 1/10, Batch 757/883, Training Loss: 0.7571
Epoch 1/10, Batch 758/883, Training Loss: 0.9099
Epoch 1/10, Batch 759/883, Training Loss: 1.0850
Epoch 1/10, Batch 760/883, Training Loss: 1.0332
Epoch 1/10, Batch 761/883, Training Loss: 0.6823
Epoch 1/10, Batch 762/883, Training Loss: 0.8193
Epoch 1/10, Batch 763/883, Training Loss: 0.8979
Epoch 1/10, Batch 764/883, Training Loss: 0.6749
Epoch 1/10, Batch 765/883, Training Loss: 0.8392
Epoch 1/10, Batch 766/883, Training Loss: 0.5810
Epoch 1/10, Batch 767/883, Training Loss: 0.7541
Epoch 1/10, Batch 768/883, Training Loss: 0.9466
Epoch 1/10, Batch 769/883, Training Loss: 1.0211
Epoch 1/10, Batch 770/883, Training Loss: 0.8268
Epoch 1/10, Batch 771/883, Training Loss: 0.7592
Epoch 1/10, Batch 772/883, Training Loss: 0.8451
Epoch 1/10, Batch 773/883, Training Loss: 0.6976
Epoch 1/10, Batch 774/883, Training Loss: 0.6811
Epoch 1/10, Batch 775/883, Training Loss: 0.6450
Epoch 1/10, Batch 776/883, Training Loss: 1.3958
Epoch 1/10, Batch 777/883, Training Loss: 1.2579
Epoch 1/10, Batch 778/883, Training Loss: 1.1883
Epoch 1/10, Batch 779/883, Training Loss: 1.2630
Epoch 1/10, Batch 780/883, Training Loss: 0.7151
Epoch 1/10, Batch 781/883, Training Loss: 1.0902
Epoch 1/10, Batch 782/883, Training Loss: 0.8818
Epoch 1/10, Batch 783/883, Training Loss: 0.7793
Epoch 1/10, Batch 784/883, Training Loss: 0.7262
Epoch 1/10, Batch 785/883, Training Loss: 0.7659
Epoch 1/10, Batch 786/883, Training Loss: 0.8518
Epoch 1/10, Batch 787/883, Training Loss: 1.0452
Epoch 1/10, Batch 788/883, Training Loss: 0.9226
Epoch 1/10, Batch 789/883, Training Loss: 0.7838
Epoch 1/10, Batch 790/883, Training Loss: 1.0022
Epoch 1/10, Batch 791/883, Training Loss: 0.8134
Epoch 1/10, Batch 792/883, Training Loss: 0.8722
Epoch 1/10, Batch 793/883, Training Loss: 0.8304
Epoch 1/10, Batch 794/883, Training Loss: 0.7926
Epoch 1/10, Batch 795/883, Training Loss: 0.8954
Epoch 1/10, Batch 796/883, Training Loss: 0.9419
Epoch 1/10, Batch 797/883, Training Loss: 0.6529
Epoch 1/10, Batch 798/883, Training Loss: 0.8676
Epoch 1/10, Batch 799/883, Training Loss: 1.1065
Epoch 1/10, Batch 800/883, Training Loss: 0.7787
Epoch 1/10, Batch 801/883, Training Loss: 0.7971
Epoch 1/10, Batch 802/883, Training Loss: 0.8197
Epoch 1/10, Batch 803/883, Training Loss: 0.7964
Epoch 1/10, Batch 804/883, Training Loss: 1.1482
Epoch 1/10, Batch 805/883, Training Loss: 0.7140
Epoch 1/10, Batch 806/883, Training Loss: 1.0671
Epoch 1/10, Batch 807/883, Training Loss: 0.7490
Epoch 1/10, Batch 808/883, Training Loss: 0.7938
Epoch 1/10, Batch 809/883, Training Loss: 0.9279
Epoch 1/10, Batch 810/883, Training Loss: 0.8947
Epoch 1/10, Batch 811/883, Training Loss: 0.8834
Epoch 1/10, Batch 812/883, Training Loss: 0.6673
Epoch 1/10, Batch 813/883, Training Loss: 0.7473
Epoch 1/10, Batch 814/883, Training Loss: 0.8786
Epoch 1/10, Batch 815/883, Training Loss: 0.9537
Epoch 1/10, Batch 816/883, Training Loss: 0.9471
Epoch 1/10, Batch 817/883, Training Loss: 0.6865
Epoch 1/10, Batch 818/883, Training Loss: 0.7686
Epoch 1/10, Batch 819/883, Training Loss: 1.1260
Epoch 1/10, Batch 820/883, Training Loss: 0.9167
Epoch 1/10, Batch 821/883, Training Loss: 0.8716
Epoch 1/10, Batch 822/883, Training Loss: 0.7421
Epoch 1/10, Batch 823/883, Training Loss: 0.9761
Epoch 1/10, Batch 824/883, Training Loss: 0.7559
Epoch 1/10, Batch 825/883, Training Loss: 0.7294
Epoch 1/10, Batch 826/883, Training Loss: 1.0411
Epoch 1/10, Batch 827/883, Training Loss: 0.9856
Epoch 1/10, Batch 828/883, Training Loss: 0.7116
Epoch 1/10, Batch 829/883, Training Loss: 0.9530
Epoch 1/10, Batch 830/883, Training Loss: 0.8480
Epoch 1/10, Batch 831/883, Training Loss: 0.8289
Epoch 1/10, Batch 832/883, Training Loss: 0.9208
Epoch 1/10, Batch 833/883, Training Loss: 0.7118
Epoch 1/10, Batch 834/883, Training Loss: 0.7213
Epoch 1/10, Batch 835/883, Training Loss: 0.8267
Epoch 1/10, Batch 836/883, Training Loss: 0.9020
Epoch 1/10, Batch 837/883, Training Loss: 0.9345
Epoch 1/10, Batch 838/883, Training Loss: 1.2673
Epoch 1/10, Batch 839/883, Training Loss: 0.7825
Epoch 1/10, Batch 840/883, Training Loss: 0.9010
Epoch 1/10, Batch 841/883, Training Loss: 0.7438
Epoch 1/10, Batch 842/883, Training Loss: 0.8206
Epoch 1/10, Batch 843/883, Training Loss: 0.8202
Epoch 1/10, Batch 844/883, Training Loss: 0.8021
Epoch 1/10, Batch 845/883, Training Loss: 0.8382
Epoch 1/10, Batch 846/883, Training Loss: 0.6955
Epoch 1/10, Batch 847/883, Training Loss: 0.8691
Epoch 1/10, Batch 848/883, Training Loss: 0.7971
Epoch 1/10, Batch 849/883, Training Loss: 0.7808
Epoch 1/10, Batch 850/883, Training Loss: 0.7167
Epoch 1/10, Batch 851/883, Training Loss: 0.7317
Epoch 1/10, Batch 852/883, Training Loss: 0.7317
Epoch 1/10, Batch 853/883, Training Loss: 0.7756
Epoch 1/10, Batch 854/883, Training Loss: 0.7320
Epoch 1/10, Batch 855/883, Training Loss: 0.9907
Epoch 1/10, Batch 856/883, Training Loss: 0.6893
Epoch 1/10, Batch 857/883, Training Loss: 0.8981
Epoch 1/10, Batch 858/883, Training Loss: 0.9428
Epoch 1/10, Batch 859/883, Training Loss: 0.8673
Epoch 1/10, Batch 860/883, Training Loss: 0.8451
Epoch 1/10, Batch 861/883, Training Loss: 1.3429
Epoch 1/10, Batch 862/883, Training Loss: 0.6304
Epoch 1/10, Batch 863/883, Training Loss: 1.0526
Epoch 1/10, Batch 864/883, Training Loss: 0.7813
Epoch 1/10, Batch 865/883, Training Loss: 0.8357
Epoch 1/10, Batch 866/883, Training Loss: 0.7073
Epoch 1/10, Batch 867/883, Training Loss: 1.0164
Epoch 1/10, Batch 868/883, Training Loss: 0.6564
Epoch 1/10, Batch 869/883, Training Loss: 0.9314
Epoch 1/10, Batch 870/883, Training Loss: 0.6999
Epoch 1/10, Batch 871/883, Training Loss: 0.7635
Epoch 1/10, Batch 872/883, Training Loss: 1.1837
Epoch 1/10, Batch 873/883, Training Loss: 0.7928
Epoch 1/10, Batch 874/883, Training Loss: 0.8446
Epoch 1/10, Batch 875/883, Training Loss: 0.9064
Epoch 1/10, Batch 876/883, Training Loss: 0.7715
Epoch 1/10, Batch 877/883, Training Loss: 1.0134
Epoch 1/10, Batch 878/883, Training Loss: 0.7840
Epoch 1/10, Batch 879/883, Training Loss: 0.9299
Epoch 1/10, Batch 880/883, Training Loss: 0.8696
Epoch 1/10, Batch 881/883, Training Loss: 0.7240
Epoch 1/10, Batch 882/883, Training Loss: 1.0254
Epoch 1/10, Batch 883/883, Training Loss: 0.8564
Epoch 1/10, Training Loss: 0.9728, Validation Loss: 0.9185, Validation Accuracy: 0.5760
Epoch 2/10, Batch 1/883, Training Loss: 0.8679
Epoch 2/10, Batch 2/883, Training Loss: 0.7416
Epoch 2/10, Batch 3/883, Training Loss: 0.8182
Epoch 2/10, Batch 4/883, Training Loss: 0.7515
Epoch 2/10, Batch 5/883, Training Loss: 0.8266
Epoch 2/10, Batch 6/883, Training Loss: 0.8285
Epoch 2/10, Batch 7/883, Training Loss: 0.6654
Epoch 2/10, Batch 8/883, Training Loss: 0.6596
Epoch 2/10, Batch 9/883, Training Loss: 0.7483
Epoch 2/10, Batch 10/883, Training Loss: 0.9642
Epoch 2/10, Batch 11/883, Training Loss: 1.0261
Epoch 2/10, Batch 12/883, Training Loss: 0.8448
Epoch 2/10, Batch 13/883, Training Loss: 0.9641
Epoch 2/10, Batch 14/883, Training Loss: 0.9180
Epoch 2/10, Batch 15/883, Training Loss: 0.8888
Epoch 2/10, Batch 16/883, Training Loss: 0.7004
Epoch 2/10, Batch 17/883, Training Loss: 0.8135
Epoch 2/10, Batch 18/883, Training Loss: 1.0068
Epoch 2/10, Batch 19/883, Training Loss: 0.9297
Epoch 2/10, Batch 20/883, Training Loss: 0.9134
Epoch 2/10, Batch 21/883, Training Loss: 0.7616
Epoch 2/10, Batch 22/883, Training Loss: 1.0268
Epoch 2/10, Batch 23/883, Training Loss: 0.8582
Epoch 2/10, Batch 24/883, Training Loss: 0.8683
Epoch 2/10, Batch 25/883, Training Loss: 0.7695
Epoch 2/10, Batch 26/883, Training Loss: 1.1861
Epoch 2/10, Batch 27/883, Training Loss: 1.1090
Epoch 2/10, Batch 28/883, Training Loss: 0.8883
Epoch 2/10, Batch 29/883, Training Loss: 0.7628
Epoch 2/10, Batch 30/883, Training Loss: 0.8299
Epoch 2/10, Batch 31/883, Training Loss: 0.9885
Epoch 2/10, Batch 32/883, Training Loss: 0.8798
Epoch 2/10, Batch 33/883, Training Loss: 0.7654
Epoch 2/10, Batch 34/883, Training Loss: 0.7754
Epoch 2/10, Batch 35/883, Training Loss: 1.0008
Epoch 2/10, Batch 36/883, Training Loss: 0.8433
Epoch 2/10, Batch 37/883, Training Loss: 0.6492
Epoch 2/10, Batch 38/883, Training Loss: 0.8076
Epoch 2/10, Batch 39/883, Training Loss: 1.0434
Epoch 2/10, Batch 40/883, Training Loss: 0.8620
Epoch 2/10, Batch 41/883, Training Loss: 1.2609
Epoch 2/10, Batch 42/883, Training Loss: 1.2138
Epoch 2/10, Batch 43/883, Training Loss: 0.9016
Epoch 2/10, Batch 44/883, Training Loss: 1.1610
Epoch 2/10, Batch 45/883, Training Loss: 0.9070
Epoch 2/10, Batch 46/883, Training Loss: 0.7454
Epoch 2/10, Batch 47/883, Training Loss: 0.7938
Epoch 2/10, Batch 48/883, Training Loss: 0.8902
Epoch 2/10, Batch 49/883, Training Loss: 0.7963
Epoch 2/10, Batch 50/883, Training Loss: 0.8256
Epoch 2/10, Batch 51/883, Training Loss: 0.9234
Epoch 2/10, Batch 52/883, Training Loss: 0.9811
Epoch 2/10, Batch 53/883, Training Loss: 1.0331
Epoch 2/10, Batch 54/883, Training Loss: 0.8565
Epoch 2/10, Batch 55/883, Training Loss: 1.0535
Epoch 2/10, Batch 56/883, Training Loss: 0.8876
Epoch 2/10, Batch 57/883, Training Loss: 0.8968
Epoch 2/10, Batch 58/883, Training Loss: 0.9823
Epoch 2/10, Batch 59/883, Training Loss: 1.0146
Epoch 2/10, Batch 60/883, Training Loss: 0.8181
Epoch 2/10, Batch 61/883, Training Loss: 0.8701
Epoch 2/10, Batch 62/883, Training Loss: 0.8119
Epoch 2/10, Batch 63/883, Training Loss: 0.8619
Epoch 2/10, Batch 64/883, Training Loss: 0.7700
Epoch 2/10, Batch 65/883, Training Loss: 0.7822
Epoch 2/10, Batch 66/883, Training Loss: 0.9736
Epoch 2/10, Batch 67/883, Training Loss: 0.7944
Epoch 2/10, Batch 68/883, Training Loss: 0.9525
Epoch 2/10, Batch 69/883, Training Loss: 0.9322
Epoch 2/10, Batch 70/883, Training Loss: 0.7215
Epoch 2/10, Batch 71/883, Training Loss: 1.1037
Epoch 2/10, Batch 72/883, Training Loss: 0.9183
Epoch 2/10, Batch 73/883, Training Loss: 0.6410
Epoch 2/10, Batch 74/883, Training Loss: 0.8722
Epoch 2/10, Batch 75/883, Training Loss: 0.7741
Epoch 2/10, Batch 76/883, Training Loss: 0.6578
Epoch 2/10, Batch 77/883, Training Loss: 1.0159
Epoch 2/10, Batch 78/883, Training Loss: 0.8114
Epoch 2/10, Batch 79/883, Training Loss: 0.9564
Epoch 2/10, Batch 80/883, Training Loss: 0.9618
Epoch 2/10, Batch 81/883, Training Loss: 0.8347
Epoch 2/10, Batch 82/883, Training Loss: 0.9379
Epoch 2/10, Batch 83/883, Training Loss: 0.8194
Epoch 2/10, Batch 84/883, Training Loss: 0.9239
Epoch 2/10, Batch 85/883, Training Loss: 0.7926
Epoch 2/10, Batch 86/883, Training Loss: 0.8805
Epoch 2/10, Batch 87/883, Training Loss: 0.7017
Epoch 2/10, Batch 88/883, Training Loss: 0.9150
Epoch 2/10, Batch 89/883, Training Loss: 1.1527
Epoch 2/10, Batch 90/883, Training Loss: 0.7988
Epoch 2/10, Batch 91/883, Training Loss: 0.9584
Epoch 2/10, Batch 92/883, Training Loss: 0.7648
Epoch 2/10, Batch 93/883, Training Loss: 0.7916
Epoch 2/10, Batch 94/883, Training Loss: 0.7981
Epoch 2/10, Batch 95/883, Training Loss: 1.1083
Epoch 2/10, Batch 96/883, Training Loss: 0.8796
Epoch 2/10, Batch 97/883, Training Loss: 1.4503
Epoch 2/10, Batch 98/883, Training Loss: 0.9584
Epoch 2/10, Batch 99/883, Training Loss: 0.6166
Epoch 2/10, Batch 100/883, Training Loss: 0.8490
Epoch 2/10, Batch 101/883, Training Loss: 1.1398
Epoch 2/10, Batch 102/883, Training Loss: 0.8072
Epoch 2/10, Batch 103/883, Training Loss: 1.0473
Epoch 2/10, Batch 104/883, Training Loss: 0.8259
Epoch 2/10, Batch 105/883, Training Loss: 1.0295
Epoch 2/10, Batch 106/883, Training Loss: 1.0513
Epoch 2/10, Batch 107/883, Training Loss: 0.8953
Epoch 2/10, Batch 108/883, Training Loss: 0.9080
Epoch 2/10, Batch 109/883, Training Loss: 1.1816
Epoch 2/10, Batch 110/883, Training Loss: 0.9529
Epoch 2/10, Batch 111/883, Training Loss: 0.9243
Epoch 2/10, Batch 112/883, Training Loss: 1.1492
Epoch 2/10, Batch 113/883, Training Loss: 0.9599
Epoch 2/10, Batch 114/883, Training Loss: 0.9304
Epoch 2/10, Batch 115/883, Training Loss: 0.8335
Epoch 2/10, Batch 116/883, Training Loss: 0.9809
Epoch 2/10, Batch 117/883, Training Loss: 0.9257
Epoch 2/10, Batch 118/883, Training Loss: 0.9367
Epoch 2/10, Batch 119/883, Training Loss: 0.8327
Epoch 2/10, Batch 120/883, Training Loss: 1.1323
Epoch 2/10, Batch 121/883, Training Loss: 0.9680
Epoch 2/10, Batch 122/883, Training Loss: 1.0250
Epoch 2/10, Batch 123/883, Training Loss: 0.9814
Epoch 2/10, Batch 124/883, Training Loss: 1.2202
Epoch 2/10, Batch 125/883, Training Loss: 0.9046
Epoch 2/10, Batch 126/883, Training Loss: 0.8626
Epoch 2/10, Batch 127/883, Training Loss: 1.0110
Epoch 2/10, Batch 128/883, Training Loss: 0.9124
Epoch 2/10, Batch 129/883, Training Loss: 0.9087
Epoch 2/10, Batch 130/883, Training Loss: 0.8568
Epoch 2/10, Batch 131/883, Training Loss: 0.8166
Epoch 2/10, Batch 132/883, Training Loss: 1.0885
Epoch 2/10, Batch 133/883, Training Loss: 0.8206
Epoch 2/10, Batch 134/883, Training Loss: 1.2173
Epoch 2/10, Batch 135/883, Training Loss: 0.8749
Epoch 2/10, Batch 136/883, Training Loss: 0.9582
Epoch 2/10, Batch 137/883, Training Loss: 0.9901
Epoch 2/10, Batch 138/883, Training Loss: 0.7612
Epoch 2/10, Batch 139/883, Training Loss: 1.1362
Epoch 2/10, Batch 140/883, Training Loss: 1.2875
Epoch 2/10, Batch 141/883, Training Loss: 0.6695
Epoch 2/10, Batch 142/883, Training Loss: 1.0825
Epoch 2/10, Batch 143/883, Training Loss: 0.8264
Epoch 2/10, Batch 144/883, Training Loss: 0.7413
Epoch 2/10, Batch 145/883, Training Loss: 0.7810
Epoch 2/10, Batch 146/883, Training Loss: 0.9030
Epoch 2/10, Batch 147/883, Training Loss: 1.0292
Epoch 2/10, Batch 148/883, Training Loss: 0.7382
Epoch 2/10, Batch 149/883, Training Loss: 0.6760
Epoch 2/10, Batch 150/883, Training Loss: 0.8394
Epoch 2/10, Batch 151/883, Training Loss: 0.7442
Epoch 2/10, Batch 152/883, Training Loss: 0.8867
Epoch 2/10, Batch 153/883, Training Loss: 1.1127
Epoch 2/10, Batch 154/883, Training Loss: 0.6243
Epoch 2/10, Batch 155/883, Training Loss: 0.8020
Epoch 2/10, Batch 156/883, Training Loss: 1.0377
Epoch 2/10, Batch 157/883, Training Loss: 1.0241
Epoch 2/10, Batch 158/883, Training Loss: 0.7627
Epoch 2/10, Batch 159/883, Training Loss: 0.7827
Epoch 2/10, Batch 160/883, Training Loss: 0.7849
Epoch 2/10, Batch 161/883, Training Loss: 1.1379
Epoch 2/10, Batch 162/883, Training Loss: 0.7289
Epoch 2/10, Batch 163/883, Training Loss: 0.8094
Epoch 2/10, Batch 164/883, Training Loss: 0.6612
Epoch 2/10, Batch 165/883, Training Loss: 0.8767
Epoch 2/10, Batch 166/883, Training Loss: 0.9277
Epoch 2/10, Batch 167/883, Training Loss: 0.7388
Epoch 2/10, Batch 168/883, Training Loss: 0.9896
Epoch 2/10, Batch 169/883, Training Loss: 0.9822
Epoch 2/10, Batch 170/883, Training Loss: 1.0889
Epoch 2/10, Batch 171/883, Training Loss: 0.8536
Epoch 2/10, Batch 172/883, Training Loss: 0.8249
Epoch 2/10, Batch 173/883, Training Loss: 0.7537
Epoch 2/10, Batch 174/883, Training Loss: 0.6363
Epoch 2/10, Batch 175/883, Training Loss: 0.7867
Epoch 2/10, Batch 176/883, Training Loss: 0.8253
Epoch 2/10, Batch 177/883, Training Loss: 0.6498
Epoch 2/10, Batch 178/883, Training Loss: 0.9379
Epoch 2/10, Batch 179/883, Training Loss: 0.8459
Epoch 2/10, Batch 180/883, Training Loss: 0.9379
Epoch 2/10, Batch 181/883, Training Loss: 0.7917
Epoch 2/10, Batch 182/883, Training Loss: 0.7695
Epoch 2/10, Batch 183/883, Training Loss: 0.9892
Epoch 2/10, Batch 184/883, Training Loss: 0.8435
Epoch 2/10, Batch 185/883, Training Loss: 0.7060
Epoch 2/10, Batch 186/883, Training Loss: 0.5956
Epoch 2/10, Batch 187/883, Training Loss: 0.8795
Epoch 2/10, Batch 188/883, Training Loss: 0.6425
Epoch 2/10, Batch 189/883, Training Loss: 1.0845
Epoch 2/10, Batch 190/883, Training Loss: 1.0305
Epoch 2/10, Batch 191/883, Training Loss: 1.1590
Epoch 2/10, Batch 192/883, Training Loss: 0.9016
Epoch 2/10, Batch 193/883, Training Loss: 0.7818
Epoch 2/10, Batch 194/883, Training Loss: 0.8554
Epoch 2/10, Batch 195/883, Training Loss: 0.9690
Epoch 2/10, Batch 196/883, Training Loss: 0.7799
Epoch 2/10, Batch 197/883, Training Loss: 0.6651
Epoch 2/10, Batch 198/883, Training Loss: 0.6198
Epoch 2/10, Batch 199/883, Training Loss: 0.7134
Epoch 2/10, Batch 200/883, Training Loss: 1.3542
Epoch 2/10, Batch 201/883, Training Loss: 1.0620
Epoch 2/10, Batch 202/883, Training Loss: 0.7904
Epoch 2/10, Batch 203/883, Training Loss: 1.1642
Epoch 2/10, Batch 204/883, Training Loss: 0.7430
Epoch 2/10, Batch 205/883, Training Loss: 0.9570
Epoch 2/10, Batch 206/883, Training Loss: 0.8870
Epoch 2/10, Batch 207/883, Training Loss: 0.8678
Epoch 2/10, Batch 208/883, Training Loss: 0.9317
Epoch 2/10, Batch 209/883, Training Loss: 0.9237
Epoch 2/10, Batch 210/883, Training Loss: 0.9479
Epoch 2/10, Batch 211/883, Training Loss: 0.7161
Epoch 2/10, Batch 212/883, Training Loss: 0.8780
Epoch 2/10, Batch 213/883, Training Loss: 0.7934
Epoch 2/10, Batch 214/883, Training Loss: 0.8977
Epoch 2/10, Batch 215/883, Training Loss: 0.8469
Epoch 2/10, Batch 216/883, Training Loss: 0.8586
Epoch 2/10, Batch 217/883, Training Loss: 1.0023
Epoch 2/10, Batch 218/883, Training Loss: 0.8356
Epoch 2/10, Batch 219/883, Training Loss: 0.9358
Epoch 2/10, Batch 220/883, Training Loss: 0.7742
Epoch 2/10, Batch 221/883, Training Loss: 0.7918
Epoch 2/10, Batch 222/883, Training Loss: 0.7302
Epoch 2/10, Batch 223/883, Training Loss: 0.7625
Epoch 2/10, Batch 224/883, Training Loss: 0.7268
Epoch 2/10, Batch 225/883, Training Loss: 0.6587
Epoch 2/10, Batch 226/883, Training Loss: 0.7464
Epoch 2/10, Batch 227/883, Training Loss: 0.7227
Epoch 2/10, Batch 228/883, Training Loss: 1.3311
Epoch 2/10, Batch 229/883, Training Loss: 1.0198
Epoch 2/10, Batch 230/883, Training Loss: 0.6611
Epoch 2/10, Batch 231/883, Training Loss: 0.7931
Epoch 2/10, Batch 232/883, Training Loss: 0.9914
Epoch 2/10, Batch 233/883, Training Loss: 0.8280
Epoch 2/10, Batch 234/883, Training Loss: 0.8100
Epoch 2/10, Batch 235/883, Training Loss: 1.1278
Epoch 2/10, Batch 236/883, Training Loss: 1.0066
Epoch 2/10, Batch 237/883, Training Loss: 1.0155
Epoch 2/10, Batch 238/883, Training Loss: 0.8204
Epoch 2/10, Batch 239/883, Training Loss: 0.8223
Epoch 2/10, Batch 240/883, Training Loss: 1.0296
Epoch 2/10, Batch 241/883, Training Loss: 0.8751
Epoch 2/10, Batch 242/883, Training Loss: 0.9060
Epoch 2/10, Batch 243/883, Training Loss: 0.9529
Epoch 2/10, Batch 244/883, Training Loss: 0.9259
Epoch 2/10, Batch 245/883, Training Loss: 1.0064
Epoch 2/10, Batch 246/883, Training Loss: 0.9116
Epoch 2/10, Batch 247/883, Training Loss: 0.7461
Epoch 2/10, Batch 248/883, Training Loss: 0.7131
Epoch 2/10, Batch 249/883, Training Loss: 0.7264
Epoch 2/10, Batch 250/883, Training Loss: 0.8819
Epoch 2/10, Batch 251/883, Training Loss: 0.6296
Epoch 2/10, Batch 252/883, Training Loss: 0.9077
Epoch 2/10, Batch 253/883, Training Loss: 0.9451
Epoch 2/10, Batch 254/883, Training Loss: 0.7487
Epoch 2/10, Batch 255/883, Training Loss: 1.1267
Epoch 2/10, Batch 256/883, Training Loss: 0.7152
Epoch 2/10, Batch 257/883, Training Loss: 0.6687
Epoch 2/10, Batch 258/883, Training Loss: 0.8877
Epoch 2/10, Batch 259/883, Training Loss: 0.7023
Epoch 2/10, Batch 260/883, Training Loss: 0.5848
Epoch 2/10, Batch 261/883, Training Loss: 0.6305
Epoch 2/10, Batch 262/883, Training Loss: 1.0131
Epoch 2/10, Batch 263/883, Training Loss: 0.8571
Epoch 2/10, Batch 264/883, Training Loss: 0.9787
Epoch 2/10, Batch 265/883, Training Loss: 1.0109
Epoch 2/10, Batch 266/883, Training Loss: 0.9315
Epoch 2/10, Batch 267/883, Training Loss: 0.8116
Epoch 2/10, Batch 268/883, Training Loss: 0.8967
Epoch 2/10, Batch 269/883, Training Loss: 0.9788
Epoch 2/10, Batch 270/883, Training Loss: 0.9565
Epoch 2/10, Batch 271/883, Training Loss: 0.7590
Epoch 2/10, Batch 272/883, Training Loss: 1.0099
Epoch 2/10, Batch 273/883, Training Loss: 0.8334
Epoch 2/10, Batch 274/883, Training Loss: 0.9069
Epoch 2/10, Batch 275/883, Training Loss: 0.6903
Epoch 2/10, Batch 276/883, Training Loss: 1.1981
Epoch 2/10, Batch 277/883, Training Loss: 0.8770
Epoch 2/10, Batch 278/883, Training Loss: 0.8467
Epoch 2/10, Batch 279/883, Training Loss: 1.2610
Epoch 2/10, Batch 280/883, Training Loss: 0.6381
Epoch 2/10, Batch 281/883, Training Loss: 0.7132
Epoch 2/10, Batch 282/883, Training Loss: 0.8128
Epoch 2/10, Batch 283/883, Training Loss: 0.8431
Epoch 2/10, Batch 284/883, Training Loss: 0.7695
Epoch 2/10, Batch 285/883, Training Loss: 0.8627
Epoch 2/10, Batch 286/883, Training Loss: 1.0228
Epoch 2/10, Batch 287/883, Training Loss: 0.7237
Epoch 2/10, Batch 288/883, Training Loss: 0.6599
Epoch 2/10, Batch 289/883, Training Loss: 1.0213
Epoch 2/10, Batch 290/883, Training Loss: 0.7529
Epoch 2/10, Batch 291/883, Training Loss: 1.0713
Epoch 2/10, Batch 292/883, Training Loss: 1.0382
Epoch 2/10, Batch 293/883, Training Loss: 1.2610
Epoch 2/10, Batch 294/883, Training Loss: 0.9539
Epoch 2/10, Batch 295/883, Training Loss: 0.8663
Epoch 2/10, Batch 296/883, Training Loss: 0.8091
Epoch 2/10, Batch 297/883, Training Loss: 0.8857
Epoch 2/10, Batch 298/883, Training Loss: 1.0359
Epoch 2/10, Batch 299/883, Training Loss: 0.6811
Epoch 2/10, Batch 300/883, Training Loss: 1.1871
Epoch 2/10, Batch 301/883, Training Loss: 0.9666
Epoch 2/10, Batch 302/883, Training Loss: 0.7936
Epoch 2/10, Batch 303/883, Training Loss: 0.9019
Epoch 2/10, Batch 304/883, Training Loss: 0.9144
Epoch 2/10, Batch 305/883, Training Loss: 0.8512
Epoch 2/10, Batch 306/883, Training Loss: 0.9073
Epoch 2/10, Batch 307/883, Training Loss: 0.8386
Epoch 2/10, Batch 308/883, Training Loss: 0.9881
Epoch 2/10, Batch 309/883, Training Loss: 0.8464
Epoch 2/10, Batch 310/883, Training Loss: 1.0321
Epoch 2/10, Batch 311/883, Training Loss: 0.6932
Epoch 2/10, Batch 312/883, Training Loss: 0.8525
Epoch 2/10, Batch 313/883, Training Loss: 0.8386
Epoch 2/10, Batch 314/883, Training Loss: 0.8049
Epoch 2/10, Batch 315/883, Training Loss: 0.9702
Epoch 2/10, Batch 316/883, Training Loss: 0.6286
Epoch 2/10, Batch 317/883, Training Loss: 0.8555
Epoch 2/10, Batch 318/883, Training Loss: 0.9921
Epoch 2/10, Batch 319/883, Training Loss: 0.9616
Epoch 2/10, Batch 320/883, Training Loss: 0.8733
Epoch 2/10, Batch 321/883, Training Loss: 0.8519
Epoch 2/10, Batch 322/883, Training Loss: 0.9034
Epoch 2/10, Batch 323/883, Training Loss: 1.2635
Epoch 2/10, Batch 324/883, Training Loss: 0.7363
Epoch 2/10, Batch 325/883, Training Loss: 1.1071
Epoch 2/10, Batch 326/883, Training Loss: 0.8062
Epoch 2/10, Batch 327/883, Training Loss: 0.9084
Epoch 2/10, Batch 328/883, Training Loss: 0.7622
Epoch 2/10, Batch 329/883, Training Loss: 0.9725
Epoch 2/10, Batch 330/883, Training Loss: 0.8364
Epoch 2/10, Batch 331/883, Training Loss: 0.8315
Epoch 2/10, Batch 332/883, Training Loss: 1.0037
Epoch 2/10, Batch 333/883, Training Loss: 0.8486
Epoch 2/10, Batch 334/883, Training Loss: 0.7075
Epoch 2/10, Batch 335/883, Training Loss: 0.8209
Epoch 2/10, Batch 336/883, Training Loss: 0.7538
Epoch 2/10, Batch 337/883, Training Loss: 0.7147
Epoch 2/10, Batch 338/883, Training Loss: 0.7249
Epoch 2/10, Batch 339/883, Training Loss: 0.6797
Epoch 2/10, Batch 340/883, Training Loss: 0.7790
Epoch 2/10, Batch 341/883, Training Loss: 1.0159
Epoch 2/10, Batch 342/883, Training Loss: 0.6936
Epoch 2/10, Batch 343/883, Training Loss: 0.7639
Epoch 2/10, Batch 344/883, Training Loss: 1.3734
Epoch 2/10, Batch 345/883, Training Loss: 1.2067
Epoch 2/10, Batch 346/883, Training Loss: 1.0890
Epoch 2/10, Batch 347/883, Training Loss: 0.8449
Epoch 2/10, Batch 348/883, Training Loss: 0.6993
Epoch 2/10, Batch 349/883, Training Loss: 0.7048
Epoch 2/10, Batch 350/883, Training Loss: 0.9030
Epoch 2/10, Batch 351/883, Training Loss: 1.0467
Epoch 2/10, Batch 352/883, Training Loss: 0.9646
Epoch 2/10, Batch 353/883, Training Loss: 0.7146
Epoch 2/10, Batch 354/883, Training Loss: 1.2966
Epoch 2/10, Batch 355/883, Training Loss: 0.7777
Epoch 2/10, Batch 356/883, Training Loss: 1.0645
Epoch 2/10, Batch 357/883, Training Loss: 0.7492
Epoch 2/10, Batch 358/883, Training Loss: 0.9147
Epoch 2/10, Batch 359/883, Training Loss: 0.7333
Epoch 2/10, Batch 360/883, Training Loss: 0.8856
Epoch 2/10, Batch 361/883, Training Loss: 0.6938
Epoch 2/10, Batch 362/883, Training Loss: 0.8227
Epoch 2/10, Batch 363/883, Training Loss: 0.7642
Epoch 2/10, Batch 364/883, Training Loss: 1.0107
Epoch 2/10, Batch 365/883, Training Loss: 0.7563
Epoch 2/10, Batch 366/883, Training Loss: 1.0717
Epoch 2/10, Batch 367/883, Training Loss: 0.7938
Epoch 2/10, Batch 368/883, Training Loss: 0.8413
Epoch 2/10, Batch 369/883, Training Loss: 0.9433
Epoch 2/10, Batch 370/883, Training Loss: 0.9323
Epoch 2/10, Batch 371/883, Training Loss: 0.7593
Epoch 2/10, Batch 372/883, Training Loss: 0.8997
Epoch 2/10, Batch 373/883, Training Loss: 0.8432
Epoch 2/10, Batch 374/883, Training Loss: 0.9647
Epoch 2/10, Batch 375/883, Training Loss: 0.7140
Epoch 2/10, Batch 376/883, Training Loss: 0.8596
Epoch 2/10, Batch 377/883, Training Loss: 0.9114
Epoch 2/10, Batch 378/883, Training Loss: 0.7290
Epoch 2/10, Batch 379/883, Training Loss: 0.7733
Epoch 2/10, Batch 380/883, Training Loss: 0.8284
Epoch 2/10, Batch 381/883, Training Loss: 0.7862
Epoch 2/10, Batch 382/883, Training Loss: 0.8419
Epoch 2/10, Batch 383/883, Training Loss: 0.9647
Epoch 2/10, Batch 384/883, Training Loss: 0.7972
Epoch 2/10, Batch 385/883, Training Loss: 0.9034
Epoch 2/10, Batch 386/883, Training Loss: 1.3221
Epoch 2/10, Batch 387/883, Training Loss: 0.9420
Epoch 2/10, Batch 388/883, Training Loss: 1.0181
Epoch 2/10, Batch 389/883, Training Loss: 0.7763
Epoch 2/10, Batch 390/883, Training Loss: 1.0122
Epoch 2/10, Batch 391/883, Training Loss: 0.8517
Epoch 2/10, Batch 392/883, Training Loss: 0.9585
Epoch 2/10, Batch 393/883, Training Loss: 0.7957
Epoch 2/10, Batch 394/883, Training Loss: 0.7278
Epoch 2/10, Batch 395/883, Training Loss: 0.8195
Epoch 2/10, Batch 396/883, Training Loss: 0.7947
Epoch 2/10, Batch 397/883, Training Loss: 0.7317
Epoch 2/10, Batch 398/883, Training Loss: 0.9232
Epoch 2/10, Batch 399/883, Training Loss: 0.7879
Epoch 2/10, Batch 400/883, Training Loss: 0.9424
Epoch 2/10, Batch 401/883, Training Loss: 0.7736
Epoch 2/10, Batch 402/883, Training Loss: 0.8987
Epoch 2/10, Batch 403/883, Training Loss: 0.8568
Epoch 2/10, Batch 404/883, Training Loss: 0.9602
Epoch 2/10, Batch 405/883, Training Loss: 0.6389
Epoch 2/10, Batch 406/883, Training Loss: 0.8553
Epoch 2/10, Batch 407/883, Training Loss: 0.9138
Epoch 2/10, Batch 408/883, Training Loss: 0.6015
Epoch 2/10, Batch 409/883, Training Loss: 0.8927
Epoch 2/10, Batch 410/883, Training Loss: 0.8507
Epoch 2/10, Batch 411/883, Training Loss: 0.7309
Epoch 2/10, Batch 412/883, Training Loss: 0.8017
Epoch 2/10, Batch 413/883, Training Loss: 0.7349
Epoch 2/10, Batch 414/883, Training Loss: 0.5989
Epoch 2/10, Batch 415/883, Training Loss: 0.9535
Epoch 2/10, Batch 416/883, Training Loss: 0.7912
Epoch 2/10, Batch 417/883, Training Loss: 0.8529
Epoch 2/10, Batch 418/883, Training Loss: 1.0863
Epoch 2/10, Batch 419/883, Training Loss: 0.8098
Epoch 2/10, Batch 420/883, Training Loss: 0.9943
Epoch 2/10, Batch 421/883, Training Loss: 1.1002
Epoch 2/10, Batch 422/883, Training Loss: 1.1538
Epoch 2/10, Batch 423/883, Training Loss: 0.6924
Epoch 2/10, Batch 424/883, Training Loss: 0.8514
Epoch 2/10, Batch 425/883, Training Loss: 1.2791
Epoch 2/10, Batch 426/883, Training Loss: 0.7511
Epoch 2/10, Batch 427/883, Training Loss: 0.7230
Epoch 2/10, Batch 428/883, Training Loss: 0.8234
Epoch 2/10, Batch 429/883, Training Loss: 0.9422
Epoch 2/10, Batch 430/883, Training Loss: 0.7012
Epoch 2/10, Batch 431/883, Training Loss: 0.7856
Epoch 2/10, Batch 432/883, Training Loss: 0.7949
Epoch 2/10, Batch 433/883, Training Loss: 0.7503
Epoch 2/10, Batch 434/883, Training Loss: 0.8708
Epoch 2/10, Batch 435/883, Training Loss: 0.7529
Epoch 2/10, Batch 436/883, Training Loss: 0.8495
Epoch 2/10, Batch 437/883, Training Loss: 0.9506
Epoch 2/10, Batch 438/883, Training Loss: 0.8053
Epoch 2/10, Batch 439/883, Training Loss: 0.8557
Epoch 2/10, Batch 440/883, Training Loss: 0.9066
Epoch 2/10, Batch 441/883, Training Loss: 0.6349
Epoch 2/10, Batch 442/883, Training Loss: 0.8371
Epoch 2/10, Batch 443/883, Training Loss: 0.7652
Epoch 2/10, Batch 444/883, Training Loss: 0.8687
Epoch 2/10, Batch 445/883, Training Loss: 0.7803
Epoch 2/10, Batch 446/883, Training Loss: 0.7379
Epoch 2/10, Batch 447/883, Training Loss: 0.8351
Epoch 2/10, Batch 448/883, Training Loss: 0.6446
Epoch 2/10, Batch 449/883, Training Loss: 0.6919
Epoch 2/10, Batch 450/883, Training Loss: 0.8788
Epoch 2/10, Batch 451/883, Training Loss: 0.9256
Epoch 2/10, Batch 452/883, Training Loss: 0.8943
Epoch 2/10, Batch 453/883, Training Loss: 1.0848
Epoch 2/10, Batch 454/883, Training Loss: 0.7500
Epoch 2/10, Batch 455/883, Training Loss: 0.8107
Epoch 2/10, Batch 456/883, Training Loss: 0.8410
Epoch 2/10, Batch 457/883, Training Loss: 0.9380
Epoch 2/10, Batch 458/883, Training Loss: 0.8792
Epoch 2/10, Batch 459/883, Training Loss: 0.6601
Epoch 2/10, Batch 460/883, Training Loss: 1.2114
Epoch 2/10, Batch 461/883, Training Loss: 1.3189
Epoch 2/10, Batch 462/883, Training Loss: 0.7953
Epoch 2/10, Batch 463/883, Training Loss: 0.9627
Epoch 2/10, Batch 464/883, Training Loss: 0.8794
Epoch 2/10, Batch 465/883, Training Loss: 0.7163
Epoch 2/10, Batch 466/883, Training Loss: 0.6743
Epoch 2/10, Batch 467/883, Training Loss: 0.7645
Epoch 2/10, Batch 468/883, Training Loss: 1.1117
Epoch 2/10, Batch 469/883, Training Loss: 0.9786
Epoch 2/10, Batch 470/883, Training Loss: 0.7813
Epoch 2/10, Batch 471/883, Training Loss: 1.0763
Epoch 2/10, Batch 472/883, Training Loss: 0.9252
Epoch 2/10, Batch 473/883, Training Loss: 0.8841
Epoch 2/10, Batch 474/883, Training Loss: 0.7951
Epoch 2/10, Batch 475/883, Training Loss: 0.9552
Epoch 2/10, Batch 476/883, Training Loss: 1.0016
Epoch 2/10, Batch 477/883, Training Loss: 0.7941
Epoch 2/10, Batch 478/883, Training Loss: 0.6672
Epoch 2/10, Batch 479/883, Training Loss: 0.9393
Epoch 2/10, Batch 480/883, Training Loss: 0.9835
Epoch 2/10, Batch 481/883, Training Loss: 0.8219
Epoch 2/10, Batch 482/883, Training Loss: 0.7118
Epoch 2/10, Batch 483/883, Training Loss: 0.6751
Epoch 2/10, Batch 484/883, Training Loss: 0.7247
Epoch 2/10, Batch 485/883, Training Loss: 1.0409
Epoch 2/10, Batch 486/883, Training Loss: 0.8593
Epoch 2/10, Batch 487/883, Training Loss: 0.8034
Epoch 2/10, Batch 488/883, Training Loss: 0.6559
Epoch 2/10, Batch 489/883, Training Loss: 0.9641
Epoch 2/10, Batch 490/883, Training Loss: 0.8426
Epoch 2/10, Batch 491/883, Training Loss: 0.8254
Epoch 2/10, Batch 492/883, Training Loss: 0.7290
Epoch 2/10, Batch 493/883, Training Loss: 0.9951
Epoch 2/10, Batch 494/883, Training Loss: 0.7565
Epoch 2/10, Batch 495/883, Training Loss: 0.8107
Epoch 2/10, Batch 496/883, Training Loss: 0.6588
Epoch 2/10, Batch 497/883, Training Loss: 0.8447
Epoch 2/10, Batch 498/883, Training Loss: 1.1310
Epoch 2/10, Batch 499/883, Training Loss: 0.9537
Epoch 2/10, Batch 500/883, Training Loss: 0.8662
Epoch 2/10, Batch 501/883, Training Loss: 0.8978
Epoch 2/10, Batch 502/883, Training Loss: 0.9233
Epoch 2/10, Batch 503/883, Training Loss: 0.6939
Epoch 2/10, Batch 504/883, Training Loss: 0.9764
Epoch 2/10, Batch 505/883, Training Loss: 0.7374
Epoch 2/10, Batch 506/883, Training Loss: 0.9687
Epoch 2/10, Batch 507/883, Training Loss: 0.6516
Epoch 2/10, Batch 508/883, Training Loss: 0.6805
Epoch 2/10, Batch 509/883, Training Loss: 0.8733
Epoch 2/10, Batch 510/883, Training Loss: 0.8648
Epoch 2/10, Batch 511/883, Training Loss: 0.8487
Epoch 2/10, Batch 512/883, Training Loss: 0.8845
Epoch 2/10, Batch 513/883, Training Loss: 0.7617
Epoch 2/10, Batch 514/883, Training Loss: 0.6495
Epoch 2/10, Batch 515/883, Training Loss: 0.6800
Epoch 2/10, Batch 516/883, Training Loss: 0.6783
Epoch 2/10, Batch 517/883, Training Loss: 0.7922
Epoch 2/10, Batch 518/883, Training Loss: 1.0779
Epoch 2/10, Batch 519/883, Training Loss: 0.8547
Epoch 2/10, Batch 520/883, Training Loss: 0.7947
Epoch 2/10, Batch 521/883, Training Loss: 0.9232
Epoch 2/10, Batch 522/883, Training Loss: 0.7009
Epoch 2/10, Batch 523/883, Training Loss: 0.6564
Epoch 2/10, Batch 524/883, Training Loss: 0.7870
Epoch 2/10, Batch 525/883, Training Loss: 1.1016
Epoch 2/10, Batch 526/883, Training Loss: 0.8131
Epoch 2/10, Batch 527/883, Training Loss: 0.6885
Epoch 2/10, Batch 528/883, Training Loss: 1.0373
Epoch 2/10, Batch 529/883, Training Loss: 0.8815
Epoch 2/10, Batch 530/883, Training Loss: 0.8048
Epoch 2/10, Batch 531/883, Training Loss: 0.7007
Epoch 2/10, Batch 532/883, Training Loss: 0.7772
Epoch 2/10, Batch 533/883, Training Loss: 0.8604
Epoch 2/10, Batch 534/883, Training Loss: 0.6327
Epoch 2/10, Batch 535/883, Training Loss: 0.7854
Epoch 2/10, Batch 536/883, Training Loss: 0.8499
Epoch 2/10, Batch 537/883, Training Loss: 1.0792
Epoch 2/10, Batch 538/883, Training Loss: 0.7719
Epoch 2/10, Batch 539/883, Training Loss: 1.0180
Epoch 2/10, Batch 540/883, Training Loss: 0.8364
Epoch 2/10, Batch 541/883, Training Loss: 1.2427
Epoch 2/10, Batch 542/883, Training Loss: 0.8262
Epoch 2/10, Batch 543/883, Training Loss: 0.7345
Epoch 2/10, Batch 544/883, Training Loss: 0.7748
Epoch 2/10, Batch 545/883, Training Loss: 1.0380
Epoch 2/10, Batch 546/883, Training Loss: 0.8973
Epoch 2/10, Batch 547/883, Training Loss: 0.7770
Epoch 2/10, Batch 548/883, Training Loss: 0.8305
Epoch 2/10, Batch 549/883, Training Loss: 0.9161
Epoch 2/10, Batch 550/883, Training Loss: 0.9540
Epoch 2/10, Batch 551/883, Training Loss: 1.0357
Epoch 2/10, Batch 552/883, Training Loss: 1.0740
Epoch 2/10, Batch 553/883, Training Loss: 0.8701
Epoch 2/10, Batch 554/883, Training Loss: 0.9496
Epoch 2/10, Batch 555/883, Training Loss: 0.8881
Epoch 2/10, Batch 556/883, Training Loss: 0.7709
Epoch 2/10, Batch 557/883, Training Loss: 0.8659
Epoch 2/10, Batch 558/883, Training Loss: 0.9329
Epoch 2/10, Batch 559/883, Training Loss: 0.7517
Epoch 2/10, Batch 560/883, Training Loss: 0.7875
Epoch 2/10, Batch 561/883, Training Loss: 0.9946
Epoch 2/10, Batch 562/883, Training Loss: 0.8195
Epoch 2/10, Batch 563/883, Training Loss: 0.8950
Epoch 2/10, Batch 564/883, Training Loss: 0.8537
Epoch 2/10, Batch 565/883, Training Loss: 0.6647
Epoch 2/10, Batch 566/883, Training Loss: 0.7800
Epoch 2/10, Batch 567/883, Training Loss: 0.5920
Epoch 2/10, Batch 568/883, Training Loss: 0.9631
Epoch 2/10, Batch 569/883, Training Loss: 0.6821
Epoch 2/10, Batch 570/883, Training Loss: 0.6658
Epoch 2/10, Batch 571/883, Training Loss: 0.8986
Epoch 2/10, Batch 572/883, Training Loss: 0.7403
Epoch 2/10, Batch 573/883, Training Loss: 0.8798
Epoch 2/10, Batch 574/883, Training Loss: 0.7415
Epoch 2/10, Batch 575/883, Training Loss: 0.7139
Epoch 2/10, Batch 576/883, Training Loss: 1.0662
Epoch 2/10, Batch 577/883, Training Loss: 0.6353
Epoch 2/10, Batch 578/883, Training Loss: 0.7983
Epoch 2/10, Batch 579/883, Training Loss: 1.1332
Epoch 2/10, Batch 580/883, Training Loss: 1.1282
Epoch 2/10, Batch 581/883, Training Loss: 0.9820
Epoch 2/10, Batch 582/883, Training Loss: 0.8857
Epoch 2/10, Batch 583/883, Training Loss: 0.8412
Epoch 2/10, Batch 584/883, Training Loss: 0.8615
Epoch 2/10, Batch 585/883, Training Loss: 0.8001
Epoch 2/10, Batch 586/883, Training Loss: 0.9519
Epoch 2/10, Batch 587/883, Training Loss: 0.8731
Epoch 2/10, Batch 588/883, Training Loss: 0.6995
Epoch 2/10, Batch 589/883, Training Loss: 0.8156
Epoch 2/10, Batch 590/883, Training Loss: 0.8000
Epoch 2/10, Batch 591/883, Training Loss: 0.6916
Epoch 2/10, Batch 592/883, Training Loss: 0.8093
Epoch 2/10, Batch 593/883, Training Loss: 0.6374
Epoch 2/10, Batch 594/883, Training Loss: 0.7665
Epoch 2/10, Batch 595/883, Training Loss: 0.6888
Epoch 2/10, Batch 596/883, Training Loss: 0.7976
Epoch 2/10, Batch 597/883, Training Loss: 0.9984
Epoch 2/10, Batch 598/883, Training Loss: 0.7197
Epoch 2/10, Batch 599/883, Training Loss: 0.9301
Epoch 2/10, Batch 600/883, Training Loss: 1.0626
Epoch 2/10, Batch 601/883, Training Loss: 0.7011
Epoch 2/10, Batch 602/883, Training Loss: 0.7098
Epoch 2/10, Batch 603/883, Training Loss: 0.8864
Epoch 2/10, Batch 604/883, Training Loss: 0.8236
Epoch 2/10, Batch 605/883, Training Loss: 0.8337
Epoch 2/10, Batch 606/883, Training Loss: 0.7877
Epoch 2/10, Batch 607/883, Training Loss: 0.8615
Epoch 2/10, Batch 608/883, Training Loss: 0.7828
Epoch 2/10, Batch 609/883, Training Loss: 0.8807
Epoch 2/10, Batch 610/883, Training Loss: 1.1349
Epoch 2/10, Batch 611/883, Training Loss: 0.7623
Epoch 2/10, Batch 612/883, Training Loss: 0.9147
Epoch 2/10, Batch 613/883, Training Loss: 0.7756
Epoch 2/10, Batch 614/883, Training Loss: 0.8232
Epoch 2/10, Batch 615/883, Training Loss: 0.8293
Epoch 2/10, Batch 616/883, Training Loss: 0.6578
Epoch 2/10, Batch 617/883, Training Loss: 0.9103
Epoch 2/10, Batch 618/883, Training Loss: 0.7380
Epoch 2/10, Batch 619/883, Training Loss: 1.1231
Epoch 2/10, Batch 620/883, Training Loss: 0.8043
Epoch 2/10, Batch 621/883, Training Loss: 1.0545
Epoch 2/10, Batch 622/883, Training Loss: 1.3111
Epoch 2/10, Batch 623/883, Training Loss: 0.7573
Epoch 2/10, Batch 624/883, Training Loss: 0.8613
Epoch 2/10, Batch 625/883, Training Loss: 1.0449
Epoch 2/10, Batch 626/883, Training Loss: 1.0000
Epoch 2/10, Batch 627/883, Training Loss: 0.8035
Epoch 2/10, Batch 628/883, Training Loss: 0.6206
Epoch 2/10, Batch 629/883, Training Loss: 0.7677
Epoch 2/10, Batch 630/883, Training Loss: 1.0592
Epoch 2/10, Batch 631/883, Training Loss: 0.7573
Epoch 2/10, Batch 632/883, Training Loss: 0.9513
Epoch 2/10, Batch 633/883, Training Loss: 0.8624
Epoch 2/10, Batch 634/883, Training Loss: 0.8147
Epoch 2/10, Batch 635/883, Training Loss: 0.7916
Epoch 2/10, Batch 636/883, Training Loss: 0.8934
Epoch 2/10, Batch 637/883, Training Loss: 1.0486
Epoch 2/10, Batch 638/883, Training Loss: 0.7637
Epoch 2/10, Batch 639/883, Training Loss: 1.1726
Epoch 2/10, Batch 640/883, Training Loss: 0.7371
Epoch 2/10, Batch 641/883, Training Loss: 0.7908
Epoch 2/10, Batch 642/883, Training Loss: 1.0613
Epoch 2/10, Batch 643/883, Training Loss: 0.7480
Epoch 2/10, Batch 644/883, Training Loss: 0.7852
Epoch 2/10, Batch 645/883, Training Loss: 0.7196
Epoch 2/10, Batch 646/883, Training Loss: 1.0594
Epoch 2/10, Batch 647/883, Training Loss: 1.1678
Epoch 2/10, Batch 648/883, Training Loss: 0.9010
Epoch 2/10, Batch 649/883, Training Loss: 0.7995
Epoch 2/10, Batch 650/883, Training Loss: 0.6915
Epoch 2/10, Batch 651/883, Training Loss: 0.7737
Epoch 2/10, Batch 652/883, Training Loss: 0.7396
Epoch 2/10, Batch 653/883, Training Loss: 0.8278
Epoch 2/10, Batch 654/883, Training Loss: 0.9502
Epoch 2/10, Batch 655/883, Training Loss: 0.6803
Epoch 2/10, Batch 656/883, Training Loss: 1.0286
Epoch 2/10, Batch 657/883, Training Loss: 0.6543
Epoch 2/10, Batch 658/883, Training Loss: 0.6452
Epoch 2/10, Batch 659/883, Training Loss: 0.8071
Epoch 2/10, Batch 660/883, Training Loss: 0.9734
Epoch 2/10, Batch 661/883, Training Loss: 0.8448
Epoch 2/10, Batch 662/883, Training Loss: 0.6191
Epoch 2/10, Batch 663/883, Training Loss: 0.7185
Epoch 2/10, Batch 664/883, Training Loss: 0.7016
Epoch 2/10, Batch 665/883, Training Loss: 0.9564
Epoch 2/10, Batch 666/883, Training Loss: 1.4930
Epoch 2/10, Batch 667/883, Training Loss: 0.6904
Epoch 2/10, Batch 668/883, Training Loss: 0.9195
Epoch 2/10, Batch 669/883, Training Loss: 0.9648
Epoch 2/10, Batch 670/883, Training Loss: 0.6463
Epoch 2/10, Batch 671/883, Training Loss: 1.0342
Epoch 2/10, Batch 672/883, Training Loss: 0.8777
Epoch 2/10, Batch 673/883, Training Loss: 0.7117
Epoch 2/10, Batch 674/883, Training Loss: 0.6488
Epoch 2/10, Batch 675/883, Training Loss: 0.7457
Epoch 2/10, Batch 676/883, Training Loss: 0.9986
Epoch 2/10, Batch 677/883, Training Loss: 0.7321
Epoch 2/10, Batch 678/883, Training Loss: 0.9315
Epoch 2/10, Batch 679/883, Training Loss: 0.8080
Epoch 2/10, Batch 680/883, Training Loss: 0.6634
Epoch 2/10, Batch 681/883, Training Loss: 0.7682
Epoch 2/10, Batch 682/883, Training Loss: 0.8380
Epoch 2/10, Batch 683/883, Training Loss: 0.8021
Epoch 2/10, Batch 684/883, Training Loss: 0.8287
Epoch 2/10, Batch 685/883, Training Loss: 1.1450
Epoch 2/10, Batch 686/883, Training Loss: 0.9056
Epoch 2/10, Batch 687/883, Training Loss: 0.8100
Epoch 2/10, Batch 688/883, Training Loss: 1.0555
Epoch 2/10, Batch 689/883, Training Loss: 0.9708
Epoch 2/10, Batch 690/883, Training Loss: 0.7262
Epoch 2/10, Batch 691/883, Training Loss: 0.8766
Epoch 2/10, Batch 692/883, Training Loss: 1.0382
Epoch 2/10, Batch 693/883, Training Loss: 0.8686
Epoch 2/10, Batch 694/883, Training Loss: 0.9256
Epoch 2/10, Batch 695/883, Training Loss: 0.8592
Epoch 2/10, Batch 696/883, Training Loss: 0.8929
Epoch 2/10, Batch 697/883, Training Loss: 0.8205
Epoch 2/10, Batch 698/883, Training Loss: 0.7182
Epoch 2/10, Batch 699/883, Training Loss: 0.8244
Epoch 2/10, Batch 700/883, Training Loss: 0.8775
Epoch 2/10, Batch 701/883, Training Loss: 0.6893
Epoch 2/10, Batch 702/883, Training Loss: 0.6476
Epoch 2/10, Batch 703/883, Training Loss: 0.7471
Epoch 2/10, Batch 704/883, Training Loss: 0.7522
Epoch 2/10, Batch 705/883, Training Loss: 0.7610
Epoch 2/10, Batch 706/883, Training Loss: 0.9818
Epoch 2/10, Batch 707/883, Training Loss: 0.8582
Epoch 2/10, Batch 708/883, Training Loss: 1.1611
Epoch 2/10, Batch 709/883, Training Loss: 0.7461
Epoch 2/10, Batch 710/883, Training Loss: 0.7676
Epoch 2/10, Batch 711/883, Training Loss: 0.8838
Epoch 2/10, Batch 712/883, Training Loss: 1.0232
Epoch 2/10, Batch 713/883, Training Loss: 0.6769
Epoch 2/10, Batch 714/883, Training Loss: 0.7680
Epoch 2/10, Batch 715/883, Training Loss: 1.0419
Epoch 2/10, Batch 716/883, Training Loss: 0.7674
Epoch 2/10, Batch 717/883, Training Loss: 1.0576
Epoch 2/10, Batch 718/883, Training Loss: 0.7448
Epoch 2/10, Batch 719/883, Training Loss: 0.6968
Epoch 2/10, Batch 720/883, Training Loss: 0.8292
Epoch 2/10, Batch 721/883, Training Loss: 1.0064
Epoch 2/10, Batch 722/883, Training Loss: 0.7549
Epoch 2/10, Batch 723/883, Training Loss: 0.8582
Epoch 2/10, Batch 724/883, Training Loss: 0.7338
Epoch 2/10, Batch 725/883, Training Loss: 0.8587
Epoch 2/10, Batch 726/883, Training Loss: 0.8158
Epoch 2/10, Batch 727/883, Training Loss: 0.8838
Epoch 2/10, Batch 728/883, Training Loss: 0.6618
Epoch 2/10, Batch 729/883, Training Loss: 0.8192
Epoch 2/10, Batch 730/883, Training Loss: 0.8093
Epoch 2/10, Batch 731/883, Training Loss: 0.8032
Epoch 2/10, Batch 732/883, Training Loss: 0.7434
Epoch 2/10, Batch 733/883, Training Loss: 1.0149
Epoch 2/10, Batch 734/883, Training Loss: 0.8680
Epoch 2/10, Batch 735/883, Training Loss: 0.8233
Epoch 2/10, Batch 736/883, Training Loss: 0.6687
Epoch 2/10, Batch 737/883, Training Loss: 0.8515
Epoch 2/10, Batch 738/883, Training Loss: 1.2289
Epoch 2/10, Batch 739/883, Training Loss: 0.6845
Epoch 2/10, Batch 740/883, Training Loss: 0.9276
Epoch 2/10, Batch 741/883, Training Loss: 0.9525
Epoch 2/10, Batch 742/883, Training Loss: 0.8685
Epoch 2/10, Batch 743/883, Training Loss: 0.8503
Epoch 2/10, Batch 744/883, Training Loss: 0.7892
Epoch 2/10, Batch 745/883, Training Loss: 0.8086
Epoch 2/10, Batch 746/883, Training Loss: 1.0436
Epoch 2/10, Batch 747/883, Training Loss: 0.9717
Epoch 2/10, Batch 748/883, Training Loss: 0.8737
Epoch 2/10, Batch 749/883, Training Loss: 0.9797
Epoch 2/10, Batch 750/883, Training Loss: 0.7272
Epoch 2/10, Batch 751/883, Training Loss: 0.7746
Epoch 2/10, Batch 752/883, Training Loss: 0.8449
Epoch 2/10, Batch 753/883, Training Loss: 0.6971
Epoch 2/10, Batch 754/883, Training Loss: 0.7792
Epoch 2/10, Batch 755/883, Training Loss: 0.7739
Epoch 2/10, Batch 756/883, Training Loss: 0.7477
Epoch 2/10, Batch 757/883, Training Loss: 0.5858
Epoch 2/10, Batch 758/883, Training Loss: 0.6954
Epoch 2/10, Batch 759/883, Training Loss: 0.7544
Epoch 2/10, Batch 760/883, Training Loss: 0.7478
Epoch 2/10, Batch 761/883, Training Loss: 0.9186
Epoch 2/10, Batch 762/883, Training Loss: 0.8991
Epoch 2/10, Batch 763/883, Training Loss: 1.4078
Epoch 2/10, Batch 764/883, Training Loss: 0.5543
Epoch 2/10, Batch 765/883, Training Loss: 0.9360
Epoch 2/10, Batch 766/883, Training Loss: 1.0489
Epoch 2/10, Batch 767/883, Training Loss: 0.6406
Epoch 2/10, Batch 768/883, Training Loss: 0.8051
Epoch 2/10, Batch 769/883, Training Loss: 0.9718
Epoch 2/10, Batch 770/883, Training Loss: 1.0106
Epoch 2/10, Batch 771/883, Training Loss: 0.8040
Epoch 2/10, Batch 772/883, Training Loss: 0.8311
Epoch 2/10, Batch 773/883, Training Loss: 1.0060
Epoch 2/10, Batch 774/883, Training Loss: 1.0952
Epoch 2/10, Batch 775/883, Training Loss: 0.6695
Epoch 2/10, Batch 776/883, Training Loss: 0.7534
Epoch 2/10, Batch 777/883, Training Loss: 0.8547
Epoch 2/10, Batch 778/883, Training Loss: 0.9853
Epoch 2/10, Batch 779/883, Training Loss: 0.8464
Epoch 2/10, Batch 780/883, Training Loss: 0.7809
Epoch 2/10, Batch 781/883, Training Loss: 0.7489
Epoch 2/10, Batch 782/883, Training Loss: 0.7601
Epoch 2/10, Batch 783/883, Training Loss: 0.8288
Epoch 2/10, Batch 784/883, Training Loss: 0.8085
Epoch 2/10, Batch 785/883, Training Loss: 0.9601
Epoch 2/10, Batch 786/883, Training Loss: 0.8445
Epoch 2/10, Batch 787/883, Training Loss: 0.6878
Epoch 2/10, Batch 788/883, Training Loss: 0.7994
Epoch 2/10, Batch 789/883, Training Loss: 1.0271
Epoch 2/10, Batch 790/883, Training Loss: 0.7371
Epoch 2/10, Batch 791/883, Training Loss: 0.8392
Epoch 2/10, Batch 792/883, Training Loss: 0.7468
Epoch 2/10, Batch 793/883, Training Loss: 0.7814
Epoch 2/10, Batch 794/883, Training Loss: 0.8263
Epoch 2/10, Batch 795/883, Training Loss: 0.7762
Epoch 2/10, Batch 796/883, Training Loss: 0.6230
Epoch 2/10, Batch 797/883, Training Loss: 0.6134
Epoch 2/10, Batch 798/883, Training Loss: 0.7715
Epoch 2/10, Batch 799/883, Training Loss: 0.5251
Epoch 2/10, Batch 800/883, Training Loss: 0.7205
Epoch 2/10, Batch 801/883, Training Loss: 1.0849
Epoch 2/10, Batch 802/883, Training Loss: 0.7478
Epoch 2/10, Batch 803/883, Training Loss: 0.8673
Epoch 2/10, Batch 804/883, Training Loss: 0.9434
Epoch 2/10, Batch 805/883, Training Loss: 0.7227
Epoch 2/10, Batch 806/883, Training Loss: 0.8704
Epoch 2/10, Batch 807/883, Training Loss: 0.7054
Epoch 2/10, Batch 808/883, Training Loss: 0.7888
Epoch 2/10, Batch 809/883, Training Loss: 0.8069
Epoch 2/10, Batch 810/883, Training Loss: 0.9176
Epoch 2/10, Batch 811/883, Training Loss: 1.1171
Epoch 2/10, Batch 812/883, Training Loss: 0.8928
Epoch 2/10, Batch 813/883, Training Loss: 0.7331
Epoch 2/10, Batch 814/883, Training Loss: 0.7031
Epoch 2/10, Batch 815/883, Training Loss: 0.8744
Epoch 2/10, Batch 816/883, Training Loss: 0.6870
Epoch 2/10, Batch 817/883, Training Loss: 0.7668
Epoch 2/10, Batch 818/883, Training Loss: 0.7895
Epoch 2/10, Batch 819/883, Training Loss: 0.6929
Epoch 2/10, Batch 820/883, Training Loss: 0.6073
Epoch 2/10, Batch 821/883, Training Loss: 1.1154
Epoch 2/10, Batch 822/883, Training Loss: 0.8718
Epoch 2/10, Batch 823/883, Training Loss: 0.8260
Epoch 2/10, Batch 824/883, Training Loss: 0.8677
Epoch 2/10, Batch 825/883, Training Loss: 0.9657
Epoch 2/10, Batch 826/883, Training Loss: 0.7977
Epoch 2/10, Batch 827/883, Training Loss: 0.9511
Epoch 2/10, Batch 828/883, Training Loss: 0.5848
Epoch 2/10, Batch 829/883, Training Loss: 1.1140
Epoch 2/10, Batch 830/883, Training Loss: 1.2374
Epoch 2/10, Batch 831/883, Training Loss: 0.7422
Epoch 2/10, Batch 832/883, Training Loss: 1.1556
Epoch 2/10, Batch 833/883, Training Loss: 0.7726
Epoch 2/10, Batch 834/883, Training Loss: 1.0270
Epoch 2/10, Batch 835/883, Training Loss: 0.7042
Epoch 2/10, Batch 836/883, Training Loss: 0.7378
Epoch 2/10, Batch 837/883, Training Loss: 0.7863
Epoch 2/10, Batch 838/883, Training Loss: 0.8157
Epoch 2/10, Batch 839/883, Training Loss: 1.0649
Epoch 2/10, Batch 840/883, Training Loss: 0.6281
Epoch 2/10, Batch 841/883, Training Loss: 0.7324
Epoch 2/10, Batch 842/883, Training Loss: 0.6215
Epoch 2/10, Batch 843/883, Training Loss: 0.7754
Epoch 2/10, Batch 844/883, Training Loss: 0.7435
Epoch 2/10, Batch 845/883, Training Loss: 0.7744
Epoch 2/10, Batch 846/883, Training Loss: 0.6673
Epoch 2/10, Batch 847/883, Training Loss: 0.7967
Epoch 2/10, Batch 848/883, Training Loss: 0.5955
Epoch 2/10, Batch 849/883, Training Loss: 1.0007
Epoch 2/10, Batch 850/883, Training Loss: 0.6751
Epoch 2/10, Batch 851/883, Training Loss: 0.8178
Epoch 2/10, Batch 852/883, Training Loss: 0.5949
Epoch 2/10, Batch 853/883, Training Loss: 0.8571
Epoch 2/10, Batch 854/883, Training Loss: 0.6947
Epoch 2/10, Batch 855/883, Training Loss: 0.8158
Epoch 2/10, Batch 856/883, Training Loss: 1.0051
Epoch 2/10, Batch 857/883, Training Loss: 0.8608
Epoch 2/10, Batch 858/883, Training Loss: 0.7254
Epoch 2/10, Batch 859/883, Training Loss: 0.8334
Epoch 2/10, Batch 860/883, Training Loss: 0.7612
Epoch 2/10, Batch 861/883, Training Loss: 0.6302
Epoch 2/10, Batch 862/883, Training Loss: 0.9294
Epoch 2/10, Batch 863/883, Training Loss: 0.6966
Epoch 2/10, Batch 864/883, Training Loss: 0.9742
Epoch 2/10, Batch 865/883, Training Loss: 0.9341
Epoch 2/10, Batch 866/883, Training Loss: 0.8378
Epoch 2/10, Batch 867/883, Training Loss: 0.8073
Epoch 2/10, Batch 868/883, Training Loss: 0.8318
Epoch 2/10, Batch 869/883, Training Loss: 0.7473
Epoch 2/10, Batch 870/883, Training Loss: 0.9558
Epoch 2/10, Batch 871/883, Training Loss: 0.7406
Epoch 2/10, Batch 872/883, Training Loss: 0.7950
Epoch 2/10, Batch 873/883, Training Loss: 0.7695
Epoch 2/10, Batch 874/883, Training Loss: 0.8239
Epoch 2/10, Batch 875/883, Training Loss: 1.1560
Epoch 2/10, Batch 876/883, Training Loss: 0.8336
Epoch 2/10, Batch 877/883, Training Loss: 1.0038
Epoch 2/10, Batch 878/883, Training Loss: 0.7301
Epoch 2/10, Batch 879/883, Training Loss: 0.7683
Epoch 2/10, Batch 880/883, Training Loss: 0.7849
Epoch 2/10, Batch 881/883, Training Loss: 0.7202
Epoch 2/10, Batch 882/883, Training Loss: 0.6681
Epoch 2/10, Batch 883/883, Training Loss: 0.9204
Epoch 2/10, Training Loss: 0.8616, Validation Loss: 0.9708, Validation Accuracy: 0.5544
Epoch 3/10, Batch 1/883, Training Loss: 0.8459
Epoch 3/10, Batch 2/883, Training Loss: 0.6997
Epoch 3/10, Batch 3/883, Training Loss: 0.9277
Epoch 3/10, Batch 4/883, Training Loss: 0.6711
Epoch 3/10, Batch 5/883, Training Loss: 1.0212
Epoch 3/10, Batch 6/883, Training Loss: 0.6609
Epoch 3/10, Batch 7/883, Training Loss: 1.0052
Epoch 3/10, Batch 8/883, Training Loss: 0.6781
Epoch 3/10, Batch 9/883, Training Loss: 0.7524
Epoch 3/10, Batch 10/883, Training Loss: 0.5543
Epoch 3/10, Batch 11/883, Training Loss: 0.8292
Epoch 3/10, Batch 12/883, Training Loss: 0.8291
Epoch 3/10, Batch 13/883, Training Loss: 0.7266
Epoch 3/10, Batch 14/883, Training Loss: 0.8436
Epoch 3/10, Batch 15/883, Training Loss: 0.8834
Epoch 3/10, Batch 16/883, Training Loss: 0.7017
Epoch 3/10, Batch 17/883, Training Loss: 0.7260
Epoch 3/10, Batch 18/883, Training Loss: 0.8700
Epoch 3/10, Batch 19/883, Training Loss: 0.8207
Epoch 3/10, Batch 20/883, Training Loss: 0.8307
Epoch 3/10, Batch 21/883, Training Loss: 0.8289
Epoch 3/10, Batch 22/883, Training Loss: 0.9032
Epoch 3/10, Batch 23/883, Training Loss: 0.7997
Epoch 3/10, Batch 24/883, Training Loss: 0.8824
Epoch 3/10, Batch 25/883, Training Loss: 1.0967
Epoch 3/10, Batch 26/883, Training Loss: 0.8693
Epoch 3/10, Batch 27/883, Training Loss: 1.0501
Epoch 3/10, Batch 28/883, Training Loss: 0.6882
Epoch 3/10, Batch 29/883, Training Loss: 1.1168
Epoch 3/10, Batch 30/883, Training Loss: 0.6953
Epoch 3/10, Batch 31/883, Training Loss: 0.9421
Epoch 3/10, Batch 32/883, Training Loss: 0.6350
Epoch 3/10, Batch 33/883, Training Loss: 0.7714
Epoch 3/10, Batch 34/883, Training Loss: 0.7066
Epoch 3/10, Batch 35/883, Training Loss: 0.8087
Epoch 3/10, Batch 36/883, Training Loss: 0.9475
Epoch 3/10, Batch 37/883, Training Loss: 0.8471
Epoch 3/10, Batch 38/883, Training Loss: 0.8350
Epoch 3/10, Batch 39/883, Training Loss: 0.6146
Epoch 3/10, Batch 40/883, Training Loss: 0.8921
Epoch 3/10, Batch 41/883, Training Loss: 0.9403
Epoch 3/10, Batch 42/883, Training Loss: 0.8187
Epoch 3/10, Batch 43/883, Training Loss: 0.9191
Epoch 3/10, Batch 44/883, Training Loss: 1.1414
Epoch 3/10, Batch 45/883, Training Loss: 0.5687
Epoch 3/10, Batch 46/883, Training Loss: 0.7763
Epoch 3/10, Batch 47/883, Training Loss: 0.6281
Epoch 3/10, Batch 48/883, Training Loss: 0.8254
Epoch 3/10, Batch 49/883, Training Loss: 0.9551
Epoch 3/10, Batch 50/883, Training Loss: 0.8893
Epoch 3/10, Batch 51/883, Training Loss: 1.0536
Epoch 3/10, Batch 52/883, Training Loss: 0.6645
Epoch 3/10, Batch 53/883, Training Loss: 1.0697
Epoch 3/10, Batch 54/883, Training Loss: 0.9211
Epoch 3/10, Batch 55/883, Training Loss: 0.7292
Epoch 3/10, Batch 56/883, Training Loss: 1.1010
Epoch 3/10, Batch 57/883, Training Loss: 0.6328
Epoch 3/10, Batch 58/883, Training Loss: 0.6482
Epoch 3/10, Batch 59/883, Training Loss: 0.7841
Epoch 3/10, Batch 60/883, Training Loss: 0.6044
Epoch 3/10, Batch 61/883, Training Loss: 0.8001
Epoch 3/10, Batch 62/883, Training Loss: 0.8386
Epoch 3/10, Batch 63/883, Training Loss: 0.6683
Epoch 3/10, Batch 64/883, Training Loss: 0.6635
Epoch 3/10, Batch 65/883, Training Loss: 0.5597
Epoch 3/10, Batch 66/883, Training Loss: 0.7354
Epoch 3/10, Batch 67/883, Training Loss: 0.6194
Epoch 3/10, Batch 68/883, Training Loss: 0.4682
Epoch 3/10, Batch 69/883, Training Loss: 0.7178
Epoch 3/10, Batch 70/883, Training Loss: 0.7349
Epoch 3/10, Batch 71/883, Training Loss: 0.8879
Epoch 3/10, Batch 72/883, Training Loss: 0.6577
Epoch 3/10, Batch 73/883, Training Loss: 0.7627
Epoch 3/10, Batch 74/883, Training Loss: 1.2326
Epoch 3/10, Batch 75/883, Training Loss: 1.0050
Epoch 3/10, Batch 76/883, Training Loss: 0.9094
Epoch 3/10, Batch 77/883, Training Loss: 1.3170
Epoch 3/10, Batch 78/883, Training Loss: 0.8231
Epoch 3/10, Batch 79/883, Training Loss: 1.2968
Epoch 3/10, Batch 80/883, Training Loss: 0.7782
Epoch 3/10, Batch 81/883, Training Loss: 1.0120
Epoch 3/10, Batch 82/883, Training Loss: 0.6489
Epoch 3/10, Batch 83/883, Training Loss: 0.6866
Epoch 3/10, Batch 84/883, Training Loss: 0.8480
Epoch 3/10, Batch 85/883, Training Loss: 0.8342
Epoch 3/10, Batch 86/883, Training Loss: 0.7381
Epoch 3/10, Batch 87/883, Training Loss: 0.7716
Epoch 3/10, Batch 88/883, Training Loss: 0.7467
Epoch 3/10, Batch 89/883, Training Loss: 0.7678
Epoch 3/10, Batch 90/883, Training Loss: 0.8172
Epoch 3/10, Batch 91/883, Training Loss: 0.7157
Epoch 3/10, Batch 92/883, Training Loss: 0.9961
Epoch 3/10, Batch 93/883, Training Loss: 0.7758
Epoch 3/10, Batch 94/883, Training Loss: 0.8128
Epoch 3/10, Batch 95/883, Training Loss: 0.8599
Epoch 3/10, Batch 96/883, Training Loss: 0.7695
Epoch 3/10, Batch 97/883, Training Loss: 0.8943
Epoch 3/10, Batch 98/883, Training Loss: 0.7484
Epoch 3/10, Batch 99/883, Training Loss: 0.7026
Epoch 3/10, Batch 100/883, Training Loss: 0.6526
Epoch 3/10, Batch 101/883, Training Loss: 0.8726
Epoch 3/10, Batch 102/883, Training Loss: 1.1043
Epoch 3/10, Batch 103/883, Training Loss: 0.7134
Epoch 3/10, Batch 104/883, Training Loss: 0.6531
Epoch 3/10, Batch 105/883, Training Loss: 0.8337
Epoch 3/10, Batch 106/883, Training Loss: 0.8106
Epoch 3/10, Batch 107/883, Training Loss: 0.6043
Epoch 3/10, Batch 108/883, Training Loss: 0.6815
Epoch 3/10, Batch 109/883, Training Loss: 1.0665
Epoch 3/10, Batch 110/883, Training Loss: 0.8967
Epoch 3/10, Batch 111/883, Training Loss: 0.6587
Epoch 3/10, Batch 112/883, Training Loss: 0.6184
Epoch 3/10, Batch 113/883, Training Loss: 0.5967
Epoch 3/10, Batch 114/883, Training Loss: 0.8388
Epoch 3/10, Batch 115/883, Training Loss: 0.8951
Epoch 3/10, Batch 116/883, Training Loss: 0.7842
Epoch 3/10, Batch 117/883, Training Loss: 0.7569
Epoch 3/10, Batch 118/883, Training Loss: 1.0204
Epoch 3/10, Batch 119/883, Training Loss: 0.7458
Epoch 3/10, Batch 120/883, Training Loss: 0.6291
Epoch 3/10, Batch 121/883, Training Loss: 0.8243
Epoch 3/10, Batch 122/883, Training Loss: 0.7071
Epoch 3/10, Batch 123/883, Training Loss: 0.6688
Epoch 3/10, Batch 124/883, Training Loss: 1.0666
Epoch 3/10, Batch 125/883, Training Loss: 0.7092
Epoch 3/10, Batch 126/883, Training Loss: 0.7720
Epoch 3/10, Batch 127/883, Training Loss: 0.5894
Epoch 3/10, Batch 128/883, Training Loss: 0.8272
Epoch 3/10, Batch 129/883, Training Loss: 0.7383
Epoch 3/10, Batch 130/883, Training Loss: 0.6910
Epoch 3/10, Batch 131/883, Training Loss: 0.5981
Epoch 3/10, Batch 132/883, Training Loss: 0.6078
Epoch 3/10, Batch 133/883, Training Loss: 0.9942
Epoch 3/10, Batch 134/883, Training Loss: 0.8476
Epoch 3/10, Batch 135/883, Training Loss: 0.7686
Epoch 3/10, Batch 136/883, Training Loss: 0.6713
Epoch 3/10, Batch 137/883, Training Loss: 0.6772
Epoch 3/10, Batch 138/883, Training Loss: 0.8411
Epoch 3/10, Batch 139/883, Training Loss: 1.0006
Epoch 3/10, Batch 140/883, Training Loss: 0.6033
Epoch 3/10, Batch 141/883, Training Loss: 0.9087
Epoch 3/10, Batch 142/883, Training Loss: 0.8966
Epoch 3/10, Batch 143/883, Training Loss: 0.6789
Epoch 3/10, Batch 144/883, Training Loss: 0.7834
Epoch 3/10, Batch 145/883, Training Loss: 0.7692
Epoch 3/10, Batch 146/883, Training Loss: 0.7207
Epoch 3/10, Batch 147/883, Training Loss: 0.7593
Epoch 3/10, Batch 148/883, Training Loss: 0.7860
Epoch 3/10, Batch 149/883, Training Loss: 0.7297
Epoch 3/10, Batch 150/883, Training Loss: 0.7595
Epoch 3/10, Batch 151/883, Training Loss: 0.8361
Epoch 3/10, Batch 152/883, Training Loss: 0.7435
Epoch 3/10, Batch 153/883, Training Loss: 0.6729
Epoch 3/10, Batch 154/883, Training Loss: 1.0886
Epoch 3/10, Batch 155/883, Training Loss: 0.6734
Epoch 3/10, Batch 156/883, Training Loss: 0.8751
Epoch 3/10, Batch 157/883, Training Loss: 0.9158
Epoch 3/10, Batch 158/883, Training Loss: 1.0982
Epoch 3/10, Batch 159/883, Training Loss: 0.6550
Epoch 3/10, Batch 160/883, Training Loss: 0.8892
Epoch 3/10, Batch 161/883, Training Loss: 0.7147
Epoch 3/10, Batch 162/883, Training Loss: 0.9948
Epoch 3/10, Batch 163/883, Training Loss: 0.8033
Epoch 3/10, Batch 164/883, Training Loss: 1.0241
Epoch 3/10, Batch 165/883, Training Loss: 0.6097
Epoch 3/10, Batch 166/883, Training Loss: 1.2300
Epoch 3/10, Batch 167/883, Training Loss: 0.7183
Epoch 3/10, Batch 168/883, Training Loss: 1.1023
Epoch 3/10, Batch 169/883, Training Loss: 0.7067
Epoch 3/10, Batch 170/883, Training Loss: 0.6272
Epoch 3/10, Batch 171/883, Training Loss: 0.8528
Epoch 3/10, Batch 172/883, Training Loss: 0.7511
Epoch 3/10, Batch 173/883, Training Loss: 0.9567
Epoch 3/10, Batch 174/883, Training Loss: 1.0303
Epoch 3/10, Batch 175/883, Training Loss: 0.9520
Epoch 3/10, Batch 176/883, Training Loss: 0.7630
Epoch 3/10, Batch 177/883, Training Loss: 0.8976
Epoch 3/10, Batch 178/883, Training Loss: 1.0844
Epoch 3/10, Batch 179/883, Training Loss: 0.6356
Epoch 3/10, Batch 180/883, Training Loss: 0.8727
Epoch 3/10, Batch 181/883, Training Loss: 0.4982
Epoch 3/10, Batch 182/883, Training Loss: 0.9234
Epoch 3/10, Batch 183/883, Training Loss: 0.6413
Epoch 3/10, Batch 184/883, Training Loss: 1.0896
Epoch 3/10, Batch 185/883, Training Loss: 0.5392
Epoch 3/10, Batch 186/883, Training Loss: 0.9640
Epoch 3/10, Batch 187/883, Training Loss: 0.9071
Epoch 3/10, Batch 188/883, Training Loss: 0.6828
Epoch 3/10, Batch 189/883, Training Loss: 0.6915
Epoch 3/10, Batch 190/883, Training Loss: 0.8340
Epoch 3/10, Batch 191/883, Training Loss: 0.6997
Epoch 3/10, Batch 192/883, Training Loss: 0.7599
Epoch 3/10, Batch 193/883, Training Loss: 0.7832
Epoch 3/10, Batch 194/883, Training Loss: 0.7601
Epoch 3/10, Batch 195/883, Training Loss: 0.8576
Epoch 3/10, Batch 196/883, Training Loss: 0.6323
Epoch 3/10, Batch 197/883, Training Loss: 0.8745
Epoch 3/10, Batch 198/883, Training Loss: 0.9202
Epoch 3/10, Batch 199/883, Training Loss: 0.9223
Epoch 3/10, Batch 200/883, Training Loss: 0.6629
Epoch 3/10, Batch 201/883, Training Loss: 0.6789
Epoch 3/10, Batch 202/883, Training Loss: 0.6704
Epoch 3/10, Batch 203/883, Training Loss: 0.7057
Epoch 3/10, Batch 204/883, Training Loss: 0.9176
Epoch 3/10, Batch 205/883, Training Loss: 0.9487
Epoch 3/10, Batch 206/883, Training Loss: 0.7690
Epoch 3/10, Batch 207/883, Training Loss: 0.6834
Epoch 3/10, Batch 208/883, Training Loss: 0.7726
Epoch 3/10, Batch 209/883, Training Loss: 0.6471
Epoch 3/10, Batch 210/883, Training Loss: 0.8423
Epoch 3/10, Batch 211/883, Training Loss: 0.7834
Epoch 3/10, Batch 212/883, Training Loss: 0.7479
Epoch 3/10, Batch 213/883, Training Loss: 1.0804
Epoch 3/10, Batch 214/883, Training Loss: 0.7701
Epoch 3/10, Batch 215/883, Training Loss: 0.8007
Epoch 3/10, Batch 216/883, Training Loss: 0.8089
Epoch 3/10, Batch 217/883, Training Loss: 0.9442
Epoch 3/10, Batch 218/883, Training Loss: 0.8017
Epoch 3/10, Batch 219/883, Training Loss: 0.6891
Epoch 3/10, Batch 220/883, Training Loss: 0.9478
Epoch 3/10, Batch 221/883, Training Loss: 0.5867
Epoch 3/10, Batch 222/883, Training Loss: 0.5556
Epoch 3/10, Batch 223/883, Training Loss: 0.7079
Epoch 3/10, Batch 224/883, Training Loss: 0.7251
Epoch 3/10, Batch 225/883, Training Loss: 0.8224
Epoch 3/10, Batch 226/883, Training Loss: 0.6602
Epoch 3/10, Batch 227/883, Training Loss: 0.9344
Epoch 3/10, Batch 228/883, Training Loss: 1.1968
Epoch 3/10, Batch 229/883, Training Loss: 0.9611
Epoch 3/10, Batch 230/883, Training Loss: 0.8205
Epoch 3/10, Batch 231/883, Training Loss: 0.6898
Epoch 3/10, Batch 232/883, Training Loss: 0.9889
Epoch 3/10, Batch 233/883, Training Loss: 0.5325
Epoch 3/10, Batch 234/883, Training Loss: 0.6173
Epoch 3/10, Batch 235/883, Training Loss: 0.6581
Epoch 3/10, Batch 236/883, Training Loss: 0.9281
Epoch 3/10, Batch 237/883, Training Loss: 0.8963
Epoch 3/10, Batch 238/883, Training Loss: 0.7033
Epoch 3/10, Batch 239/883, Training Loss: 0.5619
Epoch 3/10, Batch 240/883, Training Loss: 0.8593
Epoch 3/10, Batch 241/883, Training Loss: 0.8996
Epoch 3/10, Batch 242/883, Training Loss: 0.7932
Epoch 3/10, Batch 243/883, Training Loss: 0.7578
Epoch 3/10, Batch 244/883, Training Loss: 0.7086
Epoch 3/10, Batch 245/883, Training Loss: 0.7290
Epoch 3/10, Batch 246/883, Training Loss: 0.9722
Epoch 3/10, Batch 247/883, Training Loss: 1.0913
Epoch 3/10, Batch 248/883, Training Loss: 0.8555
Epoch 3/10, Batch 249/883, Training Loss: 0.9531
Epoch 3/10, Batch 250/883, Training Loss: 0.9158
Epoch 3/10, Batch 251/883, Training Loss: 0.6652
Epoch 3/10, Batch 252/883, Training Loss: 1.1381
Epoch 3/10, Batch 253/883, Training Loss: 0.6981
Epoch 3/10, Batch 254/883, Training Loss: 1.0016
Epoch 3/10, Batch 255/883, Training Loss: 0.9401
Epoch 3/10, Batch 256/883, Training Loss: 0.6401
Epoch 3/10, Batch 257/883, Training Loss: 0.7681
Epoch 3/10, Batch 258/883, Training Loss: 0.9409
Epoch 3/10, Batch 259/883, Training Loss: 0.6237
Epoch 3/10, Batch 260/883, Training Loss: 0.7702
Epoch 3/10, Batch 261/883, Training Loss: 0.5940
Epoch 3/10, Batch 262/883, Training Loss: 0.7667
Epoch 3/10, Batch 263/883, Training Loss: 0.9487
Epoch 3/10, Batch 264/883, Training Loss: 0.6367
Epoch 3/10, Batch 265/883, Training Loss: 0.9319
Epoch 3/10, Batch 266/883, Training Loss: 0.8193
Epoch 3/10, Batch 267/883, Training Loss: 0.8867
Epoch 3/10, Batch 268/883, Training Loss: 0.7555
Epoch 3/10, Batch 269/883, Training Loss: 0.8245
Epoch 3/10, Batch 270/883, Training Loss: 0.6492
Epoch 3/10, Batch 271/883, Training Loss: 0.8283
Epoch 3/10, Batch 272/883, Training Loss: 0.7218
Epoch 3/10, Batch 273/883, Training Loss: 0.5758
Epoch 3/10, Batch 274/883, Training Loss: 0.7652
Epoch 3/10, Batch 275/883, Training Loss: 0.5713
Epoch 3/10, Batch 276/883, Training Loss: 1.0883
Epoch 3/10, Batch 277/883, Training Loss: 0.9254
Epoch 3/10, Batch 278/883, Training Loss: 0.7889
Epoch 3/10, Batch 279/883, Training Loss: 0.8418
Epoch 3/10, Batch 280/883, Training Loss: 1.0275
Epoch 3/10, Batch 281/883, Training Loss: 1.0334
Epoch 3/10, Batch 282/883, Training Loss: 0.9494
Epoch 3/10, Batch 283/883, Training Loss: 0.5862
Epoch 3/10, Batch 284/883, Training Loss: 0.8164
Epoch 3/10, Batch 285/883, Training Loss: 0.9594
Epoch 3/10, Batch 286/883, Training Loss: 1.0046
Epoch 3/10, Batch 287/883, Training Loss: 0.5855
Epoch 3/10, Batch 288/883, Training Loss: 0.7177
Epoch 3/10, Batch 289/883, Training Loss: 0.8753
Epoch 3/10, Batch 290/883, Training Loss: 1.0534
Epoch 3/10, Batch 291/883, Training Loss: 0.9100
Epoch 3/10, Batch 292/883, Training Loss: 0.8185
Epoch 3/10, Batch 293/883, Training Loss: 0.6595
Epoch 3/10, Batch 294/883, Training Loss: 0.7737
Epoch 3/10, Batch 295/883, Training Loss: 0.5764
Epoch 3/10, Batch 296/883, Training Loss: 1.0895
Epoch 3/10, Batch 297/883, Training Loss: 0.9700
Epoch 3/10, Batch 298/883, Training Loss: 0.8185
Epoch 3/10, Batch 299/883, Training Loss: 0.7581
Epoch 3/10, Batch 300/883, Training Loss: 0.8020
Epoch 3/10, Batch 301/883, Training Loss: 0.6170
Epoch 3/10, Batch 302/883, Training Loss: 0.7286
Epoch 3/10, Batch 303/883, Training Loss: 0.6966
Epoch 3/10, Batch 304/883, Training Loss: 0.7386
Epoch 3/10, Batch 305/883, Training Loss: 1.0271
Epoch 3/10, Batch 306/883, Training Loss: 0.8820
Epoch 3/10, Batch 307/883, Training Loss: 0.6173
Epoch 3/10, Batch 308/883, Training Loss: 0.5892
Epoch 3/10, Batch 309/883, Training Loss: 0.9744
Epoch 3/10, Batch 310/883, Training Loss: 1.1613
Epoch 3/10, Batch 311/883, Training Loss: 0.6272
Epoch 3/10, Batch 312/883, Training Loss: 0.7059
Epoch 3/10, Batch 313/883, Training Loss: 0.7506
Epoch 3/10, Batch 314/883, Training Loss: 0.6712
Epoch 3/10, Batch 315/883, Training Loss: 0.7653
Epoch 3/10, Batch 316/883, Training Loss: 1.1469
Epoch 3/10, Batch 317/883, Training Loss: 0.5956
Epoch 3/10, Batch 318/883, Training Loss: 0.6221
Epoch 3/10, Batch 319/883, Training Loss: 0.7654
Epoch 3/10, Batch 320/883, Training Loss: 0.9657
Epoch 3/10, Batch 321/883, Training Loss: 0.8280
Epoch 3/10, Batch 322/883, Training Loss: 0.6539
Epoch 3/10, Batch 323/883, Training Loss: 0.7898
Epoch 3/10, Batch 324/883, Training Loss: 0.6130
Epoch 3/10, Batch 325/883, Training Loss: 0.6902
Epoch 3/10, Batch 326/883, Training Loss: 1.0690
Epoch 3/10, Batch 327/883, Training Loss: 0.6284
Epoch 3/10, Batch 328/883, Training Loss: 0.7505
Epoch 3/10, Batch 329/883, Training Loss: 0.6345
Epoch 3/10, Batch 330/883, Training Loss: 0.6086
Epoch 3/10, Batch 331/883, Training Loss: 0.9408
Epoch 3/10, Batch 332/883, Training Loss: 1.0321
Epoch 3/10, Batch 333/883, Training Loss: 0.5783
Epoch 3/10, Batch 334/883, Training Loss: 0.7733
Epoch 3/10, Batch 335/883, Training Loss: 0.6936
Epoch 3/10, Batch 336/883, Training Loss: 0.8646
Epoch 3/10, Batch 337/883, Training Loss: 0.6586
Epoch 3/10, Batch 338/883, Training Loss: 1.1985
Epoch 3/10, Batch 339/883, Training Loss: 0.6762
Epoch 3/10, Batch 340/883, Training Loss: 0.9751
Epoch 3/10, Batch 341/883, Training Loss: 0.9739
Epoch 3/10, Batch 342/883, Training Loss: 0.7433
Epoch 3/10, Batch 343/883, Training Loss: 0.7369
Epoch 3/10, Batch 344/883, Training Loss: 0.6465
Epoch 3/10, Batch 345/883, Training Loss: 0.7517
Epoch 3/10, Batch 346/883, Training Loss: 0.8046
Epoch 3/10, Batch 347/883, Training Loss: 0.9302
Epoch 3/10, Batch 348/883, Training Loss: 0.6506
Epoch 3/10, Batch 349/883, Training Loss: 0.9797
Epoch 3/10, Batch 350/883, Training Loss: 0.6439
Epoch 3/10, Batch 351/883, Training Loss: 0.8416
Epoch 3/10, Batch 352/883, Training Loss: 0.7995
Epoch 3/10, Batch 353/883, Training Loss: 0.7278
Epoch 3/10, Batch 354/883, Training Loss: 0.7547
Epoch 3/10, Batch 355/883, Training Loss: 0.7415
Epoch 3/10, Batch 356/883, Training Loss: 0.7020
Epoch 3/10, Batch 357/883, Training Loss: 0.6951
Epoch 3/10, Batch 358/883, Training Loss: 0.8874
Epoch 3/10, Batch 359/883, Training Loss: 0.9305
Epoch 3/10, Batch 360/883, Training Loss: 0.9766
Epoch 3/10, Batch 361/883, Training Loss: 0.9262
Epoch 3/10, Batch 362/883, Training Loss: 0.7577
Epoch 3/10, Batch 363/883, Training Loss: 0.9226
Epoch 3/10, Batch 364/883, Training Loss: 0.8100
Epoch 3/10, Batch 365/883, Training Loss: 0.9324
Epoch 3/10, Batch 366/883, Training Loss: 0.8920
Epoch 3/10, Batch 367/883, Training Loss: 0.7982
Epoch 3/10, Batch 368/883, Training Loss: 0.7292
Epoch 3/10, Batch 369/883, Training Loss: 0.9207
Epoch 3/10, Batch 370/883, Training Loss: 0.7181
Epoch 3/10, Batch 371/883, Training Loss: 0.8505
Epoch 3/10, Batch 372/883, Training Loss: 0.7877
Epoch 3/10, Batch 373/883, Training Loss: 0.6071
Epoch 3/10, Batch 374/883, Training Loss: 0.5477
Epoch 3/10, Batch 375/883, Training Loss: 0.9164
Epoch 3/10, Batch 376/883, Training Loss: 0.6536
Epoch 3/10, Batch 377/883, Training Loss: 0.7117
Epoch 3/10, Batch 378/883, Training Loss: 0.6748
Epoch 3/10, Batch 379/883, Training Loss: 0.5602
Epoch 3/10, Batch 380/883, Training Loss: 0.7907
Epoch 3/10, Batch 381/883, Training Loss: 0.7581
Epoch 3/10, Batch 382/883, Training Loss: 1.0443
Epoch 3/10, Batch 383/883, Training Loss: 1.1401
Epoch 3/10, Batch 384/883, Training Loss: 0.3952
Epoch 3/10, Batch 385/883, Training Loss: 0.6875
Epoch 3/10, Batch 386/883, Training Loss: 0.9415
Epoch 3/10, Batch 387/883, Training Loss: 0.5209
Epoch 3/10, Batch 388/883, Training Loss: 0.6788
Epoch 3/10, Batch 389/883, Training Loss: 0.9132
Epoch 3/10, Batch 390/883, Training Loss: 0.8089
Epoch 3/10, Batch 391/883, Training Loss: 1.0852
Epoch 3/10, Batch 392/883, Training Loss: 0.9168
Epoch 3/10, Batch 393/883, Training Loss: 0.8817
Epoch 3/10, Batch 394/883, Training Loss: 0.8496
Epoch 3/10, Batch 395/883, Training Loss: 0.6949
Epoch 3/10, Batch 396/883, Training Loss: 0.8969
Epoch 3/10, Batch 397/883, Training Loss: 0.9206
Epoch 3/10, Batch 398/883, Training Loss: 0.8208
Epoch 3/10, Batch 399/883, Training Loss: 0.9688
Epoch 3/10, Batch 400/883, Training Loss: 0.8941
Epoch 3/10, Batch 401/883, Training Loss: 0.5893
Epoch 3/10, Batch 402/883, Training Loss: 1.0040
Epoch 3/10, Batch 403/883, Training Loss: 0.8807
Epoch 3/10, Batch 404/883, Training Loss: 0.6897
Epoch 3/10, Batch 405/883, Training Loss: 0.6811
Epoch 3/10, Batch 406/883, Training Loss: 0.7746
Epoch 3/10, Batch 407/883, Training Loss: 0.8473
Epoch 3/10, Batch 408/883, Training Loss: 0.6632
Epoch 3/10, Batch 409/883, Training Loss: 0.8059
Epoch 3/10, Batch 410/883, Training Loss: 0.6597
Epoch 3/10, Batch 411/883, Training Loss: 0.9622
Epoch 3/10, Batch 412/883, Training Loss: 0.9270
Epoch 3/10, Batch 413/883, Training Loss: 0.9053
Epoch 3/10, Batch 414/883, Training Loss: 0.7763
Epoch 3/10, Batch 415/883, Training Loss: 0.8774
Epoch 3/10, Batch 416/883, Training Loss: 0.8738
Epoch 3/10, Batch 417/883, Training Loss: 0.5501
Epoch 3/10, Batch 418/883, Training Loss: 0.6044
Epoch 3/10, Batch 419/883, Training Loss: 0.8499
Epoch 3/10, Batch 420/883, Training Loss: 1.4836
Epoch 3/10, Batch 421/883, Training Loss: 0.6743
Epoch 3/10, Batch 422/883, Training Loss: 0.8384
Epoch 3/10, Batch 423/883, Training Loss: 0.6165
Epoch 3/10, Batch 424/883, Training Loss: 0.6782
Epoch 3/10, Batch 425/883, Training Loss: 0.8299
Epoch 3/10, Batch 426/883, Training Loss: 0.7531
Epoch 3/10, Batch 427/883, Training Loss: 0.7400
Epoch 3/10, Batch 428/883, Training Loss: 0.8201
Epoch 3/10, Batch 429/883, Training Loss: 0.6676
Epoch 3/10, Batch 430/883, Training Loss: 1.0488
Epoch 3/10, Batch 431/883, Training Loss: 0.7305
Epoch 3/10, Batch 432/883, Training Loss: 0.7924
Epoch 3/10, Batch 433/883, Training Loss: 1.0972
Epoch 3/10, Batch 434/883, Training Loss: 0.6584
Epoch 3/10, Batch 435/883, Training Loss: 0.7779
Epoch 3/10, Batch 436/883, Training Loss: 0.8727
Epoch 3/10, Batch 437/883, Training Loss: 0.8085
Epoch 3/10, Batch 438/883, Training Loss: 0.8123
Epoch 3/10, Batch 439/883, Training Loss: 0.9815
Epoch 3/10, Batch 440/883, Training Loss: 0.7228
Epoch 3/10, Batch 441/883, Training Loss: 1.0177
Epoch 3/10, Batch 442/883, Training Loss: 0.5769
Epoch 3/10, Batch 443/883, Training Loss: 0.8420
Epoch 3/10, Batch 444/883, Training Loss: 0.7633
Epoch 3/10, Batch 445/883, Training Loss: 0.8968
Epoch 3/10, Batch 446/883, Training Loss: 0.7700
Epoch 3/10, Batch 447/883, Training Loss: 0.8827
Epoch 3/10, Batch 448/883, Training Loss: 0.7744
Epoch 3/10, Batch 449/883, Training Loss: 1.0458
Epoch 3/10, Batch 450/883, Training Loss: 0.9531
Epoch 3/10, Batch 451/883, Training Loss: 0.9297
Epoch 3/10, Batch 452/883, Training Loss: 1.0326
Epoch 3/10, Batch 453/883, Training Loss: 0.8781
Epoch 3/10, Batch 454/883, Training Loss: 0.8055
Epoch 3/10, Batch 455/883, Training Loss: 0.8132
Epoch 3/10, Batch 456/883, Training Loss: 0.7825
Epoch 3/10, Batch 457/883, Training Loss: 0.8203
Epoch 3/10, Batch 458/883, Training Loss: 0.8246
Epoch 3/10, Batch 459/883, Training Loss: 0.6236
Epoch 3/10, Batch 460/883, Training Loss: 0.8863
Epoch 3/10, Batch 461/883, Training Loss: 0.7553
Epoch 3/10, Batch 462/883, Training Loss: 0.8381
Epoch 3/10, Batch 463/883, Training Loss: 0.6115
Epoch 3/10, Batch 464/883, Training Loss: 0.6825
Epoch 3/10, Batch 465/883, Training Loss: 1.1937
Epoch 3/10, Batch 466/883, Training Loss: 0.8716
Epoch 3/10, Batch 467/883, Training Loss: 0.8978
Epoch 3/10, Batch 468/883, Training Loss: 0.7175
Epoch 3/10, Batch 469/883, Training Loss: 0.7566
Epoch 3/10, Batch 470/883, Training Loss: 0.8213
Epoch 3/10, Batch 471/883, Training Loss: 0.6144
Epoch 3/10, Batch 472/883, Training Loss: 0.9952
Epoch 3/10, Batch 473/883, Training Loss: 0.7523
Epoch 3/10, Batch 474/883, Training Loss: 0.5385
Epoch 3/10, Batch 475/883, Training Loss: 0.6542
Epoch 3/10, Batch 476/883, Training Loss: 0.8143
Epoch 3/10, Batch 477/883, Training Loss: 0.6855
Epoch 3/10, Batch 478/883, Training Loss: 0.6129
Epoch 3/10, Batch 479/883, Training Loss: 0.6527
Epoch 3/10, Batch 480/883, Training Loss: 0.5200
Epoch 3/10, Batch 481/883, Training Loss: 0.8720
Epoch 3/10, Batch 482/883, Training Loss: 1.1726
Epoch 3/10, Batch 483/883, Training Loss: 0.6575
Epoch 3/10, Batch 484/883, Training Loss: 1.1166
Epoch 3/10, Batch 485/883, Training Loss: 1.0645
Epoch 3/10, Batch 486/883, Training Loss: 0.9094
Epoch 3/10, Batch 487/883, Training Loss: 0.7136
Epoch 3/10, Batch 488/883, Training Loss: 0.8674
Epoch 3/10, Batch 489/883, Training Loss: 0.9372
Epoch 3/10, Batch 490/883, Training Loss: 0.8587
Epoch 3/10, Batch 491/883, Training Loss: 0.8535
Epoch 3/10, Batch 492/883, Training Loss: 0.8922
Epoch 3/10, Batch 493/883, Training Loss: 0.9407
Epoch 3/10, Batch 494/883, Training Loss: 0.6135
Epoch 3/10, Batch 495/883, Training Loss: 0.6192
Epoch 3/10, Batch 496/883, Training Loss: 0.6727
Epoch 3/10, Batch 497/883, Training Loss: 0.7934
Epoch 3/10, Batch 498/883, Training Loss: 0.6666
Epoch 3/10, Batch 499/883, Training Loss: 0.8144
Epoch 3/10, Batch 500/883, Training Loss: 0.7552
Epoch 3/10, Batch 501/883, Training Loss: 0.7582
Epoch 3/10, Batch 502/883, Training Loss: 0.9727
Epoch 3/10, Batch 503/883, Training Loss: 0.9115
Epoch 3/10, Batch 504/883, Training Loss: 0.7621
Epoch 3/10, Batch 505/883, Training Loss: 0.6610
Epoch 3/10, Batch 506/883, Training Loss: 0.7423
Epoch 3/10, Batch 507/883, Training Loss: 0.8559
Epoch 3/10, Batch 508/883, Training Loss: 0.6819
Epoch 3/10, Batch 509/883, Training Loss: 0.9180
Epoch 3/10, Batch 510/883, Training Loss: 0.6830
Epoch 3/10, Batch 511/883, Training Loss: 0.9407
Epoch 3/10, Batch 512/883, Training Loss: 0.9961
Epoch 3/10, Batch 513/883, Training Loss: 0.6555
Epoch 3/10, Batch 514/883, Training Loss: 0.7654
Epoch 3/10, Batch 515/883, Training Loss: 0.9566
Epoch 3/10, Batch 516/883, Training Loss: 0.7073
Epoch 3/10, Batch 517/883, Training Loss: 1.0961
Epoch 3/10, Batch 518/883, Training Loss: 0.8826
Epoch 3/10, Batch 519/883, Training Loss: 1.1151
Epoch 3/10, Batch 520/883, Training Loss: 0.7908
Epoch 3/10, Batch 521/883, Training Loss: 0.6283
Epoch 3/10, Batch 522/883, Training Loss: 0.6983
Epoch 3/10, Batch 523/883, Training Loss: 0.5447
Epoch 3/10, Batch 524/883, Training Loss: 0.9590
Epoch 3/10, Batch 525/883, Training Loss: 0.7463
Epoch 3/10, Batch 526/883, Training Loss: 0.6347
Epoch 3/10, Batch 527/883, Training Loss: 0.7750
Epoch 3/10, Batch 528/883, Training Loss: 0.8056
Epoch 3/10, Batch 529/883, Training Loss: 0.7325
Epoch 3/10, Batch 530/883, Training Loss: 0.8042
Epoch 3/10, Batch 531/883, Training Loss: 0.6700
Epoch 3/10, Batch 532/883, Training Loss: 1.1043
Epoch 3/10, Batch 533/883, Training Loss: 0.6183
Epoch 3/10, Batch 534/883, Training Loss: 0.9156
Epoch 3/10, Batch 535/883, Training Loss: 0.6997
Epoch 3/10, Batch 536/883, Training Loss: 0.8130
Epoch 3/10, Batch 537/883, Training Loss: 0.7233
Epoch 3/10, Batch 538/883, Training Loss: 0.8216
Epoch 3/10, Batch 539/883, Training Loss: 0.7406
Epoch 3/10, Batch 540/883, Training Loss: 0.7494
Epoch 3/10, Batch 541/883, Training Loss: 0.6807
Epoch 3/10, Batch 542/883, Training Loss: 0.8059
Epoch 3/10, Batch 543/883, Training Loss: 1.0390
Epoch 3/10, Batch 544/883, Training Loss: 0.9026
Epoch 3/10, Batch 545/883, Training Loss: 0.6122
Epoch 3/10, Batch 546/883, Training Loss: 0.8400
Epoch 3/10, Batch 547/883, Training Loss: 0.9867
Epoch 3/10, Batch 548/883, Training Loss: 0.7209
Epoch 3/10, Batch 549/883, Training Loss: 0.8641
Epoch 3/10, Batch 550/883, Training Loss: 0.9340
Epoch 3/10, Batch 551/883, Training Loss: 0.7348
Epoch 3/10, Batch 552/883, Training Loss: 1.2581
Epoch 3/10, Batch 553/883, Training Loss: 0.7983
Epoch 3/10, Batch 554/883, Training Loss: 0.6342
Epoch 3/10, Batch 555/883, Training Loss: 1.0051
Epoch 3/10, Batch 556/883, Training Loss: 0.7088
Epoch 3/10, Batch 557/883, Training Loss: 0.7065
Epoch 3/10, Batch 558/883, Training Loss: 0.9016
Epoch 3/10, Batch 559/883, Training Loss: 0.7768
Epoch 3/10, Batch 560/883, Training Loss: 0.8897
Epoch 3/10, Batch 561/883, Training Loss: 0.8926
Epoch 3/10, Batch 562/883, Training Loss: 0.8685
Epoch 3/10, Batch 563/883, Training Loss: 0.9530
Epoch 3/10, Batch 564/883, Training Loss: 0.5462
Epoch 3/10, Batch 565/883, Training Loss: 0.7611
Epoch 3/10, Batch 566/883, Training Loss: 0.7180
Epoch 3/10, Batch 567/883, Training Loss: 0.6293
Epoch 3/10, Batch 568/883, Training Loss: 0.9987
Epoch 3/10, Batch 569/883, Training Loss: 0.9531
Epoch 3/10, Batch 570/883, Training Loss: 0.6929
Epoch 3/10, Batch 571/883, Training Loss: 0.9039
Epoch 3/10, Batch 572/883, Training Loss: 0.8277
Epoch 3/10, Batch 573/883, Training Loss: 0.7497
Epoch 3/10, Batch 574/883, Training Loss: 0.7962
Epoch 3/10, Batch 575/883, Training Loss: 0.6162
Epoch 3/10, Batch 576/883, Training Loss: 0.8839
Epoch 3/10, Batch 577/883, Training Loss: 0.6790
Epoch 3/10, Batch 578/883, Training Loss: 0.8757
Epoch 3/10, Batch 579/883, Training Loss: 0.9354
Epoch 3/10, Batch 580/883, Training Loss: 0.5857
Epoch 3/10, Batch 581/883, Training Loss: 1.1574
Epoch 3/10, Batch 582/883, Training Loss: 1.1448
Epoch 3/10, Batch 583/883, Training Loss: 0.6304
Epoch 3/10, Batch 584/883, Training Loss: 0.6357
Epoch 3/10, Batch 585/883, Training Loss: 0.7580
Epoch 3/10, Batch 586/883, Training Loss: 0.6207
Epoch 3/10, Batch 587/883, Training Loss: 0.8082
Epoch 3/10, Batch 588/883, Training Loss: 0.5270
Epoch 3/10, Batch 589/883, Training Loss: 0.7562
Epoch 3/10, Batch 590/883, Training Loss: 0.6440
Epoch 3/10, Batch 591/883, Training Loss: 0.7183
Epoch 3/10, Batch 592/883, Training Loss: 0.8307
Epoch 3/10, Batch 593/883, Training Loss: 0.7278
Epoch 3/10, Batch 594/883, Training Loss: 0.7392
Epoch 3/10, Batch 595/883, Training Loss: 0.8241
Epoch 3/10, Batch 596/883, Training Loss: 0.9098
Epoch 3/10, Batch 597/883, Training Loss: 1.0327
Epoch 3/10, Batch 598/883, Training Loss: 0.8268
Epoch 3/10, Batch 599/883, Training Loss: 0.9386
Epoch 3/10, Batch 600/883, Training Loss: 0.9360
Epoch 3/10, Batch 601/883, Training Loss: 0.7570
Epoch 3/10, Batch 602/883, Training Loss: 0.5745
Epoch 3/10, Batch 603/883, Training Loss: 0.7762
Epoch 3/10, Batch 604/883, Training Loss: 0.7411
Epoch 3/10, Batch 605/883, Training Loss: 0.9466
Epoch 3/10, Batch 606/883, Training Loss: 0.6589
Epoch 3/10, Batch 607/883, Training Loss: 0.6313
Epoch 3/10, Batch 608/883, Training Loss: 0.7648
Epoch 3/10, Batch 609/883, Training Loss: 0.8017
Epoch 3/10, Batch 610/883, Training Loss: 0.6302
Epoch 3/10, Batch 611/883, Training Loss: 0.6529
Epoch 3/10, Batch 612/883, Training Loss: 0.6462
Epoch 3/10, Batch 613/883, Training Loss: 1.0131
Epoch 3/10, Batch 614/883, Training Loss: 0.5486
Epoch 3/10, Batch 615/883, Training Loss: 1.2058
Epoch 3/10, Batch 616/883, Training Loss: 0.7736
Epoch 3/10, Batch 617/883, Training Loss: 0.6853
Epoch 3/10, Batch 618/883, Training Loss: 0.5568
Epoch 3/10, Batch 619/883, Training Loss: 0.9474
Epoch 3/10, Batch 620/883, Training Loss: 0.9552
Epoch 3/10, Batch 621/883, Training Loss: 0.9058
Epoch 3/10, Batch 622/883, Training Loss: 0.6674
Epoch 3/10, Batch 623/883, Training Loss: 0.8355
Epoch 3/10, Batch 624/883, Training Loss: 0.7147
Epoch 3/10, Batch 625/883, Training Loss: 0.6796
Epoch 3/10, Batch 626/883, Training Loss: 0.6373
Epoch 3/10, Batch 627/883, Training Loss: 0.9256
Epoch 3/10, Batch 628/883, Training Loss: 0.7495
Epoch 3/10, Batch 629/883, Training Loss: 0.7989
Epoch 3/10, Batch 630/883, Training Loss: 0.7657
Epoch 3/10, Batch 631/883, Training Loss: 0.5830
Epoch 3/10, Batch 632/883, Training Loss: 0.9927
Epoch 3/10, Batch 633/883, Training Loss: 1.1534
Epoch 3/10, Batch 634/883, Training Loss: 0.6888
Epoch 3/10, Batch 635/883, Training Loss: 1.0598
Epoch 3/10, Batch 636/883, Training Loss: 0.6888
Epoch 3/10, Batch 637/883, Training Loss: 0.9598
Epoch 3/10, Batch 638/883, Training Loss: 0.7819
Epoch 3/10, Batch 639/883, Training Loss: 0.7594
Epoch 3/10, Batch 640/883, Training Loss: 0.5626
Epoch 3/10, Batch 641/883, Training Loss: 0.8209
Epoch 3/10, Batch 642/883, Training Loss: 0.8317
Epoch 3/10, Batch 643/883, Training Loss: 0.7479
Epoch 3/10, Batch 644/883, Training Loss: 0.9551
Epoch 3/10, Batch 645/883, Training Loss: 0.7283
Epoch 3/10, Batch 646/883, Training Loss: 0.8408
Epoch 3/10, Batch 647/883, Training Loss: 0.6774
Epoch 3/10, Batch 648/883, Training Loss: 0.6545
Epoch 3/10, Batch 649/883, Training Loss: 0.8732
Epoch 3/10, Batch 650/883, Training Loss: 0.8142
Epoch 3/10, Batch 651/883, Training Loss: 0.7307
Epoch 3/10, Batch 652/883, Training Loss: 0.7701
Epoch 3/10, Batch 653/883, Training Loss: 0.6769
Epoch 3/10, Batch 654/883, Training Loss: 0.8551
Epoch 3/10, Batch 655/883, Training Loss: 0.6340
Epoch 3/10, Batch 656/883, Training Loss: 0.8533
Epoch 3/10, Batch 657/883, Training Loss: 0.7572
Epoch 3/10, Batch 658/883, Training Loss: 1.3595
Epoch 3/10, Batch 659/883, Training Loss: 0.7160
Epoch 3/10, Batch 660/883, Training Loss: 0.7431
Epoch 3/10, Batch 661/883, Training Loss: 0.8929
Epoch 3/10, Batch 662/883, Training Loss: 0.7478
Epoch 3/10, Batch 663/883, Training Loss: 0.9262
Epoch 3/10, Batch 664/883, Training Loss: 0.9867
Epoch 3/10, Batch 665/883, Training Loss: 0.7967
Epoch 3/10, Batch 666/883, Training Loss: 0.8549
Epoch 3/10, Batch 667/883, Training Loss: 0.9905
Epoch 3/10, Batch 668/883, Training Loss: 0.7338
Epoch 3/10, Batch 669/883, Training Loss: 1.0074
Epoch 3/10, Batch 670/883, Training Loss: 0.7661
Epoch 3/10, Batch 671/883, Training Loss: 0.8936
Epoch 3/10, Batch 672/883, Training Loss: 0.8750
Epoch 3/10, Batch 673/883, Training Loss: 0.9419
Epoch 3/10, Batch 674/883, Training Loss: 0.6778
Epoch 3/10, Batch 675/883, Training Loss: 1.0397
Epoch 3/10, Batch 676/883, Training Loss: 0.8991
Epoch 3/10, Batch 677/883, Training Loss: 0.8071
Epoch 3/10, Batch 678/883, Training Loss: 0.9850
Epoch 3/10, Batch 679/883, Training Loss: 0.8133
Epoch 3/10, Batch 680/883, Training Loss: 0.6423
Epoch 3/10, Batch 681/883, Training Loss: 0.8150
Epoch 3/10, Batch 682/883, Training Loss: 0.8569
Epoch 3/10, Batch 683/883, Training Loss: 0.8816
Epoch 3/10, Batch 684/883, Training Loss: 1.1742
Epoch 3/10, Batch 685/883, Training Loss: 1.1191
Epoch 3/10, Batch 686/883, Training Loss: 0.8730
Epoch 3/10, Batch 687/883, Training Loss: 0.7275
Epoch 3/10, Batch 688/883, Training Loss: 0.9519
Epoch 3/10, Batch 689/883, Training Loss: 0.8505
Epoch 3/10, Batch 690/883, Training Loss: 0.7987
Epoch 3/10, Batch 691/883, Training Loss: 0.8585
Epoch 3/10, Batch 692/883, Training Loss: 0.8177
Epoch 3/10, Batch 693/883, Training Loss: 0.6780
Epoch 3/10, Batch 694/883, Training Loss: 0.9533
Epoch 3/10, Batch 695/883, Training Loss: 0.9061
Epoch 3/10, Batch 696/883, Training Loss: 0.7828
Epoch 3/10, Batch 697/883, Training Loss: 1.0655
Epoch 3/10, Batch 698/883, Training Loss: 0.5596
Epoch 3/10, Batch 699/883, Training Loss: 0.9000
Epoch 3/10, Batch 700/883, Training Loss: 0.6546
Epoch 3/10, Batch 701/883, Training Loss: 0.7590
Epoch 3/10, Batch 702/883, Training Loss: 0.7076
Epoch 3/10, Batch 703/883, Training Loss: 1.1094
Epoch 3/10, Batch 704/883, Training Loss: 0.7381
Epoch 3/10, Batch 705/883, Training Loss: 0.7521
Epoch 3/10, Batch 706/883, Training Loss: 0.6886
Epoch 3/10, Batch 707/883, Training Loss: 0.6872
Epoch 3/10, Batch 708/883, Training Loss: 0.6362
Epoch 3/10, Batch 709/883, Training Loss: 0.7441
Epoch 3/10, Batch 710/883, Training Loss: 0.9357
Epoch 3/10, Batch 711/883, Training Loss: 0.7264
Epoch 3/10, Batch 712/883, Training Loss: 0.9322
Epoch 3/10, Batch 713/883, Training Loss: 0.8736
Epoch 3/10, Batch 714/883, Training Loss: 0.9859
Epoch 3/10, Batch 715/883, Training Loss: 0.6435
Epoch 3/10, Batch 716/883, Training Loss: 0.6635
Epoch 3/10, Batch 717/883, Training Loss: 0.8109
Epoch 3/10, Batch 718/883, Training Loss: 0.6634
Epoch 3/10, Batch 719/883, Training Loss: 0.8330
Epoch 3/10, Batch 720/883, Training Loss: 0.7969
Epoch 3/10, Batch 721/883, Training Loss: 0.6131
Epoch 3/10, Batch 722/883, Training Loss: 0.9225
Epoch 3/10, Batch 723/883, Training Loss: 0.6859
Epoch 3/10, Batch 724/883, Training Loss: 0.7428
Epoch 3/10, Batch 725/883, Training Loss: 0.9004
Epoch 3/10, Batch 726/883, Training Loss: 0.7217
Epoch 3/10, Batch 727/883, Training Loss: 0.8895
Epoch 3/10, Batch 728/883, Training Loss: 0.7863
Epoch 3/10, Batch 729/883, Training Loss: 0.8088
Epoch 3/10, Batch 730/883, Training Loss: 0.5911
Epoch 3/10, Batch 731/883, Training Loss: 0.5840
Epoch 3/10, Batch 732/883, Training Loss: 0.7881
Epoch 3/10, Batch 733/883, Training Loss: 0.8494
Epoch 3/10, Batch 734/883, Training Loss: 0.6349
Epoch 3/10, Batch 735/883, Training Loss: 0.7067
Epoch 3/10, Batch 736/883, Training Loss: 0.6728
Epoch 3/10, Batch 737/883, Training Loss: 0.6404
Epoch 3/10, Batch 738/883, Training Loss: 0.5405
Epoch 3/10, Batch 739/883, Training Loss: 0.6111
Epoch 3/10, Batch 740/883, Training Loss: 0.7380
Epoch 3/10, Batch 741/883, Training Loss: 0.6670
Epoch 3/10, Batch 742/883, Training Loss: 0.8065
Epoch 3/10, Batch 743/883, Training Loss: 0.3511
Epoch 3/10, Batch 744/883, Training Loss: 1.0049
Epoch 3/10, Batch 745/883, Training Loss: 1.1118
Epoch 3/10, Batch 746/883, Training Loss: 0.6776
Epoch 3/10, Batch 747/883, Training Loss: 0.9034
Epoch 3/10, Batch 748/883, Training Loss: 0.7791
Epoch 3/10, Batch 749/883, Training Loss: 0.5086
Epoch 3/10, Batch 750/883, Training Loss: 0.5384
Epoch 3/10, Batch 751/883, Training Loss: 0.7223
Epoch 3/10, Batch 752/883, Training Loss: 0.7626
Epoch 3/10, Batch 753/883, Training Loss: 0.7113
Epoch 3/10, Batch 754/883, Training Loss: 1.2117
Epoch 3/10, Batch 755/883, Training Loss: 0.7108
Epoch 3/10, Batch 756/883, Training Loss: 0.6795
Epoch 3/10, Batch 757/883, Training Loss: 0.7302
Epoch 3/10, Batch 758/883, Training Loss: 0.7523
Epoch 3/10, Batch 759/883, Training Loss: 0.5344
Epoch 3/10, Batch 760/883, Training Loss: 1.0403
Epoch 3/10, Batch 761/883, Training Loss: 0.6288
Epoch 3/10, Batch 762/883, Training Loss: 0.7609
Epoch 3/10, Batch 763/883, Training Loss: 0.7845
Epoch 3/10, Batch 764/883, Training Loss: 0.7951
Epoch 3/10, Batch 765/883, Training Loss: 0.9131
Epoch 3/10, Batch 766/883, Training Loss: 0.7584
Epoch 3/10, Batch 767/883, Training Loss: 0.7413
Epoch 3/10, Batch 768/883, Training Loss: 0.8275
Epoch 3/10, Batch 769/883, Training Loss: 0.8979
Epoch 3/10, Batch 770/883, Training Loss: 0.6676
Epoch 3/10, Batch 771/883, Training Loss: 0.8043
Epoch 3/10, Batch 772/883, Training Loss: 1.0986
Epoch 3/10, Batch 773/883, Training Loss: 1.2860
Epoch 3/10, Batch 774/883, Training Loss: 0.7255
Epoch 3/10, Batch 775/883, Training Loss: 0.7175
Epoch 3/10, Batch 776/883, Training Loss: 0.6743
Epoch 3/10, Batch 777/883, Training Loss: 0.6825
Epoch 3/10, Batch 778/883, Training Loss: 1.1644
Epoch 3/10, Batch 779/883, Training Loss: 0.6697
Epoch 3/10, Batch 780/883, Training Loss: 0.7631
Epoch 3/10, Batch 781/883, Training Loss: 0.7182
Epoch 3/10, Batch 782/883, Training Loss: 0.7002
Epoch 3/10, Batch 783/883, Training Loss: 0.6692
Epoch 3/10, Batch 784/883, Training Loss: 0.7020
Epoch 3/10, Batch 785/883, Training Loss: 0.7603
Epoch 3/10, Batch 786/883, Training Loss: 0.7310
Epoch 3/10, Batch 787/883, Training Loss: 0.8126
Epoch 3/10, Batch 788/883, Training Loss: 0.7797
Epoch 3/10, Batch 789/883, Training Loss: 0.6606
Epoch 3/10, Batch 790/883, Training Loss: 0.8256
Epoch 3/10, Batch 791/883, Training Loss: 0.6606
Epoch 3/10, Batch 792/883, Training Loss: 0.7972
Epoch 3/10, Batch 793/883, Training Loss: 1.0118
Epoch 3/10, Batch 794/883, Training Loss: 0.9108
Epoch 3/10, Batch 795/883, Training Loss: 0.9652
Epoch 3/10, Batch 796/883, Training Loss: 0.8073
Epoch 3/10, Batch 797/883, Training Loss: 0.6759
Epoch 3/10, Batch 798/883, Training Loss: 0.5424
Epoch 3/10, Batch 799/883, Training Loss: 0.5922
Epoch 3/10, Batch 800/883, Training Loss: 0.7886
Epoch 3/10, Batch 801/883, Training Loss: 0.6391
Epoch 3/10, Batch 802/883, Training Loss: 0.7227
Epoch 3/10, Batch 803/883, Training Loss: 0.9150
Epoch 3/10, Batch 804/883, Training Loss: 0.7687
Epoch 3/10, Batch 805/883, Training Loss: 0.9465
Epoch 3/10, Batch 806/883, Training Loss: 0.8650
Epoch 3/10, Batch 807/883, Training Loss: 0.8044
Epoch 3/10, Batch 808/883, Training Loss: 0.7724
Epoch 3/10, Batch 809/883, Training Loss: 0.8435
Epoch 3/10, Batch 810/883, Training Loss: 0.6279
Epoch 3/10, Batch 811/883, Training Loss: 0.7976
Epoch 3/10, Batch 812/883, Training Loss: 0.7020
Epoch 3/10, Batch 813/883, Training Loss: 0.6991
Epoch 3/10, Batch 814/883, Training Loss: 1.1333
Epoch 3/10, Batch 815/883, Training Loss: 0.8248
Epoch 3/10, Batch 816/883, Training Loss: 0.7259
Epoch 3/10, Batch 817/883, Training Loss: 0.7001
Epoch 3/10, Batch 818/883, Training Loss: 0.8274
Epoch 3/10, Batch 819/883, Training Loss: 0.6949
Epoch 3/10, Batch 820/883, Training Loss: 0.8468
Epoch 3/10, Batch 821/883, Training Loss: 0.9104
Epoch 3/10, Batch 822/883, Training Loss: 0.9014
Epoch 3/10, Batch 823/883, Training Loss: 1.0960
Epoch 3/10, Batch 824/883, Training Loss: 0.7075
Epoch 3/10, Batch 825/883, Training Loss: 0.8606
Epoch 3/10, Batch 826/883, Training Loss: 0.6793
Epoch 3/10, Batch 827/883, Training Loss: 0.8097
Epoch 3/10, Batch 828/883, Training Loss: 0.7624
Epoch 3/10, Batch 829/883, Training Loss: 0.7887
Epoch 3/10, Batch 830/883, Training Loss: 0.9608
Epoch 3/10, Batch 831/883, Training Loss: 1.0428
Epoch 3/10, Batch 832/883, Training Loss: 0.6340
Epoch 3/10, Batch 833/883, Training Loss: 0.9586
Epoch 3/10, Batch 834/883, Training Loss: 1.0591
Epoch 3/10, Batch 835/883, Training Loss: 0.9586
Epoch 3/10, Batch 836/883, Training Loss: 0.6292
Epoch 3/10, Batch 837/883, Training Loss: 0.9258
Epoch 3/10, Batch 838/883, Training Loss: 0.9293
Epoch 3/10, Batch 839/883, Training Loss: 0.5475
Epoch 3/10, Batch 840/883, Training Loss: 0.6315
Epoch 3/10, Batch 841/883, Training Loss: 0.7727
Epoch 3/10, Batch 842/883, Training Loss: 0.7087
Epoch 3/10, Batch 843/883, Training Loss: 0.9828
Epoch 3/10, Batch 844/883, Training Loss: 0.8417
Epoch 3/10, Batch 845/883, Training Loss: 0.7285
Epoch 3/10, Batch 846/883, Training Loss: 0.8234
Epoch 3/10, Batch 847/883, Training Loss: 0.8296
Epoch 3/10, Batch 848/883, Training Loss: 0.8203
Epoch 3/10, Batch 849/883, Training Loss: 1.1684
Epoch 3/10, Batch 850/883, Training Loss: 0.6818
Epoch 3/10, Batch 851/883, Training Loss: 0.7843
Epoch 3/10, Batch 852/883, Training Loss: 0.9160
Epoch 3/10, Batch 853/883, Training Loss: 0.9024
Epoch 3/10, Batch 854/883, Training Loss: 0.7649
Epoch 3/10, Batch 855/883, Training Loss: 0.7938
Epoch 3/10, Batch 856/883, Training Loss: 0.8255
Epoch 3/10, Batch 857/883, Training Loss: 0.9843
Epoch 3/10, Batch 858/883, Training Loss: 0.7459
Epoch 3/10, Batch 859/883, Training Loss: 0.7018
Epoch 3/10, Batch 860/883, Training Loss: 0.9404
Epoch 3/10, Batch 861/883, Training Loss: 0.9332
Epoch 3/10, Batch 862/883, Training Loss: 0.7488
Epoch 3/10, Batch 863/883, Training Loss: 0.6938
Epoch 3/10, Batch 864/883, Training Loss: 0.8911
Epoch 3/10, Batch 865/883, Training Loss: 0.6313
Epoch 3/10, Batch 866/883, Training Loss: 0.7217
Epoch 3/10, Batch 867/883, Training Loss: 0.6772
Epoch 3/10, Batch 868/883, Training Loss: 0.8904
Epoch 3/10, Batch 869/883, Training Loss: 0.9374
Epoch 3/10, Batch 870/883, Training Loss: 0.4787
Epoch 3/10, Batch 871/883, Training Loss: 0.8288
Epoch 3/10, Batch 872/883, Training Loss: 0.8670
Epoch 3/10, Batch 873/883, Training Loss: 0.7289
Epoch 3/10, Batch 874/883, Training Loss: 0.7366
Epoch 3/10, Batch 875/883, Training Loss: 0.5703
Epoch 3/10, Batch 876/883, Training Loss: 0.7790
Epoch 3/10, Batch 877/883, Training Loss: 0.7204
Epoch 3/10, Batch 878/883, Training Loss: 0.5264
Epoch 3/10, Batch 879/883, Training Loss: 0.9333
Epoch 3/10, Batch 880/883, Training Loss: 0.7743
Epoch 3/10, Batch 881/883, Training Loss: 1.1182
Epoch 3/10, Batch 882/883, Training Loss: 0.6490
Epoch 3/10, Batch 883/883, Training Loss: 0.7550
Epoch 3/10, Training Loss: 0.8039, Validation Loss: 0.7748, Validation Accuracy: 0.6412
Epoch 4/10, Batch 1/883, Training Loss: 0.7349
Epoch 4/10, Batch 2/883, Training Loss: 0.9266
Epoch 4/10, Batch 3/883, Training Loss: 1.0039
Epoch 4/10, Batch 4/883, Training Loss: 0.8242
Epoch 4/10, Batch 5/883, Training Loss: 0.8595
Epoch 4/10, Batch 6/883, Training Loss: 0.8099
Epoch 4/10, Batch 7/883, Training Loss: 0.8138
Epoch 4/10, Batch 8/883, Training Loss: 0.7865
Epoch 4/10, Batch 9/883, Training Loss: 0.6925
Epoch 4/10, Batch 10/883, Training Loss: 0.7152
Epoch 4/10, Batch 11/883, Training Loss: 1.1151
Epoch 4/10, Batch 12/883, Training Loss: 0.5523
Epoch 4/10, Batch 13/883, Training Loss: 0.8165
Epoch 4/10, Batch 14/883, Training Loss: 0.7048
Epoch 4/10, Batch 15/883, Training Loss: 0.9590
Epoch 4/10, Batch 16/883, Training Loss: 0.5770
Epoch 4/10, Batch 17/883, Training Loss: 0.7917
Epoch 4/10, Batch 18/883, Training Loss: 0.4856
Epoch 4/10, Batch 19/883, Training Loss: 0.8108
Epoch 4/10, Batch 20/883, Training Loss: 1.1519
Epoch 4/10, Batch 21/883, Training Loss: 0.8655
Epoch 4/10, Batch 22/883, Training Loss: 0.7793
Epoch 4/10, Batch 23/883, Training Loss: 0.9287
Epoch 4/10, Batch 24/883, Training Loss: 0.9568
Epoch 4/10, Batch 25/883, Training Loss: 0.5529
Epoch 4/10, Batch 26/883, Training Loss: 0.8915
Epoch 4/10, Batch 27/883, Training Loss: 0.5732
Epoch 4/10, Batch 28/883, Training Loss: 0.6060
Epoch 4/10, Batch 29/883, Training Loss: 0.5842
Epoch 4/10, Batch 30/883, Training Loss: 1.0505
Epoch 4/10, Batch 31/883, Training Loss: 0.9575
Epoch 4/10, Batch 32/883, Training Loss: 0.6454
Epoch 4/10, Batch 33/883, Training Loss: 0.7261
Epoch 4/10, Batch 34/883, Training Loss: 0.7778
Epoch 4/10, Batch 35/883, Training Loss: 0.7782
Epoch 4/10, Batch 36/883, Training Loss: 0.7542
Epoch 4/10, Batch 37/883, Training Loss: 0.7229
Epoch 4/10, Batch 38/883, Training Loss: 0.7567
Epoch 4/10, Batch 39/883, Training Loss: 0.9272
Epoch 4/10, Batch 40/883, Training Loss: 0.9070
Epoch 4/10, Batch 41/883, Training Loss: 0.7312
Epoch 4/10, Batch 42/883, Training Loss: 0.9147
Epoch 4/10, Batch 43/883, Training Loss: 0.5960
Epoch 4/10, Batch 44/883, Training Loss: 0.7280
Epoch 4/10, Batch 45/883, Training Loss: 0.9034
Epoch 4/10, Batch 46/883, Training Loss: 0.7911
Epoch 4/10, Batch 47/883, Training Loss: 0.8049
Epoch 4/10, Batch 48/883, Training Loss: 1.1707
Epoch 4/10, Batch 49/883, Training Loss: 0.9855
Epoch 4/10, Batch 50/883, Training Loss: 0.9354
Epoch 4/10, Batch 51/883, Training Loss: 1.0498
Epoch 4/10, Batch 52/883, Training Loss: 0.9958
Epoch 4/10, Batch 53/883, Training Loss: 0.9969
Epoch 4/10, Batch 54/883, Training Loss: 0.8482
Epoch 4/10, Batch 55/883, Training Loss: 0.9755
Epoch 4/10, Batch 56/883, Training Loss: 0.6944
Epoch 4/10, Batch 57/883, Training Loss: 0.6986
Epoch 4/10, Batch 58/883, Training Loss: 0.6200
Epoch 4/10, Batch 59/883, Training Loss: 0.6763
Epoch 4/10, Batch 60/883, Training Loss: 0.8153
Epoch 4/10, Batch 61/883, Training Loss: 0.6887
Epoch 4/10, Batch 62/883, Training Loss: 0.9251
Epoch 4/10, Batch 63/883, Training Loss: 0.8992
Epoch 4/10, Batch 64/883, Training Loss: 0.6731
Epoch 4/10, Batch 65/883, Training Loss: 0.5264
Epoch 4/10, Batch 66/883, Training Loss: 1.4227
Epoch 4/10, Batch 67/883, Training Loss: 0.6518
Epoch 4/10, Batch 68/883, Training Loss: 0.9503
Epoch 4/10, Batch 69/883, Training Loss: 0.6551
Epoch 4/10, Batch 70/883, Training Loss: 0.6178
Epoch 4/10, Batch 71/883, Training Loss: 0.6874
Epoch 4/10, Batch 72/883, Training Loss: 1.0236
Epoch 4/10, Batch 73/883, Training Loss: 0.7234
Epoch 4/10, Batch 74/883, Training Loss: 1.0047
Epoch 4/10, Batch 75/883, Training Loss: 0.5837
Epoch 4/10, Batch 76/883, Training Loss: 0.9002
Epoch 4/10, Batch 77/883, Training Loss: 0.7762
Epoch 4/10, Batch 78/883, Training Loss: 0.9062
Epoch 4/10, Batch 79/883, Training Loss: 0.7902
Epoch 4/10, Batch 80/883, Training Loss: 1.1878
Epoch 4/10, Batch 81/883, Training Loss: 0.8553
Epoch 4/10, Batch 82/883, Training Loss: 0.8056
Epoch 4/10, Batch 83/883, Training Loss: 0.7030
Epoch 4/10, Batch 84/883, Training Loss: 0.7485
Epoch 4/10, Batch 85/883, Training Loss: 0.5713
Epoch 4/10, Batch 86/883, Training Loss: 0.7280
Epoch 4/10, Batch 87/883, Training Loss: 0.9046
Epoch 4/10, Batch 88/883, Training Loss: 0.6791
Epoch 4/10, Batch 89/883, Training Loss: 0.6547
Epoch 4/10, Batch 90/883, Training Loss: 0.8234
Epoch 4/10, Batch 91/883, Training Loss: 0.8752
Epoch 4/10, Batch 92/883, Training Loss: 0.9669
Epoch 4/10, Batch 93/883, Training Loss: 0.7604
Epoch 4/10, Batch 94/883, Training Loss: 0.6722
Epoch 4/10, Batch 95/883, Training Loss: 0.7201
Epoch 4/10, Batch 96/883, Training Loss: 0.7698
Epoch 4/10, Batch 97/883, Training Loss: 0.6235
Epoch 4/10, Batch 98/883, Training Loss: 0.5957
Epoch 4/10, Batch 99/883, Training Loss: 0.6322
Epoch 4/10, Batch 100/883, Training Loss: 1.0521
Epoch 4/10, Batch 101/883, Training Loss: 0.8433
Epoch 4/10, Batch 102/883, Training Loss: 0.8345
Epoch 4/10, Batch 103/883, Training Loss: 1.1539
Epoch 4/10, Batch 104/883, Training Loss: 0.6748
Epoch 4/10, Batch 105/883, Training Loss: 0.6915
Epoch 4/10, Batch 106/883, Training Loss: 0.7712
Epoch 4/10, Batch 107/883, Training Loss: 0.6189
Epoch 4/10, Batch 108/883, Training Loss: 0.6286
Epoch 4/10, Batch 109/883, Training Loss: 0.6454
Epoch 4/10, Batch 110/883, Training Loss: 0.9932
Epoch 4/10, Batch 111/883, Training Loss: 0.5607
Epoch 4/10, Batch 112/883, Training Loss: 0.6378
Epoch 4/10, Batch 113/883, Training Loss: 0.7888
Epoch 4/10, Batch 114/883, Training Loss: 1.3190
Epoch 4/10, Batch 115/883, Training Loss: 0.8906
Epoch 4/10, Batch 116/883, Training Loss: 0.6404
Epoch 4/10, Batch 117/883, Training Loss: 0.7970
Epoch 4/10, Batch 118/883, Training Loss: 0.9720
Epoch 4/10, Batch 119/883, Training Loss: 0.8615
Epoch 4/10, Batch 120/883, Training Loss: 0.6908
Epoch 4/10, Batch 121/883, Training Loss: 0.9605
Epoch 4/10, Batch 122/883, Training Loss: 0.7042
Epoch 4/10, Batch 123/883, Training Loss: 0.6953
Epoch 4/10, Batch 124/883, Training Loss: 0.9756
Epoch 4/10, Batch 125/883, Training Loss: 0.9634
Epoch 4/10, Batch 126/883, Training Loss: 1.1013
Epoch 4/10, Batch 127/883, Training Loss: 0.7039
Epoch 4/10, Batch 128/883, Training Loss: 0.7172
Epoch 4/10, Batch 129/883, Training Loss: 0.9091
Epoch 4/10, Batch 130/883, Training Loss: 0.7126
Epoch 4/10, Batch 131/883, Training Loss: 1.0894
Epoch 4/10, Batch 132/883, Training Loss: 0.8866
Epoch 4/10, Batch 133/883, Training Loss: 0.8169
Epoch 4/10, Batch 134/883, Training Loss: 0.5752
Epoch 4/10, Batch 135/883, Training Loss: 0.7001
Epoch 4/10, Batch 136/883, Training Loss: 0.5695
Epoch 4/10, Batch 137/883, Training Loss: 0.6029
Epoch 4/10, Batch 138/883, Training Loss: 1.0001
Epoch 4/10, Batch 139/883, Training Loss: 0.6600
Epoch 4/10, Batch 140/883, Training Loss: 0.8385
Epoch 4/10, Batch 141/883, Training Loss: 0.6929
Epoch 4/10, Batch 142/883, Training Loss: 0.6434
Epoch 4/10, Batch 143/883, Training Loss: 0.6416
Epoch 4/10, Batch 144/883, Training Loss: 0.7424
Epoch 4/10, Batch 145/883, Training Loss: 0.7077
Epoch 4/10, Batch 146/883, Training Loss: 0.6406
Epoch 4/10, Batch 147/883, Training Loss: 0.7501
Epoch 4/10, Batch 148/883, Training Loss: 0.9393
Epoch 4/10, Batch 149/883, Training Loss: 0.9458
Epoch 4/10, Batch 150/883, Training Loss: 0.5529
Epoch 4/10, Batch 151/883, Training Loss: 0.9878
Epoch 4/10, Batch 152/883, Training Loss: 0.5079
Epoch 4/10, Batch 153/883, Training Loss: 0.6970
Epoch 4/10, Batch 154/883, Training Loss: 0.6857
Epoch 4/10, Batch 155/883, Training Loss: 1.3302
Epoch 4/10, Batch 156/883, Training Loss: 0.6100
Epoch 4/10, Batch 157/883, Training Loss: 0.8018
Epoch 4/10, Batch 158/883, Training Loss: 0.7986
Epoch 4/10, Batch 159/883, Training Loss: 0.9072
Epoch 4/10, Batch 160/883, Training Loss: 0.5875
Epoch 4/10, Batch 161/883, Training Loss: 0.7034
Epoch 4/10, Batch 162/883, Training Loss: 0.6296
Epoch 4/10, Batch 163/883, Training Loss: 0.9228
Epoch 4/10, Batch 164/883, Training Loss: 0.9262
Epoch 4/10, Batch 165/883, Training Loss: 1.0263
Epoch 4/10, Batch 166/883, Training Loss: 0.8323
Epoch 4/10, Batch 167/883, Training Loss: 0.6414
Epoch 4/10, Batch 168/883, Training Loss: 0.9558
Epoch 4/10, Batch 169/883, Training Loss: 0.9570
Epoch 4/10, Batch 170/883, Training Loss: 0.9507
Epoch 4/10, Batch 171/883, Training Loss: 0.7141
Epoch 4/10, Batch 172/883, Training Loss: 0.6945
Epoch 4/10, Batch 173/883, Training Loss: 0.7619
Epoch 4/10, Batch 174/883, Training Loss: 0.7604
Epoch 4/10, Batch 175/883, Training Loss: 0.7201
Epoch 4/10, Batch 176/883, Training Loss: 0.7199
Epoch 4/10, Batch 177/883, Training Loss: 0.6899
Epoch 4/10, Batch 178/883, Training Loss: 1.1316
Epoch 4/10, Batch 179/883, Training Loss: 0.7670
Epoch 4/10, Batch 180/883, Training Loss: 0.5850
Epoch 4/10, Batch 181/883, Training Loss: 1.3745
Epoch 4/10, Batch 182/883, Training Loss: 0.9002
Epoch 4/10, Batch 183/883, Training Loss: 0.9731
Epoch 4/10, Batch 184/883, Training Loss: 0.8290
Epoch 4/10, Batch 185/883, Training Loss: 0.9697
Epoch 4/10, Batch 186/883, Training Loss: 0.5631
Epoch 4/10, Batch 187/883, Training Loss: 0.8222
Epoch 4/10, Batch 188/883, Training Loss: 0.7198
Epoch 4/10, Batch 189/883, Training Loss: 0.8216
Epoch 4/10, Batch 190/883, Training Loss: 0.6058
Epoch 4/10, Batch 191/883, Training Loss: 0.7492
Epoch 4/10, Batch 192/883, Training Loss: 0.7621
Epoch 4/10, Batch 193/883, Training Loss: 0.8986
Epoch 4/10, Batch 194/883, Training Loss: 0.8356
Epoch 4/10, Batch 195/883, Training Loss: 0.6827
Epoch 4/10, Batch 196/883, Training Loss: 0.9417
Epoch 4/10, Batch 197/883, Training Loss: 0.8547
Epoch 4/10, Batch 198/883, Training Loss: 0.7509
Epoch 4/10, Batch 199/883, Training Loss: 0.8487
Epoch 4/10, Batch 200/883, Training Loss: 0.7692
Epoch 4/10, Batch 201/883, Training Loss: 0.7298
Epoch 4/10, Batch 202/883, Training Loss: 0.4707
Epoch 4/10, Batch 203/883, Training Loss: 1.2498
Epoch 4/10, Batch 204/883, Training Loss: 0.9469
Epoch 4/10, Batch 205/883, Training Loss: 0.8372
Epoch 4/10, Batch 206/883, Training Loss: 1.1297
Epoch 4/10, Batch 207/883, Training Loss: 0.6259
Epoch 4/10, Batch 208/883, Training Loss: 0.8379
Epoch 4/10, Batch 209/883, Training Loss: 0.8759
Epoch 4/10, Batch 210/883, Training Loss: 0.7218
Epoch 4/10, Batch 211/883, Training Loss: 0.7376
Epoch 4/10, Batch 212/883, Training Loss: 0.7897
Epoch 4/10, Batch 213/883, Training Loss: 0.8854
Epoch 4/10, Batch 214/883, Training Loss: 0.9990
Epoch 4/10, Batch 215/883, Training Loss: 1.0369
Epoch 4/10, Batch 216/883, Training Loss: 0.9410
Epoch 4/10, Batch 217/883, Training Loss: 0.7591
Epoch 4/10, Batch 218/883, Training Loss: 0.7420
Epoch 4/10, Batch 219/883, Training Loss: 0.9342
Epoch 4/10, Batch 220/883, Training Loss: 0.6981
Epoch 4/10, Batch 221/883, Training Loss: 0.6861
Epoch 4/10, Batch 222/883, Training Loss: 0.5693
Epoch 4/10, Batch 223/883, Training Loss: 0.5552
Epoch 4/10, Batch 224/883, Training Loss: 1.1487
Epoch 4/10, Batch 225/883, Training Loss: 1.0753
Epoch 4/10, Batch 226/883, Training Loss: 0.7298
Epoch 4/10, Batch 227/883, Training Loss: 0.8286
Epoch 4/10, Batch 228/883, Training Loss: 0.6881
Epoch 4/10, Batch 229/883, Training Loss: 0.7458
Epoch 4/10, Batch 230/883, Training Loss: 0.7862
Epoch 4/10, Batch 231/883, Training Loss: 0.6108
Epoch 4/10, Batch 232/883, Training Loss: 1.1497
Epoch 4/10, Batch 233/883, Training Loss: 0.9449
Epoch 4/10, Batch 234/883, Training Loss: 0.9740
Epoch 4/10, Batch 235/883, Training Loss: 0.6512
Epoch 4/10, Batch 236/883, Training Loss: 0.6099
Epoch 4/10, Batch 237/883, Training Loss: 0.8460
Epoch 4/10, Batch 238/883, Training Loss: 0.7097
Epoch 4/10, Batch 239/883, Training Loss: 0.6812
Epoch 4/10, Batch 240/883, Training Loss: 1.0162
Epoch 4/10, Batch 241/883, Training Loss: 0.7354
Epoch 4/10, Batch 242/883, Training Loss: 0.6831
Epoch 4/10, Batch 243/883, Training Loss: 0.9519
Epoch 4/10, Batch 244/883, Training Loss: 0.7669
Epoch 4/10, Batch 245/883, Training Loss: 0.7826
Epoch 4/10, Batch 246/883, Training Loss: 1.1882
Epoch 4/10, Batch 247/883, Training Loss: 0.6686
Epoch 4/10, Batch 248/883, Training Loss: 0.7430
Epoch 4/10, Batch 249/883, Training Loss: 0.8524
Epoch 4/10, Batch 250/883, Training Loss: 0.7704
Epoch 4/10, Batch 251/883, Training Loss: 0.6591
Epoch 4/10, Batch 252/883, Training Loss: 0.8526
Epoch 4/10, Batch 253/883, Training Loss: 0.7400
Epoch 4/10, Batch 254/883, Training Loss: 0.6005
Epoch 4/10, Batch 255/883, Training Loss: 0.8069
Epoch 4/10, Batch 256/883, Training Loss: 0.6407
Epoch 4/10, Batch 257/883, Training Loss: 0.7434
Epoch 4/10, Batch 258/883, Training Loss: 0.6475
Epoch 4/10, Batch 259/883, Training Loss: 1.0281
Epoch 4/10, Batch 260/883, Training Loss: 0.7464
Epoch 4/10, Batch 261/883, Training Loss: 0.9242
Epoch 4/10, Batch 262/883, Training Loss: 0.8215
Epoch 4/10, Batch 263/883, Training Loss: 0.4988
Epoch 4/10, Batch 264/883, Training Loss: 0.4960
Epoch 4/10, Batch 265/883, Training Loss: 0.8526
Epoch 4/10, Batch 266/883, Training Loss: 0.5487
Epoch 4/10, Batch 267/883, Training Loss: 0.7220
Epoch 4/10, Batch 268/883, Training Loss: 0.5575
Epoch 4/10, Batch 269/883, Training Loss: 0.6812
Epoch 4/10, Batch 270/883, Training Loss: 0.7515
Epoch 4/10, Batch 271/883, Training Loss: 0.7339
Epoch 4/10, Batch 272/883, Training Loss: 0.7883
Epoch 4/10, Batch 273/883, Training Loss: 0.7296
Epoch 4/10, Batch 274/883, Training Loss: 0.6446
Epoch 4/10, Batch 275/883, Training Loss: 1.0511
Epoch 4/10, Batch 276/883, Training Loss: 0.5830
Epoch 4/10, Batch 277/883, Training Loss: 1.1111
Epoch 4/10, Batch 278/883, Training Loss: 0.6181
Epoch 4/10, Batch 279/883, Training Loss: 0.8434
Epoch 4/10, Batch 280/883, Training Loss: 0.7996
Epoch 4/10, Batch 281/883, Training Loss: 0.7490
Epoch 4/10, Batch 282/883, Training Loss: 0.5696
Epoch 4/10, Batch 283/883, Training Loss: 0.6813
Epoch 4/10, Batch 284/883, Training Loss: 1.0015
Epoch 4/10, Batch 285/883, Training Loss: 0.7916
Epoch 4/10, Batch 286/883, Training Loss: 0.5865
Epoch 4/10, Batch 287/883, Training Loss: 0.7245
Epoch 4/10, Batch 288/883, Training Loss: 0.6887
Epoch 4/10, Batch 289/883, Training Loss: 0.7330
Epoch 4/10, Batch 290/883, Training Loss: 0.6610
Epoch 4/10, Batch 291/883, Training Loss: 0.8516
Epoch 4/10, Batch 292/883, Training Loss: 0.6843
Epoch 4/10, Batch 293/883, Training Loss: 0.8565
Epoch 4/10, Batch 294/883, Training Loss: 0.6201
Epoch 4/10, Batch 295/883, Training Loss: 0.8912
Epoch 4/10, Batch 296/883, Training Loss: 0.7113
Epoch 4/10, Batch 297/883, Training Loss: 0.9516
Epoch 4/10, Batch 298/883, Training Loss: 0.9515
Epoch 4/10, Batch 299/883, Training Loss: 1.0945
Epoch 4/10, Batch 300/883, Training Loss: 0.7045
Epoch 4/10, Batch 301/883, Training Loss: 0.6227
Epoch 4/10, Batch 302/883, Training Loss: 0.9071
Epoch 4/10, Batch 303/883, Training Loss: 0.8925
Epoch 4/10, Batch 304/883, Training Loss: 0.7065
Epoch 4/10, Batch 305/883, Training Loss: 0.7576
Epoch 4/10, Batch 306/883, Training Loss: 0.6704
Epoch 4/10, Batch 307/883, Training Loss: 0.6166
Epoch 4/10, Batch 308/883, Training Loss: 0.7088
Epoch 4/10, Batch 309/883, Training Loss: 0.6212
Epoch 4/10, Batch 310/883, Training Loss: 1.2149
Epoch 4/10, Batch 311/883, Training Loss: 0.7906
Epoch 4/10, Batch 312/883, Training Loss: 0.7460
Epoch 4/10, Batch 313/883, Training Loss: 0.7295
Epoch 4/10, Batch 314/883, Training Loss: 0.7834
Epoch 4/10, Batch 315/883, Training Loss: 0.6844
Epoch 4/10, Batch 316/883, Training Loss: 0.7795
Epoch 4/10, Batch 317/883, Training Loss: 0.6296
Epoch 4/10, Batch 318/883, Training Loss: 0.7448
Epoch 4/10, Batch 319/883, Training Loss: 0.7907
Epoch 4/10, Batch 320/883, Training Loss: 1.4736
Epoch 4/10, Batch 321/883, Training Loss: 0.6824
Epoch 4/10, Batch 322/883, Training Loss: 0.6883
Epoch 4/10, Batch 323/883, Training Loss: 1.0201
Epoch 4/10, Batch 324/883, Training Loss: 0.7633
Epoch 4/10, Batch 325/883, Training Loss: 0.8676
Epoch 4/10, Batch 326/883, Training Loss: 0.9527
Epoch 4/10, Batch 327/883, Training Loss: 0.8044
Epoch 4/10, Batch 328/883, Training Loss: 0.9811
Epoch 4/10, Batch 329/883, Training Loss: 0.5902
Epoch 4/10, Batch 330/883, Training Loss: 0.7167
Epoch 4/10, Batch 331/883, Training Loss: 0.7887
Epoch 4/10, Batch 332/883, Training Loss: 0.7013
Epoch 4/10, Batch 333/883, Training Loss: 0.6284
Epoch 4/10, Batch 334/883, Training Loss: 0.6611
Epoch 4/10, Batch 335/883, Training Loss: 0.6361
Epoch 4/10, Batch 336/883, Training Loss: 0.9545
Epoch 4/10, Batch 337/883, Training Loss: 0.7733
Epoch 4/10, Batch 338/883, Training Loss: 0.5667
Epoch 4/10, Batch 339/883, Training Loss: 0.8930
Epoch 4/10, Batch 340/883, Training Loss: 0.5643
Epoch 4/10, Batch 341/883, Training Loss: 0.5690
Epoch 4/10, Batch 342/883, Training Loss: 0.7835
Epoch 4/10, Batch 343/883, Training Loss: 0.7069
Epoch 4/10, Batch 344/883, Training Loss: 0.6364
Epoch 4/10, Batch 345/883, Training Loss: 0.7084
Epoch 4/10, Batch 346/883, Training Loss: 1.0905
Epoch 4/10, Batch 347/883, Training Loss: 0.5676
Epoch 4/10, Batch 348/883, Training Loss: 0.9445
Epoch 4/10, Batch 349/883, Training Loss: 0.5034
Epoch 4/10, Batch 350/883, Training Loss: 0.9527
Epoch 4/10, Batch 351/883, Training Loss: 1.0615
Epoch 4/10, Batch 352/883, Training Loss: 0.9023
Epoch 4/10, Batch 353/883, Training Loss: 0.6380
Epoch 4/10, Batch 354/883, Training Loss: 0.7624
Epoch 4/10, Batch 355/883, Training Loss: 0.8405
Epoch 4/10, Batch 356/883, Training Loss: 0.7449
Epoch 4/10, Batch 357/883, Training Loss: 1.1182
Epoch 4/10, Batch 358/883, Training Loss: 0.9520
Epoch 4/10, Batch 359/883, Training Loss: 0.8618
Epoch 4/10, Batch 360/883, Training Loss: 0.7684
Epoch 4/10, Batch 361/883, Training Loss: 0.7894
Epoch 4/10, Batch 362/883, Training Loss: 0.8493
Epoch 4/10, Batch 363/883, Training Loss: 0.6105
Epoch 4/10, Batch 364/883, Training Loss: 0.8385
Epoch 4/10, Batch 365/883, Training Loss: 0.8018
Epoch 4/10, Batch 366/883, Training Loss: 0.5247
Epoch 4/10, Batch 367/883, Training Loss: 0.6309
Epoch 4/10, Batch 368/883, Training Loss: 0.6296
Epoch 4/10, Batch 369/883, Training Loss: 0.7257
Epoch 4/10, Batch 370/883, Training Loss: 0.5287
Epoch 4/10, Batch 371/883, Training Loss: 1.0733
Epoch 4/10, Batch 372/883, Training Loss: 0.6182
Epoch 4/10, Batch 373/883, Training Loss: 0.8876
Epoch 4/10, Batch 374/883, Training Loss: 0.9739
Epoch 4/10, Batch 375/883, Training Loss: 0.7270
Epoch 4/10, Batch 376/883, Training Loss: 0.8275
Epoch 4/10, Batch 377/883, Training Loss: 0.5872
Epoch 4/10, Batch 378/883, Training Loss: 1.2000
Epoch 4/10, Batch 379/883, Training Loss: 0.6454
Epoch 4/10, Batch 380/883, Training Loss: 0.7888
Epoch 4/10, Batch 381/883, Training Loss: 0.6808
Epoch 4/10, Batch 382/883, Training Loss: 1.0282
Epoch 4/10, Batch 383/883, Training Loss: 0.6116
Epoch 4/10, Batch 384/883, Training Loss: 0.9215
Epoch 4/10, Batch 385/883, Training Loss: 0.8250
Epoch 4/10, Batch 386/883, Training Loss: 0.7832
Epoch 4/10, Batch 387/883, Training Loss: 0.6940
Epoch 4/10, Batch 388/883, Training Loss: 0.6313
Epoch 4/10, Batch 389/883, Training Loss: 0.5705
Epoch 4/10, Batch 390/883, Training Loss: 0.7967
Epoch 4/10, Batch 391/883, Training Loss: 0.7932
Epoch 4/10, Batch 392/883, Training Loss: 1.1985
Epoch 4/10, Batch 393/883, Training Loss: 0.5820
Epoch 4/10, Batch 394/883, Training Loss: 0.5739
Epoch 4/10, Batch 395/883, Training Loss: 0.6215
Epoch 4/10, Batch 396/883, Training Loss: 0.9938
Epoch 4/10, Batch 397/883, Training Loss: 0.9290
Epoch 4/10, Batch 398/883, Training Loss: 0.6335
Epoch 4/10, Batch 399/883, Training Loss: 1.0665
Epoch 4/10, Batch 400/883, Training Loss: 0.6826
Epoch 4/10, Batch 401/883, Training Loss: 0.6501
Epoch 4/10, Batch 402/883, Training Loss: 0.5301
Epoch 4/10, Batch 403/883, Training Loss: 0.8235
Epoch 4/10, Batch 404/883, Training Loss: 0.7158
Epoch 4/10, Batch 405/883, Training Loss: 0.6371
Epoch 4/10, Batch 406/883, Training Loss: 0.5197
Epoch 4/10, Batch 407/883, Training Loss: 0.8489
Epoch 4/10, Batch 408/883, Training Loss: 0.7941
Epoch 4/10, Batch 409/883, Training Loss: 0.7065
Epoch 4/10, Batch 410/883, Training Loss: 0.7763
Epoch 4/10, Batch 411/883, Training Loss: 0.7323
Epoch 4/10, Batch 412/883, Training Loss: 0.8270
Epoch 4/10, Batch 413/883, Training Loss: 0.7171
Epoch 4/10, Batch 414/883, Training Loss: 1.0927
Epoch 4/10, Batch 415/883, Training Loss: 1.0219
Epoch 4/10, Batch 416/883, Training Loss: 0.6640
Epoch 4/10, Batch 417/883, Training Loss: 0.8486
Epoch 4/10, Batch 418/883, Training Loss: 0.6544
Epoch 4/10, Batch 419/883, Training Loss: 0.7966
Epoch 4/10, Batch 420/883, Training Loss: 0.6834
Epoch 4/10, Batch 421/883, Training Loss: 1.2178
Epoch 4/10, Batch 422/883, Training Loss: 0.7798
Epoch 4/10, Batch 423/883, Training Loss: 0.7319
Epoch 4/10, Batch 424/883, Training Loss: 0.6668
Epoch 4/10, Batch 425/883, Training Loss: 0.9393
Epoch 4/10, Batch 426/883, Training Loss: 0.8745
Epoch 4/10, Batch 427/883, Training Loss: 0.6732
Epoch 4/10, Batch 428/883, Training Loss: 0.6125
Epoch 4/10, Batch 429/883, Training Loss: 0.7484
Epoch 4/10, Batch 430/883, Training Loss: 1.0531
Epoch 4/10, Batch 431/883, Training Loss: 0.8111
Epoch 4/10, Batch 432/883, Training Loss: 0.5361
Epoch 4/10, Batch 433/883, Training Loss: 0.9803
Epoch 4/10, Batch 434/883, Training Loss: 0.6890
Epoch 4/10, Batch 435/883, Training Loss: 0.6493
Epoch 4/10, Batch 436/883, Training Loss: 0.8474
Epoch 4/10, Batch 437/883, Training Loss: 0.6891
Epoch 4/10, Batch 438/883, Training Loss: 0.9640
Epoch 4/10, Batch 439/883, Training Loss: 0.7979
Epoch 4/10, Batch 440/883, Training Loss: 0.7323
Epoch 4/10, Batch 441/883, Training Loss: 0.5693
Epoch 4/10, Batch 442/883, Training Loss: 0.6104
Epoch 4/10, Batch 443/883, Training Loss: 1.0513
Epoch 4/10, Batch 444/883, Training Loss: 0.6896
Epoch 4/10, Batch 445/883, Training Loss: 0.8756
Epoch 4/10, Batch 446/883, Training Loss: 1.2775
Epoch 4/10, Batch 447/883, Training Loss: 0.9574
Epoch 4/10, Batch 448/883, Training Loss: 0.5883
Epoch 4/10, Batch 449/883, Training Loss: 0.9914
Epoch 4/10, Batch 450/883, Training Loss: 0.7197
Epoch 4/10, Batch 451/883, Training Loss: 0.8232
Epoch 4/10, Batch 452/883, Training Loss: 0.7044
Epoch 4/10, Batch 453/883, Training Loss: 0.8064
Epoch 4/10, Batch 454/883, Training Loss: 0.7527
Epoch 4/10, Batch 455/883, Training Loss: 0.5776
Epoch 4/10, Batch 456/883, Training Loss: 0.7430
Epoch 4/10, Batch 457/883, Training Loss: 0.8754
Epoch 4/10, Batch 458/883, Training Loss: 0.6729
Epoch 4/10, Batch 459/883, Training Loss: 0.8194
Epoch 4/10, Batch 460/883, Training Loss: 0.7209
Epoch 4/10, Batch 461/883, Training Loss: 0.7073
Epoch 4/10, Batch 462/883, Training Loss: 0.8758
Epoch 4/10, Batch 463/883, Training Loss: 0.7658
Epoch 4/10, Batch 464/883, Training Loss: 0.9311
Epoch 4/10, Batch 465/883, Training Loss: 0.7393
Epoch 4/10, Batch 466/883, Training Loss: 0.9685
Epoch 4/10, Batch 467/883, Training Loss: 0.8703
Epoch 4/10, Batch 468/883, Training Loss: 0.8519
Epoch 4/10, Batch 469/883, Training Loss: 0.5621
Epoch 4/10, Batch 470/883, Training Loss: 1.1586
Epoch 4/10, Batch 471/883, Training Loss: 0.7709
Epoch 4/10, Batch 472/883, Training Loss: 0.5425
Epoch 4/10, Batch 473/883, Training Loss: 0.8998
Epoch 4/10, Batch 474/883, Training Loss: 0.8309
Epoch 4/10, Batch 475/883, Training Loss: 0.9097
Epoch 4/10, Batch 476/883, Training Loss: 0.7960
Epoch 4/10, Batch 477/883, Training Loss: 0.8112
Epoch 4/10, Batch 478/883, Training Loss: 0.8496
Epoch 4/10, Batch 479/883, Training Loss: 0.8660
Epoch 4/10, Batch 480/883, Training Loss: 0.6911
Epoch 4/10, Batch 481/883, Training Loss: 0.9232
Epoch 4/10, Batch 482/883, Training Loss: 0.7720
Epoch 4/10, Batch 483/883, Training Loss: 1.1599
Epoch 4/10, Batch 484/883, Training Loss: 0.8440
Epoch 4/10, Batch 485/883, Training Loss: 0.6632
Epoch 4/10, Batch 486/883, Training Loss: 0.7111
Epoch 4/10, Batch 487/883, Training Loss: 0.7710
Epoch 4/10, Batch 488/883, Training Loss: 0.8305
Epoch 4/10, Batch 489/883, Training Loss: 0.8371
Epoch 4/10, Batch 490/883, Training Loss: 0.6093
Epoch 4/10, Batch 491/883, Training Loss: 1.0856
Epoch 4/10, Batch 492/883, Training Loss: 0.8063
Epoch 4/10, Batch 493/883, Training Loss: 1.0367
Epoch 4/10, Batch 494/883, Training Loss: 0.7574
Epoch 4/10, Batch 495/883, Training Loss: 0.7100
Epoch 4/10, Batch 496/883, Training Loss: 0.9248
Epoch 4/10, Batch 497/883, Training Loss: 0.6538
Epoch 4/10, Batch 498/883, Training Loss: 0.9390
Epoch 4/10, Batch 499/883, Training Loss: 0.7671
Epoch 4/10, Batch 500/883, Training Loss: 0.6666
Epoch 4/10, Batch 501/883, Training Loss: 0.5625
Epoch 4/10, Batch 502/883, Training Loss: 0.7515
Epoch 4/10, Batch 503/883, Training Loss: 0.9358
Epoch 4/10, Batch 504/883, Training Loss: 0.7774
Epoch 4/10, Batch 505/883, Training Loss: 0.6102
Epoch 4/10, Batch 506/883, Training Loss: 0.8785
Epoch 4/10, Batch 507/883, Training Loss: 0.7995
Epoch 4/10, Batch 508/883, Training Loss: 0.6138
Epoch 4/10, Batch 509/883, Training Loss: 0.8395
Epoch 4/10, Batch 510/883, Training Loss: 0.8618
Epoch 4/10, Batch 511/883, Training Loss: 0.9293
Epoch 4/10, Batch 512/883, Training Loss: 0.6330
Epoch 4/10, Batch 513/883, Training Loss: 0.7930
Epoch 4/10, Batch 514/883, Training Loss: 0.6335
Epoch 4/10, Batch 515/883, Training Loss: 0.7908
Epoch 4/10, Batch 516/883, Training Loss: 0.7281
Epoch 4/10, Batch 517/883, Training Loss: 0.6334
Epoch 4/10, Batch 518/883, Training Loss: 0.6629
Epoch 4/10, Batch 519/883, Training Loss: 0.7964
Epoch 4/10, Batch 520/883, Training Loss: 0.7292
Epoch 4/10, Batch 521/883, Training Loss: 1.0415
Epoch 4/10, Batch 522/883, Training Loss: 0.7436
Epoch 4/10, Batch 523/883, Training Loss: 0.7705
Epoch 4/10, Batch 524/883, Training Loss: 0.8184
Epoch 4/10, Batch 525/883, Training Loss: 0.8189
Epoch 4/10, Batch 526/883, Training Loss: 0.7020
Epoch 4/10, Batch 527/883, Training Loss: 0.7014
Epoch 4/10, Batch 528/883, Training Loss: 0.6867
Epoch 4/10, Batch 529/883, Training Loss: 0.8206
Epoch 4/10, Batch 530/883, Training Loss: 0.9652
Epoch 4/10, Batch 531/883, Training Loss: 0.7090
Epoch 4/10, Batch 532/883, Training Loss: 0.7603
Epoch 4/10, Batch 533/883, Training Loss: 0.7928
Epoch 4/10, Batch 534/883, Training Loss: 0.7288
Epoch 4/10, Batch 535/883, Training Loss: 0.6957
Epoch 4/10, Batch 536/883, Training Loss: 0.6291
Epoch 4/10, Batch 537/883, Training Loss: 0.9022
Epoch 4/10, Batch 538/883, Training Loss: 0.7291
Epoch 4/10, Batch 539/883, Training Loss: 0.6987
Epoch 4/10, Batch 540/883, Training Loss: 0.8631
Epoch 4/10, Batch 541/883, Training Loss: 0.8909
Epoch 4/10, Batch 542/883, Training Loss: 0.6022
Epoch 4/10, Batch 543/883, Training Loss: 0.7035
Epoch 4/10, Batch 544/883, Training Loss: 0.6761
Epoch 4/10, Batch 545/883, Training Loss: 0.8252
Epoch 4/10, Batch 546/883, Training Loss: 0.9199
Epoch 4/10, Batch 547/883, Training Loss: 0.8309
Epoch 4/10, Batch 548/883, Training Loss: 0.6879
Epoch 4/10, Batch 549/883, Training Loss: 0.6062
Epoch 4/10, Batch 550/883, Training Loss: 0.5379
Epoch 4/10, Batch 551/883, Training Loss: 0.9462
Epoch 4/10, Batch 552/883, Training Loss: 0.8604
Epoch 4/10, Batch 553/883, Training Loss: 0.6961
Epoch 4/10, Batch 554/883, Training Loss: 0.6594
Epoch 4/10, Batch 555/883, Training Loss: 0.6278
Epoch 4/10, Batch 556/883, Training Loss: 0.7706
Epoch 4/10, Batch 557/883, Training Loss: 0.7018
Epoch 4/10, Batch 558/883, Training Loss: 0.5213
Epoch 4/10, Batch 559/883, Training Loss: 0.5694
Epoch 4/10, Batch 560/883, Training Loss: 0.6439
Epoch 4/10, Batch 561/883, Training Loss: 0.7158
Epoch 4/10, Batch 562/883, Training Loss: 0.5447
Epoch 4/10, Batch 563/883, Training Loss: 0.5283
Epoch 4/10, Batch 564/883, Training Loss: 0.8120
Epoch 4/10, Batch 565/883, Training Loss: 0.8644
Epoch 4/10, Batch 566/883, Training Loss: 0.5340
Epoch 4/10, Batch 567/883, Training Loss: 0.6021
Epoch 4/10, Batch 568/883, Training Loss: 0.7982
Epoch 4/10, Batch 569/883, Training Loss: 0.8876
Epoch 4/10, Batch 570/883, Training Loss: 0.9086
Epoch 4/10, Batch 571/883, Training Loss: 0.8387
Epoch 4/10, Batch 572/883, Training Loss: 0.6927
Epoch 4/10, Batch 573/883, Training Loss: 0.8251
Epoch 4/10, Batch 574/883, Training Loss: 0.7050
Epoch 4/10, Batch 575/883, Training Loss: 0.6262
Epoch 4/10, Batch 576/883, Training Loss: 0.8558
Epoch 4/10, Batch 577/883, Training Loss: 0.6726
Epoch 4/10, Batch 578/883, Training Loss: 0.6282
Epoch 4/10, Batch 579/883, Training Loss: 0.8309
Epoch 4/10, Batch 580/883, Training Loss: 0.7521
Epoch 4/10, Batch 581/883, Training Loss: 0.7198
Epoch 4/10, Batch 582/883, Training Loss: 0.6351
Epoch 4/10, Batch 583/883, Training Loss: 0.7002
Epoch 4/10, Batch 584/883, Training Loss: 0.7602
Epoch 4/10, Batch 585/883, Training Loss: 0.7112
Epoch 4/10, Batch 586/883, Training Loss: 1.0761
Epoch 4/10, Batch 587/883, Training Loss: 0.7990
Epoch 4/10, Batch 588/883, Training Loss: 0.5220
Epoch 4/10, Batch 589/883, Training Loss: 0.7812
Epoch 4/10, Batch 590/883, Training Loss: 1.1321
Epoch 4/10, Batch 591/883, Training Loss: 0.5208
Epoch 4/10, Batch 592/883, Training Loss: 0.8392
Epoch 4/10, Batch 593/883, Training Loss: 0.8197
Epoch 4/10, Batch 594/883, Training Loss: 0.9076
Epoch 4/10, Batch 595/883, Training Loss: 0.7172
Epoch 4/10, Batch 596/883, Training Loss: 0.8100
Epoch 4/10, Batch 597/883, Training Loss: 0.6050
Epoch 4/10, Batch 598/883, Training Loss: 0.8324
Epoch 4/10, Batch 599/883, Training Loss: 1.0092
Epoch 4/10, Batch 600/883, Training Loss: 0.6108
Epoch 4/10, Batch 601/883, Training Loss: 0.6813
Epoch 4/10, Batch 602/883, Training Loss: 0.7605
Epoch 4/10, Batch 603/883, Training Loss: 0.7616
Epoch 4/10, Batch 604/883, Training Loss: 0.7856
Epoch 4/10, Batch 605/883, Training Loss: 0.6502
Epoch 4/10, Batch 606/883, Training Loss: 0.7190
Epoch 4/10, Batch 607/883, Training Loss: 0.5645
Epoch 4/10, Batch 608/883, Training Loss: 0.9648
Epoch 4/10, Batch 609/883, Training Loss: 0.8129
Epoch 4/10, Batch 610/883, Training Loss: 0.8348
Epoch 4/10, Batch 611/883, Training Loss: 0.6943
Epoch 4/10, Batch 612/883, Training Loss: 0.7339
Epoch 4/10, Batch 613/883, Training Loss: 0.9859
Epoch 4/10, Batch 614/883, Training Loss: 1.0513
Epoch 4/10, Batch 615/883, Training Loss: 0.6566
Epoch 4/10, Batch 616/883, Training Loss: 0.8272
Epoch 4/10, Batch 617/883, Training Loss: 0.7596
Epoch 4/10, Batch 618/883, Training Loss: 0.5331
Epoch 4/10, Batch 619/883, Training Loss: 0.7768
Epoch 4/10, Batch 620/883, Training Loss: 0.7166
Epoch 4/10, Batch 621/883, Training Loss: 0.8120
Epoch 4/10, Batch 622/883, Training Loss: 0.6911
Epoch 4/10, Batch 623/883, Training Loss: 0.6504
Epoch 4/10, Batch 624/883, Training Loss: 1.0980
Epoch 4/10, Batch 625/883, Training Loss: 0.7224
Epoch 4/10, Batch 626/883, Training Loss: 0.6788
Epoch 4/10, Batch 627/883, Training Loss: 0.6835
Epoch 4/10, Batch 628/883, Training Loss: 0.5828
Epoch 4/10, Batch 629/883, Training Loss: 0.5433
Epoch 4/10, Batch 630/883, Training Loss: 0.5705
Epoch 4/10, Batch 631/883, Training Loss: 0.5069
Epoch 4/10, Batch 632/883, Training Loss: 0.8696
Epoch 4/10, Batch 633/883, Training Loss: 0.8163
Epoch 4/10, Batch 634/883, Training Loss: 0.7712
Epoch 4/10, Batch 635/883, Training Loss: 0.8627
Epoch 4/10, Batch 636/883, Training Loss: 0.5045
Epoch 4/10, Batch 637/883, Training Loss: 1.1626
Epoch 4/10, Batch 638/883, Training Loss: 0.7845
Epoch 4/10, Batch 639/883, Training Loss: 0.6773
Epoch 4/10, Batch 640/883, Training Loss: 0.8807
Epoch 4/10, Batch 641/883, Training Loss: 0.8899
Epoch 4/10, Batch 642/883, Training Loss: 0.9422
Epoch 4/10, Batch 643/883, Training Loss: 0.5689
Epoch 4/10, Batch 644/883, Training Loss: 0.8656
Epoch 4/10, Batch 645/883, Training Loss: 0.8780
Epoch 4/10, Batch 646/883, Training Loss: 0.7389
Epoch 4/10, Batch 647/883, Training Loss: 0.8272
Epoch 4/10, Batch 648/883, Training Loss: 0.8549
Epoch 4/10, Batch 649/883, Training Loss: 0.8622
Epoch 4/10, Batch 650/883, Training Loss: 0.7214
Epoch 4/10, Batch 651/883, Training Loss: 0.9308
Epoch 4/10, Batch 652/883, Training Loss: 0.8209
Epoch 4/10, Batch 653/883, Training Loss: 0.8747
Epoch 4/10, Batch 654/883, Training Loss: 1.0575
Epoch 4/10, Batch 655/883, Training Loss: 0.6500
Epoch 4/10, Batch 656/883, Training Loss: 0.8061
Epoch 4/10, Batch 657/883, Training Loss: 0.6592
Epoch 4/10, Batch 658/883, Training Loss: 0.6182
Epoch 4/10, Batch 659/883, Training Loss: 0.8555
Epoch 4/10, Batch 660/883, Training Loss: 0.6570
Epoch 4/10, Batch 661/883, Training Loss: 0.6754
Epoch 4/10, Batch 662/883, Training Loss: 0.7235
Epoch 4/10, Batch 663/883, Training Loss: 0.8065
Epoch 4/10, Batch 664/883, Training Loss: 1.1466
Epoch 4/10, Batch 665/883, Training Loss: 0.4646
Epoch 4/10, Batch 666/883, Training Loss: 0.8337
Epoch 4/10, Batch 667/883, Training Loss: 0.8776
Epoch 4/10, Batch 668/883, Training Loss: 0.5553
Epoch 4/10, Batch 669/883, Training Loss: 0.8044
Epoch 4/10, Batch 670/883, Training Loss: 0.8467
Epoch 4/10, Batch 671/883, Training Loss: 0.8688
Epoch 4/10, Batch 672/883, Training Loss: 0.6863
Epoch 4/10, Batch 673/883, Training Loss: 0.5357
Epoch 4/10, Batch 674/883, Training Loss: 0.7794
Epoch 4/10, Batch 675/883, Training Loss: 0.5504
Epoch 4/10, Batch 676/883, Training Loss: 0.8992
Epoch 4/10, Batch 677/883, Training Loss: 0.6125
Epoch 4/10, Batch 678/883, Training Loss: 0.7905
Epoch 4/10, Batch 679/883, Training Loss: 0.5785
Epoch 4/10, Batch 680/883, Training Loss: 0.7896
Epoch 4/10, Batch 681/883, Training Loss: 0.6721
Epoch 4/10, Batch 682/883, Training Loss: 0.5676
Epoch 4/10, Batch 683/883, Training Loss: 0.7156
Epoch 4/10, Batch 684/883, Training Loss: 0.5846
Epoch 4/10, Batch 685/883, Training Loss: 0.8580
Epoch 4/10, Batch 686/883, Training Loss: 1.1482
Epoch 4/10, Batch 687/883, Training Loss: 0.5146
Epoch 4/10, Batch 688/883, Training Loss: 0.9536
Epoch 4/10, Batch 689/883, Training Loss: 0.4575
Epoch 4/10, Batch 690/883, Training Loss: 0.7753
Epoch 4/10, Batch 691/883, Training Loss: 0.7074
Epoch 4/10, Batch 692/883, Training Loss: 0.8607
Epoch 4/10, Batch 693/883, Training Loss: 0.9258
Epoch 4/10, Batch 694/883, Training Loss: 0.8275
Epoch 4/10, Batch 695/883, Training Loss: 0.6385
Epoch 4/10, Batch 696/883, Training Loss: 0.7124
Epoch 4/10, Batch 697/883, Training Loss: 0.5764
Epoch 4/10, Batch 698/883, Training Loss: 0.6279
Epoch 4/10, Batch 699/883, Training Loss: 0.9249
Epoch 4/10, Batch 700/883, Training Loss: 1.0268
Epoch 4/10, Batch 701/883, Training Loss: 0.6867
Epoch 4/10, Batch 702/883, Training Loss: 0.6467
Epoch 4/10, Batch 703/883, Training Loss: 0.8012
Epoch 4/10, Batch 704/883, Training Loss: 0.6410
Epoch 4/10, Batch 705/883, Training Loss: 0.7306
Epoch 4/10, Batch 706/883, Training Loss: 0.5994
Epoch 4/10, Batch 707/883, Training Loss: 0.7564
Epoch 4/10, Batch 708/883, Training Loss: 0.8455
Epoch 4/10, Batch 709/883, Training Loss: 0.8895
Epoch 4/10, Batch 710/883, Training Loss: 1.1027
Epoch 4/10, Batch 711/883, Training Loss: 0.6177
Epoch 4/10, Batch 712/883, Training Loss: 0.8988
Epoch 4/10, Batch 713/883, Training Loss: 0.9713
Epoch 4/10, Batch 714/883, Training Loss: 1.0094
Epoch 4/10, Batch 715/883, Training Loss: 0.4280
Epoch 4/10, Batch 716/883, Training Loss: 0.9140
Epoch 4/10, Batch 717/883, Training Loss: 0.7487
Epoch 4/10, Batch 718/883, Training Loss: 0.7973
Epoch 4/10, Batch 719/883, Training Loss: 0.7374
Epoch 4/10, Batch 720/883, Training Loss: 0.7752
Epoch 4/10, Batch 721/883, Training Loss: 0.7912
Epoch 4/10, Batch 722/883, Training Loss: 1.0323
Epoch 4/10, Batch 723/883, Training Loss: 0.7687
Epoch 4/10, Batch 724/883, Training Loss: 0.6591
Epoch 4/10, Batch 725/883, Training Loss: 0.8191
Epoch 4/10, Batch 726/883, Training Loss: 0.5931
Epoch 4/10, Batch 727/883, Training Loss: 0.8600
Epoch 4/10, Batch 728/883, Training Loss: 0.6008
Epoch 4/10, Batch 729/883, Training Loss: 0.6421
Epoch 4/10, Batch 730/883, Training Loss: 0.6771
Epoch 4/10, Batch 731/883, Training Loss: 0.4896
Epoch 4/10, Batch 732/883, Training Loss: 0.6390
Epoch 4/10, Batch 733/883, Training Loss: 0.8260
Epoch 4/10, Batch 734/883, Training Loss: 0.5928
Epoch 4/10, Batch 735/883, Training Loss: 0.9623
Epoch 4/10, Batch 736/883, Training Loss: 0.5715
Epoch 4/10, Batch 737/883, Training Loss: 0.5864
Epoch 4/10, Batch 738/883, Training Loss: 0.6205
Epoch 4/10, Batch 739/883, Training Loss: 0.8048
Epoch 4/10, Batch 740/883, Training Loss: 0.8283
Epoch 4/10, Batch 741/883, Training Loss: 1.0147
Epoch 4/10, Batch 742/883, Training Loss: 0.6497
Epoch 4/10, Batch 743/883, Training Loss: 0.7991
Epoch 4/10, Batch 744/883, Training Loss: 0.7502
Epoch 4/10, Batch 745/883, Training Loss: 0.8762
Epoch 4/10, Batch 746/883, Training Loss: 1.3503
Epoch 4/10, Batch 747/883, Training Loss: 0.7704
Epoch 4/10, Batch 748/883, Training Loss: 0.6809
Epoch 4/10, Batch 749/883, Training Loss: 0.9620
Epoch 4/10, Batch 750/883, Training Loss: 0.6150
Epoch 4/10, Batch 751/883, Training Loss: 0.5403
Epoch 4/10, Batch 752/883, Training Loss: 0.6573
Epoch 4/10, Batch 753/883, Training Loss: 1.1387
Epoch 4/10, Batch 754/883, Training Loss: 0.6053
Epoch 4/10, Batch 755/883, Training Loss: 0.6252
Epoch 4/10, Batch 756/883, Training Loss: 0.5500
Epoch 4/10, Batch 757/883, Training Loss: 0.7559
Epoch 4/10, Batch 758/883, Training Loss: 0.5468
Epoch 4/10, Batch 759/883, Training Loss: 0.7147
Epoch 4/10, Batch 760/883, Training Loss: 0.6856
Epoch 4/10, Batch 761/883, Training Loss: 0.6833
Epoch 4/10, Batch 762/883, Training Loss: 0.6223
Epoch 4/10, Batch 763/883, Training Loss: 0.6261
Epoch 4/10, Batch 764/883, Training Loss: 0.7448
Epoch 4/10, Batch 765/883, Training Loss: 0.7336
Epoch 4/10, Batch 766/883, Training Loss: 0.7791
Epoch 4/10, Batch 767/883, Training Loss: 0.6069
Epoch 4/10, Batch 768/883, Training Loss: 0.4769
Epoch 4/10, Batch 769/883, Training Loss: 0.7832
Epoch 4/10, Batch 770/883, Training Loss: 0.7262
Epoch 4/10, Batch 771/883, Training Loss: 0.7173
Epoch 4/10, Batch 772/883, Training Loss: 0.9214
Epoch 4/10, Batch 773/883, Training Loss: 1.0282
Epoch 4/10, Batch 774/883, Training Loss: 0.6845
Epoch 4/10, Batch 775/883, Training Loss: 0.5927
Epoch 4/10, Batch 776/883, Training Loss: 0.6473
Epoch 4/10, Batch 777/883, Training Loss: 0.3949
Epoch 4/10, Batch 778/883, Training Loss: 0.6625
Epoch 4/10, Batch 779/883, Training Loss: 0.6704
Epoch 4/10, Batch 780/883, Training Loss: 0.6525
Epoch 4/10, Batch 781/883, Training Loss: 0.9904
Epoch 4/10, Batch 782/883, Training Loss: 0.7072
Epoch 4/10, Batch 783/883, Training Loss: 1.2229
Epoch 4/10, Batch 784/883, Training Loss: 0.5215
Epoch 4/10, Batch 785/883, Training Loss: 0.4499
Epoch 4/10, Batch 786/883, Training Loss: 0.6182
Epoch 4/10, Batch 787/883, Training Loss: 0.5020
Epoch 4/10, Batch 788/883, Training Loss: 0.7259
Epoch 4/10, Batch 789/883, Training Loss: 0.8736
Epoch 4/10, Batch 790/883, Training Loss: 0.7755
Epoch 4/10, Batch 791/883, Training Loss: 0.8958
Epoch 4/10, Batch 792/883, Training Loss: 0.8093
Epoch 4/10, Batch 793/883, Training Loss: 0.7233
Epoch 4/10, Batch 794/883, Training Loss: 0.7469
Epoch 4/10, Batch 795/883, Training Loss: 0.8404
Epoch 4/10, Batch 796/883, Training Loss: 0.7040
Epoch 4/10, Batch 797/883, Training Loss: 0.8194
Epoch 4/10, Batch 798/883, Training Loss: 0.8797
Epoch 4/10, Batch 799/883, Training Loss: 0.5345
Epoch 4/10, Batch 800/883, Training Loss: 0.6207
Epoch 4/10, Batch 801/883, Training Loss: 0.8436
Epoch 4/10, Batch 802/883, Training Loss: 0.8745
Epoch 4/10, Batch 803/883, Training Loss: 0.7328
Epoch 4/10, Batch 804/883, Training Loss: 0.5879
Epoch 4/10, Batch 805/883, Training Loss: 0.5903
Epoch 4/10, Batch 806/883, Training Loss: 1.0393
Epoch 4/10, Batch 807/883, Training Loss: 0.8450
Epoch 4/10, Batch 808/883, Training Loss: 0.7600
Epoch 4/10, Batch 809/883, Training Loss: 0.8439
Epoch 4/10, Batch 810/883, Training Loss: 0.6728
Epoch 4/10, Batch 811/883, Training Loss: 0.9724
Epoch 4/10, Batch 812/883, Training Loss: 0.8072
Epoch 4/10, Batch 813/883, Training Loss: 0.6794
Epoch 4/10, Batch 814/883, Training Loss: 0.7175
Epoch 4/10, Batch 815/883, Training Loss: 0.9995
Epoch 4/10, Batch 816/883, Training Loss: 0.7057
Epoch 4/10, Batch 817/883, Training Loss: 0.6437
Epoch 4/10, Batch 818/883, Training Loss: 0.6105
Epoch 4/10, Batch 819/883, Training Loss: 0.6671
Epoch 4/10, Batch 820/883, Training Loss: 0.8254
Epoch 4/10, Batch 821/883, Training Loss: 0.6306
Epoch 4/10, Batch 822/883, Training Loss: 0.5446
Epoch 4/10, Batch 823/883, Training Loss: 0.8794
Epoch 4/10, Batch 824/883, Training Loss: 0.9509
Epoch 4/10, Batch 825/883, Training Loss: 0.6982
Epoch 4/10, Batch 826/883, Training Loss: 1.0195
Epoch 4/10, Batch 827/883, Training Loss: 0.6463
Epoch 4/10, Batch 828/883, Training Loss: 0.9449
Epoch 4/10, Batch 829/883, Training Loss: 0.4100
Epoch 4/10, Batch 830/883, Training Loss: 0.6047
Epoch 4/10, Batch 831/883, Training Loss: 0.8305
Epoch 4/10, Batch 832/883, Training Loss: 0.7355
Epoch 4/10, Batch 833/883, Training Loss: 0.7444
Epoch 4/10, Batch 834/883, Training Loss: 0.5216
Epoch 4/10, Batch 835/883, Training Loss: 0.7542
Epoch 4/10, Batch 836/883, Training Loss: 0.7998
Epoch 4/10, Batch 837/883, Training Loss: 0.6757
Epoch 4/10, Batch 838/883, Training Loss: 0.5747
Epoch 4/10, Batch 839/883, Training Loss: 0.8575
Epoch 4/10, Batch 840/883, Training Loss: 0.5841
Epoch 4/10, Batch 841/883, Training Loss: 0.9155
Epoch 4/10, Batch 842/883, Training Loss: 0.7644
Epoch 4/10, Batch 843/883, Training Loss: 0.8989
Epoch 4/10, Batch 844/883, Training Loss: 1.1407
Epoch 4/10, Batch 845/883, Training Loss: 0.7390
Epoch 4/10, Batch 846/883, Training Loss: 0.8817
Epoch 4/10, Batch 847/883, Training Loss: 0.8004
Epoch 4/10, Batch 848/883, Training Loss: 0.9473
Epoch 4/10, Batch 849/883, Training Loss: 0.6901
Epoch 4/10, Batch 850/883, Training Loss: 0.9573
Epoch 4/10, Batch 851/883, Training Loss: 0.7919
Epoch 4/10, Batch 852/883, Training Loss: 0.7139
Epoch 4/10, Batch 853/883, Training Loss: 0.9788
Epoch 4/10, Batch 854/883, Training Loss: 0.6234
Epoch 4/10, Batch 855/883, Training Loss: 0.8883
Epoch 4/10, Batch 856/883, Training Loss: 0.8263
Epoch 4/10, Batch 857/883, Training Loss: 0.7002
Epoch 4/10, Batch 858/883, Training Loss: 1.0051
Epoch 4/10, Batch 859/883, Training Loss: 0.8069
Epoch 4/10, Batch 860/883, Training Loss: 0.9345
Epoch 4/10, Batch 861/883, Training Loss: 0.6652
Epoch 4/10, Batch 862/883, Training Loss: 0.9179
Epoch 4/10, Batch 863/883, Training Loss: 0.7092
Epoch 4/10, Batch 864/883, Training Loss: 0.6299
Epoch 4/10, Batch 865/883, Training Loss: 0.7511
Epoch 4/10, Batch 866/883, Training Loss: 0.7411
Epoch 4/10, Batch 867/883, Training Loss: 0.7890
Epoch 4/10, Batch 868/883, Training Loss: 0.8097
Epoch 4/10, Batch 869/883, Training Loss: 0.7639
Epoch 4/10, Batch 870/883, Training Loss: 0.7906
Epoch 4/10, Batch 871/883, Training Loss: 0.8590
Epoch 4/10, Batch 872/883, Training Loss: 0.6948
Epoch 4/10, Batch 873/883, Training Loss: 0.5382
Epoch 4/10, Batch 874/883, Training Loss: 0.9090
Epoch 4/10, Batch 875/883, Training Loss: 0.7289
Epoch 4/10, Batch 876/883, Training Loss: 0.7394
Epoch 4/10, Batch 877/883, Training Loss: 0.6628
Epoch 4/10, Batch 878/883, Training Loss: 0.8392
Epoch 4/10, Batch 879/883, Training Loss: 0.5539
Epoch 4/10, Batch 880/883, Training Loss: 0.7618
Epoch 4/10, Batch 881/883, Training Loss: 0.7590
Epoch 4/10, Batch 882/883, Training Loss: 0.6095
Epoch 4/10, Batch 883/883, Training Loss: 1.0621
Epoch 4/10, Training Loss: 0.7774, Validation Loss: 0.7572, Validation Accuracy: 0.6302
Epoch 5/10, Batch 1/883, Training Loss: 0.7539
Epoch 5/10, Batch 2/883, Training Loss: 0.6696
Epoch 5/10, Batch 3/883, Training Loss: 0.6319
Epoch 5/10, Batch 4/883, Training Loss: 0.6744
Epoch 5/10, Batch 5/883, Training Loss: 0.5925
Epoch 5/10, Batch 6/883, Training Loss: 0.6359
Epoch 5/10, Batch 7/883, Training Loss: 0.6939
Epoch 5/10, Batch 8/883, Training Loss: 0.6797
Epoch 5/10, Batch 9/883, Training Loss: 0.7663
Epoch 5/10, Batch 10/883, Training Loss: 0.7977
Epoch 5/10, Batch 11/883, Training Loss: 1.1873
Epoch 5/10, Batch 12/883, Training Loss: 0.8022
Epoch 5/10, Batch 13/883, Training Loss: 0.8017
Epoch 5/10, Batch 14/883, Training Loss: 0.6450
Epoch 5/10, Batch 15/883, Training Loss: 0.5170
Epoch 5/10, Batch 16/883, Training Loss: 0.8005
Epoch 5/10, Batch 17/883, Training Loss: 0.7689
Epoch 5/10, Batch 18/883, Training Loss: 0.6687
Epoch 5/10, Batch 19/883, Training Loss: 0.5597
Epoch 5/10, Batch 20/883, Training Loss: 0.7675
Epoch 5/10, Batch 21/883, Training Loss: 0.5777
Epoch 5/10, Batch 22/883, Training Loss: 0.5617
Epoch 5/10, Batch 23/883, Training Loss: 0.9636
Epoch 5/10, Batch 24/883, Training Loss: 0.7208
Epoch 5/10, Batch 25/883, Training Loss: 0.7436
Epoch 5/10, Batch 26/883, Training Loss: 0.7061
Epoch 5/10, Batch 27/883, Training Loss: 0.5501
Epoch 5/10, Batch 28/883, Training Loss: 0.8211
Epoch 5/10, Batch 29/883, Training Loss: 0.7572
Epoch 5/10, Batch 30/883, Training Loss: 0.7298
Epoch 5/10, Batch 31/883, Training Loss: 0.8703
Epoch 5/10, Batch 32/883, Training Loss: 0.9298
Epoch 5/10, Batch 33/883, Training Loss: 0.9588
Epoch 5/10, Batch 34/883, Training Loss: 0.8495
Epoch 5/10, Batch 35/883, Training Loss: 0.7892
Epoch 5/10, Batch 36/883, Training Loss: 0.8194
Epoch 5/10, Batch 37/883, Training Loss: 0.6118
Epoch 5/10, Batch 38/883, Training Loss: 0.8762
Epoch 5/10, Batch 39/883, Training Loss: 0.9098
Epoch 5/10, Batch 40/883, Training Loss: 0.5803
Epoch 5/10, Batch 41/883, Training Loss: 1.0622
Epoch 5/10, Batch 42/883, Training Loss: 1.1080
Epoch 5/10, Batch 43/883, Training Loss: 0.9584
Epoch 5/10, Batch 44/883, Training Loss: 0.7230
Epoch 5/10, Batch 45/883, Training Loss: 0.5878
Epoch 5/10, Batch 46/883, Training Loss: 0.8055
Epoch 5/10, Batch 47/883, Training Loss: 0.5776
Epoch 5/10, Batch 48/883, Training Loss: 0.6151
Epoch 5/10, Batch 49/883, Training Loss: 0.5818
Epoch 5/10, Batch 50/883, Training Loss: 0.6366
Epoch 5/10, Batch 51/883, Training Loss: 0.6588
Epoch 5/10, Batch 52/883, Training Loss: 0.6000
Epoch 5/10, Batch 53/883, Training Loss: 0.7942
Epoch 5/10, Batch 54/883, Training Loss: 0.6161
Epoch 5/10, Batch 55/883, Training Loss: 0.7916
Epoch 5/10, Batch 56/883, Training Loss: 0.7317
Epoch 5/10, Batch 57/883, Training Loss: 0.5915
Epoch 5/10, Batch 58/883, Training Loss: 0.5670
Epoch 5/10, Batch 59/883, Training Loss: 0.5508
Epoch 5/10, Batch 60/883, Training Loss: 0.7144
Epoch 5/10, Batch 61/883, Training Loss: 0.7087
Epoch 5/10, Batch 62/883, Training Loss: 0.9354
Epoch 5/10, Batch 63/883, Training Loss: 0.5434
Epoch 5/10, Batch 64/883, Training Loss: 0.9521
Epoch 5/10, Batch 65/883, Training Loss: 0.7716
Epoch 5/10, Batch 66/883, Training Loss: 0.6937
Epoch 5/10, Batch 67/883, Training Loss: 1.0196
Epoch 5/10, Batch 68/883, Training Loss: 0.5986
Epoch 5/10, Batch 69/883, Training Loss: 0.4438
Epoch 5/10, Batch 70/883, Training Loss: 0.7400
Epoch 5/10, Batch 71/883, Training Loss: 0.5882
Epoch 5/10, Batch 72/883, Training Loss: 0.8004
Epoch 5/10, Batch 73/883, Training Loss: 0.6360
Epoch 5/10, Batch 74/883, Training Loss: 0.7287
Epoch 5/10, Batch 75/883, Training Loss: 0.7581
Epoch 5/10, Batch 76/883, Training Loss: 0.8224
Epoch 5/10, Batch 77/883, Training Loss: 1.0724
Epoch 5/10, Batch 78/883, Training Loss: 0.6726
Epoch 5/10, Batch 79/883, Training Loss: 0.6888
Epoch 5/10, Batch 80/883, Training Loss: 0.7637
Epoch 5/10, Batch 81/883, Training Loss: 0.5286
Epoch 5/10, Batch 82/883, Training Loss: 0.6614
Epoch 5/10, Batch 83/883, Training Loss: 0.5668
Epoch 5/10, Batch 84/883, Training Loss: 0.6968
Epoch 5/10, Batch 85/883, Training Loss: 0.7047
Epoch 5/10, Batch 86/883, Training Loss: 0.4760
Epoch 5/10, Batch 87/883, Training Loss: 0.7275
Epoch 5/10, Batch 88/883, Training Loss: 0.5829
Epoch 5/10, Batch 89/883, Training Loss: 0.5765
Epoch 5/10, Batch 90/883, Training Loss: 0.5513
Epoch 5/10, Batch 91/883, Training Loss: 0.3833
Epoch 5/10, Batch 92/883, Training Loss: 0.9102
Epoch 5/10, Batch 93/883, Training Loss: 0.5648
Epoch 5/10, Batch 94/883, Training Loss: 0.9543
Epoch 5/10, Batch 95/883, Training Loss: 0.4991
Epoch 5/10, Batch 96/883, Training Loss: 0.8598
Epoch 5/10, Batch 97/883, Training Loss: 0.9155
Epoch 5/10, Batch 98/883, Training Loss: 0.5354
Epoch 5/10, Batch 99/883, Training Loss: 0.5674
Epoch 5/10, Batch 100/883, Training Loss: 0.5885
Epoch 5/10, Batch 101/883, Training Loss: 0.8776
Epoch 5/10, Batch 102/883, Training Loss: 0.7992
Epoch 5/10, Batch 103/883, Training Loss: 0.6837
Epoch 5/10, Batch 104/883, Training Loss: 0.6962
Epoch 5/10, Batch 105/883, Training Loss: 0.7227
Epoch 5/10, Batch 106/883, Training Loss: 0.5901
Epoch 5/10, Batch 107/883, Training Loss: 0.9153
Epoch 5/10, Batch 108/883, Training Loss: 0.7877
Epoch 5/10, Batch 109/883, Training Loss: 0.8044
Epoch 5/10, Batch 110/883, Training Loss: 0.5352
Epoch 5/10, Batch 111/883, Training Loss: 1.2396
Epoch 5/10, Batch 112/883, Training Loss: 0.4947
Epoch 5/10, Batch 113/883, Training Loss: 0.6512
Epoch 5/10, Batch 114/883, Training Loss: 0.5006
Epoch 5/10, Batch 115/883, Training Loss: 0.9801
Epoch 5/10, Batch 116/883, Training Loss: 0.7253
Epoch 5/10, Batch 117/883, Training Loss: 1.1239
Epoch 5/10, Batch 118/883, Training Loss: 0.5064
Epoch 5/10, Batch 119/883, Training Loss: 0.5593
Epoch 5/10, Batch 120/883, Training Loss: 0.6606
Epoch 5/10, Batch 121/883, Training Loss: 0.6844
Epoch 5/10, Batch 122/883, Training Loss: 0.8334
Epoch 5/10, Batch 123/883, Training Loss: 0.6934
Epoch 5/10, Batch 124/883, Training Loss: 0.7545
Epoch 5/10, Batch 125/883, Training Loss: 0.6049
Epoch 5/10, Batch 126/883, Training Loss: 0.8221
Epoch 5/10, Batch 127/883, Training Loss: 0.9838
Epoch 5/10, Batch 128/883, Training Loss: 0.8178
Epoch 5/10, Batch 129/883, Training Loss: 0.6542
Epoch 5/10, Batch 130/883, Training Loss: 0.4459
Epoch 5/10, Batch 131/883, Training Loss: 0.6662
Epoch 5/10, Batch 132/883, Training Loss: 0.6038
Epoch 5/10, Batch 133/883, Training Loss: 0.5895
Epoch 5/10, Batch 134/883, Training Loss: 0.6643
Epoch 5/10, Batch 135/883, Training Loss: 0.6661
Epoch 5/10, Batch 136/883, Training Loss: 0.8131
Epoch 5/10, Batch 137/883, Training Loss: 0.5801
Epoch 5/10, Batch 138/883, Training Loss: 0.6200
Epoch 5/10, Batch 139/883, Training Loss: 0.8318
Epoch 5/10, Batch 140/883, Training Loss: 0.5888
Epoch 5/10, Batch 141/883, Training Loss: 0.6857
Epoch 5/10, Batch 142/883, Training Loss: 0.5811
Epoch 5/10, Batch 143/883, Training Loss: 0.6389
Epoch 5/10, Batch 144/883, Training Loss: 0.5154
Epoch 5/10, Batch 145/883, Training Loss: 0.9875
Epoch 5/10, Batch 146/883, Training Loss: 0.6996
Epoch 5/10, Batch 147/883, Training Loss: 0.5693
Epoch 5/10, Batch 148/883, Training Loss: 0.9965
Epoch 5/10, Batch 149/883, Training Loss: 0.6682
Epoch 5/10, Batch 150/883, Training Loss: 0.6209
Epoch 5/10, Batch 151/883, Training Loss: 0.5785
Epoch 5/10, Batch 152/883, Training Loss: 0.7353
Epoch 5/10, Batch 153/883, Training Loss: 0.7098
Epoch 5/10, Batch 154/883, Training Loss: 1.2994
Epoch 5/10, Batch 155/883, Training Loss: 0.8653
Epoch 5/10, Batch 156/883, Training Loss: 1.0931
Epoch 5/10, Batch 157/883, Training Loss: 0.6246
Epoch 5/10, Batch 158/883, Training Loss: 0.6698
Epoch 5/10, Batch 159/883, Training Loss: 0.6772
Epoch 5/10, Batch 160/883, Training Loss: 0.8142
Epoch 5/10, Batch 161/883, Training Loss: 0.6656
Epoch 5/10, Batch 162/883, Training Loss: 0.7812
Epoch 5/10, Batch 163/883, Training Loss: 0.7472
Epoch 5/10, Batch 164/883, Training Loss: 0.7120
Epoch 5/10, Batch 165/883, Training Loss: 0.5313
Epoch 5/10, Batch 166/883, Training Loss: 0.7304
Epoch 5/10, Batch 167/883, Training Loss: 0.5439
Epoch 5/10, Batch 168/883, Training Loss: 0.7671
Epoch 5/10, Batch 169/883, Training Loss: 0.8800
Epoch 5/10, Batch 170/883, Training Loss: 0.5590
Epoch 5/10, Batch 171/883, Training Loss: 0.7706
Epoch 5/10, Batch 172/883, Training Loss: 0.7066
Epoch 5/10, Batch 173/883, Training Loss: 0.6407
Epoch 5/10, Batch 174/883, Training Loss: 0.4849
Epoch 5/10, Batch 175/883, Training Loss: 0.8768
Epoch 5/10, Batch 176/883, Training Loss: 0.5969
Epoch 5/10, Batch 177/883, Training Loss: 0.6478
Epoch 5/10, Batch 178/883, Training Loss: 0.7687
Epoch 5/10, Batch 179/883, Training Loss: 0.7051
Epoch 5/10, Batch 180/883, Training Loss: 0.6754
Epoch 5/10, Batch 181/883, Training Loss: 0.8206
Epoch 5/10, Batch 182/883, Training Loss: 0.8327
Epoch 5/10, Batch 183/883, Training Loss: 0.6480
Epoch 5/10, Batch 184/883, Training Loss: 0.7597
Epoch 5/10, Batch 185/883, Training Loss: 0.6390
Epoch 5/10, Batch 186/883, Training Loss: 0.9638
Epoch 5/10, Batch 187/883, Training Loss: 0.5700
Epoch 5/10, Batch 188/883, Training Loss: 0.7185
Epoch 5/10, Batch 189/883, Training Loss: 0.8522
Epoch 5/10, Batch 190/883, Training Loss: 0.6684
Epoch 5/10, Batch 191/883, Training Loss: 0.8727
Epoch 5/10, Batch 192/883, Training Loss: 0.7380
Epoch 5/10, Batch 193/883, Training Loss: 0.6207
Epoch 5/10, Batch 194/883, Training Loss: 0.6214
Epoch 5/10, Batch 195/883, Training Loss: 1.0650
Epoch 5/10, Batch 196/883, Training Loss: 0.7016
Epoch 5/10, Batch 197/883, Training Loss: 0.6298
Epoch 5/10, Batch 198/883, Training Loss: 0.4955
Epoch 5/10, Batch 199/883, Training Loss: 0.7322
Epoch 5/10, Batch 200/883, Training Loss: 0.8081
Epoch 5/10, Batch 201/883, Training Loss: 0.6654
Epoch 5/10, Batch 202/883, Training Loss: 0.6887
Epoch 5/10, Batch 203/883, Training Loss: 0.5960
Epoch 5/10, Batch 204/883, Training Loss: 0.4596
Epoch 5/10, Batch 205/883, Training Loss: 0.5268
Epoch 5/10, Batch 206/883, Training Loss: 0.5984
Epoch 5/10, Batch 207/883, Training Loss: 0.6468
Epoch 5/10, Batch 208/883, Training Loss: 1.0043
Epoch 5/10, Batch 209/883, Training Loss: 0.5472
Epoch 5/10, Batch 210/883, Training Loss: 0.5780
Epoch 5/10, Batch 211/883, Training Loss: 0.7919
Epoch 5/10, Batch 212/883, Training Loss: 1.0228
Epoch 5/10, Batch 213/883, Training Loss: 0.7686
Epoch 5/10, Batch 214/883, Training Loss: 0.7451
Epoch 5/10, Batch 215/883, Training Loss: 0.6681
Epoch 5/10, Batch 216/883, Training Loss: 0.6712
Epoch 5/10, Batch 217/883, Training Loss: 0.6160
Epoch 5/10, Batch 218/883, Training Loss: 0.9587
Epoch 5/10, Batch 219/883, Training Loss: 0.9172
Epoch 5/10, Batch 220/883, Training Loss: 0.8093
Epoch 5/10, Batch 221/883, Training Loss: 0.7143
Epoch 5/10, Batch 222/883, Training Loss: 0.5587
Epoch 5/10, Batch 223/883, Training Loss: 0.5102
Epoch 5/10, Batch 224/883, Training Loss: 0.4094
Epoch 5/10, Batch 225/883, Training Loss: 0.6555
Epoch 5/10, Batch 226/883, Training Loss: 0.9733
Epoch 5/10, Batch 227/883, Training Loss: 0.7328
Epoch 5/10, Batch 228/883, Training Loss: 1.1283
Epoch 5/10, Batch 229/883, Training Loss: 0.8775
Epoch 5/10, Batch 230/883, Training Loss: 0.6076
Epoch 5/10, Batch 231/883, Training Loss: 0.5517
Epoch 5/10, Batch 232/883, Training Loss: 0.6438
Epoch 5/10, Batch 233/883, Training Loss: 0.5630
Epoch 5/10, Batch 234/883, Training Loss: 0.5534
Epoch 5/10, Batch 235/883, Training Loss: 1.1551
Epoch 5/10, Batch 236/883, Training Loss: 0.8453
Epoch 5/10, Batch 237/883, Training Loss: 0.7967
Epoch 5/10, Batch 238/883, Training Loss: 0.5744
Epoch 5/10, Batch 239/883, Training Loss: 0.7149
Epoch 5/10, Batch 240/883, Training Loss: 1.1451
Epoch 5/10, Batch 241/883, Training Loss: 0.7884
Epoch 5/10, Batch 242/883, Training Loss: 0.5644
Epoch 5/10, Batch 243/883, Training Loss: 0.5854
Epoch 5/10, Batch 244/883, Training Loss: 0.6003
Epoch 5/10, Batch 245/883, Training Loss: 0.4894
Epoch 5/10, Batch 246/883, Training Loss: 0.5688
Epoch 5/10, Batch 247/883, Training Loss: 0.7835
Epoch 5/10, Batch 248/883, Training Loss: 0.6023
Epoch 5/10, Batch 249/883, Training Loss: 0.6415
Epoch 5/10, Batch 250/883, Training Loss: 0.7839
Epoch 5/10, Batch 251/883, Training Loss: 0.4422
Epoch 5/10, Batch 252/883, Training Loss: 0.6698
Epoch 5/10, Batch 253/883, Training Loss: 0.6054
Epoch 5/10, Batch 254/883, Training Loss: 0.5666
Epoch 5/10, Batch 255/883, Training Loss: 0.5028
Epoch 5/10, Batch 256/883, Training Loss: 0.5978
Epoch 5/10, Batch 257/883, Training Loss: 0.7781
Epoch 5/10, Batch 258/883, Training Loss: 0.5105
Epoch 5/10, Batch 259/883, Training Loss: 0.6769
Epoch 5/10, Batch 260/883, Training Loss: 0.9813
Epoch 5/10, Batch 261/883, Training Loss: 0.5755
Epoch 5/10, Batch 262/883, Training Loss: 1.2995
Epoch 5/10, Batch 263/883, Training Loss: 0.5288
Epoch 5/10, Batch 264/883, Training Loss: 0.5096
Epoch 5/10, Batch 265/883, Training Loss: 0.6860
Epoch 5/10, Batch 266/883, Training Loss: 0.9939
Epoch 5/10, Batch 267/883, Training Loss: 0.6254
Epoch 5/10, Batch 268/883, Training Loss: 0.4429
Epoch 5/10, Batch 269/883, Training Loss: 0.9212
Epoch 5/10, Batch 270/883, Training Loss: 0.6701
Epoch 5/10, Batch 271/883, Training Loss: 0.8882
Epoch 5/10, Batch 272/883, Training Loss: 0.8220
Epoch 5/10, Batch 273/883, Training Loss: 0.7364
Epoch 5/10, Batch 274/883, Training Loss: 0.7530
Epoch 5/10, Batch 275/883, Training Loss: 0.7543
Epoch 5/10, Batch 276/883, Training Loss: 0.7776
Epoch 5/10, Batch 277/883, Training Loss: 0.8845
Epoch 5/10, Batch 278/883, Training Loss: 0.9052
Epoch 5/10, Batch 279/883, Training Loss: 0.4280
Epoch 5/10, Batch 280/883, Training Loss: 0.5188
Epoch 5/10, Batch 281/883, Training Loss: 0.8120
Epoch 5/10, Batch 282/883, Training Loss: 0.6732
Epoch 5/10, Batch 283/883, Training Loss: 0.8115
Epoch 5/10, Batch 284/883, Training Loss: 0.9863
Epoch 5/10, Batch 285/883, Training Loss: 0.7132
Epoch 5/10, Batch 286/883, Training Loss: 0.8041
Epoch 5/10, Batch 287/883, Training Loss: 0.4491
Epoch 5/10, Batch 288/883, Training Loss: 0.4969
Epoch 5/10, Batch 289/883, Training Loss: 0.5236
Epoch 5/10, Batch 290/883, Training Loss: 1.0463
Epoch 5/10, Batch 291/883, Training Loss: 0.4173
Epoch 5/10, Batch 292/883, Training Loss: 0.4481
Epoch 5/10, Batch 293/883, Training Loss: 0.7470
Epoch 5/10, Batch 294/883, Training Loss: 0.6595
Epoch 5/10, Batch 295/883, Training Loss: 0.4146
Epoch 5/10, Batch 296/883, Training Loss: 0.9873
Epoch 5/10, Batch 297/883, Training Loss: 0.8228
Epoch 5/10, Batch 298/883, Training Loss: 0.8752
Epoch 5/10, Batch 299/883, Training Loss: 1.0438
Epoch 5/10, Batch 300/883, Training Loss: 0.6558
Epoch 5/10, Batch 301/883, Training Loss: 0.7228
Epoch 5/10, Batch 302/883, Training Loss: 0.8668
Epoch 5/10, Batch 303/883, Training Loss: 0.7360
Epoch 5/10, Batch 304/883, Training Loss: 0.8323
Epoch 5/10, Batch 305/883, Training Loss: 0.6678
Epoch 5/10, Batch 306/883, Training Loss: 0.6745
Epoch 5/10, Batch 307/883, Training Loss: 0.7294
Epoch 5/10, Batch 308/883, Training Loss: 0.8643
Epoch 5/10, Batch 309/883, Training Loss: 0.6391
Epoch 5/10, Batch 310/883, Training Loss: 0.5402
Epoch 5/10, Batch 311/883, Training Loss: 0.6159
Epoch 5/10, Batch 312/883, Training Loss: 0.8057
Epoch 5/10, Batch 313/883, Training Loss: 0.8982
Epoch 5/10, Batch 314/883, Training Loss: 0.6005
Epoch 5/10, Batch 315/883, Training Loss: 0.6072
Epoch 5/10, Batch 316/883, Training Loss: 0.5095
Epoch 5/10, Batch 317/883, Training Loss: 0.9432
Epoch 5/10, Batch 318/883, Training Loss: 0.7402
Epoch 5/10, Batch 319/883, Training Loss: 0.6752
Epoch 5/10, Batch 320/883, Training Loss: 0.6427
Epoch 5/10, Batch 321/883, Training Loss: 0.7046
Epoch 5/10, Batch 322/883, Training Loss: 0.6105
Epoch 5/10, Batch 323/883, Training Loss: 0.9387
Epoch 5/10, Batch 324/883, Training Loss: 1.3540
Epoch 5/10, Batch 325/883, Training Loss: 0.7771
Epoch 5/10, Batch 326/883, Training Loss: 0.7532
Epoch 5/10, Batch 327/883, Training Loss: 0.9286
Epoch 5/10, Batch 328/883, Training Loss: 0.5326
Epoch 5/10, Batch 329/883, Training Loss: 0.6065
Epoch 5/10, Batch 330/883, Training Loss: 0.7045
Epoch 5/10, Batch 331/883, Training Loss: 0.8636
Epoch 5/10, Batch 332/883, Training Loss: 0.4926
Epoch 5/10, Batch 333/883, Training Loss: 0.8644
Epoch 5/10, Batch 334/883, Training Loss: 0.5211
Epoch 5/10, Batch 335/883, Training Loss: 0.7699
Epoch 5/10, Batch 336/883, Training Loss: 1.0493
Epoch 5/10, Batch 337/883, Training Loss: 0.5386
Epoch 5/10, Batch 338/883, Training Loss: 0.6894
Epoch 5/10, Batch 339/883, Training Loss: 0.7952
Epoch 5/10, Batch 340/883, Training Loss: 0.5750
Epoch 5/10, Batch 341/883, Training Loss: 0.7247
Epoch 5/10, Batch 342/883, Training Loss: 0.6507
Epoch 5/10, Batch 343/883, Training Loss: 0.8907
Epoch 5/10, Batch 344/883, Training Loss: 0.6640
Epoch 5/10, Batch 345/883, Training Loss: 0.5984
Epoch 5/10, Batch 346/883, Training Loss: 0.6969
Epoch 5/10, Batch 347/883, Training Loss: 0.8704
Epoch 5/10, Batch 348/883, Training Loss: 0.6195
Epoch 5/10, Batch 349/883, Training Loss: 0.6177
Epoch 5/10, Batch 350/883, Training Loss: 0.5636
Epoch 5/10, Batch 351/883, Training Loss: 0.5819
Epoch 5/10, Batch 352/883, Training Loss: 0.6666
Epoch 5/10, Batch 353/883, Training Loss: 0.8534
Epoch 5/10, Batch 354/883, Training Loss: 0.9230
Epoch 5/10, Batch 355/883, Training Loss: 0.6680
Epoch 5/10, Batch 356/883, Training Loss: 0.7168
Epoch 5/10, Batch 357/883, Training Loss: 0.6070
Epoch 5/10, Batch 358/883, Training Loss: 0.9722
Epoch 5/10, Batch 359/883, Training Loss: 0.6853
Epoch 5/10, Batch 360/883, Training Loss: 0.7323
Epoch 5/10, Batch 361/883, Training Loss: 0.5508
Epoch 5/10, Batch 362/883, Training Loss: 0.6717
Epoch 5/10, Batch 363/883, Training Loss: 0.7952
Epoch 5/10, Batch 364/883, Training Loss: 0.8474
Epoch 5/10, Batch 365/883, Training Loss: 1.0774
Epoch 5/10, Batch 366/883, Training Loss: 0.6949
Epoch 5/10, Batch 367/883, Training Loss: 1.0638
Epoch 5/10, Batch 368/883, Training Loss: 0.6161
Epoch 5/10, Batch 369/883, Training Loss: 0.6732
Epoch 5/10, Batch 370/883, Training Loss: 0.8237
Epoch 5/10, Batch 371/883, Training Loss: 0.6226
Epoch 5/10, Batch 372/883, Training Loss: 0.7865
Epoch 5/10, Batch 373/883, Training Loss: 0.5777
Epoch 5/10, Batch 374/883, Training Loss: 0.7137
Epoch 5/10, Batch 375/883, Training Loss: 0.7475
Epoch 5/10, Batch 376/883, Training Loss: 1.0257
Epoch 5/10, Batch 377/883, Training Loss: 1.0379
Epoch 5/10, Batch 378/883, Training Loss: 0.6764
Epoch 5/10, Batch 379/883, Training Loss: 0.8685
Epoch 5/10, Batch 380/883, Training Loss: 0.5924
Epoch 5/10, Batch 381/883, Training Loss: 0.6627
Epoch 5/10, Batch 382/883, Training Loss: 0.6294
Epoch 5/10, Batch 383/883, Training Loss: 0.7069
Epoch 5/10, Batch 384/883, Training Loss: 0.7133
Epoch 5/10, Batch 385/883, Training Loss: 0.6800
Epoch 5/10, Batch 386/883, Training Loss: 0.5942
Epoch 5/10, Batch 387/883, Training Loss: 0.5737
Epoch 5/10, Batch 388/883, Training Loss: 0.5319
Epoch 5/10, Batch 389/883, Training Loss: 0.9325
Epoch 5/10, Batch 390/883, Training Loss: 0.7645
Epoch 5/10, Batch 391/883, Training Loss: 0.4446
Epoch 5/10, Batch 392/883, Training Loss: 0.7231
Epoch 5/10, Batch 393/883, Training Loss: 0.7819
Epoch 5/10, Batch 394/883, Training Loss: 0.6047
Epoch 5/10, Batch 395/883, Training Loss: 0.9530
Epoch 5/10, Batch 396/883, Training Loss: 0.8950
Epoch 5/10, Batch 397/883, Training Loss: 0.7145
Epoch 5/10, Batch 398/883, Training Loss: 0.7246
Epoch 5/10, Batch 399/883, Training Loss: 0.4656
Epoch 5/10, Batch 400/883, Training Loss: 0.7129
Epoch 5/10, Batch 401/883, Training Loss: 0.7696
Epoch 5/10, Batch 402/883, Training Loss: 0.7875
Epoch 5/10, Batch 403/883, Training Loss: 0.7374
Epoch 5/10, Batch 404/883, Training Loss: 0.7432
Epoch 5/10, Batch 405/883, Training Loss: 0.6926
Epoch 5/10, Batch 406/883, Training Loss: 0.6906
Epoch 5/10, Batch 407/883, Training Loss: 0.7309
Epoch 5/10, Batch 408/883, Training Loss: 0.5995
Epoch 5/10, Batch 409/883, Training Loss: 0.6066
Epoch 5/10, Batch 410/883, Training Loss: 0.5837
Epoch 5/10, Batch 411/883, Training Loss: 0.9710
Epoch 5/10, Batch 412/883, Training Loss: 0.7699
Epoch 5/10, Batch 413/883, Training Loss: 1.4018
Epoch 5/10, Batch 414/883, Training Loss: 0.6125
Epoch 5/10, Batch 415/883, Training Loss: 0.6806
Epoch 5/10, Batch 416/883, Training Loss: 0.7547
Epoch 5/10, Batch 417/883, Training Loss: 0.6725
Epoch 5/10, Batch 418/883, Training Loss: 0.5921
Epoch 5/10, Batch 419/883, Training Loss: 0.6853
Epoch 5/10, Batch 420/883, Training Loss: 0.5122
Epoch 5/10, Batch 421/883, Training Loss: 0.7112
Epoch 5/10, Batch 422/883, Training Loss: 0.4727
Epoch 5/10, Batch 423/883, Training Loss: 0.5817
Epoch 5/10, Batch 424/883, Training Loss: 0.6117
Epoch 5/10, Batch 425/883, Training Loss: 0.6727
Epoch 5/10, Batch 426/883, Training Loss: 0.6208
Epoch 5/10, Batch 427/883, Training Loss: 0.5197
Epoch 5/10, Batch 428/883, Training Loss: 0.6159
Epoch 5/10, Batch 429/883, Training Loss: 0.4840
Epoch 5/10, Batch 430/883, Training Loss: 0.6675
Epoch 5/10, Batch 431/883, Training Loss: 0.7011
Epoch 5/10, Batch 432/883, Training Loss: 0.6016
Epoch 5/10, Batch 433/883, Training Loss: 0.5864
Epoch 5/10, Batch 434/883, Training Loss: 0.7988
Epoch 5/10, Batch 435/883, Training Loss: 0.7968
Epoch 5/10, Batch 436/883, Training Loss: 0.9030
Epoch 5/10, Batch 437/883, Training Loss: 0.7684
Epoch 5/10, Batch 438/883, Training Loss: 0.7228
Epoch 5/10, Batch 439/883, Training Loss: 0.6091
Epoch 5/10, Batch 440/883, Training Loss: 0.8988
Epoch 5/10, Batch 441/883, Training Loss: 0.5571
Epoch 5/10, Batch 442/883, Training Loss: 0.8472
Epoch 5/10, Batch 443/883, Training Loss: 0.4441
Epoch 5/10, Batch 444/883, Training Loss: 0.9740
Epoch 5/10, Batch 445/883, Training Loss: 0.9986
Epoch 5/10, Batch 446/883, Training Loss: 0.6892
Epoch 5/10, Batch 447/883, Training Loss: 0.7709
Epoch 5/10, Batch 448/883, Training Loss: 0.6288
Epoch 5/10, Batch 449/883, Training Loss: 0.7472
Epoch 5/10, Batch 450/883, Training Loss: 0.8330
Epoch 5/10, Batch 451/883, Training Loss: 1.0621
Epoch 5/10, Batch 452/883, Training Loss: 0.7588
Epoch 5/10, Batch 453/883, Training Loss: 1.1645
Epoch 5/10, Batch 454/883, Training Loss: 0.6599
Epoch 5/10, Batch 455/883, Training Loss: 0.7092
Epoch 5/10, Batch 456/883, Training Loss: 0.6723
Epoch 5/10, Batch 457/883, Training Loss: 0.7784
Epoch 5/10, Batch 458/883, Training Loss: 0.9118
Epoch 5/10, Batch 459/883, Training Loss: 0.8432
Epoch 5/10, Batch 460/883, Training Loss: 0.7332
Epoch 5/10, Batch 461/883, Training Loss: 1.0303
Epoch 5/10, Batch 462/883, Training Loss: 0.7720
Epoch 5/10, Batch 463/883, Training Loss: 0.6959
Epoch 5/10, Batch 464/883, Training Loss: 0.6215
Epoch 5/10, Batch 465/883, Training Loss: 0.8060
Epoch 5/10, Batch 466/883, Training Loss: 0.8584
Epoch 5/10, Batch 467/883, Training Loss: 0.8078
Epoch 5/10, Batch 468/883, Training Loss: 0.7431
Epoch 5/10, Batch 469/883, Training Loss: 0.8841
Epoch 5/10, Batch 470/883, Training Loss: 0.6078
Epoch 5/10, Batch 471/883, Training Loss: 0.8087
Epoch 5/10, Batch 472/883, Training Loss: 0.4950
Epoch 5/10, Batch 473/883, Training Loss: 0.5919
Epoch 5/10, Batch 474/883, Training Loss: 0.6984
Epoch 5/10, Batch 475/883, Training Loss: 0.9283
Epoch 5/10, Batch 476/883, Training Loss: 0.5711
Epoch 5/10, Batch 477/883, Training Loss: 0.6416
Epoch 5/10, Batch 478/883, Training Loss: 0.8716
Epoch 5/10, Batch 479/883, Training Loss: 0.6911
Epoch 5/10, Batch 480/883, Training Loss: 0.6820
Epoch 5/10, Batch 481/883, Training Loss: 0.7394
Epoch 5/10, Batch 482/883, Training Loss: 0.6119
Epoch 5/10, Batch 483/883, Training Loss: 0.6423
Epoch 5/10, Batch 484/883, Training Loss: 0.7213
Epoch 5/10, Batch 485/883, Training Loss: 0.4881
Epoch 5/10, Batch 486/883, Training Loss: 0.6854
Epoch 5/10, Batch 487/883, Training Loss: 1.1372
Epoch 5/10, Batch 488/883, Training Loss: 0.6252
Epoch 5/10, Batch 489/883, Training Loss: 0.5222
Epoch 5/10, Batch 490/883, Training Loss: 0.9176
Epoch 5/10, Batch 491/883, Training Loss: 1.0199
Epoch 5/10, Batch 492/883, Training Loss: 0.7568
Epoch 5/10, Batch 493/883, Training Loss: 0.9045
Epoch 5/10, Batch 494/883, Training Loss: 0.8044
Epoch 5/10, Batch 495/883, Training Loss: 0.8905
Epoch 5/10, Batch 496/883, Training Loss: 0.7424
Epoch 5/10, Batch 497/883, Training Loss: 0.7835
Epoch 5/10, Batch 498/883, Training Loss: 0.7651
Epoch 5/10, Batch 499/883, Training Loss: 0.5800
Epoch 5/10, Batch 500/883, Training Loss: 0.8537
Epoch 5/10, Batch 501/883, Training Loss: 0.6834
Epoch 5/10, Batch 502/883, Training Loss: 0.6773
Epoch 5/10, Batch 503/883, Training Loss: 0.8688
Epoch 5/10, Batch 504/883, Training Loss: 0.5267
Epoch 5/10, Batch 505/883, Training Loss: 0.8941
Epoch 5/10, Batch 506/883, Training Loss: 0.9266
Epoch 5/10, Batch 507/883, Training Loss: 0.6908
Epoch 5/10, Batch 508/883, Training Loss: 0.7230
Epoch 5/10, Batch 509/883, Training Loss: 0.5751
Epoch 5/10, Batch 510/883, Training Loss: 0.7701
Epoch 5/10, Batch 511/883, Training Loss: 0.5608
Epoch 5/10, Batch 512/883, Training Loss: 0.8341
Epoch 5/10, Batch 513/883, Training Loss: 0.6308
Epoch 5/10, Batch 514/883, Training Loss: 0.9180
Epoch 5/10, Batch 515/883, Training Loss: 0.6335
Epoch 5/10, Batch 516/883, Training Loss: 1.0094
Epoch 5/10, Batch 517/883, Training Loss: 0.5689
Epoch 5/10, Batch 518/883, Training Loss: 0.6870
Epoch 5/10, Batch 519/883, Training Loss: 0.5774
Epoch 5/10, Batch 520/883, Training Loss: 0.5724
Epoch 5/10, Batch 521/883, Training Loss: 0.8660
Epoch 5/10, Batch 522/883, Training Loss: 0.7104
Epoch 5/10, Batch 523/883, Training Loss: 0.8371
Epoch 5/10, Batch 524/883, Training Loss: 0.7677
Epoch 5/10, Batch 525/883, Training Loss: 0.5818
Epoch 5/10, Batch 526/883, Training Loss: 0.7574
Epoch 5/10, Batch 527/883, Training Loss: 0.5092
Epoch 5/10, Batch 528/883, Training Loss: 0.4816
Epoch 5/10, Batch 529/883, Training Loss: 0.8302
Epoch 5/10, Batch 530/883, Training Loss: 0.8115
Epoch 5/10, Batch 531/883, Training Loss: 0.5980
Epoch 5/10, Batch 532/883, Training Loss: 0.6179
Epoch 5/10, Batch 533/883, Training Loss: 0.7619
Epoch 5/10, Batch 534/883, Training Loss: 0.6619
Epoch 5/10, Batch 535/883, Training Loss: 0.5204
Epoch 5/10, Batch 536/883, Training Loss: 0.4929
Epoch 5/10, Batch 537/883, Training Loss: 0.8616
Epoch 5/10, Batch 538/883, Training Loss: 0.7771
Epoch 5/10, Batch 539/883, Training Loss: 0.5471
Epoch 5/10, Batch 540/883, Training Loss: 0.5438
Epoch 5/10, Batch 541/883, Training Loss: 0.8915
Epoch 5/10, Batch 542/883, Training Loss: 0.7813
Epoch 5/10, Batch 543/883, Training Loss: 0.8498
Epoch 5/10, Batch 544/883, Training Loss: 0.8023
Epoch 5/10, Batch 545/883, Training Loss: 0.6063
Epoch 5/10, Batch 546/883, Training Loss: 0.6790
Epoch 5/10, Batch 547/883, Training Loss: 0.8443
Epoch 5/10, Batch 548/883, Training Loss: 0.7253
Epoch 5/10, Batch 549/883, Training Loss: 0.8800
Epoch 5/10, Batch 550/883, Training Loss: 0.6881
Epoch 5/10, Batch 551/883, Training Loss: 1.0766
Epoch 5/10, Batch 552/883, Training Loss: 0.7905
Epoch 5/10, Batch 553/883, Training Loss: 0.5813
Epoch 5/10, Batch 554/883, Training Loss: 0.6617
Epoch 5/10, Batch 555/883, Training Loss: 0.6446
Epoch 5/10, Batch 556/883, Training Loss: 0.8671
Epoch 5/10, Batch 557/883, Training Loss: 0.5813
Epoch 5/10, Batch 558/883, Training Loss: 0.7760
Epoch 5/10, Batch 559/883, Training Loss: 0.5446
Epoch 5/10, Batch 560/883, Training Loss: 0.5936
Epoch 5/10, Batch 561/883, Training Loss: 0.7066
Epoch 5/10, Batch 562/883, Training Loss: 0.9386
Epoch 5/10, Batch 563/883, Training Loss: 0.6229
Epoch 5/10, Batch 564/883, Training Loss: 0.6776
Epoch 5/10, Batch 565/883, Training Loss: 0.6280
Epoch 5/10, Batch 566/883, Training Loss: 0.5919
Epoch 5/10, Batch 567/883, Training Loss: 0.7636
Epoch 5/10, Batch 568/883, Training Loss: 0.7067
Epoch 5/10, Batch 569/883, Training Loss: 0.6734
Epoch 5/10, Batch 570/883, Training Loss: 0.5388
Epoch 5/10, Batch 571/883, Training Loss: 1.0102
Epoch 5/10, Batch 572/883, Training Loss: 0.4669
Epoch 5/10, Batch 573/883, Training Loss: 0.7744
Epoch 5/10, Batch 574/883, Training Loss: 0.5186
Epoch 5/10, Batch 575/883, Training Loss: 1.1706
Epoch 5/10, Batch 576/883, Training Loss: 0.8640
Epoch 5/10, Batch 577/883, Training Loss: 0.5824
Epoch 5/10, Batch 578/883, Training Loss: 0.4080
Epoch 5/10, Batch 579/883, Training Loss: 0.7197
Epoch 5/10, Batch 580/883, Training Loss: 0.4522
Epoch 5/10, Batch 581/883, Training Loss: 0.9293
Epoch 5/10, Batch 582/883, Training Loss: 0.5172
Epoch 5/10, Batch 583/883, Training Loss: 0.8308
Epoch 5/10, Batch 584/883, Training Loss: 0.6736
Epoch 5/10, Batch 585/883, Training Loss: 0.7052
Epoch 5/10, Batch 586/883, Training Loss: 0.4707
Epoch 5/10, Batch 587/883, Training Loss: 0.8115
Epoch 5/10, Batch 588/883, Training Loss: 0.9393
Epoch 5/10, Batch 589/883, Training Loss: 0.7223
Epoch 5/10, Batch 590/883, Training Loss: 0.5997
Epoch 5/10, Batch 591/883, Training Loss: 0.8809
Epoch 5/10, Batch 592/883, Training Loss: 0.6962
Epoch 5/10, Batch 593/883, Training Loss: 0.7844
Epoch 5/10, Batch 594/883, Training Loss: 0.7032
Epoch 5/10, Batch 595/883, Training Loss: 0.7210
Epoch 5/10, Batch 596/883, Training Loss: 1.0484
Epoch 5/10, Batch 597/883, Training Loss: 0.5647
Epoch 5/10, Batch 598/883, Training Loss: 0.6288
Epoch 5/10, Batch 599/883, Training Loss: 0.6972
Epoch 5/10, Batch 600/883, Training Loss: 0.6796
Epoch 5/10, Batch 601/883, Training Loss: 1.0535
Epoch 5/10, Batch 602/883, Training Loss: 0.8228
Epoch 5/10, Batch 603/883, Training Loss: 0.5716
Epoch 5/10, Batch 604/883, Training Loss: 0.7093
Epoch 5/10, Batch 605/883, Training Loss: 0.6716
Epoch 5/10, Batch 606/883, Training Loss: 0.5597
Epoch 5/10, Batch 607/883, Training Loss: 0.6719
Epoch 5/10, Batch 608/883, Training Loss: 0.9152
Epoch 5/10, Batch 609/883, Training Loss: 0.8061
Epoch 5/10, Batch 610/883, Training Loss: 0.4904
Epoch 5/10, Batch 611/883, Training Loss: 0.8725
Epoch 5/10, Batch 612/883, Training Loss: 0.8089
Epoch 5/10, Batch 613/883, Training Loss: 0.4233
Epoch 5/10, Batch 614/883, Training Loss: 0.9271
Epoch 5/10, Batch 615/883, Training Loss: 0.6930
Epoch 5/10, Batch 616/883, Training Loss: 0.4689
Epoch 5/10, Batch 617/883, Training Loss: 0.6412
Epoch 5/10, Batch 618/883, Training Loss: 0.4657
Epoch 5/10, Batch 619/883, Training Loss: 0.9342
Epoch 5/10, Batch 620/883, Training Loss: 0.7685
Epoch 5/10, Batch 621/883, Training Loss: 0.6745
Epoch 5/10, Batch 622/883, Training Loss: 0.5296
Epoch 5/10, Batch 623/883, Training Loss: 0.7170
Epoch 5/10, Batch 624/883, Training Loss: 0.6812
Epoch 5/10, Batch 625/883, Training Loss: 0.5960
Epoch 5/10, Batch 626/883, Training Loss: 0.5788
Epoch 5/10, Batch 627/883, Training Loss: 0.5615
Epoch 5/10, Batch 628/883, Training Loss: 0.6342
Epoch 5/10, Batch 629/883, Training Loss: 0.6903
Epoch 5/10, Batch 630/883, Training Loss: 0.8387
Epoch 5/10, Batch 631/883, Training Loss: 0.6111
Epoch 5/10, Batch 632/883, Training Loss: 1.0591
Epoch 5/10, Batch 633/883, Training Loss: 0.6149
Epoch 5/10, Batch 634/883, Training Loss: 0.6867
Epoch 5/10, Batch 635/883, Training Loss: 0.5404
Epoch 5/10, Batch 636/883, Training Loss: 0.6607
Epoch 5/10, Batch 637/883, Training Loss: 0.4602
Epoch 5/10, Batch 638/883, Training Loss: 0.6144
Epoch 5/10, Batch 639/883, Training Loss: 0.7528
Epoch 5/10, Batch 640/883, Training Loss: 0.6081
Epoch 5/10, Batch 641/883, Training Loss: 0.3959
Epoch 5/10, Batch 642/883, Training Loss: 0.4472
Epoch 5/10, Batch 643/883, Training Loss: 0.7523
Epoch 5/10, Batch 644/883, Training Loss: 0.6760
Epoch 5/10, Batch 645/883, Training Loss: 0.6545
Epoch 5/10, Batch 646/883, Training Loss: 0.6362
Epoch 5/10, Batch 647/883, Training Loss: 0.8701
Epoch 5/10, Batch 648/883, Training Loss: 0.8108
Epoch 5/10, Batch 649/883, Training Loss: 0.7021
Epoch 5/10, Batch 650/883, Training Loss: 0.6756
Epoch 5/10, Batch 651/883, Training Loss: 0.8710
Epoch 5/10, Batch 652/883, Training Loss: 0.5962
Epoch 5/10, Batch 653/883, Training Loss: 0.5046
Epoch 5/10, Batch 654/883, Training Loss: 0.5829
Epoch 5/10, Batch 655/883, Training Loss: 0.5302
Epoch 5/10, Batch 656/883, Training Loss: 0.5259
Epoch 5/10, Batch 657/883, Training Loss: 0.7344
Epoch 5/10, Batch 658/883, Training Loss: 1.4186
Epoch 5/10, Batch 659/883, Training Loss: 0.4340
Epoch 5/10, Batch 660/883, Training Loss: 0.7977
Epoch 5/10, Batch 661/883, Training Loss: 0.6260
Epoch 5/10, Batch 662/883, Training Loss: 0.5160
Epoch 5/10, Batch 663/883, Training Loss: 0.5621
Epoch 5/10, Batch 664/883, Training Loss: 0.9355
Epoch 5/10, Batch 665/883, Training Loss: 0.7347
Epoch 5/10, Batch 666/883, Training Loss: 0.5906
Epoch 5/10, Batch 667/883, Training Loss: 0.5331
Epoch 5/10, Batch 668/883, Training Loss: 0.8438
Epoch 5/10, Batch 669/883, Training Loss: 0.5605
Epoch 5/10, Batch 670/883, Training Loss: 0.6170
Epoch 5/10, Batch 671/883, Training Loss: 0.6661
Epoch 5/10, Batch 672/883, Training Loss: 0.6266
Epoch 5/10, Batch 673/883, Training Loss: 0.6723
Epoch 5/10, Batch 674/883, Training Loss: 0.4888
Epoch 5/10, Batch 675/883, Training Loss: 0.9707
Epoch 5/10, Batch 676/883, Training Loss: 0.7898
Epoch 5/10, Batch 677/883, Training Loss: 0.6427
Epoch 5/10, Batch 678/883, Training Loss: 0.4509
Epoch 5/10, Batch 679/883, Training Loss: 1.2197
Epoch 5/10, Batch 680/883, Training Loss: 0.6237
Epoch 5/10, Batch 681/883, Training Loss: 0.5473
Epoch 5/10, Batch 682/883, Training Loss: 0.6076
Epoch 5/10, Batch 683/883, Training Loss: 0.4649
Epoch 5/10, Batch 684/883, Training Loss: 1.0522
Epoch 5/10, Batch 685/883, Training Loss: 0.6040
Epoch 5/10, Batch 686/883, Training Loss: 0.5343
Epoch 5/10, Batch 687/883, Training Loss: 0.6099
Epoch 5/10, Batch 688/883, Training Loss: 1.0862
Epoch 5/10, Batch 689/883, Training Loss: 0.5349
Epoch 5/10, Batch 690/883, Training Loss: 0.8774
Epoch 5/10, Batch 691/883, Training Loss: 0.6710
Epoch 5/10, Batch 692/883, Training Loss: 0.7787
Epoch 5/10, Batch 693/883, Training Loss: 0.6238
Epoch 5/10, Batch 694/883, Training Loss: 0.6906
Epoch 5/10, Batch 695/883, Training Loss: 0.4915
Epoch 5/10, Batch 696/883, Training Loss: 0.6552
Epoch 5/10, Batch 697/883, Training Loss: 0.4794
Epoch 5/10, Batch 698/883, Training Loss: 0.6226
Epoch 5/10, Batch 699/883, Training Loss: 0.7427
Epoch 5/10, Batch 700/883, Training Loss: 0.5615
Epoch 5/10, Batch 701/883, Training Loss: 0.4443
Epoch 5/10, Batch 702/883, Training Loss: 0.6727
Epoch 5/10, Batch 703/883, Training Loss: 0.8454
Epoch 5/10, Batch 704/883, Training Loss: 0.8615
Epoch 5/10, Batch 705/883, Training Loss: 0.7404
Epoch 5/10, Batch 706/883, Training Loss: 0.5155
Epoch 5/10, Batch 707/883, Training Loss: 0.5248
Epoch 5/10, Batch 708/883, Training Loss: 0.6205
Epoch 5/10, Batch 709/883, Training Loss: 0.5607
Epoch 5/10, Batch 710/883, Training Loss: 0.8278
Epoch 5/10, Batch 711/883, Training Loss: 0.3770
Epoch 5/10, Batch 712/883, Training Loss: 0.7117
Epoch 5/10, Batch 713/883, Training Loss: 0.6473
Epoch 5/10, Batch 714/883, Training Loss: 0.4158
Epoch 5/10, Batch 715/883, Training Loss: 0.7140
Epoch 5/10, Batch 716/883, Training Loss: 0.7942
Epoch 5/10, Batch 717/883, Training Loss: 0.6643
Epoch 5/10, Batch 718/883, Training Loss: 0.7052
Epoch 5/10, Batch 719/883, Training Loss: 0.4561
Epoch 5/10, Batch 720/883, Training Loss: 0.8278
Epoch 5/10, Batch 721/883, Training Loss: 0.6643
Epoch 5/10, Batch 722/883, Training Loss: 0.6205
Epoch 5/10, Batch 723/883, Training Loss: 0.8644
Epoch 5/10, Batch 724/883, Training Loss: 0.8559
Epoch 5/10, Batch 725/883, Training Loss: 1.1966
Epoch 5/10, Batch 726/883, Training Loss: 0.6236
Epoch 5/10, Batch 727/883, Training Loss: 0.6637
Epoch 5/10, Batch 728/883, Training Loss: 0.6184
Epoch 5/10, Batch 729/883, Training Loss: 0.7211
Epoch 5/10, Batch 730/883, Training Loss: 0.6334
Epoch 5/10, Batch 731/883, Training Loss: 0.5022
Epoch 5/10, Batch 732/883, Training Loss: 0.5961
Epoch 5/10, Batch 733/883, Training Loss: 0.5693
Epoch 5/10, Batch 734/883, Training Loss: 0.5823
Epoch 5/10, Batch 735/883, Training Loss: 0.6514
Epoch 5/10, Batch 736/883, Training Loss: 0.9204
Epoch 5/10, Batch 737/883, Training Loss: 0.9090
Epoch 5/10, Batch 738/883, Training Loss: 0.8207
Epoch 5/10, Batch 739/883, Training Loss: 0.6433
Epoch 5/10, Batch 740/883, Training Loss: 0.9071
Epoch 5/10, Batch 741/883, Training Loss: 1.1841
Epoch 5/10, Batch 742/883, Training Loss: 0.5694
Epoch 5/10, Batch 743/883, Training Loss: 0.6541
Epoch 5/10, Batch 744/883, Training Loss: 0.6269
Epoch 5/10, Batch 745/883, Training Loss: 0.8345
Epoch 5/10, Batch 746/883, Training Loss: 0.7515
Epoch 5/10, Batch 747/883, Training Loss: 0.7525
Epoch 5/10, Batch 748/883, Training Loss: 0.4874
Epoch 5/10, Batch 749/883, Training Loss: 0.3928
Epoch 5/10, Batch 750/883, Training Loss: 0.5351
Epoch 5/10, Batch 751/883, Training Loss: 0.4724
Epoch 5/10, Batch 752/883, Training Loss: 0.7191
Epoch 5/10, Batch 753/883, Training Loss: 0.5443
Epoch 5/10, Batch 754/883, Training Loss: 0.6844
Epoch 5/10, Batch 755/883, Training Loss: 0.6283
Epoch 5/10, Batch 756/883, Training Loss: 0.4906
Epoch 5/10, Batch 757/883, Training Loss: 1.0309
Epoch 5/10, Batch 758/883, Training Loss: 0.8837
Epoch 5/10, Batch 759/883, Training Loss: 0.5703
Epoch 5/10, Batch 760/883, Training Loss: 0.9721
Epoch 5/10, Batch 761/883, Training Loss: 0.7431
Epoch 5/10, Batch 762/883, Training Loss: 1.0317
Epoch 5/10, Batch 763/883, Training Loss: 0.4834
Epoch 5/10, Batch 764/883, Training Loss: 0.9890
Epoch 5/10, Batch 765/883, Training Loss: 0.6370
Epoch 5/10, Batch 766/883, Training Loss: 0.8014
Epoch 5/10, Batch 767/883, Training Loss: 0.7973
Epoch 5/10, Batch 768/883, Training Loss: 0.6875
Epoch 5/10, Batch 769/883, Training Loss: 0.7279
Epoch 5/10, Batch 770/883, Training Loss: 0.8515
Epoch 5/10, Batch 771/883, Training Loss: 0.7925
Epoch 5/10, Batch 772/883, Training Loss: 0.8040
Epoch 5/10, Batch 773/883, Training Loss: 1.0125
Epoch 5/10, Batch 774/883, Training Loss: 0.8925
Epoch 5/10, Batch 775/883, Training Loss: 0.7816
Epoch 5/10, Batch 776/883, Training Loss: 0.8211
Epoch 5/10, Batch 777/883, Training Loss: 0.7264
Epoch 5/10, Batch 778/883, Training Loss: 0.8013
Epoch 5/10, Batch 779/883, Training Loss: 0.6280
Epoch 5/10, Batch 780/883, Training Loss: 0.9834
Epoch 5/10, Batch 781/883, Training Loss: 0.6810
Epoch 5/10, Batch 782/883, Training Loss: 0.7487
Epoch 5/10, Batch 783/883, Training Loss: 0.6408
Epoch 5/10, Batch 784/883, Training Loss: 0.6900
Epoch 5/10, Batch 785/883, Training Loss: 0.5659
Epoch 5/10, Batch 786/883, Training Loss: 0.6048
Epoch 5/10, Batch 787/883, Training Loss: 0.8292
Epoch 5/10, Batch 788/883, Training Loss: 0.4887
Epoch 5/10, Batch 789/883, Training Loss: 0.7399
Epoch 5/10, Batch 790/883, Training Loss: 0.5517
Epoch 5/10, Batch 791/883, Training Loss: 0.4974
Epoch 5/10, Batch 792/883, Training Loss: 1.0182
Epoch 5/10, Batch 793/883, Training Loss: 0.7044
Epoch 5/10, Batch 794/883, Training Loss: 0.6436
Epoch 5/10, Batch 795/883, Training Loss: 0.8458
Epoch 5/10, Batch 796/883, Training Loss: 1.2249
Epoch 5/10, Batch 797/883, Training Loss: 0.8415
Epoch 5/10, Batch 798/883, Training Loss: 0.4998
Epoch 5/10, Batch 799/883, Training Loss: 0.6934
Epoch 5/10, Batch 800/883, Training Loss: 0.9951
Epoch 5/10, Batch 801/883, Training Loss: 0.8097
Epoch 5/10, Batch 802/883, Training Loss: 0.6980
Epoch 5/10, Batch 803/883, Training Loss: 0.5029
Epoch 5/10, Batch 804/883, Training Loss: 0.6660
Epoch 5/10, Batch 805/883, Training Loss: 0.4509
Epoch 5/10, Batch 806/883, Training Loss: 0.8799
Epoch 5/10, Batch 807/883, Training Loss: 0.6405
Epoch 5/10, Batch 808/883, Training Loss: 0.4167
Epoch 5/10, Batch 809/883, Training Loss: 0.7419
Epoch 5/10, Batch 810/883, Training Loss: 0.6790
Epoch 5/10, Batch 811/883, Training Loss: 0.6754
Epoch 5/10, Batch 812/883, Training Loss: 0.6694
Epoch 5/10, Batch 813/883, Training Loss: 0.6647
Epoch 5/10, Batch 814/883, Training Loss: 0.5891
Epoch 5/10, Batch 815/883, Training Loss: 0.7905
Epoch 5/10, Batch 816/883, Training Loss: 0.5594
Epoch 5/10, Batch 817/883, Training Loss: 0.7499
Epoch 5/10, Batch 818/883, Training Loss: 0.9007
Epoch 5/10, Batch 819/883, Training Loss: 0.6155
Epoch 5/10, Batch 820/883, Training Loss: 0.8681
Epoch 5/10, Batch 821/883, Training Loss: 0.8576
Epoch 5/10, Batch 822/883, Training Loss: 0.5288
Epoch 5/10, Batch 823/883, Training Loss: 0.7365
Epoch 5/10, Batch 824/883, Training Loss: 0.6995
Epoch 5/10, Batch 825/883, Training Loss: 0.5629
Epoch 5/10, Batch 826/883, Training Loss: 0.6894
Epoch 5/10, Batch 827/883, Training Loss: 0.8612
Epoch 5/10, Batch 828/883, Training Loss: 0.7564
Epoch 5/10, Batch 829/883, Training Loss: 0.6388
Epoch 5/10, Batch 830/883, Training Loss: 0.5322
Epoch 5/10, Batch 831/883, Training Loss: 0.7469
Epoch 5/10, Batch 832/883, Training Loss: 0.5413
Epoch 5/10, Batch 833/883, Training Loss: 0.6600
Epoch 5/10, Batch 834/883, Training Loss: 0.8686
Epoch 5/10, Batch 835/883, Training Loss: 0.6783
Epoch 5/10, Batch 836/883, Training Loss: 0.6170
Epoch 5/10, Batch 837/883, Training Loss: 0.6364
Epoch 5/10, Batch 838/883, Training Loss: 0.6284
Epoch 5/10, Batch 839/883, Training Loss: 0.4666
Epoch 5/10, Batch 840/883, Training Loss: 0.6057
Epoch 5/10, Batch 841/883, Training Loss: 0.3891
Epoch 5/10, Batch 842/883, Training Loss: 0.6732
Epoch 5/10, Batch 843/883, Training Loss: 0.5828
Epoch 5/10, Batch 844/883, Training Loss: 0.6965
Epoch 5/10, Batch 845/883, Training Loss: 0.6277
Epoch 5/10, Batch 846/883, Training Loss: 0.4456
Epoch 5/10, Batch 847/883, Training Loss: 0.6302
Epoch 5/10, Batch 848/883, Training Loss: 0.6184
Epoch 5/10, Batch 849/883, Training Loss: 1.0881
Epoch 5/10, Batch 850/883, Training Loss: 0.5832
Epoch 5/10, Batch 851/883, Training Loss: 0.6835
Epoch 5/10, Batch 852/883, Training Loss: 0.5540
Epoch 5/10, Batch 853/883, Training Loss: 0.6498
Epoch 5/10, Batch 854/883, Training Loss: 0.6068
Epoch 5/10, Batch 855/883, Training Loss: 0.8754
Epoch 5/10, Batch 856/883, Training Loss: 0.5953
Epoch 5/10, Batch 857/883, Training Loss: 0.4557
Epoch 5/10, Batch 858/883, Training Loss: 0.9791
Epoch 5/10, Batch 859/883, Training Loss: 0.5702
Epoch 5/10, Batch 860/883, Training Loss: 0.4877
Epoch 5/10, Batch 861/883, Training Loss: 0.5869
Epoch 5/10, Batch 862/883, Training Loss: 1.1658
Epoch 5/10, Batch 863/883, Training Loss: 0.8162
Epoch 5/10, Batch 864/883, Training Loss: 0.7619
Epoch 5/10, Batch 865/883, Training Loss: 0.7492
Epoch 5/10, Batch 866/883, Training Loss: 0.5961
Epoch 5/10, Batch 867/883, Training Loss: 0.8139
Epoch 5/10, Batch 868/883, Training Loss: 1.1662
Epoch 5/10, Batch 869/883, Training Loss: 0.6963
Epoch 5/10, Batch 870/883, Training Loss: 0.6499
Epoch 5/10, Batch 871/883, Training Loss: 0.8995
Epoch 5/10, Batch 872/883, Training Loss: 0.6973
Epoch 5/10, Batch 873/883, Training Loss: 0.7040
Epoch 5/10, Batch 874/883, Training Loss: 0.4425
Epoch 5/10, Batch 875/883, Training Loss: 0.4772
Epoch 5/10, Batch 876/883, Training Loss: 0.5516
Epoch 5/10, Batch 877/883, Training Loss: 0.6161
Epoch 5/10, Batch 878/883, Training Loss: 0.9709
Epoch 5/10, Batch 879/883, Training Loss: 0.4739
Epoch 5/10, Batch 880/883, Training Loss: 0.6192
Epoch 5/10, Batch 881/883, Training Loss: 0.5330
Epoch 5/10, Batch 882/883, Training Loss: 0.7912
Epoch 5/10, Batch 883/883, Training Loss: 1.0330
Epoch 5/10, Training Loss: 0.7123, Validation Loss: 0.7127, Validation Accuracy: 0.6651
Epoch 6/10, Batch 1/883, Training Loss: 0.7975
Epoch 6/10, Batch 2/883, Training Loss: 0.5944
Epoch 6/10, Batch 3/883, Training Loss: 0.7200
Epoch 6/10, Batch 4/883, Training Loss: 0.7564
Epoch 6/10, Batch 5/883, Training Loss: 0.8470
Epoch 6/10, Batch 6/883, Training Loss: 0.8593
Epoch 6/10, Batch 7/883, Training Loss: 0.6810
Epoch 6/10, Batch 8/883, Training Loss: 0.8327
Epoch 6/10, Batch 9/883, Training Loss: 0.4515
Epoch 6/10, Batch 10/883, Training Loss: 0.6417
Epoch 6/10, Batch 11/883, Training Loss: 0.5250
Epoch 6/10, Batch 12/883, Training Loss: 0.5532
Epoch 6/10, Batch 13/883, Training Loss: 0.6641
Epoch 6/10, Batch 14/883, Training Loss: 0.6187
Epoch 6/10, Batch 15/883, Training Loss: 0.5914
Epoch 6/10, Batch 16/883, Training Loss: 0.5811
Epoch 6/10, Batch 17/883, Training Loss: 0.6690
Epoch 6/10, Batch 18/883, Training Loss: 0.6371
Epoch 6/10, Batch 19/883, Training Loss: 0.7635
Epoch 6/10, Batch 20/883, Training Loss: 0.6631
Epoch 6/10, Batch 21/883, Training Loss: 0.7605
Epoch 6/10, Batch 22/883, Training Loss: 0.6974
Epoch 6/10, Batch 23/883, Training Loss: 0.9976
Epoch 6/10, Batch 24/883, Training Loss: 0.6246
Epoch 6/10, Batch 25/883, Training Loss: 0.9363
Epoch 6/10, Batch 26/883, Training Loss: 0.4404
Epoch 6/10, Batch 27/883, Training Loss: 0.4857
Epoch 6/10, Batch 28/883, Training Loss: 0.5423
Epoch 6/10, Batch 29/883, Training Loss: 0.4720
Epoch 6/10, Batch 30/883, Training Loss: 0.3837
Epoch 6/10, Batch 31/883, Training Loss: 0.6123
Epoch 6/10, Batch 32/883, Training Loss: 0.9869
Epoch 6/10, Batch 33/883, Training Loss: 0.5019
Epoch 6/10, Batch 34/883, Training Loss: 0.7946
Epoch 6/10, Batch 35/883, Training Loss: 0.4207
Epoch 6/10, Batch 36/883, Training Loss: 0.8437
Epoch 6/10, Batch 37/883, Training Loss: 0.5080
Epoch 6/10, Batch 38/883, Training Loss: 0.8330
Epoch 6/10, Batch 39/883, Training Loss: 0.4480
Epoch 6/10, Batch 40/883, Training Loss: 0.6550
Epoch 6/10, Batch 41/883, Training Loss: 0.5016
Epoch 6/10, Batch 42/883, Training Loss: 0.6005
Epoch 6/10, Batch 43/883, Training Loss: 0.5887
Epoch 6/10, Batch 44/883, Training Loss: 0.7254
Epoch 6/10, Batch 45/883, Training Loss: 0.5422
Epoch 6/10, Batch 46/883, Training Loss: 0.7979
Epoch 6/10, Batch 47/883, Training Loss: 0.8051
Epoch 6/10, Batch 48/883, Training Loss: 0.6807
Epoch 6/10, Batch 49/883, Training Loss: 0.6176
Epoch 6/10, Batch 50/883, Training Loss: 0.5233
Epoch 6/10, Batch 51/883, Training Loss: 0.6858
Epoch 6/10, Batch 52/883, Training Loss: 0.5340
Epoch 6/10, Batch 53/883, Training Loss: 0.9231
Epoch 6/10, Batch 54/883, Training Loss: 0.5863
Epoch 6/10, Batch 55/883, Training Loss: 1.3122
Epoch 6/10, Batch 56/883, Training Loss: 0.7499
Epoch 6/10, Batch 57/883, Training Loss: 0.4466
Epoch 6/10, Batch 58/883, Training Loss: 0.5446
Epoch 6/10, Batch 59/883, Training Loss: 0.5579
Epoch 6/10, Batch 60/883, Training Loss: 0.7105
Epoch 6/10, Batch 61/883, Training Loss: 0.7805
Epoch 6/10, Batch 62/883, Training Loss: 0.5127
Epoch 6/10, Batch 63/883, Training Loss: 0.5121
Epoch 6/10, Batch 64/883, Training Loss: 0.5892
Epoch 6/10, Batch 65/883, Training Loss: 0.5943
Epoch 6/10, Batch 66/883, Training Loss: 0.5569
Epoch 6/10, Batch 67/883, Training Loss: 0.6485
Epoch 6/10, Batch 68/883, Training Loss: 0.5369
Epoch 6/10, Batch 69/883, Training Loss: 0.6631
Epoch 6/10, Batch 70/883, Training Loss: 0.4987
Epoch 6/10, Batch 71/883, Training Loss: 1.0868
Epoch 6/10, Batch 72/883, Training Loss: 0.6592
Epoch 6/10, Batch 73/883, Training Loss: 0.4880
Epoch 6/10, Batch 74/883, Training Loss: 0.7658
Epoch 6/10, Batch 75/883, Training Loss: 0.5723
Epoch 6/10, Batch 76/883, Training Loss: 0.4560
Epoch 6/10, Batch 77/883, Training Loss: 0.7938
Epoch 6/10, Batch 78/883, Training Loss: 0.6393
Epoch 6/10, Batch 79/883, Training Loss: 0.6397
Epoch 6/10, Batch 80/883, Training Loss: 0.5430
Epoch 6/10, Batch 81/883, Training Loss: 0.6049
Epoch 6/10, Batch 82/883, Training Loss: 0.5437
Epoch 6/10, Batch 83/883, Training Loss: 0.4079
Epoch 6/10, Batch 84/883, Training Loss: 0.5088
Epoch 6/10, Batch 85/883, Training Loss: 0.9115
Epoch 6/10, Batch 86/883, Training Loss: 0.5032
Epoch 6/10, Batch 87/883, Training Loss: 0.8222
Epoch 6/10, Batch 88/883, Training Loss: 0.6986
Epoch 6/10, Batch 89/883, Training Loss: 1.1086
Epoch 6/10, Batch 90/883, Training Loss: 1.0207
Epoch 6/10, Batch 91/883, Training Loss: 0.8220
Epoch 6/10, Batch 92/883, Training Loss: 0.5904
Epoch 6/10, Batch 93/883, Training Loss: 0.5150
Epoch 6/10, Batch 94/883, Training Loss: 0.6208
Epoch 6/10, Batch 95/883, Training Loss: 0.5982
Epoch 6/10, Batch 96/883, Training Loss: 0.5410
Epoch 6/10, Batch 97/883, Training Loss: 0.7923
Epoch 6/10, Batch 98/883, Training Loss: 0.7179
Epoch 6/10, Batch 99/883, Training Loss: 0.9393
Epoch 6/10, Batch 100/883, Training Loss: 0.3780
Epoch 6/10, Batch 101/883, Training Loss: 0.6608
Epoch 6/10, Batch 102/883, Training Loss: 0.7050
Epoch 6/10, Batch 103/883, Training Loss: 1.0940
Epoch 6/10, Batch 104/883, Training Loss: 0.6786
Epoch 6/10, Batch 105/883, Training Loss: 0.6563
Epoch 6/10, Batch 106/883, Training Loss: 0.4307
Epoch 6/10, Batch 107/883, Training Loss: 0.9096
Epoch 6/10, Batch 108/883, Training Loss: 0.5599
Epoch 6/10, Batch 109/883, Training Loss: 0.8505
Epoch 6/10, Batch 110/883, Training Loss: 1.2139
Epoch 6/10, Batch 111/883, Training Loss: 0.9364
Epoch 6/10, Batch 112/883, Training Loss: 0.8673
Epoch 6/10, Batch 113/883, Training Loss: 0.5996
Epoch 6/10, Batch 114/883, Training Loss: 0.6717
Epoch 6/10, Batch 115/883, Training Loss: 0.7066
Epoch 6/10, Batch 116/883, Training Loss: 0.6982
Epoch 6/10, Batch 117/883, Training Loss: 0.4437
Epoch 6/10, Batch 118/883, Training Loss: 0.6123
Epoch 6/10, Batch 119/883, Training Loss: 0.5978
Epoch 6/10, Batch 120/883, Training Loss: 0.6240
Epoch 6/10, Batch 121/883, Training Loss: 0.5569
Epoch 6/10, Batch 122/883, Training Loss: 0.5793
Epoch 6/10, Batch 123/883, Training Loss: 0.6424
Epoch 6/10, Batch 124/883, Training Loss: 0.5819
Epoch 6/10, Batch 125/883, Training Loss: 0.7961
Epoch 6/10, Batch 126/883, Training Loss: 0.7918
Epoch 6/10, Batch 127/883, Training Loss: 0.9378
Epoch 6/10, Batch 128/883, Training Loss: 1.1070
Epoch 6/10, Batch 129/883, Training Loss: 0.3823
Epoch 6/10, Batch 130/883, Training Loss: 0.6537
Epoch 6/10, Batch 131/883, Training Loss: 0.5212
Epoch 6/10, Batch 132/883, Training Loss: 0.3292
Epoch 6/10, Batch 133/883, Training Loss: 0.7466
Epoch 6/10, Batch 134/883, Training Loss: 1.4683
Epoch 6/10, Batch 135/883, Training Loss: 0.6187
Epoch 6/10, Batch 136/883, Training Loss: 1.1171
Epoch 6/10, Batch 137/883, Training Loss: 1.0343
Epoch 6/10, Batch 138/883, Training Loss: 0.5687
Epoch 6/10, Batch 139/883, Training Loss: 0.5283
Epoch 6/10, Batch 140/883, Training Loss: 0.4300
Epoch 6/10, Batch 141/883, Training Loss: 0.7395
Epoch 6/10, Batch 142/883, Training Loss: 0.5934
Epoch 6/10, Batch 143/883, Training Loss: 0.6256
Epoch 6/10, Batch 144/883, Training Loss: 0.4819
Epoch 6/10, Batch 145/883, Training Loss: 0.7740
Epoch 6/10, Batch 146/883, Training Loss: 0.6565
Epoch 6/10, Batch 147/883, Training Loss: 0.7163
Epoch 6/10, Batch 148/883, Training Loss: 0.6441
Epoch 6/10, Batch 149/883, Training Loss: 0.7446
Epoch 6/10, Batch 150/883, Training Loss: 0.5428
Epoch 6/10, Batch 151/883, Training Loss: 0.5692
Epoch 6/10, Batch 152/883, Training Loss: 0.5291
Epoch 6/10, Batch 153/883, Training Loss: 0.4864
Epoch 6/10, Batch 154/883, Training Loss: 0.7215
Epoch 6/10, Batch 155/883, Training Loss: 0.7296
Epoch 6/10, Batch 156/883, Training Loss: 0.5344
Epoch 6/10, Batch 157/883, Training Loss: 0.6472
Epoch 6/10, Batch 158/883, Training Loss: 0.4841
Epoch 6/10, Batch 159/883, Training Loss: 0.4154
Epoch 6/10, Batch 160/883, Training Loss: 0.6291
Epoch 6/10, Batch 161/883, Training Loss: 0.6842
Epoch 6/10, Batch 162/883, Training Loss: 0.7659
Epoch 6/10, Batch 163/883, Training Loss: 0.6339
Epoch 6/10, Batch 164/883, Training Loss: 0.7571
Epoch 6/10, Batch 165/883, Training Loss: 0.6749
Epoch 6/10, Batch 166/883, Training Loss: 0.6506
Epoch 6/10, Batch 167/883, Training Loss: 0.6501
Epoch 6/10, Batch 168/883, Training Loss: 0.5279
Epoch 6/10, Batch 169/883, Training Loss: 0.6925
Epoch 6/10, Batch 170/883, Training Loss: 0.3778
Epoch 6/10, Batch 171/883, Training Loss: 0.9532
Epoch 6/10, Batch 172/883, Training Loss: 0.7035
Epoch 6/10, Batch 173/883, Training Loss: 0.6121
Epoch 6/10, Batch 174/883, Training Loss: 1.0538
Epoch 6/10, Batch 175/883, Training Loss: 0.6435
Epoch 6/10, Batch 176/883, Training Loss: 0.5422
Epoch 6/10, Batch 177/883, Training Loss: 0.6042
Epoch 6/10, Batch 178/883, Training Loss: 0.5870
Epoch 6/10, Batch 179/883, Training Loss: 0.4150
Epoch 6/10, Batch 180/883, Training Loss: 0.7234
Epoch 6/10, Batch 181/883, Training Loss: 0.5972
Epoch 6/10, Batch 182/883, Training Loss: 0.8346
Epoch 6/10, Batch 183/883, Training Loss: 0.8220
Epoch 6/10, Batch 184/883, Training Loss: 0.8893
Epoch 6/10, Batch 185/883, Training Loss: 0.6891
Epoch 6/10, Batch 186/883, Training Loss: 0.5412
Epoch 6/10, Batch 187/883, Training Loss: 0.8834
Epoch 6/10, Batch 188/883, Training Loss: 0.7504
Epoch 6/10, Batch 189/883, Training Loss: 0.4861
Epoch 6/10, Batch 190/883, Training Loss: 0.7565
Epoch 6/10, Batch 191/883, Training Loss: 0.7161
Epoch 6/10, Batch 192/883, Training Loss: 0.9456
Epoch 6/10, Batch 193/883, Training Loss: 0.5656
Epoch 6/10, Batch 194/883, Training Loss: 0.6650
Epoch 6/10, Batch 195/883, Training Loss: 1.0114
Epoch 6/10, Batch 196/883, Training Loss: 0.4732
Epoch 6/10, Batch 197/883, Training Loss: 0.6744
Epoch 6/10, Batch 198/883, Training Loss: 0.8825
Epoch 6/10, Batch 199/883, Training Loss: 0.6868
Epoch 6/10, Batch 200/883, Training Loss: 0.3976
Epoch 6/10, Batch 201/883, Training Loss: 0.4128
Epoch 6/10, Batch 202/883, Training Loss: 0.4257
Epoch 6/10, Batch 203/883, Training Loss: 0.6316
Epoch 6/10, Batch 204/883, Training Loss: 0.6336
Epoch 6/10, Batch 205/883, Training Loss: 0.5242
Epoch 6/10, Batch 206/883, Training Loss: 0.6926
Epoch 6/10, Batch 207/883, Training Loss: 0.5919
Epoch 6/10, Batch 208/883, Training Loss: 0.3708
Epoch 6/10, Batch 209/883, Training Loss: 0.4942
Epoch 6/10, Batch 210/883, Training Loss: 0.6392
Epoch 6/10, Batch 211/883, Training Loss: 0.8360
Epoch 6/10, Batch 212/883, Training Loss: 0.6649
Epoch 6/10, Batch 213/883, Training Loss: 0.6968
Epoch 6/10, Batch 214/883, Training Loss: 0.7867
Epoch 6/10, Batch 215/883, Training Loss: 0.8164
Epoch 6/10, Batch 216/883, Training Loss: 0.8402
Epoch 6/10, Batch 217/883, Training Loss: 0.7665
Epoch 6/10, Batch 218/883, Training Loss: 0.6686
Epoch 6/10, Batch 219/883, Training Loss: 0.4783
Epoch 6/10, Batch 220/883, Training Loss: 0.9276
Epoch 6/10, Batch 221/883, Training Loss: 0.8482
Epoch 6/10, Batch 222/883, Training Loss: 0.6096
Epoch 6/10, Batch 223/883, Training Loss: 0.6212
Epoch 6/10, Batch 224/883, Training Loss: 0.5067
Epoch 6/10, Batch 225/883, Training Loss: 0.6832
Epoch 6/10, Batch 226/883, Training Loss: 0.6062
Epoch 6/10, Batch 227/883, Training Loss: 0.7600
Epoch 6/10, Batch 228/883, Training Loss: 0.5613
Epoch 6/10, Batch 229/883, Training Loss: 0.6237
Epoch 6/10, Batch 230/883, Training Loss: 0.7456
Epoch 6/10, Batch 231/883, Training Loss: 0.7221
Epoch 6/10, Batch 232/883, Training Loss: 0.8915
Epoch 6/10, Batch 233/883, Training Loss: 0.5894
Epoch 6/10, Batch 234/883, Training Loss: 0.7060
Epoch 6/10, Batch 235/883, Training Loss: 0.7737
Epoch 6/10, Batch 236/883, Training Loss: 0.8932
Epoch 6/10, Batch 237/883, Training Loss: 0.7264
Epoch 6/10, Batch 238/883, Training Loss: 0.5645
Epoch 6/10, Batch 239/883, Training Loss: 0.8971
Epoch 6/10, Batch 240/883, Training Loss: 0.6028
Epoch 6/10, Batch 241/883, Training Loss: 0.9305
Epoch 6/10, Batch 242/883, Training Loss: 0.4076
Epoch 6/10, Batch 243/883, Training Loss: 0.6297
Epoch 6/10, Batch 244/883, Training Loss: 0.7323
Epoch 6/10, Batch 245/883, Training Loss: 1.0686
Epoch 6/10, Batch 246/883, Training Loss: 0.6115
Epoch 6/10, Batch 247/883, Training Loss: 0.5586
Epoch 6/10, Batch 248/883, Training Loss: 0.6087
Epoch 6/10, Batch 249/883, Training Loss: 0.6066
Epoch 6/10, Batch 250/883, Training Loss: 0.9781
Epoch 6/10, Batch 251/883, Training Loss: 0.6290
Epoch 6/10, Batch 252/883, Training Loss: 0.8465
Epoch 6/10, Batch 253/883, Training Loss: 0.5854
Epoch 6/10, Batch 254/883, Training Loss: 0.7957
Epoch 6/10, Batch 255/883, Training Loss: 0.6079
Epoch 6/10, Batch 256/883, Training Loss: 0.8058
Epoch 6/10, Batch 257/883, Training Loss: 0.6160
Epoch 6/10, Batch 258/883, Training Loss: 0.6205
Epoch 6/10, Batch 259/883, Training Loss: 0.4849
Epoch 6/10, Batch 260/883, Training Loss: 0.4079
Epoch 6/10, Batch 261/883, Training Loss: 0.5905
Epoch 6/10, Batch 262/883, Training Loss: 0.5205
Epoch 6/10, Batch 263/883, Training Loss: 0.5939
Epoch 6/10, Batch 264/883, Training Loss: 0.8582
Epoch 6/10, Batch 265/883, Training Loss: 0.9110
Epoch 6/10, Batch 266/883, Training Loss: 0.5905
Epoch 6/10, Batch 267/883, Training Loss: 0.8436
Epoch 6/10, Batch 268/883, Training Loss: 0.6640
Epoch 6/10, Batch 269/883, Training Loss: 0.5935
Epoch 6/10, Batch 270/883, Training Loss: 0.4422
Epoch 6/10, Batch 271/883, Training Loss: 0.5284
Epoch 6/10, Batch 272/883, Training Loss: 0.8928
Epoch 6/10, Batch 273/883, Training Loss: 0.7293
Epoch 6/10, Batch 274/883, Training Loss: 0.4396
Epoch 6/10, Batch 275/883, Training Loss: 0.6723
Epoch 6/10, Batch 276/883, Training Loss: 0.3982
Epoch 6/10, Batch 277/883, Training Loss: 0.5465
Epoch 6/10, Batch 278/883, Training Loss: 0.4711
Epoch 6/10, Batch 279/883, Training Loss: 0.8188
Epoch 6/10, Batch 280/883, Training Loss: 0.8315
Epoch 6/10, Batch 281/883, Training Loss: 0.7530
Epoch 6/10, Batch 282/883, Training Loss: 0.6849
Epoch 6/10, Batch 283/883, Training Loss: 0.6826
Epoch 6/10, Batch 284/883, Training Loss: 0.7431
Epoch 6/10, Batch 285/883, Training Loss: 0.8171
Epoch 6/10, Batch 286/883, Training Loss: 0.5326
Epoch 6/10, Batch 287/883, Training Loss: 0.5832
Epoch 6/10, Batch 288/883, Training Loss: 0.6971
Epoch 6/10, Batch 289/883, Training Loss: 1.0367
Epoch 6/10, Batch 290/883, Training Loss: 0.5103
Epoch 6/10, Batch 291/883, Training Loss: 0.7697
Epoch 6/10, Batch 292/883, Training Loss: 0.6245
Epoch 6/10, Batch 293/883, Training Loss: 0.6307
Epoch 6/10, Batch 294/883, Training Loss: 0.5763
Epoch 6/10, Batch 295/883, Training Loss: 0.3499
Epoch 6/10, Batch 296/883, Training Loss: 0.7378
Epoch 6/10, Batch 297/883, Training Loss: 0.4779
Epoch 6/10, Batch 298/883, Training Loss: 0.5823
Epoch 6/10, Batch 299/883, Training Loss: 0.3806
Epoch 6/10, Batch 300/883, Training Loss: 0.5491
Epoch 6/10, Batch 301/883, Training Loss: 0.6883
Epoch 6/10, Batch 302/883, Training Loss: 1.1155
Epoch 6/10, Batch 303/883, Training Loss: 0.6037
Epoch 6/10, Batch 304/883, Training Loss: 0.5329
Epoch 6/10, Batch 305/883, Training Loss: 0.4601
Epoch 6/10, Batch 306/883, Training Loss: 0.6414
Epoch 6/10, Batch 307/883, Training Loss: 0.4907
Epoch 6/10, Batch 308/883, Training Loss: 0.5901
Epoch 6/10, Batch 309/883, Training Loss: 0.5704
Epoch 6/10, Batch 310/883, Training Loss: 0.7668
Epoch 6/10, Batch 311/883, Training Loss: 0.5555
Epoch 6/10, Batch 312/883, Training Loss: 1.0393
Epoch 6/10, Batch 313/883, Training Loss: 0.6963
Epoch 6/10, Batch 314/883, Training Loss: 0.4917
Epoch 6/10, Batch 315/883, Training Loss: 0.7243
Epoch 6/10, Batch 316/883, Training Loss: 0.6008
Epoch 6/10, Batch 317/883, Training Loss: 0.7465
Epoch 6/10, Batch 318/883, Training Loss: 0.7094
Epoch 6/10, Batch 319/883, Training Loss: 0.6999
Epoch 6/10, Batch 320/883, Training Loss: 0.5938
Epoch 6/10, Batch 321/883, Training Loss: 0.4591
Epoch 6/10, Batch 322/883, Training Loss: 0.6114
Epoch 6/10, Batch 323/883, Training Loss: 0.9261
Epoch 6/10, Batch 324/883, Training Loss: 0.3547
Epoch 6/10, Batch 325/883, Training Loss: 0.5748
Epoch 6/10, Batch 326/883, Training Loss: 1.3027
Epoch 6/10, Batch 327/883, Training Loss: 0.4870
Epoch 6/10, Batch 328/883, Training Loss: 0.8001
Epoch 6/10, Batch 329/883, Training Loss: 0.8179
Epoch 6/10, Batch 330/883, Training Loss: 0.9325
Epoch 6/10, Batch 331/883, Training Loss: 0.5305
Epoch 6/10, Batch 332/883, Training Loss: 0.5020
Epoch 6/10, Batch 333/883, Training Loss: 0.8495
Epoch 6/10, Batch 334/883, Training Loss: 0.7382
Epoch 6/10, Batch 335/883, Training Loss: 0.5602
Epoch 6/10, Batch 336/883, Training Loss: 0.6749
Epoch 6/10, Batch 337/883, Training Loss: 1.2200
Epoch 6/10, Batch 338/883, Training Loss: 0.3743
Epoch 6/10, Batch 339/883, Training Loss: 0.4619
Epoch 6/10, Batch 340/883, Training Loss: 0.5331
Epoch 6/10, Batch 341/883, Training Loss: 0.5336
Epoch 6/10, Batch 342/883, Training Loss: 0.4400
Epoch 6/10, Batch 343/883, Training Loss: 0.6270
Epoch 6/10, Batch 344/883, Training Loss: 0.5100
Epoch 6/10, Batch 345/883, Training Loss: 0.5002
Epoch 6/10, Batch 346/883, Training Loss: 0.6745
Epoch 6/10, Batch 347/883, Training Loss: 0.7058
Epoch 6/10, Batch 348/883, Training Loss: 0.8112
Epoch 6/10, Batch 349/883, Training Loss: 0.7382
Epoch 6/10, Batch 350/883, Training Loss: 0.8429
Epoch 6/10, Batch 351/883, Training Loss: 0.4043
Epoch 6/10, Batch 352/883, Training Loss: 0.5125
Epoch 6/10, Batch 353/883, Training Loss: 0.6538
Epoch 6/10, Batch 354/883, Training Loss: 0.6736
Epoch 6/10, Batch 355/883, Training Loss: 0.4035
Epoch 6/10, Batch 356/883, Training Loss: 0.8441
Epoch 6/10, Batch 357/883, Training Loss: 0.5408
Epoch 6/10, Batch 358/883, Training Loss: 0.6833
Epoch 6/10, Batch 359/883, Training Loss: 0.3463
Epoch 6/10, Batch 360/883, Training Loss: 0.4434
Epoch 6/10, Batch 361/883, Training Loss: 0.7313
Epoch 6/10, Batch 362/883, Training Loss: 0.7898
Epoch 6/10, Batch 363/883, Training Loss: 0.7003
Epoch 6/10, Batch 364/883, Training Loss: 0.4831
Epoch 6/10, Batch 365/883, Training Loss: 0.5177
Epoch 6/10, Batch 366/883, Training Loss: 0.6309
Epoch 6/10, Batch 367/883, Training Loss: 0.5452
Epoch 6/10, Batch 368/883, Training Loss: 0.4356
Epoch 6/10, Batch 369/883, Training Loss: 0.9225
Epoch 6/10, Batch 370/883, Training Loss: 0.8807
Epoch 6/10, Batch 371/883, Training Loss: 0.4926
Epoch 6/10, Batch 372/883, Training Loss: 0.6712
Epoch 6/10, Batch 373/883, Training Loss: 0.6884
Epoch 6/10, Batch 374/883, Training Loss: 0.5876
Epoch 6/10, Batch 375/883, Training Loss: 0.7366
Epoch 6/10, Batch 376/883, Training Loss: 0.8579
Epoch 6/10, Batch 377/883, Training Loss: 0.5625
Epoch 6/10, Batch 378/883, Training Loss: 0.5922
Epoch 6/10, Batch 379/883, Training Loss: 0.4614
Epoch 6/10, Batch 380/883, Training Loss: 0.7667
Epoch 6/10, Batch 381/883, Training Loss: 0.5303
Epoch 6/10, Batch 382/883, Training Loss: 0.7343
Epoch 6/10, Batch 383/883, Training Loss: 0.6754
Epoch 6/10, Batch 384/883, Training Loss: 0.6778
Epoch 6/10, Batch 385/883, Training Loss: 0.4864
Epoch 6/10, Batch 386/883, Training Loss: 0.6068
Epoch 6/10, Batch 387/883, Training Loss: 0.6684
Epoch 6/10, Batch 388/883, Training Loss: 0.8063
Epoch 6/10, Batch 389/883, Training Loss: 0.6745
Epoch 6/10, Batch 390/883, Training Loss: 0.8269
Epoch 6/10, Batch 391/883, Training Loss: 0.4932
Epoch 6/10, Batch 392/883, Training Loss: 0.4629
Epoch 6/10, Batch 393/883, Training Loss: 0.6613
Epoch 6/10, Batch 394/883, Training Loss: 0.7775
Epoch 6/10, Batch 395/883, Training Loss: 0.6805
Epoch 6/10, Batch 396/883, Training Loss: 0.8536
Epoch 6/10, Batch 397/883, Training Loss: 1.1177
Epoch 6/10, Batch 398/883, Training Loss: 1.1471
Epoch 6/10, Batch 399/883, Training Loss: 0.7821
Epoch 6/10, Batch 400/883, Training Loss: 0.5698
Epoch 6/10, Batch 401/883, Training Loss: 0.6427
Epoch 6/10, Batch 402/883, Training Loss: 0.6319
Epoch 6/10, Batch 403/883, Training Loss: 0.5662
Epoch 6/10, Batch 404/883, Training Loss: 0.7809
Epoch 6/10, Batch 405/883, Training Loss: 0.6185
Epoch 6/10, Batch 406/883, Training Loss: 0.9065
Epoch 6/10, Batch 407/883, Training Loss: 0.4405
Epoch 6/10, Batch 408/883, Training Loss: 0.7481
Epoch 6/10, Batch 409/883, Training Loss: 0.9805
Epoch 6/10, Batch 410/883, Training Loss: 0.6134
Epoch 6/10, Batch 411/883, Training Loss: 0.5472
Epoch 6/10, Batch 412/883, Training Loss: 0.6348
Epoch 6/10, Batch 413/883, Training Loss: 0.6297
Epoch 6/10, Batch 414/883, Training Loss: 0.8036
Epoch 6/10, Batch 415/883, Training Loss: 0.8420
Epoch 6/10, Batch 416/883, Training Loss: 1.0131
Epoch 6/10, Batch 417/883, Training Loss: 0.6403
Epoch 6/10, Batch 418/883, Training Loss: 0.4091
Epoch 6/10, Batch 419/883, Training Loss: 0.6284
Epoch 6/10, Batch 420/883, Training Loss: 0.7015
Epoch 6/10, Batch 421/883, Training Loss: 0.7447
Epoch 6/10, Batch 422/883, Training Loss: 0.6149
Epoch 6/10, Batch 423/883, Training Loss: 0.9442
Epoch 6/10, Batch 424/883, Training Loss: 0.9678
Epoch 6/10, Batch 425/883, Training Loss: 0.6684
Epoch 6/10, Batch 426/883, Training Loss: 0.6758
Epoch 6/10, Batch 427/883, Training Loss: 0.5975
Epoch 6/10, Batch 428/883, Training Loss: 0.5212
Epoch 6/10, Batch 429/883, Training Loss: 0.9147
Epoch 6/10, Batch 430/883, Training Loss: 0.5103
Epoch 6/10, Batch 431/883, Training Loss: 0.7549
Epoch 6/10, Batch 432/883, Training Loss: 0.6659
Epoch 6/10, Batch 433/883, Training Loss: 0.6504
Epoch 6/10, Batch 434/883, Training Loss: 1.0625
Epoch 6/10, Batch 435/883, Training Loss: 0.7420
Epoch 6/10, Batch 436/883, Training Loss: 0.8660
Epoch 6/10, Batch 437/883, Training Loss: 0.7534
Epoch 6/10, Batch 438/883, Training Loss: 0.6372
Epoch 6/10, Batch 439/883, Training Loss: 0.8942
Epoch 6/10, Batch 440/883, Training Loss: 0.7073
Epoch 6/10, Batch 441/883, Training Loss: 0.5615
Epoch 6/10, Batch 442/883, Training Loss: 0.7222
Epoch 6/10, Batch 443/883, Training Loss: 0.5370
Epoch 6/10, Batch 444/883, Training Loss: 0.6511
Epoch 6/10, Batch 445/883, Training Loss: 0.9662
Epoch 6/10, Batch 446/883, Training Loss: 0.4684
Epoch 6/10, Batch 447/883, Training Loss: 0.7090
Epoch 6/10, Batch 448/883, Training Loss: 0.4870
Epoch 6/10, Batch 449/883, Training Loss: 0.9267
Epoch 6/10, Batch 450/883, Training Loss: 0.8461
Epoch 6/10, Batch 451/883, Training Loss: 0.8235
Epoch 6/10, Batch 452/883, Training Loss: 0.7640
Epoch 6/10, Batch 453/883, Training Loss: 0.5855
Epoch 6/10, Batch 454/883, Training Loss: 0.5001
Epoch 6/10, Batch 455/883, Training Loss: 0.6253
Epoch 6/10, Batch 456/883, Training Loss: 0.5590
Epoch 6/10, Batch 457/883, Training Loss: 0.6514
Epoch 6/10, Batch 458/883, Training Loss: 0.9995
Epoch 6/10, Batch 459/883, Training Loss: 0.5662
Epoch 6/10, Batch 460/883, Training Loss: 0.9262
Epoch 6/10, Batch 461/883, Training Loss: 1.0933
Epoch 6/10, Batch 462/883, Training Loss: 0.6550
Epoch 6/10, Batch 463/883, Training Loss: 0.9322
Epoch 6/10, Batch 464/883, Training Loss: 0.6623
Epoch 6/10, Batch 465/883, Training Loss: 0.6579
Epoch 6/10, Batch 466/883, Training Loss: 0.6957
Epoch 6/10, Batch 467/883, Training Loss: 0.5519
Epoch 6/10, Batch 468/883, Training Loss: 0.6159
Epoch 6/10, Batch 469/883, Training Loss: 0.6937
Epoch 6/10, Batch 470/883, Training Loss: 0.7550
Epoch 6/10, Batch 471/883, Training Loss: 0.6807
Epoch 6/10, Batch 472/883, Training Loss: 0.5745
Epoch 6/10, Batch 473/883, Training Loss: 0.6221
Epoch 6/10, Batch 474/883, Training Loss: 0.9177
Epoch 6/10, Batch 475/883, Training Loss: 0.6679
Epoch 6/10, Batch 476/883, Training Loss: 0.5683
Epoch 6/10, Batch 477/883, Training Loss: 0.5826
Epoch 6/10, Batch 478/883, Training Loss: 0.4760
Epoch 6/10, Batch 479/883, Training Loss: 0.6675
Epoch 6/10, Batch 480/883, Training Loss: 0.6188
Epoch 6/10, Batch 481/883, Training Loss: 0.9979
Epoch 6/10, Batch 482/883, Training Loss: 0.5538
Epoch 6/10, Batch 483/883, Training Loss: 0.6518
Epoch 6/10, Batch 484/883, Training Loss: 0.5820
Epoch 6/10, Batch 485/883, Training Loss: 0.5610
Epoch 6/10, Batch 486/883, Training Loss: 0.7128
Epoch 6/10, Batch 487/883, Training Loss: 0.6291
Epoch 6/10, Batch 488/883, Training Loss: 0.7901
Epoch 6/10, Batch 489/883, Training Loss: 0.5533
Epoch 6/10, Batch 490/883, Training Loss: 0.7513
Epoch 6/10, Batch 491/883, Training Loss: 0.7066
Epoch 6/10, Batch 492/883, Training Loss: 0.8587
Epoch 6/10, Batch 493/883, Training Loss: 0.6595
Epoch 6/10, Batch 494/883, Training Loss: 0.4833
Epoch 6/10, Batch 495/883, Training Loss: 0.5017
Epoch 6/10, Batch 496/883, Training Loss: 0.5523
Epoch 6/10, Batch 497/883, Training Loss: 0.5621
Epoch 6/10, Batch 498/883, Training Loss: 0.7170
Epoch 6/10, Batch 499/883, Training Loss: 0.6308
Epoch 6/10, Batch 500/883, Training Loss: 0.6160
Epoch 6/10, Batch 501/883, Training Loss: 0.5382
Epoch 6/10, Batch 502/883, Training Loss: 0.4252
Epoch 6/10, Batch 503/883, Training Loss: 0.8007
Epoch 6/10, Batch 504/883, Training Loss: 0.8011
Epoch 6/10, Batch 505/883, Training Loss: 0.9296
Epoch 6/10, Batch 506/883, Training Loss: 0.5999
Epoch 6/10, Batch 507/883, Training Loss: 0.6009
Epoch 6/10, Batch 508/883, Training Loss: 0.7452
Epoch 6/10, Batch 509/883, Training Loss: 0.7000
Epoch 6/10, Batch 510/883, Training Loss: 0.8171
Epoch 6/10, Batch 511/883, Training Loss: 0.7951
Epoch 6/10, Batch 512/883, Training Loss: 0.5691
Epoch 6/10, Batch 513/883, Training Loss: 0.4046
Epoch 6/10, Batch 514/883, Training Loss: 0.6756
Epoch 6/10, Batch 515/883, Training Loss: 0.6283
Epoch 6/10, Batch 516/883, Training Loss: 0.7316
Epoch 6/10, Batch 517/883, Training Loss: 0.5790
Epoch 6/10, Batch 518/883, Training Loss: 0.6452
Epoch 6/10, Batch 519/883, Training Loss: 0.6197
Epoch 6/10, Batch 520/883, Training Loss: 0.7011
Epoch 6/10, Batch 521/883, Training Loss: 0.5631
Epoch 6/10, Batch 522/883, Training Loss: 0.5419
Epoch 6/10, Batch 523/883, Training Loss: 0.6154
Epoch 6/10, Batch 524/883, Training Loss: 0.9197
Epoch 6/10, Batch 525/883, Training Loss: 0.7715
Epoch 6/10, Batch 526/883, Training Loss: 0.5636
Epoch 6/10, Batch 527/883, Training Loss: 0.4859
Epoch 6/10, Batch 528/883, Training Loss: 0.5583
Epoch 6/10, Batch 529/883, Training Loss: 0.6834
Epoch 6/10, Batch 530/883, Training Loss: 0.3327
Epoch 6/10, Batch 531/883, Training Loss: 0.4904
Epoch 6/10, Batch 532/883, Training Loss: 0.6610
Epoch 6/10, Batch 533/883, Training Loss: 1.2374
Epoch 6/10, Batch 534/883, Training Loss: 0.4462
Epoch 6/10, Batch 535/883, Training Loss: 0.7695
Epoch 6/10, Batch 536/883, Training Loss: 0.4621
Epoch 6/10, Batch 537/883, Training Loss: 0.7541
Epoch 6/10, Batch 538/883, Training Loss: 0.6434
Epoch 6/10, Batch 539/883, Training Loss: 0.3962
Epoch 6/10, Batch 540/883, Training Loss: 0.8542
Epoch 6/10, Batch 541/883, Training Loss: 0.3734
Epoch 6/10, Batch 542/883, Training Loss: 0.6921
Epoch 6/10, Batch 543/883, Training Loss: 0.8675
Epoch 6/10, Batch 544/883, Training Loss: 1.0417
Epoch 6/10, Batch 545/883, Training Loss: 0.6403
Epoch 6/10, Batch 546/883, Training Loss: 0.6485
Epoch 6/10, Batch 547/883, Training Loss: 0.7601
Epoch 6/10, Batch 548/883, Training Loss: 0.4849
Epoch 6/10, Batch 549/883, Training Loss: 0.4538
Epoch 6/10, Batch 550/883, Training Loss: 0.6934
Epoch 6/10, Batch 551/883, Training Loss: 0.8001
Epoch 6/10, Batch 552/883, Training Loss: 0.4714
Epoch 6/10, Batch 553/883, Training Loss: 0.6854
Epoch 6/10, Batch 554/883, Training Loss: 0.4882
Epoch 6/10, Batch 555/883, Training Loss: 0.9497
Epoch 6/10, Batch 556/883, Training Loss: 0.7057
Epoch 6/10, Batch 557/883, Training Loss: 0.5036
Epoch 6/10, Batch 558/883, Training Loss: 0.7168
Epoch 6/10, Batch 559/883, Training Loss: 0.6317
Epoch 6/10, Batch 560/883, Training Loss: 0.5904
Epoch 6/10, Batch 561/883, Training Loss: 0.5417
Epoch 6/10, Batch 562/883, Training Loss: 0.5587
Epoch 6/10, Batch 563/883, Training Loss: 0.3474
Epoch 6/10, Batch 564/883, Training Loss: 0.5636
Epoch 6/10, Batch 565/883, Training Loss: 1.1583
Epoch 6/10, Batch 566/883, Training Loss: 0.6598
Epoch 6/10, Batch 567/883, Training Loss: 0.5742
Epoch 6/10, Batch 568/883, Training Loss: 0.5615
Epoch 6/10, Batch 569/883, Training Loss: 0.4509
Epoch 6/10, Batch 570/883, Training Loss: 0.5345
Epoch 6/10, Batch 571/883, Training Loss: 0.5407
Epoch 6/10, Batch 572/883, Training Loss: 0.8057
Epoch 6/10, Batch 573/883, Training Loss: 0.4018
Epoch 6/10, Batch 574/883, Training Loss: 0.5924
Epoch 6/10, Batch 575/883, Training Loss: 0.7156
Epoch 6/10, Batch 576/883, Training Loss: 0.4616
Epoch 6/10, Batch 577/883, Training Loss: 0.6843
Epoch 6/10, Batch 578/883, Training Loss: 0.5049
Epoch 6/10, Batch 579/883, Training Loss: 0.4742
Epoch 6/10, Batch 580/883, Training Loss: 0.8797
Epoch 6/10, Batch 581/883, Training Loss: 0.5965
Epoch 6/10, Batch 582/883, Training Loss: 0.5507
Epoch 6/10, Batch 583/883, Training Loss: 0.3343
Epoch 6/10, Batch 584/883, Training Loss: 1.0787
Epoch 6/10, Batch 585/883, Training Loss: 0.4326
Epoch 6/10, Batch 586/883, Training Loss: 0.7880
Epoch 6/10, Batch 587/883, Training Loss: 0.7328
Epoch 6/10, Batch 588/883, Training Loss: 0.5475
Epoch 6/10, Batch 589/883, Training Loss: 0.7285
Epoch 6/10, Batch 590/883, Training Loss: 0.6560
Epoch 6/10, Batch 591/883, Training Loss: 0.4834
Epoch 6/10, Batch 592/883, Training Loss: 0.5531
Epoch 6/10, Batch 593/883, Training Loss: 0.7235
Epoch 6/10, Batch 594/883, Training Loss: 0.7600
Epoch 6/10, Batch 595/883, Training Loss: 1.2930
Epoch 6/10, Batch 596/883, Training Loss: 0.5497
Epoch 6/10, Batch 597/883, Training Loss: 0.6916
Epoch 6/10, Batch 598/883, Training Loss: 0.5708
Epoch 6/10, Batch 599/883, Training Loss: 0.6086
Epoch 6/10, Batch 600/883, Training Loss: 1.0420
Epoch 6/10, Batch 601/883, Training Loss: 0.6040
Epoch 6/10, Batch 602/883, Training Loss: 0.5122
Epoch 6/10, Batch 603/883, Training Loss: 0.6975
Epoch 6/10, Batch 604/883, Training Loss: 0.6008
Epoch 6/10, Batch 605/883, Training Loss: 0.7226
Epoch 6/10, Batch 606/883, Training Loss: 0.5990
Epoch 6/10, Batch 607/883, Training Loss: 0.5798
Epoch 6/10, Batch 608/883, Training Loss: 0.4485
Epoch 6/10, Batch 609/883, Training Loss: 0.8358
Epoch 6/10, Batch 610/883, Training Loss: 0.4687
Epoch 6/10, Batch 611/883, Training Loss: 0.5941
Epoch 6/10, Batch 612/883, Training Loss: 0.4566
Epoch 6/10, Batch 613/883, Training Loss: 0.6080
Epoch 6/10, Batch 614/883, Training Loss: 0.8654
Epoch 6/10, Batch 615/883, Training Loss: 0.4911
Epoch 6/10, Batch 616/883, Training Loss: 0.6722
Epoch 6/10, Batch 617/883, Training Loss: 0.6005
Epoch 6/10, Batch 618/883, Training Loss: 0.5766
Epoch 6/10, Batch 619/883, Training Loss: 0.8645
Epoch 6/10, Batch 620/883, Training Loss: 0.5262
Epoch 6/10, Batch 621/883, Training Loss: 0.7396
Epoch 6/10, Batch 622/883, Training Loss: 0.6516
Epoch 6/10, Batch 623/883, Training Loss: 0.6461
Epoch 6/10, Batch 624/883, Training Loss: 0.9340
Epoch 6/10, Batch 625/883, Training Loss: 0.4749
Epoch 6/10, Batch 626/883, Training Loss: 0.5910
Epoch 6/10, Batch 627/883, Training Loss: 0.3889
Epoch 6/10, Batch 628/883, Training Loss: 0.9230
Epoch 6/10, Batch 629/883, Training Loss: 0.5779
Epoch 6/10, Batch 630/883, Training Loss: 0.4457
Epoch 6/10, Batch 631/883, Training Loss: 0.7280
Epoch 6/10, Batch 632/883, Training Loss: 0.4778
Epoch 6/10, Batch 633/883, Training Loss: 0.4872
Epoch 6/10, Batch 634/883, Training Loss: 0.4183
Epoch 6/10, Batch 635/883, Training Loss: 0.8506
Epoch 6/10, Batch 636/883, Training Loss: 0.4413
Epoch 6/10, Batch 637/883, Training Loss: 0.6594
Epoch 6/10, Batch 638/883, Training Loss: 0.6352
Epoch 6/10, Batch 639/883, Training Loss: 0.6440
Epoch 6/10, Batch 640/883, Training Loss: 0.3742
Epoch 6/10, Batch 641/883, Training Loss: 0.6997
Epoch 6/10, Batch 642/883, Training Loss: 0.6279
Epoch 6/10, Batch 643/883, Training Loss: 0.5458
Epoch 6/10, Batch 644/883, Training Loss: 0.8651
Epoch 6/10, Batch 645/883, Training Loss: 0.5577
Epoch 6/10, Batch 646/883, Training Loss: 0.4443
Epoch 6/10, Batch 647/883, Training Loss: 0.5051
Epoch 6/10, Batch 648/883, Training Loss: 0.5525
Epoch 6/10, Batch 649/883, Training Loss: 0.3965
Epoch 6/10, Batch 650/883, Training Loss: 0.4748
Epoch 6/10, Batch 651/883, Training Loss: 0.6927
Epoch 6/10, Batch 652/883, Training Loss: 0.6140
Epoch 6/10, Batch 653/883, Training Loss: 0.5459
Epoch 6/10, Batch 654/883, Training Loss: 0.8193
Epoch 6/10, Batch 655/883, Training Loss: 0.7851
Epoch 6/10, Batch 656/883, Training Loss: 0.7592
Epoch 6/10, Batch 657/883, Training Loss: 0.6625
Epoch 6/10, Batch 658/883, Training Loss: 0.7098
Epoch 6/10, Batch 659/883, Training Loss: 0.7210
Epoch 6/10, Batch 660/883, Training Loss: 0.6771
Epoch 6/10, Batch 661/883, Training Loss: 0.7125
Epoch 6/10, Batch 662/883, Training Loss: 0.6101
Epoch 6/10, Batch 663/883, Training Loss: 0.9160
Epoch 6/10, Batch 664/883, Training Loss: 0.4933
Epoch 6/10, Batch 665/883, Training Loss: 0.5359
Epoch 6/10, Batch 666/883, Training Loss: 0.4347
Epoch 6/10, Batch 667/883, Training Loss: 0.6111
Epoch 6/10, Batch 668/883, Training Loss: 0.4743
Epoch 6/10, Batch 669/883, Training Loss: 0.5147
Epoch 6/10, Batch 670/883, Training Loss: 0.7140
Epoch 6/10, Batch 671/883, Training Loss: 0.7144
Epoch 6/10, Batch 672/883, Training Loss: 1.1258
Epoch 6/10, Batch 673/883, Training Loss: 0.5696
Epoch 6/10, Batch 674/883, Training Loss: 0.5470
Epoch 6/10, Batch 675/883, Training Loss: 0.4945
Epoch 6/10, Batch 676/883, Training Loss: 0.6039
Epoch 6/10, Batch 677/883, Training Loss: 0.5130
Epoch 6/10, Batch 678/883, Training Loss: 0.4198
Epoch 6/10, Batch 679/883, Training Loss: 0.6803
Epoch 6/10, Batch 680/883, Training Loss: 0.4111
Epoch 6/10, Batch 681/883, Training Loss: 0.7573
Epoch 6/10, Batch 682/883, Training Loss: 0.7542
Epoch 6/10, Batch 683/883, Training Loss: 1.0399
Epoch 6/10, Batch 684/883, Training Loss: 0.5883
Epoch 6/10, Batch 685/883, Training Loss: 0.4946
Epoch 6/10, Batch 686/883, Training Loss: 0.6239
Epoch 6/10, Batch 687/883, Training Loss: 0.6796
Epoch 6/10, Batch 688/883, Training Loss: 0.5454
Epoch 6/10, Batch 689/883, Training Loss: 0.7154
Epoch 6/10, Batch 690/883, Training Loss: 0.6544
Epoch 6/10, Batch 691/883, Training Loss: 0.8139
Epoch 6/10, Batch 692/883, Training Loss: 0.4448
Epoch 6/10, Batch 693/883, Training Loss: 0.5903
Epoch 6/10, Batch 694/883, Training Loss: 0.4347
Epoch 6/10, Batch 695/883, Training Loss: 0.4985
Epoch 6/10, Batch 696/883, Training Loss: 0.2735
Epoch 6/10, Batch 697/883, Training Loss: 0.7117
Epoch 6/10, Batch 698/883, Training Loss: 0.8474
Epoch 6/10, Batch 699/883, Training Loss: 0.3860
Epoch 6/10, Batch 700/883, Training Loss: 0.3187
Epoch 6/10, Batch 701/883, Training Loss: 0.8526
Epoch 6/10, Batch 702/883, Training Loss: 0.5291
Epoch 6/10, Batch 703/883, Training Loss: 1.3159
Epoch 6/10, Batch 704/883, Training Loss: 0.4239
Epoch 6/10, Batch 705/883, Training Loss: 0.5615
Epoch 6/10, Batch 706/883, Training Loss: 0.7566
Epoch 6/10, Batch 707/883, Training Loss: 0.3421
Epoch 6/10, Batch 708/883, Training Loss: 0.4530
Epoch 6/10, Batch 709/883, Training Loss: 0.7307
Epoch 6/10, Batch 710/883, Training Loss: 0.5451
Epoch 6/10, Batch 711/883, Training Loss: 0.4344
Epoch 6/10, Batch 712/883, Training Loss: 0.4479
Epoch 6/10, Batch 713/883, Training Loss: 0.5836
Epoch 6/10, Batch 714/883, Training Loss: 0.8026
Epoch 6/10, Batch 715/883, Training Loss: 0.7270
Epoch 6/10, Batch 716/883, Training Loss: 0.3616
Epoch 6/10, Batch 717/883, Training Loss: 0.6481
Epoch 6/10, Batch 718/883, Training Loss: 0.6870
Epoch 6/10, Batch 719/883, Training Loss: 0.7206
Epoch 6/10, Batch 720/883, Training Loss: 0.5726
Epoch 6/10, Batch 721/883, Training Loss: 0.8160
Epoch 6/10, Batch 722/883, Training Loss: 0.7679
Epoch 6/10, Batch 723/883, Training Loss: 0.8212
Epoch 6/10, Batch 724/883, Training Loss: 0.5751
Epoch 6/10, Batch 725/883, Training Loss: 0.4698
Epoch 6/10, Batch 726/883, Training Loss: 0.5445
Epoch 6/10, Batch 727/883, Training Loss: 0.4276
Epoch 6/10, Batch 728/883, Training Loss: 0.6741
Epoch 6/10, Batch 729/883, Training Loss: 0.6014
Epoch 6/10, Batch 730/883, Training Loss: 0.6879
Epoch 6/10, Batch 731/883, Training Loss: 0.7639
Epoch 6/10, Batch 732/883, Training Loss: 0.3981
Epoch 6/10, Batch 733/883, Training Loss: 0.6139
Epoch 6/10, Batch 734/883, Training Loss: 0.9267
Epoch 6/10, Batch 735/883, Training Loss: 0.7084
Epoch 6/10, Batch 736/883, Training Loss: 0.6655
Epoch 6/10, Batch 737/883, Training Loss: 0.4183
Epoch 6/10, Batch 738/883, Training Loss: 0.6183
Epoch 6/10, Batch 739/883, Training Loss: 0.5181
Epoch 6/10, Batch 740/883, Training Loss: 0.9239
Epoch 6/10, Batch 741/883, Training Loss: 0.5655
Epoch 6/10, Batch 742/883, Training Loss: 0.7653
Epoch 6/10, Batch 743/883, Training Loss: 0.6347
Epoch 6/10, Batch 744/883, Training Loss: 0.3776
Epoch 6/10, Batch 745/883, Training Loss: 0.6385
Epoch 6/10, Batch 746/883, Training Loss: 0.7809
Epoch 6/10, Batch 747/883, Training Loss: 0.4845
Epoch 6/10, Batch 748/883, Training Loss: 0.7582
Epoch 6/10, Batch 749/883, Training Loss: 0.8085
Epoch 6/10, Batch 750/883, Training Loss: 0.6888
Epoch 6/10, Batch 751/883, Training Loss: 0.4330
Epoch 6/10, Batch 752/883, Training Loss: 0.7232
Epoch 6/10, Batch 753/883, Training Loss: 0.5899
Epoch 6/10, Batch 754/883, Training Loss: 0.9699
Epoch 6/10, Batch 755/883, Training Loss: 0.6022
Epoch 6/10, Batch 756/883, Training Loss: 0.6775
Epoch 6/10, Batch 757/883, Training Loss: 0.3467
Epoch 6/10, Batch 758/883, Training Loss: 0.6362
Epoch 6/10, Batch 759/883, Training Loss: 0.5333
Epoch 6/10, Batch 760/883, Training Loss: 0.5831
Epoch 6/10, Batch 761/883, Training Loss: 1.3835
Epoch 6/10, Batch 762/883, Training Loss: 0.6512
Epoch 6/10, Batch 763/883, Training Loss: 0.7551
Epoch 6/10, Batch 764/883, Training Loss: 0.7291
Epoch 6/10, Batch 765/883, Training Loss: 0.5289
Epoch 6/10, Batch 766/883, Training Loss: 0.7707
Epoch 6/10, Batch 767/883, Training Loss: 0.6477
Epoch 6/10, Batch 768/883, Training Loss: 0.5794
Epoch 6/10, Batch 769/883, Training Loss: 0.9175
Epoch 6/10, Batch 770/883, Training Loss: 0.6499
Epoch 6/10, Batch 771/883, Training Loss: 0.7781
Epoch 6/10, Batch 772/883, Training Loss: 0.5302
Epoch 6/10, Batch 773/883, Training Loss: 0.7369
Epoch 6/10, Batch 774/883, Training Loss: 0.8162
Epoch 6/10, Batch 775/883, Training Loss: 0.5833
Epoch 6/10, Batch 776/883, Training Loss: 0.7835
Epoch 6/10, Batch 777/883, Training Loss: 0.4641
Epoch 6/10, Batch 778/883, Training Loss: 0.6164
Epoch 6/10, Batch 779/883, Training Loss: 0.9037
Epoch 6/10, Batch 780/883, Training Loss: 0.4686
Epoch 6/10, Batch 781/883, Training Loss: 0.7197
Epoch 6/10, Batch 782/883, Training Loss: 0.4555
Epoch 6/10, Batch 783/883, Training Loss: 0.5780
Epoch 6/10, Batch 784/883, Training Loss: 0.6753
Epoch 6/10, Batch 785/883, Training Loss: 0.4432
Epoch 6/10, Batch 786/883, Training Loss: 0.4811
Epoch 6/10, Batch 787/883, Training Loss: 0.7435
Epoch 6/10, Batch 788/883, Training Loss: 0.7622
Epoch 6/10, Batch 789/883, Training Loss: 0.4488
Epoch 6/10, Batch 790/883, Training Loss: 0.6040
Epoch 6/10, Batch 791/883, Training Loss: 0.4855
Epoch 6/10, Batch 792/883, Training Loss: 0.4871
Epoch 6/10, Batch 793/883, Training Loss: 0.4851
Epoch 6/10, Batch 794/883, Training Loss: 0.4992
Epoch 6/10, Batch 795/883, Training Loss: 1.1279
Epoch 6/10, Batch 796/883, Training Loss: 0.7372
Epoch 6/10, Batch 797/883, Training Loss: 0.6710
Epoch 6/10, Batch 798/883, Training Loss: 0.7017
Epoch 6/10, Batch 799/883, Training Loss: 0.3434
Epoch 6/10, Batch 800/883, Training Loss: 0.9366
Epoch 6/10, Batch 801/883, Training Loss: 0.5262
Epoch 6/10, Batch 802/883, Training Loss: 0.4305
Epoch 6/10, Batch 803/883, Training Loss: 0.6543
Epoch 6/10, Batch 804/883, Training Loss: 0.6378
Epoch 6/10, Batch 805/883, Training Loss: 0.7720
Epoch 6/10, Batch 806/883, Training Loss: 0.6158
Epoch 6/10, Batch 807/883, Training Loss: 0.7614
Epoch 6/10, Batch 808/883, Training Loss: 1.1170
Epoch 6/10, Batch 809/883, Training Loss: 0.7412
Epoch 6/10, Batch 810/883, Training Loss: 1.0559
Epoch 6/10, Batch 811/883, Training Loss: 0.5573
Epoch 6/10, Batch 812/883, Training Loss: 0.3713
Epoch 6/10, Batch 813/883, Training Loss: 0.7714
Epoch 6/10, Batch 814/883, Training Loss: 0.4470
Epoch 6/10, Batch 815/883, Training Loss: 0.4943
Epoch 6/10, Batch 816/883, Training Loss: 0.5295
Epoch 6/10, Batch 817/883, Training Loss: 0.9972
Epoch 6/10, Batch 818/883, Training Loss: 0.4673
Epoch 6/10, Batch 819/883, Training Loss: 0.5801
Epoch 6/10, Batch 820/883, Training Loss: 0.5003
Epoch 6/10, Batch 821/883, Training Loss: 0.7265
Epoch 6/10, Batch 822/883, Training Loss: 0.4547
Epoch 6/10, Batch 823/883, Training Loss: 0.8128
Epoch 6/10, Batch 824/883, Training Loss: 0.6053
Epoch 6/10, Batch 825/883, Training Loss: 0.5810
Epoch 6/10, Batch 826/883, Training Loss: 0.5428
Epoch 6/10, Batch 827/883, Training Loss: 0.5765
Epoch 6/10, Batch 828/883, Training Loss: 0.7067
Epoch 6/10, Batch 829/883, Training Loss: 0.6017
Epoch 6/10, Batch 830/883, Training Loss: 0.5117
Epoch 6/10, Batch 831/883, Training Loss: 0.6765
Epoch 6/10, Batch 832/883, Training Loss: 0.6045
Epoch 6/10, Batch 833/883, Training Loss: 0.6996
Epoch 6/10, Batch 834/883, Training Loss: 0.4805
Epoch 6/10, Batch 835/883, Training Loss: 0.6554
Epoch 6/10, Batch 836/883, Training Loss: 0.6280
Epoch 6/10, Batch 837/883, Training Loss: 0.4951
Epoch 6/10, Batch 838/883, Training Loss: 0.8174
Epoch 6/10, Batch 839/883, Training Loss: 0.9376
Epoch 6/10, Batch 840/883, Training Loss: 0.5366
Epoch 6/10, Batch 841/883, Training Loss: 0.3787
Epoch 6/10, Batch 842/883, Training Loss: 0.7829
Epoch 6/10, Batch 843/883, Training Loss: 0.5891
Epoch 6/10, Batch 844/883, Training Loss: 0.5648
Epoch 6/10, Batch 845/883, Training Loss: 0.5086
Epoch 6/10, Batch 846/883, Training Loss: 0.6612
Epoch 6/10, Batch 847/883, Training Loss: 0.4770
Epoch 6/10, Batch 848/883, Training Loss: 0.5731
Epoch 6/10, Batch 849/883, Training Loss: 0.6169
Epoch 6/10, Batch 850/883, Training Loss: 0.6650
Epoch 6/10, Batch 851/883, Training Loss: 0.9201
Epoch 6/10, Batch 852/883, Training Loss: 0.4189
Epoch 6/10, Batch 853/883, Training Loss: 0.5034
Epoch 6/10, Batch 854/883, Training Loss: 0.7683
Epoch 6/10, Batch 855/883, Training Loss: 0.9364
Epoch 6/10, Batch 856/883, Training Loss: 0.4828
Epoch 6/10, Batch 857/883, Training Loss: 0.7020
Epoch 6/10, Batch 858/883, Training Loss: 0.5653
Epoch 6/10, Batch 859/883, Training Loss: 1.2307
Epoch 6/10, Batch 860/883, Training Loss: 0.7836
Epoch 6/10, Batch 861/883, Training Loss: 0.7376
Epoch 6/10, Batch 862/883, Training Loss: 0.9089
Epoch 6/10, Batch 863/883, Training Loss: 0.4844
Epoch 6/10, Batch 864/883, Training Loss: 0.6291
Epoch 6/10, Batch 865/883, Training Loss: 0.6447
Epoch 6/10, Batch 866/883, Training Loss: 0.7623
Epoch 6/10, Batch 867/883, Training Loss: 0.8160
Epoch 6/10, Batch 868/883, Training Loss: 0.5743
Epoch 6/10, Batch 869/883, Training Loss: 0.7803
Epoch 6/10, Batch 870/883, Training Loss: 0.5826
Epoch 6/10, Batch 871/883, Training Loss: 0.6865
Epoch 6/10, Batch 872/883, Training Loss: 0.7493
Epoch 6/10, Batch 873/883, Training Loss: 0.6184
Epoch 6/10, Batch 874/883, Training Loss: 0.5733
Epoch 6/10, Batch 875/883, Training Loss: 1.0846
Epoch 6/10, Batch 876/883, Training Loss: 0.7391
Epoch 6/10, Batch 877/883, Training Loss: 0.5254
Epoch 6/10, Batch 878/883, Training Loss: 0.5997
Epoch 6/10, Batch 879/883, Training Loss: 0.8316
Epoch 6/10, Batch 880/883, Training Loss: 0.5958
Epoch 6/10, Batch 881/883, Training Loss: 0.8933
Epoch 6/10, Batch 882/883, Training Loss: 0.8164
Epoch 6/10, Batch 883/883, Training Loss: 0.8744
Epoch 6/10, Training Loss: 0.6598, Validation Loss: 0.6413, Validation Accuracy: 0.7167
Epoch 7/10, Batch 1/883, Training Loss: 0.8161
Epoch 7/10, Batch 2/883, Training Loss: 0.5776
Epoch 7/10, Batch 3/883, Training Loss: 0.7187
Epoch 7/10, Batch 4/883, Training Loss: 0.3607
Epoch 7/10, Batch 5/883, Training Loss: 0.6935
Epoch 7/10, Batch 6/883, Training Loss: 0.5935
Epoch 7/10, Batch 7/883, Training Loss: 0.7840
Epoch 7/10, Batch 8/883, Training Loss: 0.7579
Epoch 7/10, Batch 9/883, Training Loss: 0.6014
Epoch 7/10, Batch 10/883, Training Loss: 0.6255
Epoch 7/10, Batch 11/883, Training Loss: 0.4426
Epoch 7/10, Batch 12/883, Training Loss: 0.6181
Epoch 7/10, Batch 13/883, Training Loss: 0.7579
Epoch 7/10, Batch 14/883, Training Loss: 0.5455
Epoch 7/10, Batch 15/883, Training Loss: 0.6183
Epoch 7/10, Batch 16/883, Training Loss: 0.5273
Epoch 7/10, Batch 17/883, Training Loss: 0.7578
Epoch 7/10, Batch 18/883, Training Loss: 0.6291
Epoch 7/10, Batch 19/883, Training Loss: 0.5280
Epoch 7/10, Batch 20/883, Training Loss: 0.5758
Epoch 7/10, Batch 21/883, Training Loss: 0.5076
Epoch 7/10, Batch 22/883, Training Loss: 0.6122
Epoch 7/10, Batch 23/883, Training Loss: 0.5807
Epoch 7/10, Batch 24/883, Training Loss: 0.9291
Epoch 7/10, Batch 25/883, Training Loss: 0.5047
Epoch 7/10, Batch 26/883, Training Loss: 0.7037
Epoch 7/10, Batch 27/883, Training Loss: 0.8831
Epoch 7/10, Batch 28/883, Training Loss: 0.5643
Epoch 7/10, Batch 29/883, Training Loss: 0.6062
Epoch 7/10, Batch 30/883, Training Loss: 0.7318
Epoch 7/10, Batch 31/883, Training Loss: 0.4704
Epoch 7/10, Batch 32/883, Training Loss: 0.4118
Epoch 7/10, Batch 33/883, Training Loss: 0.5329
Epoch 7/10, Batch 34/883, Training Loss: 0.6442
Epoch 7/10, Batch 35/883, Training Loss: 0.9146
Epoch 7/10, Batch 36/883, Training Loss: 0.7251
Epoch 7/10, Batch 37/883, Training Loss: 0.3592
Epoch 7/10, Batch 38/883, Training Loss: 0.5924
Epoch 7/10, Batch 39/883, Training Loss: 0.4175
Epoch 7/10, Batch 40/883, Training Loss: 0.5930
Epoch 7/10, Batch 41/883, Training Loss: 0.6904
Epoch 7/10, Batch 42/883, Training Loss: 0.6176
Epoch 7/10, Batch 43/883, Training Loss: 0.5366
Epoch 7/10, Batch 44/883, Training Loss: 0.9140
Epoch 7/10, Batch 45/883, Training Loss: 0.8084
Epoch 7/10, Batch 46/883, Training Loss: 0.6608
Epoch 7/10, Batch 47/883, Training Loss: 0.6042
Epoch 7/10, Batch 48/883, Training Loss: 0.5099
Epoch 7/10, Batch 49/883, Training Loss: 0.7983
Epoch 7/10, Batch 50/883, Training Loss: 0.6379
Epoch 7/10, Batch 51/883, Training Loss: 0.6682
Epoch 7/10, Batch 52/883, Training Loss: 0.4656
Epoch 7/10, Batch 53/883, Training Loss: 0.5772
Epoch 7/10, Batch 54/883, Training Loss: 0.5616
Epoch 7/10, Batch 55/883, Training Loss: 0.4104
Epoch 7/10, Batch 56/883, Training Loss: 0.8854
Epoch 7/10, Batch 57/883, Training Loss: 0.3720
Epoch 7/10, Batch 58/883, Training Loss: 0.7754
Epoch 7/10, Batch 59/883, Training Loss: 0.7489
Epoch 7/10, Batch 60/883, Training Loss: 0.6256
Epoch 7/10, Batch 61/883, Training Loss: 0.5355
Epoch 7/10, Batch 62/883, Training Loss: 0.4116
Epoch 7/10, Batch 63/883, Training Loss: 0.7501
Epoch 7/10, Batch 64/883, Training Loss: 0.6103
Epoch 7/10, Batch 65/883, Training Loss: 0.7377
Epoch 7/10, Batch 66/883, Training Loss: 1.1253
Epoch 7/10, Batch 67/883, Training Loss: 0.7224
Epoch 7/10, Batch 68/883, Training Loss: 0.5846
Epoch 7/10, Batch 69/883, Training Loss: 0.5588
Epoch 7/10, Batch 70/883, Training Loss: 0.7337
Epoch 7/10, Batch 71/883, Training Loss: 0.4700
Epoch 7/10, Batch 72/883, Training Loss: 0.6608
Epoch 7/10, Batch 73/883, Training Loss: 0.5196
Epoch 7/10, Batch 74/883, Training Loss: 0.5529
Epoch 7/10, Batch 75/883, Training Loss: 0.7781
Epoch 7/10, Batch 76/883, Training Loss: 0.4477
Epoch 7/10, Batch 77/883, Training Loss: 0.7899
Epoch 7/10, Batch 78/883, Training Loss: 0.9046
Epoch 7/10, Batch 79/883, Training Loss: 0.5527
Epoch 7/10, Batch 80/883, Training Loss: 0.3057
Epoch 7/10, Batch 81/883, Training Loss: 0.5746
Epoch 7/10, Batch 82/883, Training Loss: 0.5599
Epoch 7/10, Batch 83/883, Training Loss: 0.8761
Epoch 7/10, Batch 84/883, Training Loss: 0.7567
Epoch 7/10, Batch 85/883, Training Loss: 0.6418
Epoch 7/10, Batch 86/883, Training Loss: 0.5291
Epoch 7/10, Batch 87/883, Training Loss: 0.4005
Epoch 7/10, Batch 88/883, Training Loss: 0.4306
Epoch 7/10, Batch 89/883, Training Loss: 0.3476
Epoch 7/10, Batch 90/883, Training Loss: 0.5284
Epoch 7/10, Batch 91/883, Training Loss: 0.5842
Epoch 7/10, Batch 92/883, Training Loss: 0.6865
Epoch 7/10, Batch 93/883, Training Loss: 0.6732
Epoch 7/10, Batch 94/883, Training Loss: 0.6821
Epoch 7/10, Batch 95/883, Training Loss: 0.3975
Epoch 7/10, Batch 96/883, Training Loss: 0.6495
Epoch 7/10, Batch 97/883, Training Loss: 0.8955
Epoch 7/10, Batch 98/883, Training Loss: 0.3591
Epoch 7/10, Batch 99/883, Training Loss: 0.5924
Epoch 7/10, Batch 100/883, Training Loss: 0.7575
Epoch 7/10, Batch 101/883, Training Loss: 0.6349
Epoch 7/10, Batch 102/883, Training Loss: 0.4791
Epoch 7/10, Batch 103/883, Training Loss: 0.5249
Epoch 7/10, Batch 104/883, Training Loss: 0.6792
Epoch 7/10, Batch 105/883, Training Loss: 0.6139
Epoch 7/10, Batch 106/883, Training Loss: 0.6825
Epoch 7/10, Batch 107/883, Training Loss: 0.4034
Epoch 7/10, Batch 108/883, Training Loss: 0.6896
Epoch 7/10, Batch 109/883, Training Loss: 0.7146
Epoch 7/10, Batch 110/883, Training Loss: 0.4988
Epoch 7/10, Batch 111/883, Training Loss: 0.6489
Epoch 7/10, Batch 112/883, Training Loss: 0.4913
Epoch 7/10, Batch 113/883, Training Loss: 0.7859
Epoch 7/10, Batch 114/883, Training Loss: 0.4707
Epoch 7/10, Batch 115/883, Training Loss: 0.5172
Epoch 7/10, Batch 116/883, Training Loss: 0.5450
Epoch 7/10, Batch 117/883, Training Loss: 0.4591
Epoch 7/10, Batch 118/883, Training Loss: 0.5305
Epoch 7/10, Batch 119/883, Training Loss: 0.5029
Epoch 7/10, Batch 120/883, Training Loss: 0.5960
Epoch 7/10, Batch 121/883, Training Loss: 0.7929
Epoch 7/10, Batch 122/883, Training Loss: 0.6192
Epoch 7/10, Batch 123/883, Training Loss: 0.4187
Epoch 7/10, Batch 124/883, Training Loss: 0.6938
Epoch 7/10, Batch 125/883, Training Loss: 1.0013
Epoch 7/10, Batch 126/883, Training Loss: 0.6225
Epoch 7/10, Batch 127/883, Training Loss: 0.7495
Epoch 7/10, Batch 128/883, Training Loss: 0.5659
Epoch 7/10, Batch 129/883, Training Loss: 0.8076
Epoch 7/10, Batch 130/883, Training Loss: 0.3792
Epoch 7/10, Batch 131/883, Training Loss: 0.3311
Epoch 7/10, Batch 132/883, Training Loss: 0.3811
Epoch 7/10, Batch 133/883, Training Loss: 0.8356
Epoch 7/10, Batch 134/883, Training Loss: 0.4511
Epoch 7/10, Batch 135/883, Training Loss: 0.6699
Epoch 7/10, Batch 136/883, Training Loss: 0.3573
Epoch 7/10, Batch 137/883, Training Loss: 0.5731
Epoch 7/10, Batch 138/883, Training Loss: 0.5789
Epoch 7/10, Batch 139/883, Training Loss: 0.8192
Epoch 7/10, Batch 140/883, Training Loss: 0.5627
Epoch 7/10, Batch 141/883, Training Loss: 0.5364
Epoch 7/10, Batch 142/883, Training Loss: 0.7250
Epoch 7/10, Batch 143/883, Training Loss: 0.7644
Epoch 7/10, Batch 144/883, Training Loss: 1.1216
Epoch 7/10, Batch 145/883, Training Loss: 0.7832
Epoch 7/10, Batch 146/883, Training Loss: 0.4127
Epoch 7/10, Batch 147/883, Training Loss: 0.4719
Epoch 7/10, Batch 148/883, Training Loss: 0.5998
Epoch 7/10, Batch 149/883, Training Loss: 0.5638
Epoch 7/10, Batch 150/883, Training Loss: 0.6610
Epoch 7/10, Batch 151/883, Training Loss: 0.7642
Epoch 7/10, Batch 152/883, Training Loss: 0.7361
Epoch 7/10, Batch 153/883, Training Loss: 0.4215
Epoch 7/10, Batch 154/883, Training Loss: 1.0638
Epoch 7/10, Batch 155/883, Training Loss: 0.5832
Epoch 7/10, Batch 156/883, Training Loss: 0.4568
Epoch 7/10, Batch 157/883, Training Loss: 0.5807
Epoch 7/10, Batch 158/883, Training Loss: 0.5742
Epoch 7/10, Batch 159/883, Training Loss: 0.7638
Epoch 7/10, Batch 160/883, Training Loss: 0.5693
Epoch 7/10, Batch 161/883, Training Loss: 0.7351
Epoch 7/10, Batch 162/883, Training Loss: 0.4938
Epoch 7/10, Batch 163/883, Training Loss: 0.5595
Epoch 7/10, Batch 164/883, Training Loss: 0.4999
Epoch 7/10, Batch 165/883, Training Loss: 0.5307
Epoch 7/10, Batch 166/883, Training Loss: 0.5760
Epoch 7/10, Batch 167/883, Training Loss: 0.4734
Epoch 7/10, Batch 168/883, Training Loss: 0.5409
Epoch 7/10, Batch 169/883, Training Loss: 0.7380
Epoch 7/10, Batch 170/883, Training Loss: 0.5380
Epoch 7/10, Batch 171/883, Training Loss: 0.7294
Epoch 7/10, Batch 172/883, Training Loss: 0.5310
Epoch 7/10, Batch 173/883, Training Loss: 0.7557
Epoch 7/10, Batch 174/883, Training Loss: 0.4788
Epoch 7/10, Batch 175/883, Training Loss: 0.5956
Epoch 7/10, Batch 176/883, Training Loss: 0.4089
Epoch 7/10, Batch 177/883, Training Loss: 1.0801
Epoch 7/10, Batch 178/883, Training Loss: 0.4999
Epoch 7/10, Batch 179/883, Training Loss: 0.5275
Epoch 7/10, Batch 180/883, Training Loss: 0.8916
Epoch 7/10, Batch 181/883, Training Loss: 0.8346
Epoch 7/10, Batch 182/883, Training Loss: 0.6300
Epoch 7/10, Batch 183/883, Training Loss: 1.0860
Epoch 7/10, Batch 184/883, Training Loss: 0.5760
Epoch 7/10, Batch 185/883, Training Loss: 0.7499
Epoch 7/10, Batch 186/883, Training Loss: 0.7810
Epoch 7/10, Batch 187/883, Training Loss: 0.7911
Epoch 7/10, Batch 188/883, Training Loss: 0.6317
Epoch 7/10, Batch 189/883, Training Loss: 0.5053
Epoch 7/10, Batch 190/883, Training Loss: 0.5625
Epoch 7/10, Batch 191/883, Training Loss: 0.5003
Epoch 7/10, Batch 192/883, Training Loss: 0.7231
Epoch 7/10, Batch 193/883, Training Loss: 0.3404
Epoch 7/10, Batch 194/883, Training Loss: 0.7225
Epoch 7/10, Batch 195/883, Training Loss: 0.5488
Epoch 7/10, Batch 196/883, Training Loss: 1.0381
Epoch 7/10, Batch 197/883, Training Loss: 0.7165
Epoch 7/10, Batch 198/883, Training Loss: 0.5292
Epoch 7/10, Batch 199/883, Training Loss: 0.7154
Epoch 7/10, Batch 200/883, Training Loss: 0.5948
Epoch 7/10, Batch 201/883, Training Loss: 0.5362
Epoch 7/10, Batch 202/883, Training Loss: 0.5353
Epoch 7/10, Batch 203/883, Training Loss: 0.4353
Epoch 7/10, Batch 204/883, Training Loss: 0.9540
Epoch 7/10, Batch 205/883, Training Loss: 0.5818
Epoch 7/10, Batch 206/883, Training Loss: 0.4454
Epoch 7/10, Batch 207/883, Training Loss: 0.7288
Epoch 7/10, Batch 208/883, Training Loss: 0.9562
Epoch 7/10, Batch 209/883, Training Loss: 0.4771
Epoch 7/10, Batch 210/883, Training Loss: 0.4558
Epoch 7/10, Batch 211/883, Training Loss: 0.6859
Epoch 7/10, Batch 212/883, Training Loss: 0.8392
Epoch 7/10, Batch 213/883, Training Loss: 0.5683
Epoch 7/10, Batch 214/883, Training Loss: 0.7055
Epoch 7/10, Batch 215/883, Training Loss: 0.6928
Epoch 7/10, Batch 216/883, Training Loss: 0.3924
Epoch 7/10, Batch 217/883, Training Loss: 0.5179
Epoch 7/10, Batch 218/883, Training Loss: 0.6152
Epoch 7/10, Batch 219/883, Training Loss: 0.6191
Epoch 7/10, Batch 220/883, Training Loss: 0.4887
Epoch 7/10, Batch 221/883, Training Loss: 0.6894
Epoch 7/10, Batch 222/883, Training Loss: 0.8210
Epoch 7/10, Batch 223/883, Training Loss: 0.8692
Epoch 7/10, Batch 224/883, Training Loss: 0.8833
Epoch 7/10, Batch 225/883, Training Loss: 0.6526
Epoch 7/10, Batch 226/883, Training Loss: 0.5804
Epoch 7/10, Batch 227/883, Training Loss: 0.6696
Epoch 7/10, Batch 228/883, Training Loss: 0.5178
Epoch 7/10, Batch 229/883, Training Loss: 0.3465
Epoch 7/10, Batch 230/883, Training Loss: 0.5520
Epoch 7/10, Batch 231/883, Training Loss: 0.6137
Epoch 7/10, Batch 232/883, Training Loss: 0.5450
Epoch 7/10, Batch 233/883, Training Loss: 0.6605
Epoch 7/10, Batch 234/883, Training Loss: 0.8067
Epoch 7/10, Batch 235/883, Training Loss: 0.4773
Epoch 7/10, Batch 236/883, Training Loss: 0.5177
Epoch 7/10, Batch 237/883, Training Loss: 0.7771
Epoch 7/10, Batch 238/883, Training Loss: 0.4996
Epoch 7/10, Batch 239/883, Training Loss: 0.6709
Epoch 7/10, Batch 240/883, Training Loss: 0.5776
Epoch 7/10, Batch 241/883, Training Loss: 0.6708
Epoch 7/10, Batch 242/883, Training Loss: 0.6788
Epoch 7/10, Batch 243/883, Training Loss: 0.5666
Epoch 7/10, Batch 244/883, Training Loss: 0.7193
Epoch 7/10, Batch 245/883, Training Loss: 0.5686
Epoch 7/10, Batch 246/883, Training Loss: 0.4626
Epoch 7/10, Batch 247/883, Training Loss: 1.1088
Epoch 7/10, Batch 248/883, Training Loss: 0.6001
Epoch 7/10, Batch 249/883, Training Loss: 0.5548
Epoch 7/10, Batch 250/883, Training Loss: 0.5224
Epoch 7/10, Batch 251/883, Training Loss: 0.8714
Epoch 7/10, Batch 252/883, Training Loss: 0.7604
Epoch 7/10, Batch 253/883, Training Loss: 0.4498
Epoch 7/10, Batch 254/883, Training Loss: 0.5982
Epoch 7/10, Batch 255/883, Training Loss: 0.4206
Epoch 7/10, Batch 256/883, Training Loss: 0.8735
Epoch 7/10, Batch 257/883, Training Loss: 0.6331
Epoch 7/10, Batch 258/883, Training Loss: 0.8632
Epoch 7/10, Batch 259/883, Training Loss: 0.5650
Epoch 7/10, Batch 260/883, Training Loss: 0.4553
Epoch 7/10, Batch 261/883, Training Loss: 0.5423
Epoch 7/10, Batch 262/883, Training Loss: 0.7836
Epoch 7/10, Batch 263/883, Training Loss: 0.6074
Epoch 7/10, Batch 264/883, Training Loss: 0.4158
Epoch 7/10, Batch 265/883, Training Loss: 0.5883
Epoch 7/10, Batch 266/883, Training Loss: 0.7897
Epoch 7/10, Batch 267/883, Training Loss: 0.7895
Epoch 7/10, Batch 268/883, Training Loss: 0.5519
Epoch 7/10, Batch 269/883, Training Loss: 0.3794
Epoch 7/10, Batch 270/883, Training Loss: 0.8164
Epoch 7/10, Batch 271/883, Training Loss: 0.7743
Epoch 7/10, Batch 272/883, Training Loss: 0.5258
Epoch 7/10, Batch 273/883, Training Loss: 0.4609
Epoch 7/10, Batch 274/883, Training Loss: 0.6012
Epoch 7/10, Batch 275/883, Training Loss: 0.6497
Epoch 7/10, Batch 276/883, Training Loss: 0.5488
Epoch 7/10, Batch 277/883, Training Loss: 0.6984
Epoch 7/10, Batch 278/883, Training Loss: 0.6709
Epoch 7/10, Batch 279/883, Training Loss: 0.4644
Epoch 7/10, Batch 280/883, Training Loss: 0.3589
Epoch 7/10, Batch 281/883, Training Loss: 0.4302
Epoch 7/10, Batch 282/883, Training Loss: 0.4309
Epoch 7/10, Batch 283/883, Training Loss: 0.8908
Epoch 7/10, Batch 284/883, Training Loss: 0.6899
Epoch 7/10, Batch 285/883, Training Loss: 0.5380
Epoch 7/10, Batch 286/883, Training Loss: 0.7661
Epoch 7/10, Batch 287/883, Training Loss: 0.4189
Epoch 7/10, Batch 288/883, Training Loss: 0.4613
Epoch 7/10, Batch 289/883, Training Loss: 1.0331
Epoch 7/10, Batch 290/883, Training Loss: 0.7613
Epoch 7/10, Batch 291/883, Training Loss: 0.5055
Epoch 7/10, Batch 292/883, Training Loss: 0.4580
Epoch 7/10, Batch 293/883, Training Loss: 0.5098
Epoch 7/10, Batch 294/883, Training Loss: 1.0092
Epoch 7/10, Batch 295/883, Training Loss: 0.7344
Epoch 7/10, Batch 296/883, Training Loss: 0.4720
Epoch 7/10, Batch 297/883, Training Loss: 0.5439
Epoch 7/10, Batch 298/883, Training Loss: 0.7483
Epoch 7/10, Batch 299/883, Training Loss: 0.6702
Epoch 7/10, Batch 300/883, Training Loss: 0.5829
Epoch 7/10, Batch 301/883, Training Loss: 0.5898
Epoch 7/10, Batch 302/883, Training Loss: 0.6825
Epoch 7/10, Batch 303/883, Training Loss: 0.5466
Epoch 7/10, Batch 304/883, Training Loss: 0.7089
Epoch 7/10, Batch 305/883, Training Loss: 0.8580
Epoch 7/10, Batch 306/883, Training Loss: 0.6965
Epoch 7/10, Batch 307/883, Training Loss: 0.7768
Epoch 7/10, Batch 308/883, Training Loss: 0.7305
Epoch 7/10, Batch 309/883, Training Loss: 0.6571
Epoch 7/10, Batch 310/883, Training Loss: 0.4455
Epoch 7/10, Batch 311/883, Training Loss: 0.3926
Epoch 7/10, Batch 312/883, Training Loss: 0.3891
Epoch 7/10, Batch 313/883, Training Loss: 0.5663
Epoch 7/10, Batch 314/883, Training Loss: 0.5806
Epoch 7/10, Batch 315/883, Training Loss: 0.6853
Epoch 7/10, Batch 316/883, Training Loss: 0.6863
Epoch 7/10, Batch 317/883, Training Loss: 0.5175
Epoch 7/10, Batch 318/883, Training Loss: 0.4806
Epoch 7/10, Batch 319/883, Training Loss: 0.6211
Epoch 7/10, Batch 320/883, Training Loss: 0.4628
Epoch 7/10, Batch 321/883, Training Loss: 0.6782
Epoch 7/10, Batch 322/883, Training Loss: 0.2978
Epoch 7/10, Batch 323/883, Training Loss: 0.5758
Epoch 7/10, Batch 324/883, Training Loss: 0.3579
Epoch 7/10, Batch 325/883, Training Loss: 0.5445
Epoch 7/10, Batch 326/883, Training Loss: 0.8144
Epoch 7/10, Batch 327/883, Training Loss: 0.6418
Epoch 7/10, Batch 328/883, Training Loss: 0.9879
Epoch 7/10, Batch 329/883, Training Loss: 0.7218
Epoch 7/10, Batch 330/883, Training Loss: 0.5632
Epoch 7/10, Batch 331/883, Training Loss: 0.7077
Epoch 7/10, Batch 332/883, Training Loss: 0.5451
Epoch 7/10, Batch 333/883, Training Loss: 0.5194
Epoch 7/10, Batch 334/883, Training Loss: 0.5548
Epoch 7/10, Batch 335/883, Training Loss: 0.9114
Epoch 7/10, Batch 336/883, Training Loss: 0.6847
Epoch 7/10, Batch 337/883, Training Loss: 0.7433
Epoch 7/10, Batch 338/883, Training Loss: 0.6280
Epoch 7/10, Batch 339/883, Training Loss: 0.7436
Epoch 7/10, Batch 340/883, Training Loss: 0.8210
Epoch 7/10, Batch 341/883, Training Loss: 0.3916
Epoch 7/10, Batch 342/883, Training Loss: 0.3553
Epoch 7/10, Batch 343/883, Training Loss: 0.5755
Epoch 7/10, Batch 344/883, Training Loss: 0.7636
Epoch 7/10, Batch 345/883, Training Loss: 0.7823
Epoch 7/10, Batch 346/883, Training Loss: 0.4574
Epoch 7/10, Batch 347/883, Training Loss: 0.4172
Epoch 7/10, Batch 348/883, Training Loss: 0.4520
Epoch 7/10, Batch 349/883, Training Loss: 0.5209
Epoch 7/10, Batch 350/883, Training Loss: 0.7106
Epoch 7/10, Batch 351/883, Training Loss: 0.6144
Epoch 7/10, Batch 352/883, Training Loss: 0.5088
Epoch 7/10, Batch 353/883, Training Loss: 0.7588
Epoch 7/10, Batch 354/883, Training Loss: 0.7523
Epoch 7/10, Batch 355/883, Training Loss: 0.7136
Epoch 7/10, Batch 356/883, Training Loss: 0.6459
Epoch 7/10, Batch 357/883, Training Loss: 0.7755
Epoch 7/10, Batch 358/883, Training Loss: 0.7573
Epoch 7/10, Batch 359/883, Training Loss: 0.5640
Epoch 7/10, Batch 360/883, Training Loss: 0.6829
Epoch 7/10, Batch 361/883, Training Loss: 0.9730
Epoch 7/10, Batch 362/883, Training Loss: 0.8749
Epoch 7/10, Batch 363/883, Training Loss: 0.8404
Epoch 7/10, Batch 364/883, Training Loss: 0.5303
Epoch 7/10, Batch 365/883, Training Loss: 0.7612
Epoch 7/10, Batch 366/883, Training Loss: 0.6687
Epoch 7/10, Batch 367/883, Training Loss: 0.4683
Epoch 7/10, Batch 368/883, Training Loss: 0.6450
Epoch 7/10, Batch 369/883, Training Loss: 0.5234
Epoch 7/10, Batch 370/883, Training Loss: 0.4994
Epoch 7/10, Batch 371/883, Training Loss: 0.5619
Epoch 7/10, Batch 372/883, Training Loss: 0.6083
Epoch 7/10, Batch 373/883, Training Loss: 0.6193
Epoch 7/10, Batch 374/883, Training Loss: 0.4386
Epoch 7/10, Batch 375/883, Training Loss: 0.5149
Epoch 7/10, Batch 376/883, Training Loss: 0.7988
Epoch 7/10, Batch 377/883, Training Loss: 0.7458
Epoch 7/10, Batch 378/883, Training Loss: 0.6420
Epoch 7/10, Batch 379/883, Training Loss: 0.4505
Epoch 7/10, Batch 380/883, Training Loss: 0.3753
Epoch 7/10, Batch 381/883, Training Loss: 0.7634
Epoch 7/10, Batch 382/883, Training Loss: 0.5298
Epoch 7/10, Batch 383/883, Training Loss: 0.5020
Epoch 7/10, Batch 384/883, Training Loss: 0.5705
Epoch 7/10, Batch 385/883, Training Loss: 0.7143
Epoch 7/10, Batch 386/883, Training Loss: 0.8096
Epoch 7/10, Batch 387/883, Training Loss: 0.6479
Epoch 7/10, Batch 388/883, Training Loss: 0.7763
Epoch 7/10, Batch 389/883, Training Loss: 0.5374
Epoch 7/10, Batch 390/883, Training Loss: 0.4562
Epoch 7/10, Batch 391/883, Training Loss: 0.5688
Epoch 7/10, Batch 392/883, Training Loss: 0.4540
Epoch 7/10, Batch 393/883, Training Loss: 0.8602
Epoch 7/10, Batch 394/883, Training Loss: 0.5591
Epoch 7/10, Batch 395/883, Training Loss: 0.7064
Epoch 7/10, Batch 396/883, Training Loss: 0.3701
Epoch 7/10, Batch 397/883, Training Loss: 0.6148
Epoch 7/10, Batch 398/883, Training Loss: 0.8737
Epoch 7/10, Batch 399/883, Training Loss: 0.3720
Epoch 7/10, Batch 400/883, Training Loss: 0.3731
Epoch 7/10, Batch 401/883, Training Loss: 0.5318
Epoch 7/10, Batch 402/883, Training Loss: 0.5038
Epoch 7/10, Batch 403/883, Training Loss: 0.8044
Epoch 7/10, Batch 404/883, Training Loss: 0.5810
Epoch 7/10, Batch 405/883, Training Loss: 0.7381
Epoch 7/10, Batch 406/883, Training Loss: 0.4121
Epoch 7/10, Batch 407/883, Training Loss: 0.5984
Epoch 7/10, Batch 408/883, Training Loss: 0.5646
Epoch 7/10, Batch 409/883, Training Loss: 0.8873
Epoch 7/10, Batch 410/883, Training Loss: 0.6814
Epoch 7/10, Batch 411/883, Training Loss: 0.4091
Epoch 7/10, Batch 412/883, Training Loss: 0.3759
Epoch 7/10, Batch 413/883, Training Loss: 0.7394
Epoch 7/10, Batch 414/883, Training Loss: 0.5783
Epoch 7/10, Batch 415/883, Training Loss: 0.6440
Epoch 7/10, Batch 416/883, Training Loss: 0.4811
Epoch 7/10, Batch 417/883, Training Loss: 0.6457
Epoch 7/10, Batch 418/883, Training Loss: 0.6826
Epoch 7/10, Batch 419/883, Training Loss: 0.4970
Epoch 7/10, Batch 420/883, Training Loss: 0.6749
Epoch 7/10, Batch 421/883, Training Loss: 0.8392
Epoch 7/10, Batch 422/883, Training Loss: 0.3388
Epoch 7/10, Batch 423/883, Training Loss: 0.8206
Epoch 7/10, Batch 424/883, Training Loss: 0.5020
Epoch 7/10, Batch 425/883, Training Loss: 0.6010
Epoch 7/10, Batch 426/883, Training Loss: 0.6796
Epoch 7/10, Batch 427/883, Training Loss: 0.5353
Epoch 7/10, Batch 428/883, Training Loss: 0.4670
Epoch 7/10, Batch 429/883, Training Loss: 0.4459
Epoch 7/10, Batch 430/883, Training Loss: 0.4436
Epoch 7/10, Batch 431/883, Training Loss: 0.5407
Epoch 7/10, Batch 432/883, Training Loss: 0.6367
Epoch 7/10, Batch 433/883, Training Loss: 0.6813
Epoch 7/10, Batch 434/883, Training Loss: 0.7381
Epoch 7/10, Batch 435/883, Training Loss: 0.6236
Epoch 7/10, Batch 436/883, Training Loss: 0.8495
Epoch 7/10, Batch 437/883, Training Loss: 0.4384
Epoch 7/10, Batch 438/883, Training Loss: 0.4598
Epoch 7/10, Batch 439/883, Training Loss: 0.7955
Epoch 7/10, Batch 440/883, Training Loss: 0.3397
Epoch 7/10, Batch 441/883, Training Loss: 0.6850
Epoch 7/10, Batch 442/883, Training Loss: 0.5804
Epoch 7/10, Batch 443/883, Training Loss: 0.8354
Epoch 7/10, Batch 444/883, Training Loss: 0.9890
Epoch 7/10, Batch 445/883, Training Loss: 0.5231
Epoch 7/10, Batch 446/883, Training Loss: 0.6536
Epoch 7/10, Batch 447/883, Training Loss: 0.7425
Epoch 7/10, Batch 448/883, Training Loss: 0.9278
Epoch 7/10, Batch 449/883, Training Loss: 0.5885
Epoch 7/10, Batch 450/883, Training Loss: 0.7281
Epoch 7/10, Batch 451/883, Training Loss: 0.6314
Epoch 7/10, Batch 452/883, Training Loss: 0.7895
Epoch 7/10, Batch 453/883, Training Loss: 0.6241
Epoch 7/10, Batch 454/883, Training Loss: 0.5175
Epoch 7/10, Batch 455/883, Training Loss: 0.4152
Epoch 7/10, Batch 456/883, Training Loss: 0.8092
Epoch 7/10, Batch 457/883, Training Loss: 0.4228
Epoch 7/10, Batch 458/883, Training Loss: 0.6005
Epoch 7/10, Batch 459/883, Training Loss: 1.1065
Epoch 7/10, Batch 460/883, Training Loss: 0.3710
Epoch 7/10, Batch 461/883, Training Loss: 0.4555
Epoch 7/10, Batch 462/883, Training Loss: 0.6467
Epoch 7/10, Batch 463/883, Training Loss: 0.6531
Epoch 7/10, Batch 464/883, Training Loss: 0.4770
Epoch 7/10, Batch 465/883, Training Loss: 0.4746
Epoch 7/10, Batch 466/883, Training Loss: 0.8071
Epoch 7/10, Batch 467/883, Training Loss: 0.3805
Epoch 7/10, Batch 468/883, Training Loss: 0.5057
Epoch 7/10, Batch 469/883, Training Loss: 0.2689
Epoch 7/10, Batch 470/883, Training Loss: 0.5659
Epoch 7/10, Batch 471/883, Training Loss: 0.4540
Epoch 7/10, Batch 472/883, Training Loss: 0.5840
Epoch 7/10, Batch 473/883, Training Loss: 0.4854
Epoch 7/10, Batch 474/883, Training Loss: 0.9178
Epoch 7/10, Batch 475/883, Training Loss: 0.6279
Epoch 7/10, Batch 476/883, Training Loss: 0.5438
Epoch 7/10, Batch 477/883, Training Loss: 0.6232
Epoch 7/10, Batch 478/883, Training Loss: 0.9986
Epoch 7/10, Batch 479/883, Training Loss: 0.5632
Epoch 7/10, Batch 480/883, Training Loss: 0.4826
Epoch 7/10, Batch 481/883, Training Loss: 0.7896
Epoch 7/10, Batch 482/883, Training Loss: 0.5469
Epoch 7/10, Batch 483/883, Training Loss: 0.8981
Epoch 7/10, Batch 484/883, Training Loss: 0.6264
Epoch 7/10, Batch 485/883, Training Loss: 0.5008
Epoch 7/10, Batch 486/883, Training Loss: 0.4714
Epoch 7/10, Batch 487/883, Training Loss: 0.4911
Epoch 7/10, Batch 488/883, Training Loss: 0.4693
Epoch 7/10, Batch 489/883, Training Loss: 0.7308
Epoch 7/10, Batch 490/883, Training Loss: 0.3619
Epoch 7/10, Batch 491/883, Training Loss: 0.6300
Epoch 7/10, Batch 492/883, Training Loss: 0.2839
Epoch 7/10, Batch 493/883, Training Loss: 0.5031
Epoch 7/10, Batch 494/883, Training Loss: 0.8522
Epoch 7/10, Batch 495/883, Training Loss: 0.4483
Epoch 7/10, Batch 496/883, Training Loss: 0.6415
Epoch 7/10, Batch 497/883, Training Loss: 0.7796
Epoch 7/10, Batch 498/883, Training Loss: 0.5500
Epoch 7/10, Batch 499/883, Training Loss: 0.8469
Epoch 7/10, Batch 500/883, Training Loss: 0.6758
Epoch 7/10, Batch 501/883, Training Loss: 0.9162
Epoch 7/10, Batch 502/883, Training Loss: 0.5179
Epoch 7/10, Batch 503/883, Training Loss: 0.4407
Epoch 7/10, Batch 504/883, Training Loss: 0.5936
Epoch 7/10, Batch 505/883, Training Loss: 0.5446
Epoch 7/10, Batch 506/883, Training Loss: 0.4405
Epoch 7/10, Batch 507/883, Training Loss: 0.4615
Epoch 7/10, Batch 508/883, Training Loss: 0.7288
Epoch 7/10, Batch 509/883, Training Loss: 0.8157
Epoch 7/10, Batch 510/883, Training Loss: 0.8608
Epoch 7/10, Batch 511/883, Training Loss: 0.5619
Epoch 7/10, Batch 512/883, Training Loss: 0.5779
Epoch 7/10, Batch 513/883, Training Loss: 0.4644
Epoch 7/10, Batch 514/883, Training Loss: 0.8814
Epoch 7/10, Batch 515/883, Training Loss: 0.8684
Epoch 7/10, Batch 516/883, Training Loss: 0.7404
Epoch 7/10, Batch 517/883, Training Loss: 0.7345
Epoch 7/10, Batch 518/883, Training Loss: 0.7544
Epoch 7/10, Batch 519/883, Training Loss: 0.9283
Epoch 7/10, Batch 520/883, Training Loss: 0.8039
Epoch 7/10, Batch 521/883, Training Loss: 0.4137
Epoch 7/10, Batch 522/883, Training Loss: 0.5151
Epoch 7/10, Batch 523/883, Training Loss: 0.5508
Epoch 7/10, Batch 524/883, Training Loss: 0.6170
Epoch 7/10, Batch 525/883, Training Loss: 0.4538
Epoch 7/10, Batch 526/883, Training Loss: 0.7217
Epoch 7/10, Batch 527/883, Training Loss: 0.4525
Epoch 7/10, Batch 528/883, Training Loss: 0.5059
Epoch 7/10, Batch 529/883, Training Loss: 0.5012
Epoch 7/10, Batch 530/883, Training Loss: 0.4414
Epoch 7/10, Batch 531/883, Training Loss: 0.9219
Epoch 7/10, Batch 532/883, Training Loss: 1.1188
Epoch 7/10, Batch 533/883, Training Loss: 0.4031
Epoch 7/10, Batch 534/883, Training Loss: 0.5193
Epoch 7/10, Batch 535/883, Training Loss: 0.8611
Epoch 7/10, Batch 536/883, Training Loss: 0.6561
Epoch 7/10, Batch 537/883, Training Loss: 0.4766
Epoch 7/10, Batch 538/883, Training Loss: 0.5422
Epoch 7/10, Batch 539/883, Training Loss: 0.5616
Epoch 7/10, Batch 540/883, Training Loss: 0.6245
Epoch 7/10, Batch 541/883, Training Loss: 0.7910
Epoch 7/10, Batch 542/883, Training Loss: 0.4788
Epoch 7/10, Batch 543/883, Training Loss: 0.4482
Epoch 7/10, Batch 544/883, Training Loss: 0.6239
Epoch 7/10, Batch 545/883, Training Loss: 0.6139
Epoch 7/10, Batch 546/883, Training Loss: 0.7035
Epoch 7/10, Batch 547/883, Training Loss: 0.6256
Epoch 7/10, Batch 548/883, Training Loss: 0.5263
Epoch 7/10, Batch 549/883, Training Loss: 0.4897
Epoch 7/10, Batch 550/883, Training Loss: 0.6868
Epoch 7/10, Batch 551/883, Training Loss: 0.3730
Epoch 7/10, Batch 552/883, Training Loss: 0.7311
Epoch 7/10, Batch 553/883, Training Loss: 0.4407
Epoch 7/10, Batch 554/883, Training Loss: 0.4292
Epoch 7/10, Batch 555/883, Training Loss: 0.3010
Epoch 7/10, Batch 556/883, Training Loss: 0.5791
Epoch 7/10, Batch 557/883, Training Loss: 0.5838
Epoch 7/10, Batch 558/883, Training Loss: 0.5944
Epoch 7/10, Batch 559/883, Training Loss: 0.5650
Epoch 7/10, Batch 560/883, Training Loss: 0.5809
Epoch 7/10, Batch 561/883, Training Loss: 0.6669
Epoch 7/10, Batch 562/883, Training Loss: 0.3720
Epoch 7/10, Batch 563/883, Training Loss: 0.5773
Epoch 7/10, Batch 564/883, Training Loss: 0.3851
Epoch 7/10, Batch 565/883, Training Loss: 0.5206
Epoch 7/10, Batch 566/883, Training Loss: 0.7684
Epoch 7/10, Batch 567/883, Training Loss: 0.3798
Epoch 7/10, Batch 568/883, Training Loss: 0.4001
Epoch 7/10, Batch 569/883, Training Loss: 0.4006
Epoch 7/10, Batch 570/883, Training Loss: 0.3845
Epoch 7/10, Batch 571/883, Training Loss: 0.6398
Epoch 7/10, Batch 572/883, Training Loss: 0.8658
Epoch 7/10, Batch 573/883, Training Loss: 0.4849
Epoch 7/10, Batch 574/883, Training Loss: 0.7483
Epoch 7/10, Batch 575/883, Training Loss: 0.5014
Epoch 7/10, Batch 576/883, Training Loss: 0.5124
Epoch 7/10, Batch 577/883, Training Loss: 0.5419
Epoch 7/10, Batch 578/883, Training Loss: 0.5179
Epoch 7/10, Batch 579/883, Training Loss: 0.4541
Epoch 7/10, Batch 580/883, Training Loss: 0.4726
Epoch 7/10, Batch 581/883, Training Loss: 0.5066
Epoch 7/10, Batch 582/883, Training Loss: 0.6478
Epoch 7/10, Batch 583/883, Training Loss: 0.4955
Epoch 7/10, Batch 584/883, Training Loss: 0.4186
Epoch 7/10, Batch 585/883, Training Loss: 0.4871
Epoch 7/10, Batch 586/883, Training Loss: 0.4795
Epoch 7/10, Batch 587/883, Training Loss: 0.5509
Epoch 7/10, Batch 588/883, Training Loss: 0.6406
Epoch 7/10, Batch 589/883, Training Loss: 0.6564
Epoch 7/10, Batch 590/883, Training Loss: 0.5864
Epoch 7/10, Batch 591/883, Training Loss: 0.7733
Epoch 7/10, Batch 592/883, Training Loss: 0.4261
Epoch 7/10, Batch 593/883, Training Loss: 0.6631
Epoch 7/10, Batch 594/883, Training Loss: 1.1747
Epoch 7/10, Batch 595/883, Training Loss: 0.2891
Epoch 7/10, Batch 596/883, Training Loss: 0.4896
Epoch 7/10, Batch 597/883, Training Loss: 0.3216
Epoch 7/10, Batch 598/883, Training Loss: 0.5516
Epoch 7/10, Batch 599/883, Training Loss: 0.5952
Epoch 7/10, Batch 600/883, Training Loss: 0.5649
Epoch 7/10, Batch 601/883, Training Loss: 0.4265
Epoch 7/10, Batch 602/883, Training Loss: 0.8043
Epoch 7/10, Batch 603/883, Training Loss: 0.6159
Epoch 7/10, Batch 604/883, Training Loss: 0.6935
Epoch 7/10, Batch 605/883, Training Loss: 0.5232
Epoch 7/10, Batch 606/883, Training Loss: 0.3041
Epoch 7/10, Batch 607/883, Training Loss: 0.5547
Epoch 7/10, Batch 608/883, Training Loss: 0.9274
Epoch 7/10, Batch 609/883, Training Loss: 0.5177
Epoch 7/10, Batch 610/883, Training Loss: 0.6617
Epoch 7/10, Batch 611/883, Training Loss: 1.1103
Epoch 7/10, Batch 612/883, Training Loss: 0.4718
Epoch 7/10, Batch 613/883, Training Loss: 0.6263
Epoch 7/10, Batch 614/883, Training Loss: 0.5518
Epoch 7/10, Batch 615/883, Training Loss: 0.5870
Epoch 7/10, Batch 616/883, Training Loss: 0.5162
Epoch 7/10, Batch 617/883, Training Loss: 0.9705
Epoch 7/10, Batch 618/883, Training Loss: 0.3812
Epoch 7/10, Batch 619/883, Training Loss: 0.5838
Epoch 7/10, Batch 620/883, Training Loss: 0.6022
Epoch 7/10, Batch 621/883, Training Loss: 0.4197
Epoch 7/10, Batch 622/883, Training Loss: 0.6343
Epoch 7/10, Batch 623/883, Training Loss: 0.8055
Epoch 7/10, Batch 624/883, Training Loss: 0.5702
Epoch 7/10, Batch 625/883, Training Loss: 0.2652
Epoch 7/10, Batch 626/883, Training Loss: 0.5286
Epoch 7/10, Batch 627/883, Training Loss: 0.5788
Epoch 7/10, Batch 628/883, Training Loss: 0.6645
Epoch 7/10, Batch 629/883, Training Loss: 0.6226
Epoch 7/10, Batch 630/883, Training Loss: 0.4763
Epoch 7/10, Batch 631/883, Training Loss: 0.6461
Epoch 7/10, Batch 632/883, Training Loss: 0.3654
Epoch 7/10, Batch 633/883, Training Loss: 0.6497
Epoch 7/10, Batch 634/883, Training Loss: 1.1845
Epoch 7/10, Batch 635/883, Training Loss: 0.7080
Epoch 7/10, Batch 636/883, Training Loss: 0.7136
Epoch 7/10, Batch 637/883, Training Loss: 1.0674
Epoch 7/10, Batch 638/883, Training Loss: 0.5693
Epoch 7/10, Batch 639/883, Training Loss: 0.4110
Epoch 7/10, Batch 640/883, Training Loss: 0.9593
Epoch 7/10, Batch 641/883, Training Loss: 0.4186
Epoch 7/10, Batch 642/883, Training Loss: 0.6633
Epoch 7/10, Batch 643/883, Training Loss: 0.4072
Epoch 7/10, Batch 644/883, Training Loss: 0.5119
Epoch 7/10, Batch 645/883, Training Loss: 0.4991
Epoch 7/10, Batch 646/883, Training Loss: 0.5628
Epoch 7/10, Batch 647/883, Training Loss: 0.4765
Epoch 7/10, Batch 648/883, Training Loss: 0.5162
Epoch 7/10, Batch 649/883, Training Loss: 0.6350
Epoch 7/10, Batch 650/883, Training Loss: 0.6713
Epoch 7/10, Batch 651/883, Training Loss: 0.7100
Epoch 7/10, Batch 652/883, Training Loss: 0.9373
Epoch 7/10, Batch 653/883, Training Loss: 0.4626
Epoch 7/10, Batch 654/883, Training Loss: 0.5591
Epoch 7/10, Batch 655/883, Training Loss: 0.4475
Epoch 7/10, Batch 656/883, Training Loss: 0.9886
Epoch 7/10, Batch 657/883, Training Loss: 0.9764
Epoch 7/10, Batch 658/883, Training Loss: 0.4663
Epoch 7/10, Batch 659/883, Training Loss: 0.4935
Epoch 7/10, Batch 660/883, Training Loss: 0.6628
Epoch 7/10, Batch 661/883, Training Loss: 0.5926
Epoch 7/10, Batch 662/883, Training Loss: 0.7812
Epoch 7/10, Batch 663/883, Training Loss: 0.7031
Epoch 7/10, Batch 664/883, Training Loss: 0.7674
Epoch 7/10, Batch 665/883, Training Loss: 0.7221
Epoch 7/10, Batch 666/883, Training Loss: 0.7349
Epoch 7/10, Batch 667/883, Training Loss: 0.3833
Epoch 7/10, Batch 668/883, Training Loss: 0.6540
Epoch 7/10, Batch 669/883, Training Loss: 0.3268
Epoch 7/10, Batch 670/883, Training Loss: 0.4622
Epoch 7/10, Batch 671/883, Training Loss: 0.7577
Epoch 7/10, Batch 672/883, Training Loss: 0.9661
Epoch 7/10, Batch 673/883, Training Loss: 0.4211
Epoch 7/10, Batch 674/883, Training Loss: 0.6377
Epoch 7/10, Batch 675/883, Training Loss: 0.6484
Epoch 7/10, Batch 676/883, Training Loss: 0.5689
Epoch 7/10, Batch 677/883, Training Loss: 0.5528
Epoch 7/10, Batch 678/883, Training Loss: 0.8139
Epoch 7/10, Batch 679/883, Training Loss: 0.5265
Epoch 7/10, Batch 680/883, Training Loss: 0.4119
Epoch 7/10, Batch 681/883, Training Loss: 0.3906
Epoch 7/10, Batch 682/883, Training Loss: 0.4584
Epoch 7/10, Batch 683/883, Training Loss: 0.8523
Epoch 7/10, Batch 684/883, Training Loss: 0.8738
Epoch 7/10, Batch 685/883, Training Loss: 0.5803
Epoch 7/10, Batch 686/883, Training Loss: 0.4286
Epoch 7/10, Batch 687/883, Training Loss: 0.3975
Epoch 7/10, Batch 688/883, Training Loss: 0.5254
Epoch 7/10, Batch 689/883, Training Loss: 0.4860
Epoch 7/10, Batch 690/883, Training Loss: 0.6530
Epoch 7/10, Batch 691/883, Training Loss: 0.3902
Epoch 7/10, Batch 692/883, Training Loss: 0.6597
Epoch 7/10, Batch 693/883, Training Loss: 0.7588
Epoch 7/10, Batch 694/883, Training Loss: 0.4030
Epoch 7/10, Batch 695/883, Training Loss: 0.7498
Epoch 7/10, Batch 696/883, Training Loss: 0.5488
Epoch 7/10, Batch 697/883, Training Loss: 0.6441
Epoch 7/10, Batch 698/883, Training Loss: 0.3983
Epoch 7/10, Batch 699/883, Training Loss: 0.5610
Epoch 7/10, Batch 700/883, Training Loss: 0.5320
Epoch 7/10, Batch 701/883, Training Loss: 0.6035
Epoch 7/10, Batch 702/883, Training Loss: 0.6674
Epoch 7/10, Batch 703/883, Training Loss: 0.5380
Epoch 7/10, Batch 704/883, Training Loss: 0.3731
Epoch 7/10, Batch 705/883, Training Loss: 0.5117
Epoch 7/10, Batch 706/883, Training Loss: 0.4826
Epoch 7/10, Batch 707/883, Training Loss: 0.4363
Epoch 7/10, Batch 708/883, Training Loss: 0.4511
Epoch 7/10, Batch 709/883, Training Loss: 0.3443
Epoch 7/10, Batch 710/883, Training Loss: 0.5472
Epoch 7/10, Batch 711/883, Training Loss: 0.6783
Epoch 7/10, Batch 712/883, Training Loss: 0.5667
Epoch 7/10, Batch 713/883, Training Loss: 0.6889
Epoch 7/10, Batch 714/883, Training Loss: 0.8067
Epoch 7/10, Batch 715/883, Training Loss: 0.4007
Epoch 7/10, Batch 716/883, Training Loss: 0.4109
Epoch 7/10, Batch 717/883, Training Loss: 0.2886
Epoch 7/10, Batch 718/883, Training Loss: 0.7260
Epoch 7/10, Batch 719/883, Training Loss: 0.5047
Epoch 7/10, Batch 720/883, Training Loss: 0.5415
Epoch 7/10, Batch 721/883, Training Loss: 0.5598
Epoch 7/10, Batch 722/883, Training Loss: 0.7827
Epoch 7/10, Batch 723/883, Training Loss: 0.7646
Epoch 7/10, Batch 724/883, Training Loss: 0.6186
Epoch 7/10, Batch 725/883, Training Loss: 0.5015
Epoch 7/10, Batch 726/883, Training Loss: 0.4254
Epoch 7/10, Batch 727/883, Training Loss: 0.3653
Epoch 7/10, Batch 728/883, Training Loss: 0.4609
Epoch 7/10, Batch 729/883, Training Loss: 0.7422
Epoch 7/10, Batch 730/883, Training Loss: 0.6354
Epoch 7/10, Batch 731/883, Training Loss: 1.0767
Epoch 7/10, Batch 732/883, Training Loss: 0.5435
Epoch 7/10, Batch 733/883, Training Loss: 0.7547
Epoch 7/10, Batch 734/883, Training Loss: 0.8430
Epoch 7/10, Batch 735/883, Training Loss: 0.7138
Epoch 7/10, Batch 736/883, Training Loss: 0.6802
Epoch 7/10, Batch 737/883, Training Loss: 0.4818
Epoch 7/10, Batch 738/883, Training Loss: 0.4083
Epoch 7/10, Batch 739/883, Training Loss: 0.7051
Epoch 7/10, Batch 740/883, Training Loss: 0.6507
Epoch 7/10, Batch 741/883, Training Loss: 0.8098
Epoch 7/10, Batch 742/883, Training Loss: 0.5679
Epoch 7/10, Batch 743/883, Training Loss: 0.5092
Epoch 7/10, Batch 744/883, Training Loss: 0.5003
Epoch 7/10, Batch 745/883, Training Loss: 0.6095
Epoch 7/10, Batch 746/883, Training Loss: 0.6127
Epoch 7/10, Batch 747/883, Training Loss: 0.4650
Epoch 7/10, Batch 748/883, Training Loss: 0.6018
Epoch 7/10, Batch 749/883, Training Loss: 0.5263
Epoch 7/10, Batch 750/883, Training Loss: 0.5241
Epoch 7/10, Batch 751/883, Training Loss: 0.8370
Epoch 7/10, Batch 752/883, Training Loss: 1.1826
Epoch 7/10, Batch 753/883, Training Loss: 1.1832
Epoch 7/10, Batch 754/883, Training Loss: 0.6883
Epoch 7/10, Batch 755/883, Training Loss: 0.5053
Epoch 7/10, Batch 756/883, Training Loss: 0.4615
Epoch 7/10, Batch 757/883, Training Loss: 0.5033
Epoch 7/10, Batch 758/883, Training Loss: 0.7176
Epoch 7/10, Batch 759/883, Training Loss: 0.5573
Epoch 7/10, Batch 760/883, Training Loss: 0.6059
Epoch 7/10, Batch 761/883, Training Loss: 0.6362
Epoch 7/10, Batch 762/883, Training Loss: 0.6385
Epoch 7/10, Batch 763/883, Training Loss: 0.7905
Epoch 7/10, Batch 764/883, Training Loss: 0.5068
Epoch 7/10, Batch 765/883, Training Loss: 0.5664
Epoch 7/10, Batch 766/883, Training Loss: 0.5210
Epoch 7/10, Batch 767/883, Training Loss: 0.7066
Epoch 7/10, Batch 768/883, Training Loss: 0.6995
Epoch 7/10, Batch 769/883, Training Loss: 0.5029
Epoch 7/10, Batch 770/883, Training Loss: 0.5815
Epoch 7/10, Batch 771/883, Training Loss: 0.5403
Epoch 7/10, Batch 772/883, Training Loss: 0.6216
Epoch 7/10, Batch 773/883, Training Loss: 0.7289
Epoch 7/10, Batch 774/883, Training Loss: 0.3689
Epoch 7/10, Batch 775/883, Training Loss: 1.0828
Epoch 7/10, Batch 776/883, Training Loss: 0.8326
Epoch 7/10, Batch 777/883, Training Loss: 0.6726
Epoch 7/10, Batch 778/883, Training Loss: 0.7530
Epoch 7/10, Batch 779/883, Training Loss: 0.5150
Epoch 7/10, Batch 780/883, Training Loss: 0.4716
Epoch 7/10, Batch 781/883, Training Loss: 0.7170
Epoch 7/10, Batch 782/883, Training Loss: 0.6654
Epoch 7/10, Batch 783/883, Training Loss: 0.6390
Epoch 7/10, Batch 784/883, Training Loss: 0.7616
Epoch 7/10, Batch 785/883, Training Loss: 0.3855
Epoch 7/10, Batch 786/883, Training Loss: 0.4991
Epoch 7/10, Batch 787/883, Training Loss: 0.6209
Epoch 7/10, Batch 788/883, Training Loss: 0.5126
Epoch 7/10, Batch 789/883, Training Loss: 0.5422
Epoch 7/10, Batch 790/883, Training Loss: 0.4090
Epoch 7/10, Batch 791/883, Training Loss: 0.5506
Epoch 7/10, Batch 792/883, Training Loss: 0.6198
Epoch 7/10, Batch 793/883, Training Loss: 0.7994
Epoch 7/10, Batch 794/883, Training Loss: 0.2849
Epoch 7/10, Batch 795/883, Training Loss: 0.3795
Epoch 7/10, Batch 796/883, Training Loss: 0.8325
Epoch 7/10, Batch 797/883, Training Loss: 0.6319
Epoch 7/10, Batch 798/883, Training Loss: 0.5246
Epoch 7/10, Batch 799/883, Training Loss: 0.3590
Epoch 7/10, Batch 800/883, Training Loss: 0.6048
Epoch 7/10, Batch 801/883, Training Loss: 0.6720
Epoch 7/10, Batch 802/883, Training Loss: 0.9532
Epoch 7/10, Batch 803/883, Training Loss: 0.6924
Epoch 7/10, Batch 804/883, Training Loss: 0.3939
Epoch 7/10, Batch 805/883, Training Loss: 0.8637
Epoch 7/10, Batch 806/883, Training Loss: 0.7506
Epoch 7/10, Batch 807/883, Training Loss: 0.3986
Epoch 7/10, Batch 808/883, Training Loss: 0.5117
Epoch 7/10, Batch 809/883, Training Loss: 0.4574
Epoch 7/10, Batch 810/883, Training Loss: 0.5332
Epoch 7/10, Batch 811/883, Training Loss: 0.3399
Epoch 7/10, Batch 812/883, Training Loss: 0.5151
Epoch 7/10, Batch 813/883, Training Loss: 0.5445
Epoch 7/10, Batch 814/883, Training Loss: 0.6903
Epoch 7/10, Batch 815/883, Training Loss: 0.6640
Epoch 7/10, Batch 816/883, Training Loss: 0.3301
Epoch 7/10, Batch 817/883, Training Loss: 0.6672
Epoch 7/10, Batch 818/883, Training Loss: 0.5202
Epoch 7/10, Batch 819/883, Training Loss: 0.7561
Epoch 7/10, Batch 820/883, Training Loss: 0.5424
Epoch 7/10, Batch 821/883, Training Loss: 0.5544
Epoch 7/10, Batch 822/883, Training Loss: 0.5613
Epoch 7/10, Batch 823/883, Training Loss: 0.8211
Epoch 7/10, Batch 824/883, Training Loss: 0.5771
Epoch 7/10, Batch 825/883, Training Loss: 0.7670
Epoch 7/10, Batch 826/883, Training Loss: 0.8998
Epoch 7/10, Batch 827/883, Training Loss: 0.4813
Epoch 7/10, Batch 828/883, Training Loss: 0.5295
Epoch 7/10, Batch 829/883, Training Loss: 0.4354
Epoch 7/10, Batch 830/883, Training Loss: 0.5945
Epoch 7/10, Batch 831/883, Training Loss: 0.8869
Epoch 7/10, Batch 832/883, Training Loss: 0.3303
Epoch 7/10, Batch 833/883, Training Loss: 0.5437
Epoch 7/10, Batch 834/883, Training Loss: 0.5882
Epoch 7/10, Batch 835/883, Training Loss: 0.7079
Epoch 7/10, Batch 836/883, Training Loss: 0.5488
Epoch 7/10, Batch 837/883, Training Loss: 0.4280
Epoch 7/10, Batch 838/883, Training Loss: 0.2978
Epoch 7/10, Batch 839/883, Training Loss: 0.4289
Epoch 7/10, Batch 840/883, Training Loss: 0.9048
Epoch 7/10, Batch 841/883, Training Loss: 0.7646
Epoch 7/10, Batch 842/883, Training Loss: 0.4181
Epoch 7/10, Batch 843/883, Training Loss: 0.7304
Epoch 7/10, Batch 844/883, Training Loss: 0.4416
Epoch 7/10, Batch 845/883, Training Loss: 0.8777
Epoch 7/10, Batch 846/883, Training Loss: 0.4048
Epoch 7/10, Batch 847/883, Training Loss: 0.6755
Epoch 7/10, Batch 848/883, Training Loss: 0.5924
Epoch 7/10, Batch 849/883, Training Loss: 0.6032
Epoch 7/10, Batch 850/883, Training Loss: 0.4835
Epoch 7/10, Batch 851/883, Training Loss: 0.6346
Epoch 7/10, Batch 852/883, Training Loss: 1.1777
Epoch 7/10, Batch 853/883, Training Loss: 0.7459
Epoch 7/10, Batch 854/883, Training Loss: 0.4083
Epoch 7/10, Batch 855/883, Training Loss: 0.7622
Epoch 7/10, Batch 856/883, Training Loss: 0.6399
Epoch 7/10, Batch 857/883, Training Loss: 0.6707
Epoch 7/10, Batch 858/883, Training Loss: 0.3980
Epoch 7/10, Batch 859/883, Training Loss: 0.4792
Epoch 7/10, Batch 860/883, Training Loss: 0.6924
Epoch 7/10, Batch 861/883, Training Loss: 0.7555
Epoch 7/10, Batch 862/883, Training Loss: 0.5851
Epoch 7/10, Batch 863/883, Training Loss: 0.8594
Epoch 7/10, Batch 864/883, Training Loss: 0.7044
Epoch 7/10, Batch 865/883, Training Loss: 0.5167
Epoch 7/10, Batch 866/883, Training Loss: 0.6187
Epoch 7/10, Batch 867/883, Training Loss: 0.4939
Epoch 7/10, Batch 868/883, Training Loss: 0.5118
Epoch 7/10, Batch 869/883, Training Loss: 0.4430
Epoch 7/10, Batch 870/883, Training Loss: 0.5913
Epoch 7/10, Batch 871/883, Training Loss: 0.6214
Epoch 7/10, Batch 872/883, Training Loss: 0.4211
Epoch 7/10, Batch 873/883, Training Loss: 0.5413
Epoch 7/10, Batch 874/883, Training Loss: 0.6037
Epoch 7/10, Batch 875/883, Training Loss: 0.3753
Epoch 7/10, Batch 876/883, Training Loss: 0.4756
Epoch 7/10, Batch 877/883, Training Loss: 0.8294
Epoch 7/10, Batch 878/883, Training Loss: 0.3063
Epoch 7/10, Batch 879/883, Training Loss: 0.5360
Epoch 7/10, Batch 880/883, Training Loss: 0.4572
Epoch 7/10, Batch 881/883, Training Loss: 0.5948
Epoch 7/10, Batch 882/883, Training Loss: 0.4694
Epoch 7/10, Batch 883/883, Training Loss: 0.4803
Epoch 7/10, Training Loss: 0.6109, Validation Loss: 0.5438, Validation Accuracy: 0.7680
Epoch 8/10, Batch 1/883, Training Loss: 0.4803
Epoch 8/10, Batch 2/883, Training Loss: 1.0791
Epoch 8/10, Batch 3/883, Training Loss: 0.5777
Epoch 8/10, Batch 4/883, Training Loss: 0.4831
Epoch 8/10, Batch 5/883, Training Loss: 0.6425
Epoch 8/10, Batch 6/883, Training Loss: 0.5150
Epoch 8/10, Batch 7/883, Training Loss: 0.6408
Epoch 8/10, Batch 8/883, Training Loss: 0.6284
Epoch 8/10, Batch 9/883, Training Loss: 0.7759
Epoch 8/10, Batch 10/883, Training Loss: 0.3231
Epoch 8/10, Batch 11/883, Training Loss: 0.5574
Epoch 8/10, Batch 12/883, Training Loss: 0.5873
Epoch 8/10, Batch 13/883, Training Loss: 0.4767
Epoch 8/10, Batch 14/883, Training Loss: 0.4545
Epoch 8/10, Batch 15/883, Training Loss: 0.7357
Epoch 8/10, Batch 16/883, Training Loss: 0.4649
Epoch 8/10, Batch 17/883, Training Loss: 0.6572
Epoch 8/10, Batch 18/883, Training Loss: 0.5531
Epoch 8/10, Batch 19/883, Training Loss: 0.4532
Epoch 8/10, Batch 20/883, Training Loss: 0.8811
Epoch 8/10, Batch 21/883, Training Loss: 0.3126
Epoch 8/10, Batch 22/883, Training Loss: 0.4811
Epoch 8/10, Batch 23/883, Training Loss: 0.3946
Epoch 8/10, Batch 24/883, Training Loss: 0.3226
Epoch 8/10, Batch 25/883, Training Loss: 0.5945
Epoch 8/10, Batch 26/883, Training Loss: 0.7979
Epoch 8/10, Batch 27/883, Training Loss: 0.5108
Epoch 8/10, Batch 28/883, Training Loss: 0.4507
Epoch 8/10, Batch 29/883, Training Loss: 0.2707
Epoch 8/10, Batch 30/883, Training Loss: 0.6390
Epoch 8/10, Batch 31/883, Training Loss: 0.8056
Epoch 8/10, Batch 32/883, Training Loss: 0.3874
Epoch 8/10, Batch 33/883, Training Loss: 0.7179
Epoch 8/10, Batch 34/883, Training Loss: 0.6497
Epoch 8/10, Batch 35/883, Training Loss: 0.6552
Epoch 8/10, Batch 36/883, Training Loss: 0.5706
Epoch 8/10, Batch 37/883, Training Loss: 0.5876
Epoch 8/10, Batch 38/883, Training Loss: 0.7269
Epoch 8/10, Batch 39/883, Training Loss: 0.3429
Epoch 8/10, Batch 40/883, Training Loss: 0.9149
Epoch 8/10, Batch 41/883, Training Loss: 0.5543
Epoch 8/10, Batch 42/883, Training Loss: 0.6967
Epoch 8/10, Batch 43/883, Training Loss: 0.5647
Epoch 8/10, Batch 44/883, Training Loss: 0.5776
Epoch 8/10, Batch 45/883, Training Loss: 0.4974
Epoch 8/10, Batch 46/883, Training Loss: 0.3820
Epoch 8/10, Batch 47/883, Training Loss: 0.6070
Epoch 8/10, Batch 48/883, Training Loss: 0.5402
Epoch 8/10, Batch 49/883, Training Loss: 0.3361
Epoch 8/10, Batch 50/883, Training Loss: 0.4822
Epoch 8/10, Batch 51/883, Training Loss: 0.6293
Epoch 8/10, Batch 52/883, Training Loss: 0.4710
Epoch 8/10, Batch 53/883, Training Loss: 0.3243
Epoch 8/10, Batch 54/883, Training Loss: 0.7393
Epoch 8/10, Batch 55/883, Training Loss: 0.9886
Epoch 8/10, Batch 56/883, Training Loss: 0.5854
Epoch 8/10, Batch 57/883, Training Loss: 0.5137
Epoch 8/10, Batch 58/883, Training Loss: 0.4758
Epoch 8/10, Batch 59/883, Training Loss: 0.7101
Epoch 8/10, Batch 60/883, Training Loss: 0.4898
Epoch 8/10, Batch 61/883, Training Loss: 0.3287
Epoch 8/10, Batch 62/883, Training Loss: 0.7492
Epoch 8/10, Batch 63/883, Training Loss: 0.5225
Epoch 8/10, Batch 64/883, Training Loss: 0.9812
Epoch 8/10, Batch 65/883, Training Loss: 0.2759
Epoch 8/10, Batch 66/883, Training Loss: 0.6070
Epoch 8/10, Batch 67/883, Training Loss: 0.6158
Epoch 8/10, Batch 68/883, Training Loss: 0.4854
Epoch 8/10, Batch 69/883, Training Loss: 0.5801
Epoch 8/10, Batch 70/883, Training Loss: 1.3265
Epoch 8/10, Batch 71/883, Training Loss: 0.7254
Epoch 8/10, Batch 72/883, Training Loss: 0.3957
Epoch 8/10, Batch 73/883, Training Loss: 0.7707
Epoch 8/10, Batch 74/883, Training Loss: 0.6696
Epoch 8/10, Batch 75/883, Training Loss: 0.4772
Epoch 8/10, Batch 76/883, Training Loss: 0.5186
Epoch 8/10, Batch 77/883, Training Loss: 0.7773
Epoch 8/10, Batch 78/883, Training Loss: 0.4607
Epoch 8/10, Batch 79/883, Training Loss: 0.5460
Epoch 8/10, Batch 80/883, Training Loss: 0.5648
Epoch 8/10, Batch 81/883, Training Loss: 0.5534
Epoch 8/10, Batch 82/883, Training Loss: 0.7441
Epoch 8/10, Batch 83/883, Training Loss: 0.6517
Epoch 8/10, Batch 84/883, Training Loss: 0.3817
Epoch 8/10, Batch 85/883, Training Loss: 0.4735
Epoch 8/10, Batch 86/883, Training Loss: 0.7814
Epoch 8/10, Batch 87/883, Training Loss: 0.4719
Epoch 8/10, Batch 88/883, Training Loss: 0.5834
Epoch 8/10, Batch 89/883, Training Loss: 0.5712
Epoch 8/10, Batch 90/883, Training Loss: 0.4829
Epoch 8/10, Batch 91/883, Training Loss: 0.5992
Epoch 8/10, Batch 92/883, Training Loss: 0.4916
Epoch 8/10, Batch 93/883, Training Loss: 0.4970
Epoch 8/10, Batch 94/883, Training Loss: 0.7348
Epoch 8/10, Batch 95/883, Training Loss: 0.4672
Epoch 8/10, Batch 96/883, Training Loss: 0.5765
Epoch 8/10, Batch 97/883, Training Loss: 0.5886
Epoch 8/10, Batch 98/883, Training Loss: 0.3920
Epoch 8/10, Batch 99/883, Training Loss: 0.7261
Epoch 8/10, Batch 100/883, Training Loss: 0.8362
Epoch 8/10, Batch 101/883, Training Loss: 0.6295
Epoch 8/10, Batch 102/883, Training Loss: 0.6596
Epoch 8/10, Batch 103/883, Training Loss: 0.7811
Epoch 8/10, Batch 104/883, Training Loss: 0.6563
Epoch 8/10, Batch 105/883, Training Loss: 0.6133
Epoch 8/10, Batch 106/883, Training Loss: 0.3862
Epoch 8/10, Batch 107/883, Training Loss: 0.5187
Epoch 8/10, Batch 108/883, Training Loss: 0.4296
Epoch 8/10, Batch 109/883, Training Loss: 0.7625
Epoch 8/10, Batch 110/883, Training Loss: 0.5846
Epoch 8/10, Batch 111/883, Training Loss: 0.3846
Epoch 8/10, Batch 112/883, Training Loss: 0.6984
Epoch 8/10, Batch 113/883, Training Loss: 0.5007
Epoch 8/10, Batch 114/883, Training Loss: 0.6349
Epoch 8/10, Batch 115/883, Training Loss: 0.6734
Epoch 8/10, Batch 116/883, Training Loss: 0.4557
Epoch 8/10, Batch 117/883, Training Loss: 0.7072
Epoch 8/10, Batch 118/883, Training Loss: 0.5205
Epoch 8/10, Batch 119/883, Training Loss: 0.4102
Epoch 8/10, Batch 120/883, Training Loss: 0.5522
Epoch 8/10, Batch 121/883, Training Loss: 0.4559
Epoch 8/10, Batch 122/883, Training Loss: 0.6873
Epoch 8/10, Batch 123/883, Training Loss: 0.7415
Epoch 8/10, Batch 124/883, Training Loss: 0.6313
Epoch 8/10, Batch 125/883, Training Loss: 0.2463
Epoch 8/10, Batch 126/883, Training Loss: 0.4422
Epoch 8/10, Batch 127/883, Training Loss: 0.6005
Epoch 8/10, Batch 128/883, Training Loss: 0.9077
Epoch 8/10, Batch 129/883, Training Loss: 0.4561
Epoch 8/10, Batch 130/883, Training Loss: 0.3956
Epoch 8/10, Batch 131/883, Training Loss: 0.5659
Epoch 8/10, Batch 132/883, Training Loss: 0.4378
Epoch 8/10, Batch 133/883, Training Loss: 0.2887
Epoch 8/10, Batch 134/883, Training Loss: 0.5428
Epoch 8/10, Batch 135/883, Training Loss: 0.6401
Epoch 8/10, Batch 136/883, Training Loss: 0.6858
Epoch 8/10, Batch 137/883, Training Loss: 0.8281
Epoch 8/10, Batch 138/883, Training Loss: 0.6376
Epoch 8/10, Batch 139/883, Training Loss: 0.8348
Epoch 8/10, Batch 140/883, Training Loss: 0.6388
Epoch 8/10, Batch 141/883, Training Loss: 0.7608
Epoch 8/10, Batch 142/883, Training Loss: 0.5179
Epoch 8/10, Batch 143/883, Training Loss: 0.2600
Epoch 8/10, Batch 144/883, Training Loss: 0.6223
Epoch 8/10, Batch 145/883, Training Loss: 0.4028
Epoch 8/10, Batch 146/883, Training Loss: 0.4409
Epoch 8/10, Batch 147/883, Training Loss: 0.4676
Epoch 8/10, Batch 148/883, Training Loss: 0.5332
Epoch 8/10, Batch 149/883, Training Loss: 0.3066
Epoch 8/10, Batch 150/883, Training Loss: 0.5288
Epoch 8/10, Batch 151/883, Training Loss: 0.5861
Epoch 8/10, Batch 152/883, Training Loss: 0.5847
Epoch 8/10, Batch 153/883, Training Loss: 0.3533
Epoch 8/10, Batch 154/883, Training Loss: 0.3847
Epoch 8/10, Batch 155/883, Training Loss: 0.3779
Epoch 8/10, Batch 156/883, Training Loss: 0.5443
Epoch 8/10, Batch 157/883, Training Loss: 0.7197
Epoch 8/10, Batch 158/883, Training Loss: 0.4542
Epoch 8/10, Batch 159/883, Training Loss: 0.4455
Epoch 8/10, Batch 160/883, Training Loss: 0.7794
Epoch 8/10, Batch 161/883, Training Loss: 0.6989
Epoch 8/10, Batch 162/883, Training Loss: 0.5229
Epoch 8/10, Batch 163/883, Training Loss: 0.8210
Epoch 8/10, Batch 164/883, Training Loss: 0.6673
Epoch 8/10, Batch 165/883, Training Loss: 0.5314
Epoch 8/10, Batch 166/883, Training Loss: 0.3840
Epoch 8/10, Batch 167/883, Training Loss: 0.3428
Epoch 8/10, Batch 168/883, Training Loss: 0.7585
Epoch 8/10, Batch 169/883, Training Loss: 0.3976
Epoch 8/10, Batch 170/883, Training Loss: 0.5024
Epoch 8/10, Batch 171/883, Training Loss: 0.9304
Epoch 8/10, Batch 172/883, Training Loss: 0.8049
Epoch 8/10, Batch 173/883, Training Loss: 0.4656
Epoch 8/10, Batch 174/883, Training Loss: 0.5134
Epoch 8/10, Batch 175/883, Training Loss: 0.4671
Epoch 8/10, Batch 176/883, Training Loss: 0.4424
Epoch 8/10, Batch 177/883, Training Loss: 0.5876
Epoch 8/10, Batch 178/883, Training Loss: 0.4373
Epoch 8/10, Batch 179/883, Training Loss: 0.3151
Epoch 8/10, Batch 180/883, Training Loss: 0.4821
Epoch 8/10, Batch 181/883, Training Loss: 0.3920
Epoch 8/10, Batch 182/883, Training Loss: 0.6627
Epoch 8/10, Batch 183/883, Training Loss: 0.3998
Epoch 8/10, Batch 184/883, Training Loss: 0.4537
Epoch 8/10, Batch 185/883, Training Loss: 0.4775
Epoch 8/10, Batch 186/883, Training Loss: 0.3551
Epoch 8/10, Batch 187/883, Training Loss: 0.2854
Epoch 8/10, Batch 188/883, Training Loss: 0.6305
Epoch 8/10, Batch 189/883, Training Loss: 0.5531
Epoch 8/10, Batch 190/883, Training Loss: 0.4335
Epoch 8/10, Batch 191/883, Training Loss: 0.7002
Epoch 8/10, Batch 192/883, Training Loss: 0.3340
Epoch 8/10, Batch 193/883, Training Loss: 0.4925
Epoch 8/10, Batch 194/883, Training Loss: 0.7254
Epoch 8/10, Batch 195/883, Training Loss: 0.4849
Epoch 8/10, Batch 196/883, Training Loss: 0.3722
Epoch 8/10, Batch 197/883, Training Loss: 0.6890
Epoch 8/10, Batch 198/883, Training Loss: 0.4370
Epoch 8/10, Batch 199/883, Training Loss: 0.5739
Epoch 8/10, Batch 200/883, Training Loss: 0.6222
Epoch 8/10, Batch 201/883, Training Loss: 0.4528
Epoch 8/10, Batch 202/883, Training Loss: 0.8683
Epoch 8/10, Batch 203/883, Training Loss: 0.5546
Epoch 8/10, Batch 204/883, Training Loss: 0.3438
Epoch 8/10, Batch 205/883, Training Loss: 0.3120
Epoch 8/10, Batch 206/883, Training Loss: 0.1495
Epoch 8/10, Batch 207/883, Training Loss: 0.4617
Epoch 8/10, Batch 208/883, Training Loss: 0.4318
Epoch 8/10, Batch 209/883, Training Loss: 0.5696
Epoch 8/10, Batch 210/883, Training Loss: 0.8650
Epoch 8/10, Batch 211/883, Training Loss: 0.3535
Epoch 8/10, Batch 212/883, Training Loss: 0.6937
Epoch 8/10, Batch 213/883, Training Loss: 0.3361
Epoch 8/10, Batch 214/883, Training Loss: 0.5103
Epoch 8/10, Batch 215/883, Training Loss: 0.3249
Epoch 8/10, Batch 216/883, Training Loss: 0.8953
Epoch 8/10, Batch 217/883, Training Loss: 0.6604
Epoch 8/10, Batch 218/883, Training Loss: 0.3940
Epoch 8/10, Batch 219/883, Training Loss: 0.6260
Epoch 8/10, Batch 220/883, Training Loss: 0.4388
Epoch 8/10, Batch 221/883, Training Loss: 0.4976
Epoch 8/10, Batch 222/883, Training Loss: 0.7609
Epoch 8/10, Batch 223/883, Training Loss: 0.4791
Epoch 8/10, Batch 224/883, Training Loss: 0.5292
Epoch 8/10, Batch 225/883, Training Loss: 0.8000
Epoch 8/10, Batch 226/883, Training Loss: 0.5848
Epoch 8/10, Batch 227/883, Training Loss: 0.2407
Epoch 8/10, Batch 228/883, Training Loss: 0.4897
Epoch 8/10, Batch 229/883, Training Loss: 0.7613
Epoch 8/10, Batch 230/883, Training Loss: 0.7174
Epoch 8/10, Batch 231/883, Training Loss: 0.2885
Epoch 8/10, Batch 232/883, Training Loss: 0.3768
Epoch 8/10, Batch 233/883, Training Loss: 0.6154
Epoch 8/10, Batch 234/883, Training Loss: 0.5861
Epoch 8/10, Batch 235/883, Training Loss: 1.1012
Epoch 8/10, Batch 236/883, Training Loss: 1.1510
Epoch 8/10, Batch 237/883, Training Loss: 0.4983
Epoch 8/10, Batch 238/883, Training Loss: 0.4725
Epoch 8/10, Batch 239/883, Training Loss: 0.5929
Epoch 8/10, Batch 240/883, Training Loss: 0.4621
Epoch 8/10, Batch 241/883, Training Loss: 0.4790
Epoch 8/10, Batch 242/883, Training Loss: 0.5679
Epoch 8/10, Batch 243/883, Training Loss: 0.3966
Epoch 8/10, Batch 244/883, Training Loss: 0.4274
Epoch 8/10, Batch 245/883, Training Loss: 0.8952
Epoch 8/10, Batch 246/883, Training Loss: 0.6039
Epoch 8/10, Batch 247/883, Training Loss: 0.8298
Epoch 8/10, Batch 248/883, Training Loss: 0.4733
Epoch 8/10, Batch 249/883, Training Loss: 0.5114
Epoch 8/10, Batch 250/883, Training Loss: 1.1624
Epoch 8/10, Batch 251/883, Training Loss: 0.8081
Epoch 8/10, Batch 252/883, Training Loss: 0.5561
Epoch 8/10, Batch 253/883, Training Loss: 0.4819
Epoch 8/10, Batch 254/883, Training Loss: 0.4706
Epoch 8/10, Batch 255/883, Training Loss: 0.6693
Epoch 8/10, Batch 256/883, Training Loss: 0.4053
Epoch 8/10, Batch 257/883, Training Loss: 0.5836
Epoch 8/10, Batch 258/883, Training Loss: 0.4345
Epoch 8/10, Batch 259/883, Training Loss: 0.6162
Epoch 8/10, Batch 260/883, Training Loss: 0.4415
Epoch 8/10, Batch 261/883, Training Loss: 0.7546
Epoch 8/10, Batch 262/883, Training Loss: 0.5257
Epoch 8/10, Batch 263/883, Training Loss: 0.7459
Epoch 8/10, Batch 264/883, Training Loss: 0.8099
Epoch 8/10, Batch 265/883, Training Loss: 0.5590
Epoch 8/10, Batch 266/883, Training Loss: 0.7549
Epoch 8/10, Batch 267/883, Training Loss: 1.0027
Epoch 8/10, Batch 268/883, Training Loss: 0.5207
Epoch 8/10, Batch 269/883, Training Loss: 0.8906
Epoch 8/10, Batch 270/883, Training Loss: 0.6116
Epoch 8/10, Batch 271/883, Training Loss: 0.5247
Epoch 8/10, Batch 272/883, Training Loss: 0.5800
Epoch 8/10, Batch 273/883, Training Loss: 0.6257
Epoch 8/10, Batch 274/883, Training Loss: 0.9161
Epoch 8/10, Batch 275/883, Training Loss: 0.5847
Epoch 8/10, Batch 276/883, Training Loss: 0.6316
Epoch 8/10, Batch 277/883, Training Loss: 0.4910
Epoch 8/10, Batch 278/883, Training Loss: 0.3145
Epoch 8/10, Batch 279/883, Training Loss: 0.9164
Epoch 8/10, Batch 280/883, Training Loss: 0.6268
Epoch 8/10, Batch 281/883, Training Loss: 0.6285
Epoch 8/10, Batch 282/883, Training Loss: 0.4274
Epoch 8/10, Batch 283/883, Training Loss: 0.5999
Epoch 8/10, Batch 284/883, Training Loss: 0.6210
Epoch 8/10, Batch 285/883, Training Loss: 0.5115
Epoch 8/10, Batch 286/883, Training Loss: 0.8217
Epoch 8/10, Batch 287/883, Training Loss: 0.6510
Epoch 8/10, Batch 288/883, Training Loss: 0.7609
Epoch 8/10, Batch 289/883, Training Loss: 0.4674
Epoch 8/10, Batch 290/883, Training Loss: 0.4015
Epoch 8/10, Batch 291/883, Training Loss: 0.6915
Epoch 8/10, Batch 292/883, Training Loss: 0.3247
Epoch 8/10, Batch 293/883, Training Loss: 0.6368
Epoch 8/10, Batch 294/883, Training Loss: 0.9367
Epoch 8/10, Batch 295/883, Training Loss: 0.5584
Epoch 8/10, Batch 296/883, Training Loss: 0.5033
Epoch 8/10, Batch 297/883, Training Loss: 0.2777
Epoch 8/10, Batch 298/883, Training Loss: 0.3699
Epoch 8/10, Batch 299/883, Training Loss: 0.4951
Epoch 8/10, Batch 300/883, Training Loss: 0.4693
Epoch 8/10, Batch 301/883, Training Loss: 0.6521
Epoch 8/10, Batch 302/883, Training Loss: 0.4222
Epoch 8/10, Batch 303/883, Training Loss: 0.5315
Epoch 8/10, Batch 304/883, Training Loss: 0.4045
Epoch 8/10, Batch 305/883, Training Loss: 0.7569
Epoch 8/10, Batch 306/883, Training Loss: 0.5083
Epoch 8/10, Batch 307/883, Training Loss: 0.9468
Epoch 8/10, Batch 308/883, Training Loss: 0.4040
Epoch 8/10, Batch 309/883, Training Loss: 0.5223
Epoch 8/10, Batch 310/883, Training Loss: 0.4711
Epoch 8/10, Batch 311/883, Training Loss: 0.6199
Epoch 8/10, Batch 312/883, Training Loss: 0.1934
Epoch 8/10, Batch 313/883, Training Loss: 0.4887
Epoch 8/10, Batch 314/883, Training Loss: 0.6100
Epoch 8/10, Batch 315/883, Training Loss: 0.2460
Epoch 8/10, Batch 316/883, Training Loss: 0.4480
Epoch 8/10, Batch 317/883, Training Loss: 0.3096
Epoch 8/10, Batch 318/883, Training Loss: 0.4393
Epoch 8/10, Batch 319/883, Training Loss: 0.4350
Epoch 8/10, Batch 320/883, Training Loss: 0.6524
Epoch 8/10, Batch 321/883, Training Loss: 0.7849
Epoch 8/10, Batch 322/883, Training Loss: 0.7156
Epoch 8/10, Batch 323/883, Training Loss: 0.6056
Epoch 8/10, Batch 324/883, Training Loss: 0.7117
Epoch 8/10, Batch 325/883, Training Loss: 0.4335
Epoch 8/10, Batch 326/883, Training Loss: 0.6250
Epoch 8/10, Batch 327/883, Training Loss: 0.3655
Epoch 8/10, Batch 328/883, Training Loss: 0.5687
Epoch 8/10, Batch 329/883, Training Loss: 0.6614
Epoch 8/10, Batch 330/883, Training Loss: 0.5758
Epoch 8/10, Batch 331/883, Training Loss: 0.6345
Epoch 8/10, Batch 332/883, Training Loss: 0.4498
Epoch 8/10, Batch 333/883, Training Loss: 0.4831
Epoch 8/10, Batch 334/883, Training Loss: 0.3838
Epoch 8/10, Batch 335/883, Training Loss: 0.3873
Epoch 8/10, Batch 336/883, Training Loss: 0.4323
Epoch 8/10, Batch 337/883, Training Loss: 0.5350
Epoch 8/10, Batch 338/883, Training Loss: 0.4468
Epoch 8/10, Batch 339/883, Training Loss: 0.5502
Epoch 8/10, Batch 340/883, Training Loss: 0.5851
Epoch 8/10, Batch 341/883, Training Loss: 1.1656
Epoch 8/10, Batch 342/883, Training Loss: 0.5183
Epoch 8/10, Batch 343/883, Training Loss: 0.4870
Epoch 8/10, Batch 344/883, Training Loss: 0.3839
Epoch 8/10, Batch 345/883, Training Loss: 0.2431
Epoch 8/10, Batch 346/883, Training Loss: 0.4482
Epoch 8/10, Batch 347/883, Training Loss: 0.2904
Epoch 8/10, Batch 348/883, Training Loss: 0.3869
Epoch 8/10, Batch 349/883, Training Loss: 0.4670
Epoch 8/10, Batch 350/883, Training Loss: 0.3272
Epoch 8/10, Batch 351/883, Training Loss: 0.4936
Epoch 8/10, Batch 352/883, Training Loss: 0.5830
Epoch 8/10, Batch 353/883, Training Loss: 0.4813
Epoch 8/10, Batch 354/883, Training Loss: 0.3340
Epoch 8/10, Batch 355/883, Training Loss: 1.0884
Epoch 8/10, Batch 356/883, Training Loss: 0.7191
Epoch 8/10, Batch 357/883, Training Loss: 0.8484
Epoch 8/10, Batch 358/883, Training Loss: 0.4197
Epoch 8/10, Batch 359/883, Training Loss: 0.5531
Epoch 8/10, Batch 360/883, Training Loss: 0.6253
Epoch 8/10, Batch 361/883, Training Loss: 0.4715
Epoch 8/10, Batch 362/883, Training Loss: 0.9865
Epoch 8/10, Batch 363/883, Training Loss: 0.3576
Epoch 8/10, Batch 364/883, Training Loss: 0.2324
Epoch 8/10, Batch 365/883, Training Loss: 0.7831
Epoch 8/10, Batch 366/883, Training Loss: 0.5578
Epoch 8/10, Batch 367/883, Training Loss: 0.3677
Epoch 8/10, Batch 368/883, Training Loss: 0.7703
Epoch 8/10, Batch 369/883, Training Loss: 0.6428
Epoch 8/10, Batch 370/883, Training Loss: 0.6686
Epoch 8/10, Batch 371/883, Training Loss: 0.7338
Epoch 8/10, Batch 372/883, Training Loss: 0.5799
Epoch 8/10, Batch 373/883, Training Loss: 0.5184
Epoch 8/10, Batch 374/883, Training Loss: 0.7101
Epoch 8/10, Batch 375/883, Training Loss: 0.6062
Epoch 8/10, Batch 376/883, Training Loss: 0.3200
Epoch 8/10, Batch 377/883, Training Loss: 0.6377
Epoch 8/10, Batch 378/883, Training Loss: 0.4709
Epoch 8/10, Batch 379/883, Training Loss: 0.6920
Epoch 8/10, Batch 380/883, Training Loss: 0.4618
Epoch 8/10, Batch 381/883, Training Loss: 0.5516
Epoch 8/10, Batch 382/883, Training Loss: 0.5062
Epoch 8/10, Batch 383/883, Training Loss: 0.6253
Epoch 8/10, Batch 384/883, Training Loss: 0.5669
Epoch 8/10, Batch 385/883, Training Loss: 0.7436
Epoch 8/10, Batch 386/883, Training Loss: 0.4996
Epoch 8/10, Batch 387/883, Training Loss: 0.5080
Epoch 8/10, Batch 388/883, Training Loss: 0.6664
Epoch 8/10, Batch 389/883, Training Loss: 0.5324
Epoch 8/10, Batch 390/883, Training Loss: 0.5914
Epoch 8/10, Batch 391/883, Training Loss: 0.7918
Epoch 8/10, Batch 392/883, Training Loss: 0.5978
Epoch 8/10, Batch 393/883, Training Loss: 0.4527
Epoch 8/10, Batch 394/883, Training Loss: 0.5441
Epoch 8/10, Batch 395/883, Training Loss: 0.9128
Epoch 8/10, Batch 396/883, Training Loss: 0.4782
Epoch 8/10, Batch 397/883, Training Loss: 0.7828
Epoch 8/10, Batch 398/883, Training Loss: 0.5830
Epoch 8/10, Batch 399/883, Training Loss: 0.3720
Epoch 8/10, Batch 400/883, Training Loss: 0.5206
Epoch 8/10, Batch 401/883, Training Loss: 0.7109
Epoch 8/10, Batch 402/883, Training Loss: 0.5881
Epoch 8/10, Batch 403/883, Training Loss: 0.4867
Epoch 8/10, Batch 404/883, Training Loss: 0.5984
Epoch 8/10, Batch 405/883, Training Loss: 0.4467
Epoch 8/10, Batch 406/883, Training Loss: 0.6470
Epoch 8/10, Batch 407/883, Training Loss: 0.5413
Epoch 8/10, Batch 408/883, Training Loss: 0.4107
Epoch 8/10, Batch 409/883, Training Loss: 0.4391
Epoch 8/10, Batch 410/883, Training Loss: 0.3806
Epoch 8/10, Batch 411/883, Training Loss: 0.5489
Epoch 8/10, Batch 412/883, Training Loss: 0.9723
Epoch 8/10, Batch 413/883, Training Loss: 0.8189
Epoch 8/10, Batch 414/883, Training Loss: 0.6896
Epoch 8/10, Batch 415/883, Training Loss: 0.5724
Epoch 8/10, Batch 416/883, Training Loss: 1.0089
Epoch 8/10, Batch 417/883, Training Loss: 0.3580
Epoch 8/10, Batch 418/883, Training Loss: 0.3991
Epoch 8/10, Batch 419/883, Training Loss: 0.5426
Epoch 8/10, Batch 420/883, Training Loss: 0.6208
Epoch 8/10, Batch 421/883, Training Loss: 0.3582
Epoch 8/10, Batch 422/883, Training Loss: 0.4336
Epoch 8/10, Batch 423/883, Training Loss: 0.5873
Epoch 8/10, Batch 424/883, Training Loss: 0.5189
Epoch 8/10, Batch 425/883, Training Loss: 0.8658
Epoch 8/10, Batch 426/883, Training Loss: 0.4528
Epoch 8/10, Batch 427/883, Training Loss: 0.7320
Epoch 8/10, Batch 428/883, Training Loss: 0.9827
Epoch 8/10, Batch 429/883, Training Loss: 0.6145
Epoch 8/10, Batch 430/883, Training Loss: 0.6255
Epoch 8/10, Batch 431/883, Training Loss: 0.7999
Epoch 8/10, Batch 432/883, Training Loss: 0.5167
Epoch 8/10, Batch 433/883, Training Loss: 0.4312
Epoch 8/10, Batch 434/883, Training Loss: 0.2846
Epoch 8/10, Batch 435/883, Training Loss: 0.7414
Epoch 8/10, Batch 436/883, Training Loss: 0.4610
Epoch 8/10, Batch 437/883, Training Loss: 0.7897
Epoch 8/10, Batch 438/883, Training Loss: 0.7365
Epoch 8/10, Batch 439/883, Training Loss: 0.4171
Epoch 8/10, Batch 440/883, Training Loss: 0.6923
Epoch 8/10, Batch 441/883, Training Loss: 0.7992
Epoch 8/10, Batch 442/883, Training Loss: 0.5963
Epoch 8/10, Batch 443/883, Training Loss: 0.3462
Epoch 8/10, Batch 444/883, Training Loss: 0.3881
Epoch 8/10, Batch 445/883, Training Loss: 0.4152
Epoch 8/10, Batch 446/883, Training Loss: 0.4495
Epoch 8/10, Batch 447/883, Training Loss: 0.3968
Epoch 8/10, Batch 448/883, Training Loss: 0.6483
Epoch 8/10, Batch 449/883, Training Loss: 0.7177
Epoch 8/10, Batch 450/883, Training Loss: 0.8365
Epoch 8/10, Batch 451/883, Training Loss: 0.6289
Epoch 8/10, Batch 452/883, Training Loss: 0.6658
Epoch 8/10, Batch 453/883, Training Loss: 0.5169
Epoch 8/10, Batch 454/883, Training Loss: 0.3938
Epoch 8/10, Batch 455/883, Training Loss: 0.4130
Epoch 8/10, Batch 456/883, Training Loss: 0.9481
Epoch 8/10, Batch 457/883, Training Loss: 0.4173
Epoch 8/10, Batch 458/883, Training Loss: 0.5813
Epoch 8/10, Batch 459/883, Training Loss: 0.4673
Epoch 8/10, Batch 460/883, Training Loss: 0.5746
Epoch 8/10, Batch 461/883, Training Loss: 0.4219
Epoch 8/10, Batch 462/883, Training Loss: 0.5658
Epoch 8/10, Batch 463/883, Training Loss: 0.6895
Epoch 8/10, Batch 464/883, Training Loss: 0.5454
Epoch 8/10, Batch 465/883, Training Loss: 0.5544
Epoch 8/10, Batch 466/883, Training Loss: 0.5441
Epoch 8/10, Batch 467/883, Training Loss: 0.9669
Epoch 8/10, Batch 468/883, Training Loss: 0.5201
Epoch 8/10, Batch 469/883, Training Loss: 0.4674
Epoch 8/10, Batch 470/883, Training Loss: 0.6378
Epoch 8/10, Batch 471/883, Training Loss: 0.4188
Epoch 8/10, Batch 472/883, Training Loss: 0.4183
Epoch 8/10, Batch 473/883, Training Loss: 0.5200
Epoch 8/10, Batch 474/883, Training Loss: 0.9155
Epoch 8/10, Batch 475/883, Training Loss: 0.4641
Epoch 8/10, Batch 476/883, Training Loss: 0.2751
Epoch 8/10, Batch 477/883, Training Loss: 0.4005
Epoch 8/10, Batch 478/883, Training Loss: 0.5155
Epoch 8/10, Batch 479/883, Training Loss: 0.7309
Epoch 8/10, Batch 480/883, Training Loss: 0.5627
Epoch 8/10, Batch 481/883, Training Loss: 0.5568
Epoch 8/10, Batch 482/883, Training Loss: 0.5173
Epoch 8/10, Batch 483/883, Training Loss: 1.0040
Epoch 8/10, Batch 484/883, Training Loss: 0.6220
Epoch 8/10, Batch 485/883, Training Loss: 0.3664
Epoch 8/10, Batch 486/883, Training Loss: 0.6444
Epoch 8/10, Batch 487/883, Training Loss: 0.6205
Epoch 8/10, Batch 488/883, Training Loss: 0.4543
Epoch 8/10, Batch 489/883, Training Loss: 0.4055
Epoch 8/10, Batch 490/883, Training Loss: 0.4252
Epoch 8/10, Batch 491/883, Training Loss: 0.4232
Epoch 8/10, Batch 492/883, Training Loss: 0.6883
Epoch 8/10, Batch 493/883, Training Loss: 0.7019
Epoch 8/10, Batch 494/883, Training Loss: 0.7448
Epoch 8/10, Batch 495/883, Training Loss: 0.6076
Epoch 8/10, Batch 496/883, Training Loss: 0.2731
Epoch 8/10, Batch 497/883, Training Loss: 0.3963
Epoch 8/10, Batch 498/883, Training Loss: 0.4107
Epoch 8/10, Batch 499/883, Training Loss: 0.4003
Epoch 8/10, Batch 500/883, Training Loss: 0.4494
Epoch 8/10, Batch 501/883, Training Loss: 0.5996
Epoch 8/10, Batch 502/883, Training Loss: 0.5010
Epoch 8/10, Batch 503/883, Training Loss: 0.5182
Epoch 8/10, Batch 504/883, Training Loss: 0.4462
Epoch 8/10, Batch 505/883, Training Loss: 0.4359
Epoch 8/10, Batch 506/883, Training Loss: 0.4002
Epoch 8/10, Batch 507/883, Training Loss: 0.3934
Epoch 8/10, Batch 508/883, Training Loss: 0.6162
Epoch 8/10, Batch 509/883, Training Loss: 0.4574
Epoch 8/10, Batch 510/883, Training Loss: 0.5660
Epoch 8/10, Batch 511/883, Training Loss: 0.4454
Epoch 8/10, Batch 512/883, Training Loss: 0.5089
Epoch 8/10, Batch 513/883, Training Loss: 0.3220
Epoch 8/10, Batch 514/883, Training Loss: 0.5394
Epoch 8/10, Batch 515/883, Training Loss: 0.8964
Epoch 8/10, Batch 516/883, Training Loss: 0.4711
Epoch 8/10, Batch 517/883, Training Loss: 0.5726
Epoch 8/10, Batch 518/883, Training Loss: 0.7967
Epoch 8/10, Batch 519/883, Training Loss: 0.6785
Epoch 8/10, Batch 520/883, Training Loss: 0.3912
Epoch 8/10, Batch 521/883, Training Loss: 0.5611
Epoch 8/10, Batch 522/883, Training Loss: 0.5227
Epoch 8/10, Batch 523/883, Training Loss: 0.4289
Epoch 8/10, Batch 524/883, Training Loss: 0.4136
Epoch 8/10, Batch 525/883, Training Loss: 0.6923
Epoch 8/10, Batch 526/883, Training Loss: 0.4458
Epoch 8/10, Batch 527/883, Training Loss: 0.6378
Epoch 8/10, Batch 528/883, Training Loss: 0.8832
Epoch 8/10, Batch 529/883, Training Loss: 0.8253
Epoch 8/10, Batch 530/883, Training Loss: 1.0311
Epoch 8/10, Batch 531/883, Training Loss: 0.6937
Epoch 8/10, Batch 532/883, Training Loss: 0.6827
Epoch 8/10, Batch 533/883, Training Loss: 0.4616
Epoch 8/10, Batch 534/883, Training Loss: 0.5729
Epoch 8/10, Batch 535/883, Training Loss: 0.4661
Epoch 8/10, Batch 536/883, Training Loss: 0.8622
Epoch 8/10, Batch 537/883, Training Loss: 0.8317
Epoch 8/10, Batch 538/883, Training Loss: 0.2605
Epoch 8/10, Batch 539/883, Training Loss: 0.5041
Epoch 8/10, Batch 540/883, Training Loss: 0.4808
Epoch 8/10, Batch 541/883, Training Loss: 0.6639
Epoch 8/10, Batch 542/883, Training Loss: 0.6800
Epoch 8/10, Batch 543/883, Training Loss: 0.8791
Epoch 8/10, Batch 544/883, Training Loss: 0.5357
Epoch 8/10, Batch 545/883, Training Loss: 0.4943
Epoch 8/10, Batch 546/883, Training Loss: 0.7135
Epoch 8/10, Batch 547/883, Training Loss: 0.5485
Epoch 8/10, Batch 548/883, Training Loss: 0.6722
Epoch 8/10, Batch 549/883, Training Loss: 0.3516
Epoch 8/10, Batch 550/883, Training Loss: 0.3358
Epoch 8/10, Batch 551/883, Training Loss: 0.8018
Epoch 8/10, Batch 552/883, Training Loss: 0.2343
Epoch 8/10, Batch 553/883, Training Loss: 0.2596
Epoch 8/10, Batch 554/883, Training Loss: 0.3017
Epoch 8/10, Batch 555/883, Training Loss: 0.3771
Epoch 8/10, Batch 556/883, Training Loss: 0.6075
Epoch 8/10, Batch 557/883, Training Loss: 0.6433
Epoch 8/10, Batch 558/883, Training Loss: 0.3481
Epoch 8/10, Batch 559/883, Training Loss: 0.7034
Epoch 8/10, Batch 560/883, Training Loss: 0.6183
Epoch 8/10, Batch 561/883, Training Loss: 0.5504
Epoch 8/10, Batch 562/883, Training Loss: 0.7838
Epoch 8/10, Batch 563/883, Training Loss: 0.2146
Epoch 8/10, Batch 564/883, Training Loss: 0.3583
Epoch 8/10, Batch 565/883, Training Loss: 0.6407
Epoch 8/10, Batch 566/883, Training Loss: 0.6282
Epoch 8/10, Batch 567/883, Training Loss: 0.4242
Epoch 8/10, Batch 568/883, Training Loss: 0.7451
Epoch 8/10, Batch 569/883, Training Loss: 0.3152
Epoch 8/10, Batch 570/883, Training Loss: 0.3025
Epoch 8/10, Batch 571/883, Training Loss: 0.5993
Epoch 8/10, Batch 572/883, Training Loss: 0.5704
Epoch 8/10, Batch 573/883, Training Loss: 0.4009
Epoch 8/10, Batch 574/883, Training Loss: 0.5408
Epoch 8/10, Batch 575/883, Training Loss: 0.4430
Epoch 8/10, Batch 576/883, Training Loss: 0.4206
Epoch 8/10, Batch 577/883, Training Loss: 0.4783
Epoch 8/10, Batch 578/883, Training Loss: 0.5331
Epoch 8/10, Batch 579/883, Training Loss: 0.5889
Epoch 8/10, Batch 580/883, Training Loss: 0.7757
Epoch 8/10, Batch 581/883, Training Loss: 0.6358
Epoch 8/10, Batch 582/883, Training Loss: 0.4912
Epoch 8/10, Batch 583/883, Training Loss: 0.5719
Epoch 8/10, Batch 584/883, Training Loss: 0.7362
Epoch 8/10, Batch 585/883, Training Loss: 0.6568
Epoch 8/10, Batch 586/883, Training Loss: 1.0801
Epoch 8/10, Batch 587/883, Training Loss: 0.5543
Epoch 8/10, Batch 588/883, Training Loss: 1.0800
Epoch 8/10, Batch 589/883, Training Loss: 1.1315
Epoch 8/10, Batch 590/883, Training Loss: 0.7340
Epoch 8/10, Batch 591/883, Training Loss: 0.4311
Epoch 8/10, Batch 592/883, Training Loss: 0.4215
Epoch 8/10, Batch 593/883, Training Loss: 0.5539
Epoch 8/10, Batch 594/883, Training Loss: 0.3643
Epoch 8/10, Batch 595/883, Training Loss: 0.6293
Epoch 8/10, Batch 596/883, Training Loss: 0.5711
Epoch 8/10, Batch 597/883, Training Loss: 0.8011
Epoch 8/10, Batch 598/883, Training Loss: 0.8703
Epoch 8/10, Batch 599/883, Training Loss: 0.5789
Epoch 8/10, Batch 600/883, Training Loss: 0.4560
Epoch 8/10, Batch 601/883, Training Loss: 0.5572
Epoch 8/10, Batch 602/883, Training Loss: 0.4169
Epoch 8/10, Batch 603/883, Training Loss: 0.5725
Epoch 8/10, Batch 604/883, Training Loss: 0.6303
Epoch 8/10, Batch 605/883, Training Loss: 0.2806
Epoch 8/10, Batch 606/883, Training Loss: 0.3693
Epoch 8/10, Batch 607/883, Training Loss: 0.4348
Epoch 8/10, Batch 608/883, Training Loss: 0.5828
Epoch 8/10, Batch 609/883, Training Loss: 0.4123
Epoch 8/10, Batch 610/883, Training Loss: 0.4738
Epoch 8/10, Batch 611/883, Training Loss: 0.4718
Epoch 8/10, Batch 612/883, Training Loss: 0.5597
Epoch 8/10, Batch 613/883, Training Loss: 0.6162
Epoch 8/10, Batch 614/883, Training Loss: 0.4959
Epoch 8/10, Batch 615/883, Training Loss: 0.4686
Epoch 8/10, Batch 616/883, Training Loss: 0.6280
Epoch 8/10, Batch 617/883, Training Loss: 0.7476
Epoch 8/10, Batch 618/883, Training Loss: 0.3684
Epoch 8/10, Batch 619/883, Training Loss: 0.8675
Epoch 8/10, Batch 620/883, Training Loss: 0.5308
Epoch 8/10, Batch 621/883, Training Loss: 0.5704
Epoch 8/10, Batch 622/883, Training Loss: 0.6084
Epoch 8/10, Batch 623/883, Training Loss: 0.6437
Epoch 8/10, Batch 624/883, Training Loss: 0.9870
Epoch 8/10, Batch 625/883, Training Loss: 0.5450
Epoch 8/10, Batch 626/883, Training Loss: 0.6235
Epoch 8/10, Batch 627/883, Training Loss: 0.6005
Epoch 8/10, Batch 628/883, Training Loss: 0.6599
Epoch 8/10, Batch 629/883, Training Loss: 0.5696
Epoch 8/10, Batch 630/883, Training Loss: 1.0624
Epoch 8/10, Batch 631/883, Training Loss: 0.7151
Epoch 8/10, Batch 632/883, Training Loss: 0.6984
Epoch 8/10, Batch 633/883, Training Loss: 0.5222
Epoch 8/10, Batch 634/883, Training Loss: 0.5690
Epoch 8/10, Batch 635/883, Training Loss: 0.5209
Epoch 8/10, Batch 636/883, Training Loss: 0.3890
Epoch 8/10, Batch 637/883, Training Loss: 0.3551
Epoch 8/10, Batch 638/883, Training Loss: 0.5961
Epoch 8/10, Batch 639/883, Training Loss: 0.6582
Epoch 8/10, Batch 640/883, Training Loss: 0.7099
Epoch 8/10, Batch 641/883, Training Loss: 0.4056
Epoch 8/10, Batch 642/883, Training Loss: 0.4720
Epoch 8/10, Batch 643/883, Training Loss: 0.4075
Epoch 8/10, Batch 644/883, Training Loss: 0.6443
Epoch 8/10, Batch 645/883, Training Loss: 0.4471
Epoch 8/10, Batch 646/883, Training Loss: 0.7022
Epoch 8/10, Batch 647/883, Training Loss: 0.7343
Epoch 8/10, Batch 648/883, Training Loss: 0.6593
Epoch 8/10, Batch 649/883, Training Loss: 0.3890
Epoch 8/10, Batch 650/883, Training Loss: 0.7554
Epoch 8/10, Batch 651/883, Training Loss: 0.2973
Epoch 8/10, Batch 652/883, Training Loss: 0.3892
Epoch 8/10, Batch 653/883, Training Loss: 0.7107
Epoch 8/10, Batch 654/883, Training Loss: 0.4518
Epoch 8/10, Batch 655/883, Training Loss: 0.2589
Epoch 8/10, Batch 656/883, Training Loss: 0.4405
Epoch 8/10, Batch 657/883, Training Loss: 0.4007
Epoch 8/10, Batch 658/883, Training Loss: 0.5626
Epoch 8/10, Batch 659/883, Training Loss: 0.4439
Epoch 8/10, Batch 660/883, Training Loss: 0.5420
Epoch 8/10, Batch 661/883, Training Loss: 0.5460
Epoch 8/10, Batch 662/883, Training Loss: 0.8655
Epoch 8/10, Batch 663/883, Training Loss: 0.6769
Epoch 8/10, Batch 664/883, Training Loss: 0.4111
Epoch 8/10, Batch 665/883, Training Loss: 0.8463
Epoch 8/10, Batch 666/883, Training Loss: 0.6643
Epoch 8/10, Batch 667/883, Training Loss: 0.3190
Epoch 8/10, Batch 668/883, Training Loss: 0.8906
Epoch 8/10, Batch 669/883, Training Loss: 0.6121
Epoch 8/10, Batch 670/883, Training Loss: 0.6312
Epoch 8/10, Batch 671/883, Training Loss: 0.3854
Epoch 8/10, Batch 672/883, Training Loss: 0.7877
Epoch 8/10, Batch 673/883, Training Loss: 0.6056
Epoch 8/10, Batch 674/883, Training Loss: 0.6083
Epoch 8/10, Batch 675/883, Training Loss: 0.3458
Epoch 8/10, Batch 676/883, Training Loss: 0.6203
Epoch 8/10, Batch 677/883, Training Loss: 0.4014
Epoch 8/10, Batch 678/883, Training Loss: 0.3714
Epoch 8/10, Batch 679/883, Training Loss: 0.6460
Epoch 8/10, Batch 680/883, Training Loss: 0.5508
Epoch 8/10, Batch 681/883, Training Loss: 0.8366
Epoch 8/10, Batch 682/883, Training Loss: 0.8804
Epoch 8/10, Batch 683/883, Training Loss: 0.4328
Epoch 8/10, Batch 684/883, Training Loss: 0.4593
Epoch 8/10, Batch 685/883, Training Loss: 0.8271
Epoch 8/10, Batch 686/883, Training Loss: 0.2921
Epoch 8/10, Batch 687/883, Training Loss: 0.6943
Epoch 8/10, Batch 688/883, Training Loss: 0.7000
Epoch 8/10, Batch 689/883, Training Loss: 0.7969
Epoch 8/10, Batch 690/883, Training Loss: 0.6689
Epoch 8/10, Batch 691/883, Training Loss: 0.7234
Epoch 8/10, Batch 692/883, Training Loss: 0.3327
Epoch 8/10, Batch 693/883, Training Loss: 0.5093
Epoch 8/10, Batch 694/883, Training Loss: 0.4025
Epoch 8/10, Batch 695/883, Training Loss: 0.3131
Epoch 8/10, Batch 696/883, Training Loss: 0.4792
Epoch 8/10, Batch 697/883, Training Loss: 0.5537
Epoch 8/10, Batch 698/883, Training Loss: 0.4714
Epoch 8/10, Batch 699/883, Training Loss: 0.5728
Epoch 8/10, Batch 700/883, Training Loss: 0.5750
Epoch 8/10, Batch 701/883, Training Loss: 0.4150
Epoch 8/10, Batch 702/883, Training Loss: 0.5149
Epoch 8/10, Batch 703/883, Training Loss: 0.5224
Epoch 8/10, Batch 704/883, Training Loss: 0.6422
Epoch 8/10, Batch 705/883, Training Loss: 0.3753
Epoch 8/10, Batch 706/883, Training Loss: 0.3321
Epoch 8/10, Batch 707/883, Training Loss: 0.2615
Epoch 8/10, Batch 708/883, Training Loss: 0.7739
Epoch 8/10, Batch 709/883, Training Loss: 0.6298
Epoch 8/10, Batch 710/883, Training Loss: 0.8136
Epoch 8/10, Batch 711/883, Training Loss: 0.5795
Epoch 8/10, Batch 712/883, Training Loss: 0.4567
Epoch 8/10, Batch 713/883, Training Loss: 0.2505
Epoch 8/10, Batch 714/883, Training Loss: 0.6047
Epoch 8/10, Batch 715/883, Training Loss: 0.3834
Epoch 8/10, Batch 716/883, Training Loss: 0.5886
Epoch 8/10, Batch 717/883, Training Loss: 0.3533
Epoch 8/10, Batch 718/883, Training Loss: 0.3851
Epoch 8/10, Batch 719/883, Training Loss: 0.3289
Epoch 8/10, Batch 720/883, Training Loss: 0.3787
Epoch 8/10, Batch 721/883, Training Loss: 0.3647
Epoch 8/10, Batch 722/883, Training Loss: 0.4769
Epoch 8/10, Batch 723/883, Training Loss: 0.5711
Epoch 8/10, Batch 724/883, Training Loss: 0.5800
Epoch 8/10, Batch 725/883, Training Loss: 0.6487
Epoch 8/10, Batch 726/883, Training Loss: 0.4384
Epoch 8/10, Batch 727/883, Training Loss: 0.5927
Epoch 8/10, Batch 728/883, Training Loss: 0.3093
Epoch 8/10, Batch 729/883, Training Loss: 0.9087
Epoch 8/10, Batch 730/883, Training Loss: 0.4982
Epoch 8/10, Batch 731/883, Training Loss: 1.0833
Epoch 8/10, Batch 732/883, Training Loss: 0.5209
Epoch 8/10, Batch 733/883, Training Loss: 0.3619
Epoch 8/10, Batch 734/883, Training Loss: 0.3502
Epoch 8/10, Batch 735/883, Training Loss: 0.7462
Epoch 8/10, Batch 736/883, Training Loss: 0.4966
Epoch 8/10, Batch 737/883, Training Loss: 0.2971
Epoch 8/10, Batch 738/883, Training Loss: 0.9231
Epoch 8/10, Batch 739/883, Training Loss: 0.2207
Epoch 8/10, Batch 740/883, Training Loss: 0.3789
Epoch 8/10, Batch 741/883, Training Loss: 0.8438
Epoch 8/10, Batch 742/883, Training Loss: 0.5177
Epoch 8/10, Batch 743/883, Training Loss: 0.6163
Epoch 8/10, Batch 744/883, Training Loss: 0.7756
Epoch 8/10, Batch 745/883, Training Loss: 0.9916
Epoch 8/10, Batch 746/883, Training Loss: 0.9862
Epoch 8/10, Batch 747/883, Training Loss: 0.7932
Epoch 8/10, Batch 748/883, Training Loss: 0.3368
Epoch 8/10, Batch 749/883, Training Loss: 0.3295
Epoch 8/10, Batch 750/883, Training Loss: 0.4469
Epoch 8/10, Batch 751/883, Training Loss: 0.5331
Epoch 8/10, Batch 752/883, Training Loss: 0.5540
Epoch 8/10, Batch 753/883, Training Loss: 0.5696
Epoch 8/10, Batch 754/883, Training Loss: 0.7415
Epoch 8/10, Batch 755/883, Training Loss: 0.6646
Epoch 8/10, Batch 756/883, Training Loss: 0.3538
Epoch 8/10, Batch 757/883, Training Loss: 0.4471
Epoch 8/10, Batch 758/883, Training Loss: 0.6413
Epoch 8/10, Batch 759/883, Training Loss: 0.4943
Epoch 8/10, Batch 760/883, Training Loss: 0.4620
Epoch 8/10, Batch 761/883, Training Loss: 0.3987
Epoch 8/10, Batch 762/883, Training Loss: 0.5511
Epoch 8/10, Batch 763/883, Training Loss: 0.6295
Epoch 8/10, Batch 764/883, Training Loss: 0.5951
Epoch 8/10, Batch 765/883, Training Loss: 0.6840
Epoch 8/10, Batch 766/883, Training Loss: 0.6692
Epoch 8/10, Batch 767/883, Training Loss: 0.9770
Epoch 8/10, Batch 768/883, Training Loss: 0.3517
Epoch 8/10, Batch 769/883, Training Loss: 0.2509
Epoch 8/10, Batch 770/883, Training Loss: 0.6300
Epoch 8/10, Batch 771/883, Training Loss: 0.6451
Epoch 8/10, Batch 772/883, Training Loss: 0.6026
Epoch 8/10, Batch 773/883, Training Loss: 0.3956
Epoch 8/10, Batch 774/883, Training Loss: 0.7608
Epoch 8/10, Batch 775/883, Training Loss: 0.9063
Epoch 8/10, Batch 776/883, Training Loss: 0.5180
Epoch 8/10, Batch 777/883, Training Loss: 0.4108
Epoch 8/10, Batch 778/883, Training Loss: 0.6699
Epoch 8/10, Batch 779/883, Training Loss: 0.6536
Epoch 8/10, Batch 780/883, Training Loss: 0.5995
Epoch 8/10, Batch 781/883, Training Loss: 0.8572
Epoch 8/10, Batch 782/883, Training Loss: 0.5173
Epoch 8/10, Batch 783/883, Training Loss: 0.3045
Epoch 8/10, Batch 784/883, Training Loss: 0.6354
Epoch 8/10, Batch 785/883, Training Loss: 0.5339
Epoch 8/10, Batch 786/883, Training Loss: 0.6839
Epoch 8/10, Batch 787/883, Training Loss: 0.5758
Epoch 8/10, Batch 788/883, Training Loss: 0.3915
Epoch 8/10, Batch 789/883, Training Loss: 0.6033
Epoch 8/10, Batch 790/883, Training Loss: 0.4056
Epoch 8/10, Batch 791/883, Training Loss: 0.3965
Epoch 8/10, Batch 792/883, Training Loss: 0.5868
Epoch 8/10, Batch 793/883, Training Loss: 0.5739
Epoch 8/10, Batch 794/883, Training Loss: 0.5338
Epoch 8/10, Batch 795/883, Training Loss: 0.5934
Epoch 8/10, Batch 796/883, Training Loss: 0.6372
Epoch 8/10, Batch 797/883, Training Loss: 0.3786
Epoch 8/10, Batch 798/883, Training Loss: 0.6475
Epoch 8/10, Batch 799/883, Training Loss: 0.5225
Epoch 8/10, Batch 800/883, Training Loss: 0.3039
Epoch 8/10, Batch 801/883, Training Loss: 0.7455
Epoch 8/10, Batch 802/883, Training Loss: 0.6743
Epoch 8/10, Batch 803/883, Training Loss: 0.2827
Epoch 8/10, Batch 804/883, Training Loss: 0.4242
Epoch 8/10, Batch 805/883, Training Loss: 0.5999
Epoch 8/10, Batch 806/883, Training Loss: 0.3197
Epoch 8/10, Batch 807/883, Training Loss: 0.5313
Epoch 8/10, Batch 808/883, Training Loss: 0.4906
Epoch 8/10, Batch 809/883, Training Loss: 0.8806
Epoch 8/10, Batch 810/883, Training Loss: 0.6321
Epoch 8/10, Batch 811/883, Training Loss: 0.4620
Epoch 8/10, Batch 812/883, Training Loss: 0.3741
Epoch 8/10, Batch 813/883, Training Loss: 0.5487
Epoch 8/10, Batch 814/883, Training Loss: 0.5018
Epoch 8/10, Batch 815/883, Training Loss: 0.4212
Epoch 8/10, Batch 816/883, Training Loss: 0.6611
Epoch 8/10, Batch 817/883, Training Loss: 0.6720
Epoch 8/10, Batch 818/883, Training Loss: 0.5623
Epoch 8/10, Batch 819/883, Training Loss: 0.9138
Epoch 8/10, Batch 820/883, Training Loss: 0.7483
Epoch 8/10, Batch 821/883, Training Loss: 0.2757
Epoch 8/10, Batch 822/883, Training Loss: 0.5168
Epoch 8/10, Batch 823/883, Training Loss: 0.6489
Epoch 8/10, Batch 824/883, Training Loss: 0.5616
Epoch 8/10, Batch 825/883, Training Loss: 0.2934
Epoch 8/10, Batch 826/883, Training Loss: 0.4621
Epoch 8/10, Batch 827/883, Training Loss: 0.4880
Epoch 8/10, Batch 828/883, Training Loss: 0.4135
Epoch 8/10, Batch 829/883, Training Loss: 0.8697
Epoch 8/10, Batch 830/883, Training Loss: 0.5190
Epoch 8/10, Batch 831/883, Training Loss: 0.3230
Epoch 8/10, Batch 832/883, Training Loss: 0.3976
Epoch 8/10, Batch 833/883, Training Loss: 0.4457
Epoch 8/10, Batch 834/883, Training Loss: 0.7207
Epoch 8/10, Batch 835/883, Training Loss: 0.5135
Epoch 8/10, Batch 836/883, Training Loss: 0.4532
Epoch 8/10, Batch 837/883, Training Loss: 0.5449
Epoch 8/10, Batch 838/883, Training Loss: 0.4979
Epoch 8/10, Batch 839/883, Training Loss: 0.5832
Epoch 8/10, Batch 840/883, Training Loss: 0.7039
Epoch 8/10, Batch 841/883, Training Loss: 0.6700
Epoch 8/10, Batch 842/883, Training Loss: 0.4728
Epoch 8/10, Batch 843/883, Training Loss: 0.4537
Epoch 8/10, Batch 844/883, Training Loss: 0.4685
Epoch 8/10, Batch 845/883, Training Loss: 0.9928
Epoch 8/10, Batch 846/883, Training Loss: 0.4956
Epoch 8/10, Batch 847/883, Training Loss: 0.6479
Epoch 8/10, Batch 848/883, Training Loss: 0.6807
Epoch 8/10, Batch 849/883, Training Loss: 0.5535
Epoch 8/10, Batch 850/883, Training Loss: 0.3755
Epoch 8/10, Batch 851/883, Training Loss: 0.9159
Epoch 8/10, Batch 852/883, Training Loss: 0.4721
Epoch 8/10, Batch 853/883, Training Loss: 0.4879
Epoch 8/10, Batch 854/883, Training Loss: 0.6260
Epoch 8/10, Batch 855/883, Training Loss: 0.3968
Epoch 8/10, Batch 856/883, Training Loss: 1.0856
Epoch 8/10, Batch 857/883, Training Loss: 0.7652
Epoch 8/10, Batch 858/883, Training Loss: 0.6160
Epoch 8/10, Batch 859/883, Training Loss: 0.4303
Epoch 8/10, Batch 860/883, Training Loss: 0.4727
Epoch 8/10, Batch 861/883, Training Loss: 0.5376
Epoch 8/10, Batch 862/883, Training Loss: 0.6437
Epoch 8/10, Batch 863/883, Training Loss: 0.6289
Epoch 8/10, Batch 864/883, Training Loss: 0.7778
Epoch 8/10, Batch 865/883, Training Loss: 0.5179
Epoch 8/10, Batch 866/883, Training Loss: 0.5550
Epoch 8/10, Batch 867/883, Training Loss: 0.3712
Epoch 8/10, Batch 868/883, Training Loss: 0.5677
Epoch 8/10, Batch 869/883, Training Loss: 0.6218
Epoch 8/10, Batch 870/883, Training Loss: 0.4519
Epoch 8/10, Batch 871/883, Training Loss: 0.8766
Epoch 8/10, Batch 872/883, Training Loss: 0.3946
Epoch 8/10, Batch 873/883, Training Loss: 0.5191
Epoch 8/10, Batch 874/883, Training Loss: 0.3842
Epoch 8/10, Batch 875/883, Training Loss: 0.5154
Epoch 8/10, Batch 876/883, Training Loss: 0.6029
Epoch 8/10, Batch 877/883, Training Loss: 0.4989
Epoch 8/10, Batch 878/883, Training Loss: 0.7471
Epoch 8/10, Batch 879/883, Training Loss: 0.3375
Epoch 8/10, Batch 880/883, Training Loss: 0.3442
Epoch 8/10, Batch 881/883, Training Loss: 0.6212
Epoch 8/10, Batch 882/883, Training Loss: 0.7825
Epoch 8/10, Batch 883/883, Training Loss: 0.5881
Epoch 8/10, Training Loss: 0.5650, Validation Loss: 0.5351, Validation Accuracy: 0.7635
Epoch 9/10, Batch 1/883, Training Loss: 0.6816
Epoch 9/10, Batch 2/883, Training Loss: 0.4310
Epoch 9/10, Batch 3/883, Training Loss: 0.5692
Epoch 9/10, Batch 4/883, Training Loss: 0.4752
Epoch 9/10, Batch 5/883, Training Loss: 0.6070
Epoch 9/10, Batch 6/883, Training Loss: 0.7381
Epoch 9/10, Batch 7/883, Training Loss: 0.5315
Epoch 9/10, Batch 8/883, Training Loss: 0.3646
Epoch 9/10, Batch 9/883, Training Loss: 0.3668
Epoch 9/10, Batch 10/883, Training Loss: 0.6299
Epoch 9/10, Batch 11/883, Training Loss: 0.4473
Epoch 9/10, Batch 12/883, Training Loss: 0.2297
Epoch 9/10, Batch 13/883, Training Loss: 0.7385
Epoch 9/10, Batch 14/883, Training Loss: 0.3900
Epoch 9/10, Batch 15/883, Training Loss: 0.6622
Epoch 9/10, Batch 16/883, Training Loss: 0.7253
Epoch 9/10, Batch 17/883, Training Loss: 0.6563
Epoch 9/10, Batch 18/883, Training Loss: 0.5135
Epoch 9/10, Batch 19/883, Training Loss: 0.2996
Epoch 9/10, Batch 20/883, Training Loss: 0.4343
Epoch 9/10, Batch 21/883, Training Loss: 0.4606
Epoch 9/10, Batch 22/883, Training Loss: 0.3587
Epoch 9/10, Batch 23/883, Training Loss: 0.6831
Epoch 9/10, Batch 24/883, Training Loss: 0.8456
Epoch 9/10, Batch 25/883, Training Loss: 0.4488
Epoch 9/10, Batch 26/883, Training Loss: 0.5064
Epoch 9/10, Batch 27/883, Training Loss: 0.5760
Epoch 9/10, Batch 28/883, Training Loss: 0.4037
Epoch 9/10, Batch 29/883, Training Loss: 0.7028
Epoch 9/10, Batch 30/883, Training Loss: 0.7269
Epoch 9/10, Batch 31/883, Training Loss: 0.3676
Epoch 9/10, Batch 32/883, Training Loss: 0.4906
Epoch 9/10, Batch 33/883, Training Loss: 0.9463
Epoch 9/10, Batch 34/883, Training Loss: 0.7281
Epoch 9/10, Batch 35/883, Training Loss: 0.5138
Epoch 9/10, Batch 36/883, Training Loss: 0.5463
Epoch 9/10, Batch 37/883, Training Loss: 0.6748
Epoch 9/10, Batch 38/883, Training Loss: 0.5676
Epoch 9/10, Batch 39/883, Training Loss: 0.5999
Epoch 9/10, Batch 40/883, Training Loss: 0.8771
Epoch 9/10, Batch 41/883, Training Loss: 0.4894
Epoch 9/10, Batch 42/883, Training Loss: 0.3732
Epoch 9/10, Batch 43/883, Training Loss: 0.5533
Epoch 9/10, Batch 44/883, Training Loss: 0.4497
Epoch 9/10, Batch 45/883, Training Loss: 0.2887
Epoch 9/10, Batch 46/883, Training Loss: 0.3080
Epoch 9/10, Batch 47/883, Training Loss: 0.5344
Epoch 9/10, Batch 48/883, Training Loss: 0.4079
Epoch 9/10, Batch 49/883, Training Loss: 0.6797
Epoch 9/10, Batch 50/883, Training Loss: 0.6165
Epoch 9/10, Batch 51/883, Training Loss: 0.6049
Epoch 9/10, Batch 52/883, Training Loss: 0.3156
Epoch 9/10, Batch 53/883, Training Loss: 0.7507
Epoch 9/10, Batch 54/883, Training Loss: 0.7074
Epoch 9/10, Batch 55/883, Training Loss: 0.4042
Epoch 9/10, Batch 56/883, Training Loss: 0.4171
Epoch 9/10, Batch 57/883, Training Loss: 0.8075
Epoch 9/10, Batch 58/883, Training Loss: 0.5842
Epoch 9/10, Batch 59/883, Training Loss: 0.5271
Epoch 9/10, Batch 60/883, Training Loss: 0.9285
Epoch 9/10, Batch 61/883, Training Loss: 0.6267
Epoch 9/10, Batch 62/883, Training Loss: 0.6070
Epoch 9/10, Batch 63/883, Training Loss: 0.5996
Epoch 9/10, Batch 64/883, Training Loss: 0.4610
Epoch 9/10, Batch 65/883, Training Loss: 0.3578
Epoch 9/10, Batch 66/883, Training Loss: 0.5198
Epoch 9/10, Batch 67/883, Training Loss: 0.7208
Epoch 9/10, Batch 68/883, Training Loss: 0.3799
Epoch 9/10, Batch 69/883, Training Loss: 0.4453
Epoch 9/10, Batch 70/883, Training Loss: 0.3088
Epoch 9/10, Batch 71/883, Training Loss: 0.6597
Epoch 9/10, Batch 72/883, Training Loss: 0.5276
Epoch 9/10, Batch 73/883, Training Loss: 0.4830
Epoch 9/10, Batch 74/883, Training Loss: 0.6376
Epoch 9/10, Batch 75/883, Training Loss: 0.7733
Epoch 9/10, Batch 76/883, Training Loss: 0.3798
Epoch 9/10, Batch 77/883, Training Loss: 0.4550
Epoch 9/10, Batch 78/883, Training Loss: 0.7979
Epoch 9/10, Batch 79/883, Training Loss: 0.5991
Epoch 9/10, Batch 80/883, Training Loss: 0.6004
Epoch 9/10, Batch 81/883, Training Loss: 0.3093
Epoch 9/10, Batch 82/883, Training Loss: 0.4788
Epoch 9/10, Batch 83/883, Training Loss: 0.4483
Epoch 9/10, Batch 84/883, Training Loss: 0.4418
Epoch 9/10, Batch 85/883, Training Loss: 0.7099
Epoch 9/10, Batch 86/883, Training Loss: 0.4165
Epoch 9/10, Batch 87/883, Training Loss: 0.4829
Epoch 9/10, Batch 88/883, Training Loss: 0.3510
Epoch 9/10, Batch 89/883, Training Loss: 0.3967
Epoch 9/10, Batch 90/883, Training Loss: 0.6191
Epoch 9/10, Batch 91/883, Training Loss: 0.5682
Epoch 9/10, Batch 92/883, Training Loss: 0.3160
Epoch 9/10, Batch 93/883, Training Loss: 0.6652
Epoch 9/10, Batch 94/883, Training Loss: 0.4977
Epoch 9/10, Batch 95/883, Training Loss: 0.5032
Epoch 9/10, Batch 96/883, Training Loss: 0.4421
Epoch 9/10, Batch 97/883, Training Loss: 1.0303
Epoch 9/10, Batch 98/883, Training Loss: 0.3416
Epoch 9/10, Batch 99/883, Training Loss: 0.3222
Epoch 9/10, Batch 100/883, Training Loss: 0.4230
Epoch 9/10, Batch 101/883, Training Loss: 0.4506
Epoch 9/10, Batch 102/883, Training Loss: 0.5268
Epoch 9/10, Batch 103/883, Training Loss: 0.3098
Epoch 9/10, Batch 104/883, Training Loss: 0.3149
Epoch 9/10, Batch 105/883, Training Loss: 0.4241
Epoch 9/10, Batch 106/883, Training Loss: 0.4696
Epoch 9/10, Batch 107/883, Training Loss: 0.3917
Epoch 9/10, Batch 108/883, Training Loss: 0.4627
Epoch 9/10, Batch 109/883, Training Loss: 0.4803
Epoch 9/10, Batch 110/883, Training Loss: 0.2808
Epoch 9/10, Batch 111/883, Training Loss: 0.4375
Epoch 9/10, Batch 112/883, Training Loss: 0.4310
Epoch 9/10, Batch 113/883, Training Loss: 0.6523
Epoch 9/10, Batch 114/883, Training Loss: 0.4643
Epoch 9/10, Batch 115/883, Training Loss: 0.6075
Epoch 9/10, Batch 116/883, Training Loss: 0.3036
Epoch 9/10, Batch 117/883, Training Loss: 0.8340
Epoch 9/10, Batch 118/883, Training Loss: 0.7686
Epoch 9/10, Batch 119/883, Training Loss: 0.3147
Epoch 9/10, Batch 120/883, Training Loss: 0.5075
Epoch 9/10, Batch 121/883, Training Loss: 0.5729
Epoch 9/10, Batch 122/883, Training Loss: 0.6727
Epoch 9/10, Batch 123/883, Training Loss: 0.4002
Epoch 9/10, Batch 124/883, Training Loss: 0.7351
Epoch 9/10, Batch 125/883, Training Loss: 0.5972
Epoch 9/10, Batch 126/883, Training Loss: 0.4218
Epoch 9/10, Batch 127/883, Training Loss: 0.3745
Epoch 9/10, Batch 128/883, Training Loss: 0.3288
Epoch 9/10, Batch 129/883, Training Loss: 0.4902
Epoch 9/10, Batch 130/883, Training Loss: 0.5068
Epoch 9/10, Batch 131/883, Training Loss: 0.2981
Epoch 9/10, Batch 132/883, Training Loss: 0.4425
Epoch 9/10, Batch 133/883, Training Loss: 0.4733
Epoch 9/10, Batch 134/883, Training Loss: 0.2302
Epoch 9/10, Batch 135/883, Training Loss: 0.4451
Epoch 9/10, Batch 136/883, Training Loss: 0.8314
Epoch 9/10, Batch 137/883, Training Loss: 0.6204
Epoch 9/10, Batch 138/883, Training Loss: 0.4761
Epoch 9/10, Batch 139/883, Training Loss: 0.5058
Epoch 9/10, Batch 140/883, Training Loss: 1.5188
Epoch 9/10, Batch 141/883, Training Loss: 0.6475
Epoch 9/10, Batch 142/883, Training Loss: 0.5001
Epoch 9/10, Batch 143/883, Training Loss: 0.3123
Epoch 9/10, Batch 144/883, Training Loss: 0.2629
Epoch 9/10, Batch 145/883, Training Loss: 0.4800
Epoch 9/10, Batch 146/883, Training Loss: 0.7680
Epoch 9/10, Batch 147/883, Training Loss: 0.3870
Epoch 9/10, Batch 148/883, Training Loss: 0.7526
Epoch 9/10, Batch 149/883, Training Loss: 0.5537
Epoch 9/10, Batch 150/883, Training Loss: 0.4209
Epoch 9/10, Batch 151/883, Training Loss: 0.9565
Epoch 9/10, Batch 152/883, Training Loss: 0.5169
Epoch 9/10, Batch 153/883, Training Loss: 0.6586
Epoch 9/10, Batch 154/883, Training Loss: 0.2985
Epoch 9/10, Batch 155/883, Training Loss: 1.0721
Epoch 9/10, Batch 156/883, Training Loss: 0.4770
Epoch 9/10, Batch 157/883, Training Loss: 0.4907
Epoch 9/10, Batch 158/883, Training Loss: 0.4027
Epoch 9/10, Batch 159/883, Training Loss: 0.5751
Epoch 9/10, Batch 160/883, Training Loss: 0.6069
Epoch 9/10, Batch 161/883, Training Loss: 0.5103
Epoch 9/10, Batch 162/883, Training Loss: 0.3339
Epoch 9/10, Batch 163/883, Training Loss: 0.3205
Epoch 9/10, Batch 164/883, Training Loss: 0.6487
Epoch 9/10, Batch 165/883, Training Loss: 0.4706
Epoch 9/10, Batch 166/883, Training Loss: 0.4534
Epoch 9/10, Batch 167/883, Training Loss: 0.6168
Epoch 9/10, Batch 168/883, Training Loss: 0.4021
Epoch 9/10, Batch 169/883, Training Loss: 0.3810
Epoch 9/10, Batch 170/883, Training Loss: 0.6103
Epoch 9/10, Batch 171/883, Training Loss: 0.3176
Epoch 9/10, Batch 172/883, Training Loss: 0.5291
Epoch 9/10, Batch 173/883, Training Loss: 0.6061
Epoch 9/10, Batch 174/883, Training Loss: 0.2448
Epoch 9/10, Batch 175/883, Training Loss: 0.4086
Epoch 9/10, Batch 176/883, Training Loss: 0.3129
Epoch 9/10, Batch 177/883, Training Loss: 0.4171
Epoch 9/10, Batch 178/883, Training Loss: 0.5498
Epoch 9/10, Batch 179/883, Training Loss: 0.6388
Epoch 9/10, Batch 180/883, Training Loss: 0.3833
Epoch 9/10, Batch 181/883, Training Loss: 0.5288
Epoch 9/10, Batch 182/883, Training Loss: 0.7990
Epoch 9/10, Batch 183/883, Training Loss: 0.5636
Epoch 9/10, Batch 184/883, Training Loss: 0.5813
Epoch 9/10, Batch 185/883, Training Loss: 0.4856
Epoch 9/10, Batch 186/883, Training Loss: 0.6380
Epoch 9/10, Batch 187/883, Training Loss: 0.7561
Epoch 9/10, Batch 188/883, Training Loss: 0.5481
Epoch 9/10, Batch 189/883, Training Loss: 0.4798
Epoch 9/10, Batch 190/883, Training Loss: 0.5474
Epoch 9/10, Batch 191/883, Training Loss: 0.5790
Epoch 9/10, Batch 192/883, Training Loss: 0.5170
Epoch 9/10, Batch 193/883, Training Loss: 0.7644
Epoch 9/10, Batch 194/883, Training Loss: 0.4489
Epoch 9/10, Batch 195/883, Training Loss: 0.5012
Epoch 9/10, Batch 196/883, Training Loss: 0.3256
Epoch 9/10, Batch 197/883, Training Loss: 0.5582
Epoch 9/10, Batch 198/883, Training Loss: 0.5714
Epoch 9/10, Batch 199/883, Training Loss: 0.5073
Epoch 9/10, Batch 200/883, Training Loss: 0.4634
Epoch 9/10, Batch 201/883, Training Loss: 0.5238
Epoch 9/10, Batch 202/883, Training Loss: 0.6365
Epoch 9/10, Batch 203/883, Training Loss: 0.4506
Epoch 9/10, Batch 204/883, Training Loss: 0.3387
Epoch 9/10, Batch 205/883, Training Loss: 0.4528
Epoch 9/10, Batch 206/883, Training Loss: 0.6332
Epoch 9/10, Batch 207/883, Training Loss: 0.4960
Epoch 9/10, Batch 208/883, Training Loss: 0.6460
Epoch 9/10, Batch 209/883, Training Loss: 0.4017
Epoch 9/10, Batch 210/883, Training Loss: 0.3058
Epoch 9/10, Batch 211/883, Training Loss: 0.6037
Epoch 9/10, Batch 212/883, Training Loss: 0.8111
Epoch 9/10, Batch 213/883, Training Loss: 0.4759
Epoch 9/10, Batch 214/883, Training Loss: 0.5543
Epoch 9/10, Batch 215/883, Training Loss: 0.3070
Epoch 9/10, Batch 216/883, Training Loss: 0.6699
Epoch 9/10, Batch 217/883, Training Loss: 0.5161
Epoch 9/10, Batch 218/883, Training Loss: 0.5364
Epoch 9/10, Batch 219/883, Training Loss: 0.5446
Epoch 9/10, Batch 220/883, Training Loss: 0.4698
Epoch 9/10, Batch 221/883, Training Loss: 0.6425
Epoch 9/10, Batch 222/883, Training Loss: 0.6776
Epoch 9/10, Batch 223/883, Training Loss: 0.7903
Epoch 9/10, Batch 224/883, Training Loss: 0.4069
Epoch 9/10, Batch 225/883, Training Loss: 0.4648
Epoch 9/10, Batch 226/883, Training Loss: 0.3194
Epoch 9/10, Batch 227/883, Training Loss: 0.7138
Epoch 9/10, Batch 228/883, Training Loss: 0.5234
Epoch 9/10, Batch 229/883, Training Loss: 0.3456
Epoch 9/10, Batch 230/883, Training Loss: 0.2544
Epoch 9/10, Batch 231/883, Training Loss: 0.8342
Epoch 9/10, Batch 232/883, Training Loss: 0.6487
Epoch 9/10, Batch 233/883, Training Loss: 0.7698
Epoch 9/10, Batch 234/883, Training Loss: 0.3576
Epoch 9/10, Batch 235/883, Training Loss: 1.0335
Epoch 9/10, Batch 236/883, Training Loss: 0.4486
Epoch 9/10, Batch 237/883, Training Loss: 0.6294
Epoch 9/10, Batch 238/883, Training Loss: 0.6704
Epoch 9/10, Batch 239/883, Training Loss: 0.4298
Epoch 9/10, Batch 240/883, Training Loss: 0.3802
Epoch 9/10, Batch 241/883, Training Loss: 0.4836
Epoch 9/10, Batch 242/883, Training Loss: 0.7257
Epoch 9/10, Batch 243/883, Training Loss: 0.7893
Epoch 9/10, Batch 244/883, Training Loss: 0.5967
Epoch 9/10, Batch 245/883, Training Loss: 0.8239
Epoch 9/10, Batch 246/883, Training Loss: 0.3920
Epoch 9/10, Batch 247/883, Training Loss: 0.5120
Epoch 9/10, Batch 248/883, Training Loss: 0.6671
Epoch 9/10, Batch 249/883, Training Loss: 0.8553
Epoch 9/10, Batch 250/883, Training Loss: 0.3559
Epoch 9/10, Batch 251/883, Training Loss: 0.6262
Epoch 9/10, Batch 252/883, Training Loss: 0.6850
Epoch 9/10, Batch 253/883, Training Loss: 0.7077
Epoch 9/10, Batch 254/883, Training Loss: 0.5341
Epoch 9/10, Batch 255/883, Training Loss: 0.2795
Epoch 9/10, Batch 256/883, Training Loss: 0.5828
Epoch 9/10, Batch 257/883, Training Loss: 0.3221
Epoch 9/10, Batch 258/883, Training Loss: 0.3250
Epoch 9/10, Batch 259/883, Training Loss: 0.4398
Epoch 9/10, Batch 260/883, Training Loss: 0.4202
Epoch 9/10, Batch 261/883, Training Loss: 0.5456
Epoch 9/10, Batch 262/883, Training Loss: 0.5665
Epoch 9/10, Batch 263/883, Training Loss: 0.6910
Epoch 9/10, Batch 264/883, Training Loss: 0.6290
Epoch 9/10, Batch 265/883, Training Loss: 0.5854
Epoch 9/10, Batch 266/883, Training Loss: 0.5083
Epoch 9/10, Batch 267/883, Training Loss: 0.6276
Epoch 9/10, Batch 268/883, Training Loss: 0.2988
Epoch 9/10, Batch 269/883, Training Loss: 0.5829
Epoch 9/10, Batch 270/883, Training Loss: 0.4099
Epoch 9/10, Batch 271/883, Training Loss: 0.3370
Epoch 9/10, Batch 272/883, Training Loss: 0.3976
Epoch 9/10, Batch 273/883, Training Loss: 0.4802
Epoch 9/10, Batch 274/883, Training Loss: 0.9552
Epoch 9/10, Batch 275/883, Training Loss: 0.4908
Epoch 9/10, Batch 276/883, Training Loss: 0.9461
Epoch 9/10, Batch 277/883, Training Loss: 0.2822
Epoch 9/10, Batch 278/883, Training Loss: 0.2932
Epoch 9/10, Batch 279/883, Training Loss: 0.4727
Epoch 9/10, Batch 280/883, Training Loss: 0.3443
Epoch 9/10, Batch 281/883, Training Loss: 0.8166
Epoch 9/10, Batch 282/883, Training Loss: 0.7260
Epoch 9/10, Batch 283/883, Training Loss: 0.4154
Epoch 9/10, Batch 284/883, Training Loss: 0.4482
Epoch 9/10, Batch 285/883, Training Loss: 0.5793
Epoch 9/10, Batch 286/883, Training Loss: 0.3518
Epoch 9/10, Batch 287/883, Training Loss: 0.6043
Epoch 9/10, Batch 288/883, Training Loss: 0.3509
Epoch 9/10, Batch 289/883, Training Loss: 0.2935
Epoch 9/10, Batch 290/883, Training Loss: 0.3760
Epoch 9/10, Batch 291/883, Training Loss: 0.5770
Epoch 9/10, Batch 292/883, Training Loss: 0.7050
Epoch 9/10, Batch 293/883, Training Loss: 0.8008
Epoch 9/10, Batch 294/883, Training Loss: 0.6154
Epoch 9/10, Batch 295/883, Training Loss: 0.9182
Epoch 9/10, Batch 296/883, Training Loss: 0.6515
Epoch 9/10, Batch 297/883, Training Loss: 0.4546
Epoch 9/10, Batch 298/883, Training Loss: 0.2369
Epoch 9/10, Batch 299/883, Training Loss: 0.4002
Epoch 9/10, Batch 300/883, Training Loss: 0.5188
Epoch 9/10, Batch 301/883, Training Loss: 0.9000
Epoch 9/10, Batch 302/883, Training Loss: 0.5691
Epoch 9/10, Batch 303/883, Training Loss: 0.7310
Epoch 9/10, Batch 304/883, Training Loss: 0.6492
Epoch 9/10, Batch 305/883, Training Loss: 0.4933
Epoch 9/10, Batch 306/883, Training Loss: 0.9083
Epoch 9/10, Batch 307/883, Training Loss: 0.3605
Epoch 9/10, Batch 308/883, Training Loss: 1.1174
Epoch 9/10, Batch 309/883, Training Loss: 0.5273
Epoch 9/10, Batch 310/883, Training Loss: 0.3840
Epoch 9/10, Batch 311/883, Training Loss: 0.2672
Epoch 9/10, Batch 312/883, Training Loss: 0.4341
Epoch 9/10, Batch 313/883, Training Loss: 0.4757
Epoch 9/10, Batch 314/883, Training Loss: 0.6209
Epoch 9/10, Batch 315/883, Training Loss: 0.5943
Epoch 9/10, Batch 316/883, Training Loss: 0.3183
Epoch 9/10, Batch 317/883, Training Loss: 0.5885
Epoch 9/10, Batch 318/883, Training Loss: 0.5769
Epoch 9/10, Batch 319/883, Training Loss: 0.5911
Epoch 9/10, Batch 320/883, Training Loss: 0.6151
Epoch 9/10, Batch 321/883, Training Loss: 0.3729
Epoch 9/10, Batch 322/883, Training Loss: 0.6599
Epoch 9/10, Batch 323/883, Training Loss: 0.5237
Epoch 9/10, Batch 324/883, Training Loss: 0.3819
Epoch 9/10, Batch 325/883, Training Loss: 0.3754
Epoch 9/10, Batch 326/883, Training Loss: 0.4944
Epoch 9/10, Batch 327/883, Training Loss: 0.6690
Epoch 9/10, Batch 328/883, Training Loss: 0.7112
Epoch 9/10, Batch 329/883, Training Loss: 0.5195
Epoch 9/10, Batch 330/883, Training Loss: 0.6373
Epoch 9/10, Batch 331/883, Training Loss: 0.5508
Epoch 9/10, Batch 332/883, Training Loss: 0.4838
Epoch 9/10, Batch 333/883, Training Loss: 0.4665
Epoch 9/10, Batch 334/883, Training Loss: 0.2762
Epoch 9/10, Batch 335/883, Training Loss: 0.3228
Epoch 9/10, Batch 336/883, Training Loss: 1.1637
Epoch 9/10, Batch 337/883, Training Loss: 0.5002
Epoch 9/10, Batch 338/883, Training Loss: 0.5586
Epoch 9/10, Batch 339/883, Training Loss: 0.4550
Epoch 9/10, Batch 340/883, Training Loss: 0.6885
Epoch 9/10, Batch 341/883, Training Loss: 0.3191
Epoch 9/10, Batch 342/883, Training Loss: 0.3556
Epoch 9/10, Batch 343/883, Training Loss: 0.8172
Epoch 9/10, Batch 344/883, Training Loss: 0.4243
Epoch 9/10, Batch 345/883, Training Loss: 0.4842
Epoch 9/10, Batch 346/883, Training Loss: 0.5227
Epoch 9/10, Batch 347/883, Training Loss: 0.7842
Epoch 9/10, Batch 348/883, Training Loss: 0.5714
Epoch 9/10, Batch 349/883, Training Loss: 0.3805
Epoch 9/10, Batch 350/883, Training Loss: 0.2788
Epoch 9/10, Batch 351/883, Training Loss: 0.7248
Epoch 9/10, Batch 352/883, Training Loss: 0.4124
Epoch 9/10, Batch 353/883, Training Loss: 0.3713
Epoch 9/10, Batch 354/883, Training Loss: 0.4173
Epoch 9/10, Batch 355/883, Training Loss: 0.6058
Epoch 9/10, Batch 356/883, Training Loss: 0.6034
Epoch 9/10, Batch 357/883, Training Loss: 0.5533
Epoch 9/10, Batch 358/883, Training Loss: 0.5231
Epoch 9/10, Batch 359/883, Training Loss: 0.5311
Epoch 9/10, Batch 360/883, Training Loss: 0.6767
Epoch 9/10, Batch 361/883, Training Loss: 0.4102
Epoch 9/10, Batch 362/883, Training Loss: 0.3398
Epoch 9/10, Batch 363/883, Training Loss: 0.4755
Epoch 9/10, Batch 364/883, Training Loss: 0.8718
Epoch 9/10, Batch 365/883, Training Loss: 0.3431
Epoch 9/10, Batch 366/883, Training Loss: 0.5258
Epoch 9/10, Batch 367/883, Training Loss: 0.2506
Epoch 9/10, Batch 368/883, Training Loss: 0.4078
Epoch 9/10, Batch 369/883, Training Loss: 0.4213
Epoch 9/10, Batch 370/883, Training Loss: 0.3296
Epoch 9/10, Batch 371/883, Training Loss: 0.5948
Epoch 9/10, Batch 372/883, Training Loss: 0.4857
Epoch 9/10, Batch 373/883, Training Loss: 0.6899
Epoch 9/10, Batch 374/883, Training Loss: 0.3784
Epoch 9/10, Batch 375/883, Training Loss: 0.4514
Epoch 9/10, Batch 376/883, Training Loss: 0.3687
Epoch 9/10, Batch 377/883, Training Loss: 0.6416
Epoch 9/10, Batch 378/883, Training Loss: 0.6166
Epoch 9/10, Batch 379/883, Training Loss: 0.2264
Epoch 9/10, Batch 380/883, Training Loss: 0.7724
Epoch 9/10, Batch 381/883, Training Loss: 0.5886
Epoch 9/10, Batch 382/883, Training Loss: 0.3591
Epoch 9/10, Batch 383/883, Training Loss: 0.3253
Epoch 9/10, Batch 384/883, Training Loss: 0.7683
Epoch 9/10, Batch 385/883, Training Loss: 0.8678
Epoch 9/10, Batch 386/883, Training Loss: 0.4839
Epoch 9/10, Batch 387/883, Training Loss: 0.3987
Epoch 9/10, Batch 388/883, Training Loss: 0.6246
Epoch 9/10, Batch 389/883, Training Loss: 0.5195
Epoch 9/10, Batch 390/883, Training Loss: 0.4696
Epoch 9/10, Batch 391/883, Training Loss: 0.4213
Epoch 9/10, Batch 392/883, Training Loss: 0.4682
Epoch 9/10, Batch 393/883, Training Loss: 0.5915
Epoch 9/10, Batch 394/883, Training Loss: 0.2795
Epoch 9/10, Batch 395/883, Training Loss: 0.3548
Epoch 9/10, Batch 396/883, Training Loss: 0.4096
Epoch 9/10, Batch 397/883, Training Loss: 0.8420
Epoch 9/10, Batch 398/883, Training Loss: 0.3751
Epoch 9/10, Batch 399/883, Training Loss: 0.4597
Epoch 9/10, Batch 400/883, Training Loss: 0.6405
Epoch 9/10, Batch 401/883, Training Loss: 0.4744
Epoch 9/10, Batch 402/883, Training Loss: 0.5310
Epoch 9/10, Batch 403/883, Training Loss: 0.4480
Epoch 9/10, Batch 404/883, Training Loss: 0.6883
Epoch 9/10, Batch 405/883, Training Loss: 0.5369
Epoch 9/10, Batch 406/883, Training Loss: 0.9297
Epoch 9/10, Batch 407/883, Training Loss: 0.3322
Epoch 9/10, Batch 408/883, Training Loss: 0.4276
Epoch 9/10, Batch 409/883, Training Loss: 0.3518
Epoch 9/10, Batch 410/883, Training Loss: 0.3664
Epoch 9/10, Batch 411/883, Training Loss: 0.6623
Epoch 9/10, Batch 412/883, Training Loss: 0.5229
Epoch 9/10, Batch 413/883, Training Loss: 0.4240
Epoch 9/10, Batch 414/883, Training Loss: 0.5286
Epoch 9/10, Batch 415/883, Training Loss: 0.5800
Epoch 9/10, Batch 416/883, Training Loss: 0.3171
Epoch 9/10, Batch 417/883, Training Loss: 0.3095
Epoch 9/10, Batch 418/883, Training Loss: 0.8683
Epoch 9/10, Batch 419/883, Training Loss: 0.6936
Epoch 9/10, Batch 420/883, Training Loss: 0.4274
Epoch 9/10, Batch 421/883, Training Loss: 0.5575
Epoch 9/10, Batch 422/883, Training Loss: 0.8531
Epoch 9/10, Batch 423/883, Training Loss: 0.4301
Epoch 9/10, Batch 424/883, Training Loss: 0.3545
Epoch 9/10, Batch 425/883, Training Loss: 0.5897
Epoch 9/10, Batch 426/883, Training Loss: 0.6559
Epoch 9/10, Batch 427/883, Training Loss: 0.6848
Epoch 9/10, Batch 428/883, Training Loss: 0.2693
Epoch 9/10, Batch 429/883, Training Loss: 0.5181
Epoch 9/10, Batch 430/883, Training Loss: 0.4731
Epoch 9/10, Batch 431/883, Training Loss: 0.4614
Epoch 9/10, Batch 432/883, Training Loss: 0.3888
Epoch 9/10, Batch 433/883, Training Loss: 0.5194
Epoch 9/10, Batch 434/883, Training Loss: 0.6422
Epoch 9/10, Batch 435/883, Training Loss: 0.5421
Epoch 9/10, Batch 436/883, Training Loss: 0.3542
Epoch 9/10, Batch 437/883, Training Loss: 0.5115
Epoch 9/10, Batch 438/883, Training Loss: 0.4601
Epoch 9/10, Batch 439/883, Training Loss: 0.6127
Epoch 9/10, Batch 440/883, Training Loss: 0.4750
Epoch 9/10, Batch 441/883, Training Loss: 0.5379
Epoch 9/10, Batch 442/883, Training Loss: 0.8256
Epoch 9/10, Batch 443/883, Training Loss: 0.7640
Epoch 9/10, Batch 444/883, Training Loss: 0.6051
Epoch 9/10, Batch 445/883, Training Loss: 0.4123
Epoch 9/10, Batch 446/883, Training Loss: 0.7978
Epoch 9/10, Batch 447/883, Training Loss: 0.2092
Epoch 9/10, Batch 448/883, Training Loss: 1.4120
Epoch 9/10, Batch 449/883, Training Loss: 0.5077
Epoch 9/10, Batch 450/883, Training Loss: 0.5890
Epoch 9/10, Batch 451/883, Training Loss: 0.5160
Epoch 9/10, Batch 452/883, Training Loss: 0.4954
Epoch 9/10, Batch 453/883, Training Loss: 0.6391
Epoch 9/10, Batch 454/883, Training Loss: 0.5717
Epoch 9/10, Batch 455/883, Training Loss: 0.6109
Epoch 9/10, Batch 456/883, Training Loss: 0.4587
Epoch 9/10, Batch 457/883, Training Loss: 0.4644
Epoch 9/10, Batch 458/883, Training Loss: 0.5114
Epoch 9/10, Batch 459/883, Training Loss: 0.3329
Epoch 9/10, Batch 460/883, Training Loss: 0.5553
Epoch 9/10, Batch 461/883, Training Loss: 0.6366
Epoch 9/10, Batch 462/883, Training Loss: 0.3582
Epoch 9/10, Batch 463/883, Training Loss: 0.4142
Epoch 9/10, Batch 464/883, Training Loss: 0.2830
Epoch 9/10, Batch 465/883, Training Loss: 0.6071
Epoch 9/10, Batch 466/883, Training Loss: 0.4433
Epoch 9/10, Batch 467/883, Training Loss: 0.3792
Epoch 9/10, Batch 468/883, Training Loss: 0.3316
Epoch 9/10, Batch 469/883, Training Loss: 0.4331
Epoch 9/10, Batch 470/883, Training Loss: 0.7464
Epoch 9/10, Batch 471/883, Training Loss: 0.6424
Epoch 9/10, Batch 472/883, Training Loss: 0.6213
Epoch 9/10, Batch 473/883, Training Loss: 0.2923
Epoch 9/10, Batch 474/883, Training Loss: 0.2789
Epoch 9/10, Batch 475/883, Training Loss: 0.6159
Epoch 9/10, Batch 476/883, Training Loss: 0.4559
Epoch 9/10, Batch 477/883, Training Loss: 0.3655
Epoch 9/10, Batch 478/883, Training Loss: 0.3743
Epoch 9/10, Batch 479/883, Training Loss: 0.1541
Epoch 9/10, Batch 480/883, Training Loss: 0.9235
Epoch 9/10, Batch 481/883, Training Loss: 0.6171
Epoch 9/10, Batch 482/883, Training Loss: 0.7566
Epoch 9/10, Batch 483/883, Training Loss: 0.3923
Epoch 9/10, Batch 484/883, Training Loss: 0.5548
Epoch 9/10, Batch 485/883, Training Loss: 0.9192
Epoch 9/10, Batch 486/883, Training Loss: 0.9655
Epoch 9/10, Batch 487/883, Training Loss: 0.4766
Epoch 9/10, Batch 488/883, Training Loss: 0.5943
Epoch 9/10, Batch 489/883, Training Loss: 0.3092
Epoch 9/10, Batch 490/883, Training Loss: 0.7010
Epoch 9/10, Batch 491/883, Training Loss: 0.3515
Epoch 9/10, Batch 492/883, Training Loss: 0.7117
Epoch 9/10, Batch 493/883, Training Loss: 0.5212
Epoch 9/10, Batch 494/883, Training Loss: 0.8649
Epoch 9/10, Batch 495/883, Training Loss: 0.4418
Epoch 9/10, Batch 496/883, Training Loss: 0.3771
Epoch 9/10, Batch 497/883, Training Loss: 0.4066
Epoch 9/10, Batch 498/883, Training Loss: 0.5436
Epoch 9/10, Batch 499/883, Training Loss: 0.5942
Epoch 9/10, Batch 500/883, Training Loss: 0.4786
Epoch 9/10, Batch 501/883, Training Loss: 0.8904
Epoch 9/10, Batch 502/883, Training Loss: 0.6654
Epoch 9/10, Batch 503/883, Training Loss: 0.3663
Epoch 9/10, Batch 504/883, Training Loss: 0.5224
Epoch 9/10, Batch 505/883, Training Loss: 0.6017
Epoch 9/10, Batch 506/883, Training Loss: 0.3306
Epoch 9/10, Batch 507/883, Training Loss: 0.3175
Epoch 9/10, Batch 508/883, Training Loss: 0.5624
Epoch 9/10, Batch 509/883, Training Loss: 0.3670
Epoch 9/10, Batch 510/883, Training Loss: 0.4591
Epoch 9/10, Batch 511/883, Training Loss: 0.2887
Epoch 9/10, Batch 512/883, Training Loss: 0.5787
Epoch 9/10, Batch 513/883, Training Loss: 0.2512
Epoch 9/10, Batch 514/883, Training Loss: 0.4919
Epoch 9/10, Batch 515/883, Training Loss: 0.6594
Epoch 9/10, Batch 516/883, Training Loss: 0.5352
Epoch 9/10, Batch 517/883, Training Loss: 0.7565
Epoch 9/10, Batch 518/883, Training Loss: 0.5445
Epoch 9/10, Batch 519/883, Training Loss: 1.1073
Epoch 9/10, Batch 520/883, Training Loss: 0.4161
Epoch 9/10, Batch 521/883, Training Loss: 0.4788
Epoch 9/10, Batch 522/883, Training Loss: 0.7609
Epoch 9/10, Batch 523/883, Training Loss: 0.5488
Epoch 9/10, Batch 524/883, Training Loss: 0.2675
Epoch 9/10, Batch 525/883, Training Loss: 0.5000
Epoch 9/10, Batch 526/883, Training Loss: 0.5775
Epoch 9/10, Batch 527/883, Training Loss: 0.5948
Epoch 9/10, Batch 528/883, Training Loss: 0.3059
Epoch 9/10, Batch 529/883, Training Loss: 0.8354
Epoch 9/10, Batch 530/883, Training Loss: 0.6556
Epoch 9/10, Batch 531/883, Training Loss: 0.5676
Epoch 9/10, Batch 532/883, Training Loss: 0.8482
Epoch 9/10, Batch 533/883, Training Loss: 0.7590
Epoch 9/10, Batch 534/883, Training Loss: 0.5768
Epoch 9/10, Batch 535/883, Training Loss: 0.4142
Epoch 9/10, Batch 536/883, Training Loss: 0.5700
Epoch 9/10, Batch 537/883, Training Loss: 0.3362
Epoch 9/10, Batch 538/883, Training Loss: 0.5550
Epoch 9/10, Batch 539/883, Training Loss: 0.4954
Epoch 9/10, Batch 540/883, Training Loss: 0.5435
Epoch 9/10, Batch 541/883, Training Loss: 0.8067
Epoch 9/10, Batch 542/883, Training Loss: 0.6837
Epoch 9/10, Batch 543/883, Training Loss: 0.6059
Epoch 9/10, Batch 544/883, Training Loss: 0.1717
Epoch 9/10, Batch 545/883, Training Loss: 0.5650
Epoch 9/10, Batch 546/883, Training Loss: 0.6956
Epoch 9/10, Batch 547/883, Training Loss: 0.5153
Epoch 9/10, Batch 548/883, Training Loss: 0.7557
Epoch 9/10, Batch 549/883, Training Loss: 0.4003
Epoch 9/10, Batch 550/883, Training Loss: 0.6614
Epoch 9/10, Batch 551/883, Training Loss: 0.6583
Epoch 9/10, Batch 552/883, Training Loss: 0.4367
Epoch 9/10, Batch 553/883, Training Loss: 0.3392
Epoch 9/10, Batch 554/883, Training Loss: 0.6093
Epoch 9/10, Batch 555/883, Training Loss: 0.6871
Epoch 9/10, Batch 556/883, Training Loss: 0.4863
Epoch 9/10, Batch 557/883, Training Loss: 0.7696
Epoch 9/10, Batch 558/883, Training Loss: 0.5956
Epoch 9/10, Batch 559/883, Training Loss: 0.6357
Epoch 9/10, Batch 560/883, Training Loss: 0.5122
Epoch 9/10, Batch 561/883, Training Loss: 0.4458
Epoch 9/10, Batch 562/883, Training Loss: 0.4595
Epoch 9/10, Batch 563/883, Training Loss: 0.6286
Epoch 9/10, Batch 564/883, Training Loss: 0.4155
Epoch 9/10, Batch 565/883, Training Loss: 0.7833
Epoch 9/10, Batch 566/883, Training Loss: 0.8927
Epoch 9/10, Batch 567/883, Training Loss: 0.3483
Epoch 9/10, Batch 568/883, Training Loss: 0.5665
Epoch 9/10, Batch 569/883, Training Loss: 0.6805
Epoch 9/10, Batch 570/883, Training Loss: 0.4468
Epoch 9/10, Batch 571/883, Training Loss: 0.5707
Epoch 9/10, Batch 572/883, Training Loss: 0.5757
Epoch 9/10, Batch 573/883, Training Loss: 0.7387
Epoch 9/10, Batch 574/883, Training Loss: 0.4320
Epoch 9/10, Batch 575/883, Training Loss: 0.5107
Epoch 9/10, Batch 576/883, Training Loss: 0.5936
Epoch 9/10, Batch 577/883, Training Loss: 0.8204
Epoch 9/10, Batch 578/883, Training Loss: 0.4808
Epoch 9/10, Batch 579/883, Training Loss: 0.4290
Epoch 9/10, Batch 580/883, Training Loss: 0.6819
Epoch 9/10, Batch 581/883, Training Loss: 0.3765
Epoch 9/10, Batch 582/883, Training Loss: 0.8921
Epoch 9/10, Batch 583/883, Training Loss: 0.8347
Epoch 9/10, Batch 584/883, Training Loss: 0.2480
Epoch 9/10, Batch 585/883, Training Loss: 0.6173
Epoch 9/10, Batch 586/883, Training Loss: 0.2987
Epoch 9/10, Batch 587/883, Training Loss: 0.9759
Epoch 9/10, Batch 588/883, Training Loss: 0.6635
Epoch 9/10, Batch 589/883, Training Loss: 0.4946
Epoch 9/10, Batch 590/883, Training Loss: 0.6645
Epoch 9/10, Batch 591/883, Training Loss: 0.5700
Epoch 9/10, Batch 592/883, Training Loss: 0.6254
Epoch 9/10, Batch 593/883, Training Loss: 0.6454
Epoch 9/10, Batch 594/883, Training Loss: 0.4194
Epoch 9/10, Batch 595/883, Training Loss: 0.7052
Epoch 9/10, Batch 596/883, Training Loss: 0.6495
Epoch 9/10, Batch 597/883, Training Loss: 0.5704
Epoch 9/10, Batch 598/883, Training Loss: 0.2951
Epoch 9/10, Batch 599/883, Training Loss: 1.1971
Epoch 9/10, Batch 600/883, Training Loss: 0.3938
Epoch 9/10, Batch 601/883, Training Loss: 0.4144
Epoch 9/10, Batch 602/883, Training Loss: 0.3943
Epoch 9/10, Batch 603/883, Training Loss: 0.6548
Epoch 9/10, Batch 604/883, Training Loss: 0.3173
Epoch 9/10, Batch 605/883, Training Loss: 0.6043
Epoch 9/10, Batch 606/883, Training Loss: 0.5182
Epoch 9/10, Batch 607/883, Training Loss: 0.6387
Epoch 9/10, Batch 608/883, Training Loss: 0.3646
Epoch 9/10, Batch 609/883, Training Loss: 0.5018
Epoch 9/10, Batch 610/883, Training Loss: 0.4927
Epoch 9/10, Batch 611/883, Training Loss: 0.3940
Epoch 9/10, Batch 612/883, Training Loss: 0.4737
Epoch 9/10, Batch 613/883, Training Loss: 0.4478
Epoch 9/10, Batch 614/883, Training Loss: 0.9140
Epoch 9/10, Batch 615/883, Training Loss: 1.0212
Epoch 9/10, Batch 616/883, Training Loss: 0.7126
Epoch 9/10, Batch 617/883, Training Loss: 0.5069
Epoch 9/10, Batch 618/883, Training Loss: 0.4466
Epoch 9/10, Batch 619/883, Training Loss: 0.2813
Epoch 9/10, Batch 620/883, Training Loss: 0.5227
Epoch 9/10, Batch 621/883, Training Loss: 0.5213
Epoch 9/10, Batch 622/883, Training Loss: 0.6634
Epoch 9/10, Batch 623/883, Training Loss: 0.7324
Epoch 9/10, Batch 624/883, Training Loss: 0.9230
Epoch 9/10, Batch 625/883, Training Loss: 0.6573
Epoch 9/10, Batch 626/883, Training Loss: 0.6873
Epoch 9/10, Batch 627/883, Training Loss: 0.4114
Epoch 9/10, Batch 628/883, Training Loss: 0.3388
Epoch 9/10, Batch 629/883, Training Loss: 0.6132
Epoch 9/10, Batch 630/883, Training Loss: 0.5505
Epoch 9/10, Batch 631/883, Training Loss: 0.2627
Epoch 9/10, Batch 632/883, Training Loss: 0.5761
Epoch 9/10, Batch 633/883, Training Loss: 0.4297
Epoch 9/10, Batch 634/883, Training Loss: 0.4575
Epoch 9/10, Batch 635/883, Training Loss: 0.7235
Epoch 9/10, Batch 636/883, Training Loss: 0.5384
Epoch 9/10, Batch 637/883, Training Loss: 0.2983
Epoch 9/10, Batch 638/883, Training Loss: 0.6543
Epoch 9/10, Batch 639/883, Training Loss: 0.7808
Epoch 9/10, Batch 640/883, Training Loss: 0.5607
Epoch 9/10, Batch 641/883, Training Loss: 0.4645
Epoch 9/10, Batch 642/883, Training Loss: 0.4255
Epoch 9/10, Batch 643/883, Training Loss: 0.6668
Epoch 9/10, Batch 644/883, Training Loss: 0.4270
Epoch 9/10, Batch 645/883, Training Loss: 0.9440
Epoch 9/10, Batch 646/883, Training Loss: 0.5133
Epoch 9/10, Batch 647/883, Training Loss: 0.3491
Epoch 9/10, Batch 648/883, Training Loss: 0.5710
Epoch 9/10, Batch 649/883, Training Loss: 0.4388
Epoch 9/10, Batch 650/883, Training Loss: 0.6721
Epoch 9/10, Batch 651/883, Training Loss: 0.5943
Epoch 9/10, Batch 652/883, Training Loss: 0.4366
Epoch 9/10, Batch 653/883, Training Loss: 0.7367
Epoch 9/10, Batch 654/883, Training Loss: 0.5642
Epoch 9/10, Batch 655/883, Training Loss: 0.4494
Epoch 9/10, Batch 656/883, Training Loss: 0.4266
Epoch 9/10, Batch 657/883, Training Loss: 0.2794
Epoch 9/10, Batch 658/883, Training Loss: 0.8562
Epoch 9/10, Batch 659/883, Training Loss: 0.7695
Epoch 9/10, Batch 660/883, Training Loss: 0.5029
Epoch 9/10, Batch 661/883, Training Loss: 0.4702
Epoch 9/10, Batch 662/883, Training Loss: 0.7230
Epoch 9/10, Batch 663/883, Training Loss: 0.6780
Epoch 9/10, Batch 664/883, Training Loss: 0.5677
Epoch 9/10, Batch 665/883, Training Loss: 0.7026
Epoch 9/10, Batch 666/883, Training Loss: 0.6989
Epoch 9/10, Batch 667/883, Training Loss: 0.6303
Epoch 9/10, Batch 668/883, Training Loss: 0.3681
Epoch 9/10, Batch 669/883, Training Loss: 0.5179
Epoch 9/10, Batch 670/883, Training Loss: 0.2908
Epoch 9/10, Batch 671/883, Training Loss: 0.6161
Epoch 9/10, Batch 672/883, Training Loss: 0.6318
Epoch 9/10, Batch 673/883, Training Loss: 0.3244
Epoch 9/10, Batch 674/883, Training Loss: 0.6380
Epoch 9/10, Batch 675/883, Training Loss: 0.4188
Epoch 9/10, Batch 676/883, Training Loss: 0.4680
Epoch 9/10, Batch 677/883, Training Loss: 0.4664
Epoch 9/10, Batch 678/883, Training Loss: 0.5100
Epoch 9/10, Batch 679/883, Training Loss: 0.8063
Epoch 9/10, Batch 680/883, Training Loss: 0.4264
Epoch 9/10, Batch 681/883, Training Loss: 0.5566
Epoch 9/10, Batch 682/883, Training Loss: 0.3654
Epoch 9/10, Batch 683/883, Training Loss: 0.4417
Epoch 9/10, Batch 684/883, Training Loss: 0.5205
Epoch 9/10, Batch 685/883, Training Loss: 0.2164
Epoch 9/10, Batch 686/883, Training Loss: 0.7966
Epoch 9/10, Batch 687/883, Training Loss: 0.3063
Epoch 9/10, Batch 688/883, Training Loss: 0.7382
Epoch 9/10, Batch 689/883, Training Loss: 0.6303
Epoch 9/10, Batch 690/883, Training Loss: 0.5656
Epoch 9/10, Batch 691/883, Training Loss: 0.4085
Epoch 9/10, Batch 692/883, Training Loss: 0.6812
Epoch 9/10, Batch 693/883, Training Loss: 0.3158
Epoch 9/10, Batch 694/883, Training Loss: 0.5642
Epoch 9/10, Batch 695/883, Training Loss: 0.5308
Epoch 9/10, Batch 696/883, Training Loss: 0.2691
Epoch 9/10, Batch 697/883, Training Loss: 0.6788
Epoch 9/10, Batch 698/883, Training Loss: 0.2515
Epoch 9/10, Batch 699/883, Training Loss: 0.7436
Epoch 9/10, Batch 700/883, Training Loss: 0.2345
Epoch 9/10, Batch 701/883, Training Loss: 0.2663
Epoch 9/10, Batch 702/883, Training Loss: 0.5235
Epoch 9/10, Batch 703/883, Training Loss: 0.3189
Epoch 9/10, Batch 704/883, Training Loss: 0.3655
Epoch 9/10, Batch 705/883, Training Loss: 0.5653
Epoch 9/10, Batch 706/883, Training Loss: 0.2404
Epoch 9/10, Batch 707/883, Training Loss: 0.4940
Epoch 9/10, Batch 708/883, Training Loss: 0.6529
Epoch 9/10, Batch 709/883, Training Loss: 0.6936
Epoch 9/10, Batch 710/883, Training Loss: 0.6993
Epoch 9/10, Batch 711/883, Training Loss: 0.2602
Epoch 9/10, Batch 712/883, Training Loss: 0.5382
Epoch 9/10, Batch 713/883, Training Loss: 0.4107
Epoch 9/10, Batch 714/883, Training Loss: 0.3326
Epoch 9/10, Batch 715/883, Training Loss: 0.5697
Epoch 9/10, Batch 716/883, Training Loss: 0.4311
Epoch 9/10, Batch 717/883, Training Loss: 0.6824
Epoch 9/10, Batch 718/883, Training Loss: 0.3542
Epoch 9/10, Batch 719/883, Training Loss: 0.3539
Epoch 9/10, Batch 720/883, Training Loss: 0.4818
Epoch 9/10, Batch 721/883, Training Loss: 0.3429
Epoch 9/10, Batch 722/883, Training Loss: 0.4843
Epoch 9/10, Batch 723/883, Training Loss: 0.8711
Epoch 9/10, Batch 724/883, Training Loss: 0.4230
Epoch 9/10, Batch 725/883, Training Loss: 0.3355
Epoch 9/10, Batch 726/883, Training Loss: 0.6744
Epoch 9/10, Batch 727/883, Training Loss: 0.4360
Epoch 9/10, Batch 728/883, Training Loss: 0.4456
Epoch 9/10, Batch 729/883, Training Loss: 0.6595
Epoch 9/10, Batch 730/883, Training Loss: 0.7344
Epoch 9/10, Batch 731/883, Training Loss: 0.5257
Epoch 9/10, Batch 732/883, Training Loss: 0.5131
Epoch 9/10, Batch 733/883, Training Loss: 0.3624
Epoch 9/10, Batch 734/883, Training Loss: 0.6150
Epoch 9/10, Batch 735/883, Training Loss: 0.8712
Epoch 9/10, Batch 736/883, Training Loss: 0.6636
Epoch 9/10, Batch 737/883, Training Loss: 0.2932
Epoch 9/10, Batch 738/883, Training Loss: 1.1971
Epoch 9/10, Batch 739/883, Training Loss: 0.6348
Epoch 9/10, Batch 740/883, Training Loss: 0.6924
Epoch 9/10, Batch 741/883, Training Loss: 0.3277
Epoch 9/10, Batch 742/883, Training Loss: 0.9455
Epoch 9/10, Batch 743/883, Training Loss: 0.6552
Epoch 9/10, Batch 744/883, Training Loss: 0.7757
Epoch 9/10, Batch 745/883, Training Loss: 0.6666
Epoch 9/10, Batch 746/883, Training Loss: 0.4928
Epoch 9/10, Batch 747/883, Training Loss: 1.0145
Epoch 9/10, Batch 748/883, Training Loss: 0.5585
Epoch 9/10, Batch 749/883, Training Loss: 0.4214
Epoch 9/10, Batch 750/883, Training Loss: 0.3333
Epoch 9/10, Batch 751/883, Training Loss: 0.4668
Epoch 9/10, Batch 752/883, Training Loss: 0.7341
Epoch 9/10, Batch 753/883, Training Loss: 0.6887
Epoch 9/10, Batch 754/883, Training Loss: 0.3910
Epoch 9/10, Batch 755/883, Training Loss: 0.3063
Epoch 9/10, Batch 756/883, Training Loss: 0.8883
Epoch 9/10, Batch 757/883, Training Loss: 0.3254
Epoch 9/10, Batch 758/883, Training Loss: 0.4954
Epoch 9/10, Batch 759/883, Training Loss: 0.5189
Epoch 9/10, Batch 760/883, Training Loss: 0.5446
Epoch 9/10, Batch 761/883, Training Loss: 0.3633
Epoch 9/10, Batch 762/883, Training Loss: 0.5819
Epoch 9/10, Batch 763/883, Training Loss: 0.4555
Epoch 9/10, Batch 764/883, Training Loss: 0.3453
Epoch 9/10, Batch 765/883, Training Loss: 0.4509
Epoch 9/10, Batch 766/883, Training Loss: 0.6385
Epoch 9/10, Batch 767/883, Training Loss: 0.6765
Epoch 9/10, Batch 768/883, Training Loss: 0.6130
Epoch 9/10, Batch 769/883, Training Loss: 0.2897
Epoch 9/10, Batch 770/883, Training Loss: 0.3032
Epoch 9/10, Batch 771/883, Training Loss: 0.7825
Epoch 9/10, Batch 772/883, Training Loss: 0.3642
Epoch 9/10, Batch 773/883, Training Loss: 0.6333
Epoch 9/10, Batch 774/883, Training Loss: 0.9404
Epoch 9/10, Batch 775/883, Training Loss: 0.7279
Epoch 9/10, Batch 776/883, Training Loss: 0.1709
Epoch 9/10, Batch 777/883, Training Loss: 0.6802
Epoch 9/10, Batch 778/883, Training Loss: 0.4737
Epoch 9/10, Batch 779/883, Training Loss: 0.3150
Epoch 9/10, Batch 780/883, Training Loss: 0.6911
Epoch 9/10, Batch 781/883, Training Loss: 0.3101
Epoch 9/10, Batch 782/883, Training Loss: 0.6981
Epoch 9/10, Batch 783/883, Training Loss: 0.2980
Epoch 9/10, Batch 784/883, Training Loss: 0.3751
Epoch 9/10, Batch 785/883, Training Loss: 0.5163
Epoch 9/10, Batch 786/883, Training Loss: 0.4598
Epoch 9/10, Batch 787/883, Training Loss: 0.4636
Epoch 9/10, Batch 788/883, Training Loss: 0.5934
Epoch 9/10, Batch 789/883, Training Loss: 0.2674
Epoch 9/10, Batch 790/883, Training Loss: 0.7161
Epoch 9/10, Batch 791/883, Training Loss: 0.4643
Epoch 9/10, Batch 792/883, Training Loss: 0.7999
Epoch 9/10, Batch 793/883, Training Loss: 0.3781
Epoch 9/10, Batch 794/883, Training Loss: 0.3759
Epoch 9/10, Batch 795/883, Training Loss: 0.5095
Epoch 9/10, Batch 796/883, Training Loss: 0.6269
Epoch 9/10, Batch 797/883, Training Loss: 0.6507
Epoch 9/10, Batch 798/883, Training Loss: 0.4928
Epoch 9/10, Batch 799/883, Training Loss: 0.4908
Epoch 9/10, Batch 800/883, Training Loss: 0.8514
Epoch 9/10, Batch 801/883, Training Loss: 0.3305
Epoch 9/10, Batch 802/883, Training Loss: 0.4745
Epoch 9/10, Batch 803/883, Training Loss: 0.6262
Epoch 9/10, Batch 804/883, Training Loss: 0.3763
Epoch 9/10, Batch 805/883, Training Loss: 0.4767
Epoch 9/10, Batch 806/883, Training Loss: 0.6625
Epoch 9/10, Batch 807/883, Training Loss: 0.3054
Epoch 9/10, Batch 808/883, Training Loss: 0.7078
Epoch 9/10, Batch 809/883, Training Loss: 0.3700
Epoch 9/10, Batch 810/883, Training Loss: 0.3485
Epoch 9/10, Batch 811/883, Training Loss: 0.4595
Epoch 9/10, Batch 812/883, Training Loss: 0.2750
Epoch 9/10, Batch 813/883, Training Loss: 0.7335
Epoch 9/10, Batch 814/883, Training Loss: 0.4266
Epoch 9/10, Batch 815/883, Training Loss: 0.2781
Epoch 9/10, Batch 816/883, Training Loss: 0.3684
Epoch 9/10, Batch 817/883, Training Loss: 0.4548
Epoch 9/10, Batch 818/883, Training Loss: 0.2134
Epoch 9/10, Batch 819/883, Training Loss: 0.6626
Epoch 9/10, Batch 820/883, Training Loss: 0.3059
Epoch 9/10, Batch 821/883, Training Loss: 0.6663
Epoch 9/10, Batch 822/883, Training Loss: 0.2421
Epoch 9/10, Batch 823/883, Training Loss: 0.7108
Epoch 9/10, Batch 824/883, Training Loss: 0.4368
Epoch 9/10, Batch 825/883, Training Loss: 0.7316
Epoch 9/10, Batch 826/883, Training Loss: 0.5285
Epoch 9/10, Batch 827/883, Training Loss: 0.3558
Epoch 9/10, Batch 828/883, Training Loss: 0.2394
Epoch 9/10, Batch 829/883, Training Loss: 0.4191
Epoch 9/10, Batch 830/883, Training Loss: 0.9261
Epoch 9/10, Batch 831/883, Training Loss: 0.4143
Epoch 9/10, Batch 832/883, Training Loss: 0.4991
Epoch 9/10, Batch 833/883, Training Loss: 0.5018
Epoch 9/10, Batch 834/883, Training Loss: 0.2176
Epoch 9/10, Batch 835/883, Training Loss: 0.5603
Epoch 9/10, Batch 836/883, Training Loss: 0.7617
Epoch 9/10, Batch 837/883, Training Loss: 0.5338
Epoch 9/10, Batch 838/883, Training Loss: 0.6900
Epoch 9/10, Batch 839/883, Training Loss: 0.3619
Epoch 9/10, Batch 840/883, Training Loss: 0.4262
Epoch 9/10, Batch 841/883, Training Loss: 0.4260
Epoch 9/10, Batch 842/883, Training Loss: 0.3965
Epoch 9/10, Batch 843/883, Training Loss: 0.6398
Epoch 9/10, Batch 844/883, Training Loss: 0.6833
Epoch 9/10, Batch 845/883, Training Loss: 0.3384
Epoch 9/10, Batch 846/883, Training Loss: 0.5825
Epoch 9/10, Batch 847/883, Training Loss: 0.2937
Epoch 9/10, Batch 848/883, Training Loss: 0.6149
Epoch 9/10, Batch 849/883, Training Loss: 1.3088
Epoch 9/10, Batch 850/883, Training Loss: 0.6417
Epoch 9/10, Batch 851/883, Training Loss: 0.3415
Epoch 9/10, Batch 852/883, Training Loss: 0.6116
Epoch 9/10, Batch 853/883, Training Loss: 0.2190
Epoch 9/10, Batch 854/883, Training Loss: 0.7724
Epoch 9/10, Batch 855/883, Training Loss: 0.7162
Epoch 9/10, Batch 856/883, Training Loss: 0.3627
Epoch 9/10, Batch 857/883, Training Loss: 0.1724
Epoch 9/10, Batch 858/883, Training Loss: 0.6618
Epoch 9/10, Batch 859/883, Training Loss: 0.4182
Epoch 9/10, Batch 860/883, Training Loss: 0.4447
Epoch 9/10, Batch 861/883, Training Loss: 0.3218
Epoch 9/10, Batch 862/883, Training Loss: 0.3841
Epoch 9/10, Batch 863/883, Training Loss: 0.3043
Epoch 9/10, Batch 864/883, Training Loss: 0.3740
Epoch 9/10, Batch 865/883, Training Loss: 0.3419
Epoch 9/10, Batch 866/883, Training Loss: 0.6479
Epoch 9/10, Batch 867/883, Training Loss: 1.1710
Epoch 9/10, Batch 868/883, Training Loss: 0.4846
Epoch 9/10, Batch 869/883, Training Loss: 0.5618
Epoch 9/10, Batch 870/883, Training Loss: 0.3245
Epoch 9/10, Batch 871/883, Training Loss: 0.3882
Epoch 9/10, Batch 872/883, Training Loss: 0.4574
Epoch 9/10, Batch 873/883, Training Loss: 0.5554
Epoch 9/10, Batch 874/883, Training Loss: 0.5018
Epoch 9/10, Batch 875/883, Training Loss: 0.4304
Epoch 9/10, Batch 876/883, Training Loss: 0.2530
Epoch 9/10, Batch 877/883, Training Loss: 0.4306
Epoch 9/10, Batch 878/883, Training Loss: 0.2554
Epoch 9/10, Batch 879/883, Training Loss: 0.5861
Epoch 9/10, Batch 880/883, Training Loss: 0.4287
Epoch 9/10, Batch 881/883, Training Loss: 0.6843
Epoch 9/10, Batch 882/883, Training Loss: 0.7893
Epoch 9/10, Batch 883/883, Training Loss: 0.4372
Epoch 9/10, Training Loss: 0.5339, Validation Loss: 0.5395, Validation Accuracy: 0.7751
Epoch 10/10, Batch 1/883, Training Loss: 0.4911
Epoch 10/10, Batch 2/883, Training Loss: 0.5951
Epoch 10/10, Batch 3/883, Training Loss: 0.5705
Epoch 10/10, Batch 4/883, Training Loss: 0.4477
Epoch 10/10, Batch 5/883, Training Loss: 0.3084
Epoch 10/10, Batch 6/883, Training Loss: 0.3799
Epoch 10/10, Batch 7/883, Training Loss: 1.0152
Epoch 10/10, Batch 8/883, Training Loss: 0.5508
Epoch 10/10, Batch 9/883, Training Loss: 0.2887
Epoch 10/10, Batch 10/883, Training Loss: 0.5617
Epoch 10/10, Batch 11/883, Training Loss: 0.5272
Epoch 10/10, Batch 12/883, Training Loss: 0.5122
Epoch 10/10, Batch 13/883, Training Loss: 0.4411
Epoch 10/10, Batch 14/883, Training Loss: 0.5164
Epoch 10/10, Batch 15/883, Training Loss: 0.5967
Epoch 10/10, Batch 16/883, Training Loss: 0.5171
Epoch 10/10, Batch 17/883, Training Loss: 0.2863
Epoch 10/10, Batch 18/883, Training Loss: 0.5739
Epoch 10/10, Batch 19/883, Training Loss: 0.2487
Epoch 10/10, Batch 20/883, Training Loss: 0.7569
Epoch 10/10, Batch 21/883, Training Loss: 0.5614
Epoch 10/10, Batch 22/883, Training Loss: 0.5171
Epoch 10/10, Batch 23/883, Training Loss: 0.3376
Epoch 10/10, Batch 24/883, Training Loss: 0.3631
Epoch 10/10, Batch 25/883, Training Loss: 0.4514
Epoch 10/10, Batch 26/883, Training Loss: 0.3222
Epoch 10/10, Batch 27/883, Training Loss: 0.4551
Epoch 10/10, Batch 28/883, Training Loss: 0.5083
Epoch 10/10, Batch 29/883, Training Loss: 0.8684
Epoch 10/10, Batch 30/883, Training Loss: 0.8445
Epoch 10/10, Batch 31/883, Training Loss: 0.3711
Epoch 10/10, Batch 32/883, Training Loss: 0.6054
Epoch 10/10, Batch 33/883, Training Loss: 0.6551
Epoch 10/10, Batch 34/883, Training Loss: 0.5559
Epoch 10/10, Batch 35/883, Training Loss: 0.3825
Epoch 10/10, Batch 36/883, Training Loss: 0.6451
Epoch 10/10, Batch 37/883, Training Loss: 0.2058
Epoch 10/10, Batch 38/883, Training Loss: 0.3824
Epoch 10/10, Batch 39/883, Training Loss: 0.5481
Epoch 10/10, Batch 40/883, Training Loss: 0.8014
Epoch 10/10, Batch 41/883, Training Loss: 0.5211
Epoch 10/10, Batch 42/883, Training Loss: 0.4666
Epoch 10/10, Batch 43/883, Training Loss: 0.7768
Epoch 10/10, Batch 44/883, Training Loss: 0.4425
Epoch 10/10, Batch 45/883, Training Loss: 0.3954
Epoch 10/10, Batch 46/883, Training Loss: 0.3655
Epoch 10/10, Batch 47/883, Training Loss: 0.2686
Epoch 10/10, Batch 48/883, Training Loss: 0.3413
Epoch 10/10, Batch 49/883, Training Loss: 0.3186
Epoch 10/10, Batch 50/883, Training Loss: 0.2476
Epoch 10/10, Batch 51/883, Training Loss: 0.4884
Epoch 10/10, Batch 52/883, Training Loss: 0.3779
Epoch 10/10, Batch 53/883, Training Loss: 0.5464
Epoch 10/10, Batch 54/883, Training Loss: 0.2930
Epoch 10/10, Batch 55/883, Training Loss: 0.4717
Epoch 10/10, Batch 56/883, Training Loss: 0.4588
Epoch 10/10, Batch 57/883, Training Loss: 0.3281
Epoch 10/10, Batch 58/883, Training Loss: 0.4987
Epoch 10/10, Batch 59/883, Training Loss: 0.4052
Epoch 10/10, Batch 60/883, Training Loss: 0.9462
Epoch 10/10, Batch 61/883, Training Loss: 0.4596
Epoch 10/10, Batch 62/883, Training Loss: 0.4160
Epoch 10/10, Batch 63/883, Training Loss: 0.7006
Epoch 10/10, Batch 64/883, Training Loss: 0.3848
Epoch 10/10, Batch 65/883, Training Loss: 0.4912
Epoch 10/10, Batch 66/883, Training Loss: 0.3908
Epoch 10/10, Batch 67/883, Training Loss: 0.2911
Epoch 10/10, Batch 68/883, Training Loss: 0.4069
Epoch 10/10, Batch 69/883, Training Loss: 0.2258
Epoch 10/10, Batch 70/883, Training Loss: 0.7488
Epoch 10/10, Batch 71/883, Training Loss: 0.2322
Epoch 10/10, Batch 72/883, Training Loss: 0.3917
Epoch 10/10, Batch 73/883, Training Loss: 0.2946
Epoch 10/10, Batch 74/883, Training Loss: 0.4669
Epoch 10/10, Batch 75/883, Training Loss: 0.4429
Epoch 10/10, Batch 76/883, Training Loss: 0.7854
Epoch 10/10, Batch 77/883, Training Loss: 0.4261
Epoch 10/10, Batch 78/883, Training Loss: 0.7019
Epoch 10/10, Batch 79/883, Training Loss: 1.0291
Epoch 10/10, Batch 80/883, Training Loss: 0.4364
Epoch 10/10, Batch 81/883, Training Loss: 0.3797
Epoch 10/10, Batch 82/883, Training Loss: 0.4739
Epoch 10/10, Batch 83/883, Training Loss: 0.4517
Epoch 10/10, Batch 84/883, Training Loss: 0.3831
Epoch 10/10, Batch 85/883, Training Loss: 0.4979
Epoch 10/10, Batch 86/883, Training Loss: 0.3611
Epoch 10/10, Batch 87/883, Training Loss: 0.7939
Epoch 10/10, Batch 88/883, Training Loss: 0.4361
Epoch 10/10, Batch 89/883, Training Loss: 0.6213
Epoch 10/10, Batch 90/883, Training Loss: 0.7146
Epoch 10/10, Batch 91/883, Training Loss: 0.3657
Epoch 10/10, Batch 92/883, Training Loss: 0.6092
Epoch 10/10, Batch 93/883, Training Loss: 0.5269
Epoch 10/10, Batch 94/883, Training Loss: 0.5102
Epoch 10/10, Batch 95/883, Training Loss: 0.8305
Epoch 10/10, Batch 96/883, Training Loss: 0.6021
Epoch 10/10, Batch 97/883, Training Loss: 0.3840
Epoch 10/10, Batch 98/883, Training Loss: 0.5388
Epoch 10/10, Batch 99/883, Training Loss: 0.3193
Epoch 10/10, Batch 100/883, Training Loss: 0.5173
Epoch 10/10, Batch 101/883, Training Loss: 0.5078
Epoch 10/10, Batch 102/883, Training Loss: 0.8237
Epoch 10/10, Batch 103/883, Training Loss: 0.4653
Epoch 10/10, Batch 104/883, Training Loss: 0.4146
Epoch 10/10, Batch 105/883, Training Loss: 0.6409
Epoch 10/10, Batch 106/883, Training Loss: 0.7814
Epoch 10/10, Batch 107/883, Training Loss: 0.3958
Epoch 10/10, Batch 108/883, Training Loss: 0.6874
Epoch 10/10, Batch 109/883, Training Loss: 0.2885
Epoch 10/10, Batch 110/883, Training Loss: 0.6132
Epoch 10/10, Batch 111/883, Training Loss: 0.7960
Epoch 10/10, Batch 112/883, Training Loss: 0.5603
Epoch 10/10, Batch 113/883, Training Loss: 0.3543
Epoch 10/10, Batch 114/883, Training Loss: 0.5549
Epoch 10/10, Batch 115/883, Training Loss: 0.8373
Epoch 10/10, Batch 116/883, Training Loss: 0.6116
Epoch 10/10, Batch 117/883, Training Loss: 0.5228
Epoch 10/10, Batch 118/883, Training Loss: 0.3633
Epoch 10/10, Batch 119/883, Training Loss: 0.6339
Epoch 10/10, Batch 120/883, Training Loss: 0.4598
Epoch 10/10, Batch 121/883, Training Loss: 0.4148
Epoch 10/10, Batch 122/883, Training Loss: 0.4955
Epoch 10/10, Batch 123/883, Training Loss: 0.6172
Epoch 10/10, Batch 124/883, Training Loss: 0.4357
Epoch 10/10, Batch 125/883, Training Loss: 0.8601
Epoch 10/10, Batch 126/883, Training Loss: 0.8132
Epoch 10/10, Batch 127/883, Training Loss: 0.2789
Epoch 10/10, Batch 128/883, Training Loss: 0.4429
Epoch 10/10, Batch 129/883, Training Loss: 0.3889
Epoch 10/10, Batch 130/883, Training Loss: 0.7290
Epoch 10/10, Batch 131/883, Training Loss: 0.2639
Epoch 10/10, Batch 132/883, Training Loss: 0.4987
Epoch 10/10, Batch 133/883, Training Loss: 0.3027
Epoch 10/10, Batch 134/883, Training Loss: 0.4400
Epoch 10/10, Batch 135/883, Training Loss: 0.5917
Epoch 10/10, Batch 136/883, Training Loss: 0.4740
Epoch 10/10, Batch 137/883, Training Loss: 0.6245
Epoch 10/10, Batch 138/883, Training Loss: 0.6228
Epoch 10/10, Batch 139/883, Training Loss: 0.5088
Epoch 10/10, Batch 140/883, Training Loss: 0.4099
Epoch 10/10, Batch 141/883, Training Loss: 0.3244
Epoch 10/10, Batch 142/883, Training Loss: 0.6167
Epoch 10/10, Batch 143/883, Training Loss: 1.0067
Epoch 10/10, Batch 144/883, Training Loss: 0.4764
Epoch 10/10, Batch 145/883, Training Loss: 0.6214
Epoch 10/10, Batch 146/883, Training Loss: 0.5514
Epoch 10/10, Batch 147/883, Training Loss: 0.5111
Epoch 10/10, Batch 148/883, Training Loss: 0.6466
Epoch 10/10, Batch 149/883, Training Loss: 0.5321
Epoch 10/10, Batch 150/883, Training Loss: 0.3771
Epoch 10/10, Batch 151/883, Training Loss: 0.4395
Epoch 10/10, Batch 152/883, Training Loss: 0.4221
Epoch 10/10, Batch 153/883, Training Loss: 0.4159
Epoch 10/10, Batch 154/883, Training Loss: 0.5403
Epoch 10/10, Batch 155/883, Training Loss: 0.4280
Epoch 10/10, Batch 156/883, Training Loss: 0.3877
Epoch 10/10, Batch 157/883, Training Loss: 0.4699
Epoch 10/10, Batch 158/883, Training Loss: 0.5307
Epoch 10/10, Batch 159/883, Training Loss: 0.6935
Epoch 10/10, Batch 160/883, Training Loss: 0.6726
Epoch 10/10, Batch 161/883, Training Loss: 0.4284
Epoch 10/10, Batch 162/883, Training Loss: 0.5752
Epoch 10/10, Batch 163/883, Training Loss: 0.5880
Epoch 10/10, Batch 164/883, Training Loss: 0.4150
Epoch 10/10, Batch 165/883, Training Loss: 0.3941
Epoch 10/10, Batch 166/883, Training Loss: 0.4297
Epoch 10/10, Batch 167/883, Training Loss: 0.5069
Epoch 10/10, Batch 168/883, Training Loss: 0.4008
Epoch 10/10, Batch 169/883, Training Loss: 0.2111
Epoch 10/10, Batch 170/883, Training Loss: 0.5762
Epoch 10/10, Batch 171/883, Training Loss: 0.4471
Epoch 10/10, Batch 172/883, Training Loss: 0.4629
Epoch 10/10, Batch 173/883, Training Loss: 0.6757
Epoch 10/10, Batch 174/883, Training Loss: 0.3828
Epoch 10/10, Batch 175/883, Training Loss: 0.4010
Epoch 10/10, Batch 176/883, Training Loss: 0.5875
Epoch 10/10, Batch 177/883, Training Loss: 0.5548
Epoch 10/10, Batch 178/883, Training Loss: 0.5222
Epoch 10/10, Batch 179/883, Training Loss: 0.9657
Epoch 10/10, Batch 180/883, Training Loss: 0.4836
Epoch 10/10, Batch 181/883, Training Loss: 0.3166
Epoch 10/10, Batch 182/883, Training Loss: 0.8618
Epoch 10/10, Batch 183/883, Training Loss: 0.4437
Epoch 10/10, Batch 184/883, Training Loss: 0.5851
Epoch 10/10, Batch 185/883, Training Loss: 0.4717
Epoch 10/10, Batch 186/883, Training Loss: 0.2853
Epoch 10/10, Batch 187/883, Training Loss: 0.5072
Epoch 10/10, Batch 188/883, Training Loss: 0.3986
Epoch 10/10, Batch 189/883, Training Loss: 0.4728
Epoch 10/10, Batch 190/883, Training Loss: 0.5783
Epoch 10/10, Batch 191/883, Training Loss: 0.3341
Epoch 10/10, Batch 192/883, Training Loss: 0.4661
Epoch 10/10, Batch 193/883, Training Loss: 0.6819
Epoch 10/10, Batch 194/883, Training Loss: 0.5198
Epoch 10/10, Batch 195/883, Training Loss: 0.6316
Epoch 10/10, Batch 196/883, Training Loss: 0.6449
Epoch 10/10, Batch 197/883, Training Loss: 0.3869
Epoch 10/10, Batch 198/883, Training Loss: 0.5644
Epoch 10/10, Batch 199/883, Training Loss: 0.4845
Epoch 10/10, Batch 200/883, Training Loss: 0.2125
Epoch 10/10, Batch 201/883, Training Loss: 0.4387
Epoch 10/10, Batch 202/883, Training Loss: 0.6354
Epoch 10/10, Batch 203/883, Training Loss: 0.6597
Epoch 10/10, Batch 204/883, Training Loss: 0.4891
Epoch 10/10, Batch 205/883, Training Loss: 0.4463
Epoch 10/10, Batch 206/883, Training Loss: 0.5554
Epoch 10/10, Batch 207/883, Training Loss: 0.4550
Epoch 10/10, Batch 208/883, Training Loss: 0.8504
Epoch 10/10, Batch 209/883, Training Loss: 0.6476
Epoch 10/10, Batch 210/883, Training Loss: 0.5600
Epoch 10/10, Batch 211/883, Training Loss: 0.5209
Epoch 10/10, Batch 212/883, Training Loss: 0.6779
Epoch 10/10, Batch 213/883, Training Loss: 0.3599
Epoch 10/10, Batch 214/883, Training Loss: 0.4166
Epoch 10/10, Batch 215/883, Training Loss: 0.6191
Epoch 10/10, Batch 216/883, Training Loss: 0.4669
Epoch 10/10, Batch 217/883, Training Loss: 0.5584
Epoch 10/10, Batch 218/883, Training Loss: 0.6062
Epoch 10/10, Batch 219/883, Training Loss: 0.5272
Epoch 10/10, Batch 220/883, Training Loss: 0.5200
Epoch 10/10, Batch 221/883, Training Loss: 0.6040
Epoch 10/10, Batch 222/883, Training Loss: 0.2448
Epoch 10/10, Batch 223/883, Training Loss: 0.4434
Epoch 10/10, Batch 224/883, Training Loss: 0.2480
Epoch 10/10, Batch 225/883, Training Loss: 0.3336
Epoch 10/10, Batch 226/883, Training Loss: 0.4130
Epoch 10/10, Batch 227/883, Training Loss: 0.5667
Epoch 10/10, Batch 228/883, Training Loss: 0.3074
Epoch 10/10, Batch 229/883, Training Loss: 0.7702
Epoch 10/10, Batch 230/883, Training Loss: 0.4427
Epoch 10/10, Batch 231/883, Training Loss: 0.4551
Epoch 10/10, Batch 232/883, Training Loss: 0.4787
Epoch 10/10, Batch 233/883, Training Loss: 0.6228
Epoch 10/10, Batch 234/883, Training Loss: 1.0142
Epoch 10/10, Batch 235/883, Training Loss: 0.3753
Epoch 10/10, Batch 236/883, Training Loss: 0.3880
Epoch 10/10, Batch 237/883, Training Loss: 0.7832
Epoch 10/10, Batch 238/883, Training Loss: 0.4711
Epoch 10/10, Batch 239/883, Training Loss: 0.3365
Epoch 10/10, Batch 240/883, Training Loss: 0.3757
Epoch 10/10, Batch 241/883, Training Loss: 0.4842
Epoch 10/10, Batch 242/883, Training Loss: 0.5105
Epoch 10/10, Batch 243/883, Training Loss: 0.4436
Epoch 10/10, Batch 244/883, Training Loss: 0.4966
Epoch 10/10, Batch 245/883, Training Loss: 0.2959
Epoch 10/10, Batch 246/883, Training Loss: 0.3560
Epoch 10/10, Batch 247/883, Training Loss: 0.6504
Epoch 10/10, Batch 248/883, Training Loss: 0.6170
Epoch 10/10, Batch 249/883, Training Loss: 0.9360
Epoch 10/10, Batch 250/883, Training Loss: 0.4116
Epoch 10/10, Batch 251/883, Training Loss: 0.4248
Epoch 10/10, Batch 252/883, Training Loss: 0.3960
Epoch 10/10, Batch 253/883, Training Loss: 0.4461
Epoch 10/10, Batch 254/883, Training Loss: 0.7467
Epoch 10/10, Batch 255/883, Training Loss: 0.2272
Epoch 10/10, Batch 256/883, Training Loss: 0.8415
Epoch 10/10, Batch 257/883, Training Loss: 0.3360
Epoch 10/10, Batch 258/883, Training Loss: 0.3752
Epoch 10/10, Batch 259/883, Training Loss: 1.0800
Epoch 10/10, Batch 260/883, Training Loss: 0.4433
Epoch 10/10, Batch 261/883, Training Loss: 0.4444
Epoch 10/10, Batch 262/883, Training Loss: 0.3735
Epoch 10/10, Batch 263/883, Training Loss: 0.3095
Epoch 10/10, Batch 264/883, Training Loss: 0.2958
Epoch 10/10, Batch 265/883, Training Loss: 0.4251
Epoch 10/10, Batch 266/883, Training Loss: 0.7656
Epoch 10/10, Batch 267/883, Training Loss: 0.8582
Epoch 10/10, Batch 268/883, Training Loss: 0.3024
Epoch 10/10, Batch 269/883, Training Loss: 0.5326
Epoch 10/10, Batch 270/883, Training Loss: 0.8836
Epoch 10/10, Batch 271/883, Training Loss: 0.4073
Epoch 10/10, Batch 272/883, Training Loss: 0.6736
Epoch 10/10, Batch 273/883, Training Loss: 0.5337
Epoch 10/10, Batch 274/883, Training Loss: 0.4670
Epoch 10/10, Batch 275/883, Training Loss: 0.4712
Epoch 10/10, Batch 276/883, Training Loss: 0.3954
Epoch 10/10, Batch 277/883, Training Loss: 0.6568
Epoch 10/10, Batch 278/883, Training Loss: 0.6739
Epoch 10/10, Batch 279/883, Training Loss: 0.6836
Epoch 10/10, Batch 280/883, Training Loss: 0.3136
Epoch 10/10, Batch 281/883, Training Loss: 0.4890
Epoch 10/10, Batch 282/883, Training Loss: 0.5828
Epoch 10/10, Batch 283/883, Training Loss: 0.3379
Epoch 10/10, Batch 284/883, Training Loss: 0.5309
Epoch 10/10, Batch 285/883, Training Loss: 0.6300
Epoch 10/10, Batch 286/883, Training Loss: 0.2408
Epoch 10/10, Batch 287/883, Training Loss: 0.2678
Epoch 10/10, Batch 288/883, Training Loss: 0.4892
Epoch 10/10, Batch 289/883, Training Loss: 0.4615
Epoch 10/10, Batch 290/883, Training Loss: 0.3809
Epoch 10/10, Batch 291/883, Training Loss: 0.4087
Epoch 10/10, Batch 292/883, Training Loss: 0.3141
Epoch 10/10, Batch 293/883, Training Loss: 0.4474
Epoch 10/10, Batch 294/883, Training Loss: 0.4669
Epoch 10/10, Batch 295/883, Training Loss: 0.4519
Epoch 10/10, Batch 296/883, Training Loss: 0.5463
Epoch 10/10, Batch 297/883, Training Loss: 0.6477
Epoch 10/10, Batch 298/883, Training Loss: 0.3466
Epoch 10/10, Batch 299/883, Training Loss: 1.0263
Epoch 10/10, Batch 300/883, Training Loss: 0.7417
Epoch 10/10, Batch 301/883, Training Loss: 0.2348
Epoch 10/10, Batch 302/883, Training Loss: 0.9755
Epoch 10/10, Batch 303/883, Training Loss: 0.4015
Epoch 10/10, Batch 304/883, Training Loss: 0.7423
Epoch 10/10, Batch 305/883, Training Loss: 0.2892
Epoch 10/10, Batch 306/883, Training Loss: 0.4140
Epoch 10/10, Batch 307/883, Training Loss: 0.2709
Epoch 10/10, Batch 308/883, Training Loss: 0.5770
Epoch 10/10, Batch 309/883, Training Loss: 0.3508
Epoch 10/10, Batch 310/883, Training Loss: 0.4191
Epoch 10/10, Batch 311/883, Training Loss: 0.4237
Epoch 10/10, Batch 312/883, Training Loss: 0.6668
Epoch 10/10, Batch 313/883, Training Loss: 0.5019
Epoch 10/10, Batch 314/883, Training Loss: 0.7611
Epoch 10/10, Batch 315/883, Training Loss: 0.2802
Epoch 10/10, Batch 316/883, Training Loss: 0.8334
Epoch 10/10, Batch 317/883, Training Loss: 0.5722
Epoch 10/10, Batch 318/883, Training Loss: 0.4958
Epoch 10/10, Batch 319/883, Training Loss: 0.6044
Epoch 10/10, Batch 320/883, Training Loss: 0.8700
Epoch 10/10, Batch 321/883, Training Loss: 0.9612
Epoch 10/10, Batch 322/883, Training Loss: 0.7642
Epoch 10/10, Batch 323/883, Training Loss: 0.4270
Epoch 10/10, Batch 324/883, Training Loss: 0.5235
Epoch 10/10, Batch 325/883, Training Loss: 0.4569
Epoch 10/10, Batch 326/883, Training Loss: 0.3577
Epoch 10/10, Batch 327/883, Training Loss: 0.6119
Epoch 10/10, Batch 328/883, Training Loss: 0.7564
Epoch 10/10, Batch 329/883, Training Loss: 0.4218
Epoch 10/10, Batch 330/883, Training Loss: 0.4719
Epoch 10/10, Batch 331/883, Training Loss: 0.3341
Epoch 10/10, Batch 332/883, Training Loss: 0.5927
Epoch 10/10, Batch 333/883, Training Loss: 0.3568
Epoch 10/10, Batch 334/883, Training Loss: 0.4356
Epoch 10/10, Batch 335/883, Training Loss: 0.5496
Epoch 10/10, Batch 336/883, Training Loss: 0.5182
Epoch 10/10, Batch 337/883, Training Loss: 0.4128
Epoch 10/10, Batch 338/883, Training Loss: 0.3373
Epoch 10/10, Batch 339/883, Training Loss: 0.2936
Epoch 10/10, Batch 340/883, Training Loss: 0.7014
Epoch 10/10, Batch 341/883, Training Loss: 0.7588
Epoch 10/10, Batch 342/883, Training Loss: 0.3714
Epoch 10/10, Batch 343/883, Training Loss: 0.2348
Epoch 10/10, Batch 344/883, Training Loss: 1.0912
Epoch 10/10, Batch 345/883, Training Loss: 0.7466
Epoch 10/10, Batch 346/883, Training Loss: 0.6271
Epoch 10/10, Batch 347/883, Training Loss: 0.5255
Epoch 10/10, Batch 348/883, Training Loss: 0.5212
Epoch 10/10, Batch 349/883, Training Loss: 0.4938
Epoch 10/10, Batch 350/883, Training Loss: 0.3835
Epoch 10/10, Batch 351/883, Training Loss: 0.5682
Epoch 10/10, Batch 352/883, Training Loss: 0.5244
Epoch 10/10, Batch 353/883, Training Loss: 0.5679
Epoch 10/10, Batch 354/883, Training Loss: 0.4878
Epoch 10/10, Batch 355/883, Training Loss: 0.5060
Epoch 10/10, Batch 356/883, Training Loss: 0.3680
Epoch 10/10, Batch 357/883, Training Loss: 0.4617
Epoch 10/10, Batch 358/883, Training Loss: 0.4509
Epoch 10/10, Batch 359/883, Training Loss: 0.3981
Epoch 10/10, Batch 360/883, Training Loss: 0.6123
Epoch 10/10, Batch 361/883, Training Loss: 0.4923
Epoch 10/10, Batch 362/883, Training Loss: 0.4283
Epoch 10/10, Batch 363/883, Training Loss: 0.7964
Epoch 10/10, Batch 364/883, Training Loss: 0.4689
Epoch 10/10, Batch 365/883, Training Loss: 0.3458
Epoch 10/10, Batch 366/883, Training Loss: 0.2918
Epoch 10/10, Batch 367/883, Training Loss: 0.7239
Epoch 10/10, Batch 368/883, Training Loss: 0.5027
Epoch 10/10, Batch 369/883, Training Loss: 0.5768
Epoch 10/10, Batch 370/883, Training Loss: 0.3378
Epoch 10/10, Batch 371/883, Training Loss: 0.5529
Epoch 10/10, Batch 372/883, Training Loss: 0.5183
Epoch 10/10, Batch 373/883, Training Loss: 0.4002
Epoch 10/10, Batch 374/883, Training Loss: 0.5111
Epoch 10/10, Batch 375/883, Training Loss: 0.3547
Epoch 10/10, Batch 376/883, Training Loss: 0.3695
Epoch 10/10, Batch 377/883, Training Loss: 0.5371
Epoch 10/10, Batch 378/883, Training Loss: 0.3544
Epoch 10/10, Batch 379/883, Training Loss: 0.5824
Epoch 10/10, Batch 380/883, Training Loss: 0.4953
Epoch 10/10, Batch 381/883, Training Loss: 0.8450
Epoch 10/10, Batch 382/883, Training Loss: 0.4451
Epoch 10/10, Batch 383/883, Training Loss: 0.5502
Epoch 10/10, Batch 384/883, Training Loss: 0.2979
Epoch 10/10, Batch 385/883, Training Loss: 0.5590
Epoch 10/10, Batch 386/883, Training Loss: 0.6178
Epoch 10/10, Batch 387/883, Training Loss: 0.3329
Epoch 10/10, Batch 388/883, Training Loss: 0.4169
Epoch 10/10, Batch 389/883, Training Loss: 0.5560
Epoch 10/10, Batch 390/883, Training Loss: 0.4227
Epoch 10/10, Batch 391/883, Training Loss: 0.9799
Epoch 10/10, Batch 392/883, Training Loss: 0.5131
Epoch 10/10, Batch 393/883, Training Loss: 0.5562
Epoch 10/10, Batch 394/883, Training Loss: 0.4119
Epoch 10/10, Batch 395/883, Training Loss: 0.4422
Epoch 10/10, Batch 396/883, Training Loss: 0.7317
Epoch 10/10, Batch 397/883, Training Loss: 0.7412
Epoch 10/10, Batch 398/883, Training Loss: 0.3211
Epoch 10/10, Batch 399/883, Training Loss: 0.5213
Epoch 10/10, Batch 400/883, Training Loss: 0.4113
Epoch 10/10, Batch 401/883, Training Loss: 0.4022
Epoch 10/10, Batch 402/883, Training Loss: 0.4166
Epoch 10/10, Batch 403/883, Training Loss: 0.4197
Epoch 10/10, Batch 404/883, Training Loss: 0.3734
Epoch 10/10, Batch 405/883, Training Loss: 0.5281
Epoch 10/10, Batch 406/883, Training Loss: 0.4150
Epoch 10/10, Batch 407/883, Training Loss: 0.3812
Epoch 10/10, Batch 408/883, Training Loss: 0.5968
Epoch 10/10, Batch 409/883, Training Loss: 0.6141
Epoch 10/10, Batch 410/883, Training Loss: 0.4414
Epoch 10/10, Batch 411/883, Training Loss: 0.7095
Epoch 10/10, Batch 412/883, Training Loss: 0.3742
Epoch 10/10, Batch 413/883, Training Loss: 0.2938
Epoch 10/10, Batch 414/883, Training Loss: 0.2948
Epoch 10/10, Batch 415/883, Training Loss: 0.2801
Epoch 10/10, Batch 416/883, Training Loss: 0.3719
Epoch 10/10, Batch 417/883, Training Loss: 0.2653
Epoch 10/10, Batch 418/883, Training Loss: 0.4580
Epoch 10/10, Batch 419/883, Training Loss: 0.4993
Epoch 10/10, Batch 420/883, Training Loss: 0.3730
Epoch 10/10, Batch 421/883, Training Loss: 0.3704
Epoch 10/10, Batch 422/883, Training Loss: 0.6050
Epoch 10/10, Batch 423/883, Training Loss: 0.4523
Epoch 10/10, Batch 424/883, Training Loss: 0.8086
Epoch 10/10, Batch 425/883, Training Loss: 0.3531
Epoch 10/10, Batch 426/883, Training Loss: 0.2380
Epoch 10/10, Batch 427/883, Training Loss: 0.5708
Epoch 10/10, Batch 428/883, Training Loss: 0.3615
Epoch 10/10, Batch 429/883, Training Loss: 0.4872
Epoch 10/10, Batch 430/883, Training Loss: 0.6181
Epoch 10/10, Batch 431/883, Training Loss: 0.2845
Epoch 10/10, Batch 432/883, Training Loss: 0.7096
Epoch 10/10, Batch 433/883, Training Loss: 0.2796
Epoch 10/10, Batch 434/883, Training Loss: 0.6629
Epoch 10/10, Batch 435/883, Training Loss: 0.3909
Epoch 10/10, Batch 436/883, Training Loss: 0.1991
Epoch 10/10, Batch 437/883, Training Loss: 0.4889
Epoch 10/10, Batch 438/883, Training Loss: 0.3333
Epoch 10/10, Batch 439/883, Training Loss: 0.4024
Epoch 10/10, Batch 440/883, Training Loss: 0.3273
Epoch 10/10, Batch 441/883, Training Loss: 0.8207
Epoch 10/10, Batch 442/883, Training Loss: 0.3577
Epoch 10/10, Batch 443/883, Training Loss: 0.7235
Epoch 10/10, Batch 444/883, Training Loss: 0.2910
Epoch 10/10, Batch 445/883, Training Loss: 0.7621
Epoch 10/10, Batch 446/883, Training Loss: 0.3213
Epoch 10/10, Batch 447/883, Training Loss: 0.2891
Epoch 10/10, Batch 448/883, Training Loss: 0.9318
Epoch 10/10, Batch 449/883, Training Loss: 0.7255
Epoch 10/10, Batch 450/883, Training Loss: 0.2862
Epoch 10/10, Batch 451/883, Training Loss: 0.5819
Epoch 10/10, Batch 452/883, Training Loss: 0.6829
Epoch 10/10, Batch 453/883, Training Loss: 0.3780
Epoch 10/10, Batch 454/883, Training Loss: 0.3937
Epoch 10/10, Batch 455/883, Training Loss: 0.4336
Epoch 10/10, Batch 456/883, Training Loss: 0.4119
Epoch 10/10, Batch 457/883, Training Loss: 0.3185
Epoch 10/10, Batch 458/883, Training Loss: 0.5397
Epoch 10/10, Batch 459/883, Training Loss: 0.4057
Epoch 10/10, Batch 460/883, Training Loss: 0.4692
Epoch 10/10, Batch 461/883, Training Loss: 0.2369
Epoch 10/10, Batch 462/883, Training Loss: 0.2235
Epoch 10/10, Batch 463/883, Training Loss: 0.5258
Epoch 10/10, Batch 464/883, Training Loss: 0.5680
Epoch 10/10, Batch 465/883, Training Loss: 0.4804
Epoch 10/10, Batch 466/883, Training Loss: 0.5043
Epoch 10/10, Batch 467/883, Training Loss: 0.3052
Epoch 10/10, Batch 468/883, Training Loss: 0.5301
Epoch 10/10, Batch 469/883, Training Loss: 0.3875
Epoch 10/10, Batch 470/883, Training Loss: 0.9901
Epoch 10/10, Batch 471/883, Training Loss: 0.3808
Epoch 10/10, Batch 472/883, Training Loss: 0.5822
Epoch 10/10, Batch 473/883, Training Loss: 0.2807
Epoch 10/10, Batch 474/883, Training Loss: 0.6309
Epoch 10/10, Batch 475/883, Training Loss: 0.2907
Epoch 10/10, Batch 476/883, Training Loss: 0.6714
Epoch 10/10, Batch 477/883, Training Loss: 0.4112
Epoch 10/10, Batch 478/883, Training Loss: 0.4666
Epoch 10/10, Batch 479/883, Training Loss: 0.4787
Epoch 10/10, Batch 480/883, Training Loss: 0.6771
Epoch 10/10, Batch 481/883, Training Loss: 0.4399
Epoch 10/10, Batch 482/883, Training Loss: 0.2712
Epoch 10/10, Batch 483/883, Training Loss: 0.6721
Epoch 10/10, Batch 484/883, Training Loss: 0.6370
Epoch 10/10, Batch 485/883, Training Loss: 0.3513
Epoch 10/10, Batch 486/883, Training Loss: 0.6021
Epoch 10/10, Batch 487/883, Training Loss: 0.5857
Epoch 10/10, Batch 488/883, Training Loss: 0.4401
Epoch 10/10, Batch 489/883, Training Loss: 0.6066
Epoch 10/10, Batch 490/883, Training Loss: 0.4589
Epoch 10/10, Batch 491/883, Training Loss: 0.4053
Epoch 10/10, Batch 492/883, Training Loss: 0.2416
Epoch 10/10, Batch 493/883, Training Loss: 0.5801
Epoch 10/10, Batch 494/883, Training Loss: 0.5037
Epoch 10/10, Batch 495/883, Training Loss: 0.4807
Epoch 10/10, Batch 496/883, Training Loss: 0.6142
Epoch 10/10, Batch 497/883, Training Loss: 0.5893
Epoch 10/10, Batch 498/883, Training Loss: 0.3835
Epoch 10/10, Batch 499/883, Training Loss: 0.4571
Epoch 10/10, Batch 500/883, Training Loss: 0.3363
Epoch 10/10, Batch 501/883, Training Loss: 0.3439
Epoch 10/10, Batch 502/883, Training Loss: 0.4835
Epoch 10/10, Batch 503/883, Training Loss: 0.6357
Epoch 10/10, Batch 504/883, Training Loss: 0.4108
Epoch 10/10, Batch 505/883, Training Loss: 0.3842
Epoch 10/10, Batch 506/883, Training Loss: 0.8527
Epoch 10/10, Batch 507/883, Training Loss: 0.5202
Epoch 10/10, Batch 508/883, Training Loss: 0.6649
Epoch 10/10, Batch 509/883, Training Loss: 0.4951
Epoch 10/10, Batch 510/883, Training Loss: 0.8760
Epoch 10/10, Batch 511/883, Training Loss: 0.2990
Epoch 10/10, Batch 512/883, Training Loss: 0.7630
Epoch 10/10, Batch 513/883, Training Loss: 0.4124
Epoch 10/10, Batch 514/883, Training Loss: 0.7253
Epoch 10/10, Batch 515/883, Training Loss: 0.4316
Epoch 10/10, Batch 516/883, Training Loss: 0.6780
Epoch 10/10, Batch 517/883, Training Loss: 0.2565
Epoch 10/10, Batch 518/883, Training Loss: 0.7355
Epoch 10/10, Batch 519/883, Training Loss: 0.6069
Epoch 10/10, Batch 520/883, Training Loss: 0.3156
Epoch 10/10, Batch 521/883, Training Loss: 0.1736
Epoch 10/10, Batch 522/883, Training Loss: 0.7670
Epoch 10/10, Batch 523/883, Training Loss: 0.5852
Epoch 10/10, Batch 524/883, Training Loss: 0.7021
Epoch 10/10, Batch 525/883, Training Loss: 0.5736
Epoch 10/10, Batch 526/883, Training Loss: 0.7423
Epoch 10/10, Batch 527/883, Training Loss: 0.5734
Epoch 10/10, Batch 528/883, Training Loss: 0.4144
Epoch 10/10, Batch 529/883, Training Loss: 0.8211
Epoch 10/10, Batch 530/883, Training Loss: 0.4045
Epoch 10/10, Batch 531/883, Training Loss: 0.7352
Epoch 10/10, Batch 532/883, Training Loss: 0.3613
Epoch 10/10, Batch 533/883, Training Loss: 0.8981
Epoch 10/10, Batch 534/883, Training Loss: 0.4312
Epoch 10/10, Batch 535/883, Training Loss: 0.4791
Epoch 10/10, Batch 536/883, Training Loss: 0.5168
Epoch 10/10, Batch 537/883, Training Loss: 0.4047
Epoch 10/10, Batch 538/883, Training Loss: 0.5642
Epoch 10/10, Batch 539/883, Training Loss: 0.5781
Epoch 10/10, Batch 540/883, Training Loss: 0.3912
Epoch 10/10, Batch 541/883, Training Loss: 0.6025
Epoch 10/10, Batch 542/883, Training Loss: 0.6066
Epoch 10/10, Batch 543/883, Training Loss: 0.5740
Epoch 10/10, Batch 544/883, Training Loss: 0.6344
Epoch 10/10, Batch 545/883, Training Loss: 0.8746
Epoch 10/10, Batch 546/883, Training Loss: 0.3152
Epoch 10/10, Batch 547/883, Training Loss: 0.5973
Epoch 10/10, Batch 548/883, Training Loss: 0.3454
Epoch 10/10, Batch 549/883, Training Loss: 0.7544
Epoch 10/10, Batch 550/883, Training Loss: 0.6893
Epoch 10/10, Batch 551/883, Training Loss: 0.5954
Epoch 10/10, Batch 552/883, Training Loss: 0.3024
Epoch 10/10, Batch 553/883, Training Loss: 0.4925
Epoch 10/10, Batch 554/883, Training Loss: 0.4235
Epoch 10/10, Batch 555/883, Training Loss: 0.5667
Epoch 10/10, Batch 556/883, Training Loss: 0.5859
Epoch 10/10, Batch 557/883, Training Loss: 0.5792
Epoch 10/10, Batch 558/883, Training Loss: 0.3915
Epoch 10/10, Batch 559/883, Training Loss: 0.3386
Epoch 10/10, Batch 560/883, Training Loss: 0.5426
Epoch 10/10, Batch 561/883, Training Loss: 0.5489
Epoch 10/10, Batch 562/883, Training Loss: 0.4490
Epoch 10/10, Batch 563/883, Training Loss: 0.3533
Epoch 10/10, Batch 564/883, Training Loss: 0.8499
Epoch 10/10, Batch 565/883, Training Loss: 0.5404
Epoch 10/10, Batch 566/883, Training Loss: 0.5781
Epoch 10/10, Batch 567/883, Training Loss: 0.7494
Epoch 10/10, Batch 568/883, Training Loss: 0.6012
Epoch 10/10, Batch 569/883, Training Loss: 0.4422
Epoch 10/10, Batch 570/883, Training Loss: 0.4568
Epoch 10/10, Batch 571/883, Training Loss: 0.3605
Epoch 10/10, Batch 572/883, Training Loss: 0.7354
Epoch 10/10, Batch 573/883, Training Loss: 0.5048
Epoch 10/10, Batch 574/883, Training Loss: 0.5947
Epoch 10/10, Batch 575/883, Training Loss: 0.2524
Epoch 10/10, Batch 576/883, Training Loss: 0.4953
Epoch 10/10, Batch 577/883, Training Loss: 0.3280
Epoch 10/10, Batch 578/883, Training Loss: 0.4855
Epoch 10/10, Batch 579/883, Training Loss: 0.3897
Epoch 10/10, Batch 580/883, Training Loss: 0.2833
Epoch 10/10, Batch 581/883, Training Loss: 0.4020
Epoch 10/10, Batch 582/883, Training Loss: 0.4798
Epoch 10/10, Batch 583/883, Training Loss: 0.3928
Epoch 10/10, Batch 584/883, Training Loss: 0.5660
Epoch 10/10, Batch 585/883, Training Loss: 0.4933
Epoch 10/10, Batch 586/883, Training Loss: 0.6317
Epoch 10/10, Batch 587/883, Training Loss: 0.9061
Epoch 10/10, Batch 588/883, Training Loss: 0.3480
Epoch 10/10, Batch 589/883, Training Loss: 0.3286
Epoch 10/10, Batch 590/883, Training Loss: 0.3430
Epoch 10/10, Batch 591/883, Training Loss: 0.6007
Epoch 10/10, Batch 592/883, Training Loss: 0.8196
Epoch 10/10, Batch 593/883, Training Loss: 0.5559
Epoch 10/10, Batch 594/883, Training Loss: 0.5438
Epoch 10/10, Batch 595/883, Training Loss: 0.7066
Epoch 10/10, Batch 596/883, Training Loss: 0.5167
Epoch 10/10, Batch 597/883, Training Loss: 1.1594
Epoch 10/10, Batch 598/883, Training Loss: 0.3612
Epoch 10/10, Batch 599/883, Training Loss: 0.2771
Epoch 10/10, Batch 600/883, Training Loss: 0.3190
Epoch 10/10, Batch 601/883, Training Loss: 0.3719
Epoch 10/10, Batch 602/883, Training Loss: 0.4552
Epoch 10/10, Batch 603/883, Training Loss: 0.3891
Epoch 10/10, Batch 604/883, Training Loss: 0.5039
Epoch 10/10, Batch 605/883, Training Loss: 0.7674
Epoch 10/10, Batch 606/883, Training Loss: 0.4203
Epoch 10/10, Batch 607/883, Training Loss: 0.5237
Epoch 10/10, Batch 608/883, Training Loss: 0.4565
Epoch 10/10, Batch 609/883, Training Loss: 0.3589
Epoch 10/10, Batch 610/883, Training Loss: 0.3880
Epoch 10/10, Batch 611/883, Training Loss: 0.3022
Epoch 10/10, Batch 612/883, Training Loss: 0.2662
Epoch 10/10, Batch 613/883, Training Loss: 0.3450
Epoch 10/10, Batch 614/883, Training Loss: 0.3802
Epoch 10/10, Batch 615/883, Training Loss: 0.5452
Epoch 10/10, Batch 616/883, Training Loss: 0.3660
Epoch 10/10, Batch 617/883, Training Loss: 0.6307
Epoch 10/10, Batch 618/883, Training Loss: 0.4335
Epoch 10/10, Batch 619/883, Training Loss: 0.2009
Epoch 10/10, Batch 620/883, Training Loss: 0.3316
Epoch 10/10, Batch 621/883, Training Loss: 0.3598
Epoch 10/10, Batch 622/883, Training Loss: 0.2549
Epoch 10/10, Batch 623/883, Training Loss: 0.3319
Epoch 10/10, Batch 624/883, Training Loss: 0.5496
Epoch 10/10, Batch 625/883, Training Loss: 0.5121
Epoch 10/10, Batch 626/883, Training Loss: 0.4190
Epoch 10/10, Batch 627/883, Training Loss: 0.4303
Epoch 10/10, Batch 628/883, Training Loss: 0.7148
Epoch 10/10, Batch 629/883, Training Loss: 0.5204
Epoch 10/10, Batch 630/883, Training Loss: 0.6969
Epoch 10/10, Batch 631/883, Training Loss: 0.5110
Epoch 10/10, Batch 632/883, Training Loss: 0.5565
Epoch 10/10, Batch 633/883, Training Loss: 0.4511
Epoch 10/10, Batch 634/883, Training Loss: 1.1117
Epoch 10/10, Batch 635/883, Training Loss: 0.3034
Epoch 10/10, Batch 636/883, Training Loss: 0.2411
Epoch 10/10, Batch 637/883, Training Loss: 0.7703
Epoch 10/10, Batch 638/883, Training Loss: 0.4650
Epoch 10/10, Batch 639/883, Training Loss: 0.8371
Epoch 10/10, Batch 640/883, Training Loss: 0.3660
Epoch 10/10, Batch 641/883, Training Loss: 0.3993
Epoch 10/10, Batch 642/883, Training Loss: 0.4468
Epoch 10/10, Batch 643/883, Training Loss: 0.6615
Epoch 10/10, Batch 644/883, Training Loss: 0.4761
Epoch 10/10, Batch 645/883, Training Loss: 0.4162
Epoch 10/10, Batch 646/883, Training Loss: 0.4618
Epoch 10/10, Batch 647/883, Training Loss: 0.9293
Epoch 10/10, Batch 648/883, Training Loss: 0.5779
Epoch 10/10, Batch 649/883, Training Loss: 0.5136
Epoch 10/10, Batch 650/883, Training Loss: 0.4014
Epoch 10/10, Batch 651/883, Training Loss: 0.6108
Epoch 10/10, Batch 652/883, Training Loss: 1.0194
Epoch 10/10, Batch 653/883, Training Loss: 0.8827
Epoch 10/10, Batch 654/883, Training Loss: 0.3810
Epoch 10/10, Batch 655/883, Training Loss: 0.3388
Epoch 10/10, Batch 656/883, Training Loss: 0.2568
Epoch 10/10, Batch 657/883, Training Loss: 0.5313
Epoch 10/10, Batch 658/883, Training Loss: 0.2783
Epoch 10/10, Batch 659/883, Training Loss: 0.8169
Epoch 10/10, Batch 660/883, Training Loss: 0.4797
Epoch 10/10, Batch 661/883, Training Loss: 0.5303
Epoch 10/10, Batch 662/883, Training Loss: 0.6349
Epoch 10/10, Batch 663/883, Training Loss: 0.5090
Epoch 10/10, Batch 664/883, Training Loss: 0.3321
Epoch 10/10, Batch 665/883, Training Loss: 0.6341
Epoch 10/10, Batch 666/883, Training Loss: 0.4566
Epoch 10/10, Batch 667/883, Training Loss: 0.3938
Epoch 10/10, Batch 668/883, Training Loss: 0.3789
Epoch 10/10, Batch 669/883, Training Loss: 0.3708
Epoch 10/10, Batch 670/883, Training Loss: 0.4584
Epoch 10/10, Batch 671/883, Training Loss: 0.4669
Epoch 10/10, Batch 672/883, Training Loss: 0.4894
Epoch 10/10, Batch 673/883, Training Loss: 0.4862
Epoch 10/10, Batch 674/883, Training Loss: 0.3328
Epoch 10/10, Batch 675/883, Training Loss: 0.6709
Epoch 10/10, Batch 676/883, Training Loss: 0.4346
Epoch 10/10, Batch 677/883, Training Loss: 0.6396
Epoch 10/10, Batch 678/883, Training Loss: 0.2808
Epoch 10/10, Batch 679/883, Training Loss: 0.4007
Epoch 10/10, Batch 680/883, Training Loss: 0.5407
Epoch 10/10, Batch 681/883, Training Loss: 0.3416
Epoch 10/10, Batch 682/883, Training Loss: 0.4641
Epoch 10/10, Batch 683/883, Training Loss: 0.4389
Epoch 10/10, Batch 684/883, Training Loss: 0.4812
Epoch 10/10, Batch 685/883, Training Loss: 0.8524
Epoch 10/10, Batch 686/883, Training Loss: 0.3235
Epoch 10/10, Batch 687/883, Training Loss: 0.5010
Epoch 10/10, Batch 688/883, Training Loss: 0.6703
Epoch 10/10, Batch 689/883, Training Loss: 0.3466
Epoch 10/10, Batch 690/883, Training Loss: 0.5725
Epoch 10/10, Batch 691/883, Training Loss: 0.6098
Epoch 10/10, Batch 692/883, Training Loss: 0.4848
Epoch 10/10, Batch 693/883, Training Loss: 0.5029
Epoch 10/10, Batch 694/883, Training Loss: 0.3330
Epoch 10/10, Batch 695/883, Training Loss: 0.3831
Epoch 10/10, Batch 696/883, Training Loss: 0.3862
Epoch 10/10, Batch 697/883, Training Loss: 0.7092
Epoch 10/10, Batch 698/883, Training Loss: 0.3478
Epoch 10/10, Batch 699/883, Training Loss: 0.4323
Epoch 10/10, Batch 700/883, Training Loss: 0.5913
Epoch 10/10, Batch 701/883, Training Loss: 0.3980
Epoch 10/10, Batch 702/883, Training Loss: 0.5350
Epoch 10/10, Batch 703/883, Training Loss: 0.7361
Epoch 10/10, Batch 704/883, Training Loss: 0.5987
Epoch 10/10, Batch 705/883, Training Loss: 0.6713
Epoch 10/10, Batch 706/883, Training Loss: 0.3584
Epoch 10/10, Batch 707/883, Training Loss: 0.4896
Epoch 10/10, Batch 708/883, Training Loss: 0.6449
Epoch 10/10, Batch 709/883, Training Loss: 0.3908
Epoch 10/10, Batch 710/883, Training Loss: 0.6394
Epoch 10/10, Batch 711/883, Training Loss: 0.6543
Epoch 10/10, Batch 712/883, Training Loss: 0.4836
Epoch 10/10, Batch 713/883, Training Loss: 0.5873
Epoch 10/10, Batch 714/883, Training Loss: 0.5248
Epoch 10/10, Batch 715/883, Training Loss: 0.3365
Epoch 10/10, Batch 716/883, Training Loss: 0.2120
Epoch 10/10, Batch 717/883, Training Loss: 0.7592
Epoch 10/10, Batch 718/883, Training Loss: 0.5126
Epoch 10/10, Batch 719/883, Training Loss: 0.3171
Epoch 10/10, Batch 720/883, Training Loss: 0.5390
Epoch 10/10, Batch 721/883, Training Loss: 0.8718
Epoch 10/10, Batch 722/883, Training Loss: 0.7763
Epoch 10/10, Batch 723/883, Training Loss: 0.4504
Epoch 10/10, Batch 724/883, Training Loss: 0.3234
Epoch 10/10, Batch 725/883, Training Loss: 0.5491
Epoch 10/10, Batch 726/883, Training Loss: 0.5152
Epoch 10/10, Batch 727/883, Training Loss: 0.3434
Epoch 10/10, Batch 728/883, Training Loss: 0.3678
Epoch 10/10, Batch 729/883, Training Loss: 0.9368
Epoch 10/10, Batch 730/883, Training Loss: 0.6301
Epoch 10/10, Batch 731/883, Training Loss: 0.7309
Epoch 10/10, Batch 732/883, Training Loss: 0.3540
Epoch 10/10, Batch 733/883, Training Loss: 0.2677
Epoch 10/10, Batch 734/883, Training Loss: 0.5081
Epoch 10/10, Batch 735/883, Training Loss: 0.3645
Epoch 10/10, Batch 736/883, Training Loss: 0.7792
Epoch 10/10, Batch 737/883, Training Loss: 0.3520
Epoch 10/10, Batch 738/883, Training Loss: 0.6245
Epoch 10/10, Batch 739/883, Training Loss: 0.5493
Epoch 10/10, Batch 740/883, Training Loss: 0.3193
Epoch 10/10, Batch 741/883, Training Loss: 0.3300
Epoch 10/10, Batch 742/883, Training Loss: 0.5834
Epoch 10/10, Batch 743/883, Training Loss: 0.3746
Epoch 10/10, Batch 744/883, Training Loss: 0.3135
Epoch 10/10, Batch 745/883, Training Loss: 0.5059
Epoch 10/10, Batch 746/883, Training Loss: 0.6514
Epoch 10/10, Batch 747/883, Training Loss: 0.4980
Epoch 10/10, Batch 748/883, Training Loss: 0.5141
Epoch 10/10, Batch 749/883, Training Loss: 0.5715
Epoch 10/10, Batch 750/883, Training Loss: 0.5032
Epoch 10/10, Batch 751/883, Training Loss: 0.6575
Epoch 10/10, Batch 752/883, Training Loss: 0.3554
Epoch 10/10, Batch 753/883, Training Loss: 0.2549
Epoch 10/10, Batch 754/883, Training Loss: 0.4181
Epoch 10/10, Batch 755/883, Training Loss: 0.3390
Epoch 10/10, Batch 756/883, Training Loss: 0.6460
Epoch 10/10, Batch 757/883, Training Loss: 0.2476
Epoch 10/10, Batch 758/883, Training Loss: 0.2680
Epoch 10/10, Batch 759/883, Training Loss: 0.3977
Epoch 10/10, Batch 760/883, Training Loss: 0.2860
Epoch 10/10, Batch 761/883, Training Loss: 0.4117
Epoch 10/10, Batch 762/883, Training Loss: 0.2342
Epoch 10/10, Batch 763/883, Training Loss: 0.4596
Epoch 10/10, Batch 764/883, Training Loss: 0.2925
Epoch 10/10, Batch 765/883, Training Loss: 0.4409
Epoch 10/10, Batch 766/883, Training Loss: 0.7927
Epoch 10/10, Batch 767/883, Training Loss: 0.4560
Epoch 10/10, Batch 768/883, Training Loss: 0.4346
Epoch 10/10, Batch 769/883, Training Loss: 0.5741
Epoch 10/10, Batch 770/883, Training Loss: 0.4391
Epoch 10/10, Batch 771/883, Training Loss: 0.7464
Epoch 10/10, Batch 772/883, Training Loss: 0.6903
Epoch 10/10, Batch 773/883, Training Loss: 0.2360
Epoch 10/10, Batch 774/883, Training Loss: 0.2591
Epoch 10/10, Batch 775/883, Training Loss: 0.3754
Epoch 10/10, Batch 776/883, Training Loss: 0.5502
Epoch 10/10, Batch 777/883, Training Loss: 0.2542
Epoch 10/10, Batch 778/883, Training Loss: 0.8730
Epoch 10/10, Batch 779/883, Training Loss: 0.3372
Epoch 10/10, Batch 780/883, Training Loss: 0.4070
Epoch 10/10, Batch 781/883, Training Loss: 0.4477
Epoch 10/10, Batch 782/883, Training Loss: 0.4377
Epoch 10/10, Batch 783/883, Training Loss: 0.5808
Epoch 10/10, Batch 784/883, Training Loss: 0.6368
Epoch 10/10, Batch 785/883, Training Loss: 0.7752
Epoch 10/10, Batch 786/883, Training Loss: 0.3253
Epoch 10/10, Batch 787/883, Training Loss: 0.3211
Epoch 10/10, Batch 788/883, Training Loss: 0.8089
Epoch 10/10, Batch 789/883, Training Loss: 0.4882
Epoch 10/10, Batch 790/883, Training Loss: 0.6397
Epoch 10/10, Batch 791/883, Training Loss: 0.5169
Epoch 10/10, Batch 792/883, Training Loss: 0.4671
Epoch 10/10, Batch 793/883, Training Loss: 0.4538
Epoch 10/10, Batch 794/883, Training Loss: 0.3275
Epoch 10/10, Batch 795/883, Training Loss: 0.5141
Epoch 10/10, Batch 796/883, Training Loss: 0.5883
Epoch 10/10, Batch 797/883, Training Loss: 0.5822
Epoch 10/10, Batch 798/883, Training Loss: 0.2989
Epoch 10/10, Batch 799/883, Training Loss: 0.3736
Epoch 10/10, Batch 800/883, Training Loss: 0.7041
Epoch 10/10, Batch 801/883, Training Loss: 0.4167
Epoch 10/10, Batch 802/883, Training Loss: 0.3195
Epoch 10/10, Batch 803/883, Training Loss: 0.4107
Epoch 10/10, Batch 804/883, Training Loss: 0.4634
Epoch 10/10, Batch 805/883, Training Loss: 0.3930
Epoch 10/10, Batch 806/883, Training Loss: 0.2148
Epoch 10/10, Batch 807/883, Training Loss: 0.2906
Epoch 10/10, Batch 808/883, Training Loss: 0.5339
Epoch 10/10, Batch 809/883, Training Loss: 0.4103
Epoch 10/10, Batch 810/883, Training Loss: 0.7542
Epoch 10/10, Batch 811/883, Training Loss: 0.5556
Epoch 10/10, Batch 812/883, Training Loss: 0.2301
Epoch 10/10, Batch 813/883, Training Loss: 0.5283
Epoch 10/10, Batch 814/883, Training Loss: 0.9541
Epoch 10/10, Batch 815/883, Training Loss: 0.4089
Epoch 10/10, Batch 816/883, Training Loss: 0.4792
Epoch 10/10, Batch 817/883, Training Loss: 0.4577
Epoch 10/10, Batch 818/883, Training Loss: 0.5167
Epoch 10/10, Batch 819/883, Training Loss: 0.5687
Epoch 10/10, Batch 820/883, Training Loss: 0.6872
Epoch 10/10, Batch 821/883, Training Loss: 0.1974
Epoch 10/10, Batch 822/883, Training Loss: 0.3530
Epoch 10/10, Batch 823/883, Training Loss: 0.5827
Epoch 10/10, Batch 824/883, Training Loss: 0.5303
Epoch 10/10, Batch 825/883, Training Loss: 0.6738
Epoch 10/10, Batch 826/883, Training Loss: 0.2938
Epoch 10/10, Batch 827/883, Training Loss: 0.4983
Epoch 10/10, Batch 828/883, Training Loss: 0.3110
Epoch 10/10, Batch 829/883, Training Loss: 0.3360
Epoch 10/10, Batch 830/883, Training Loss: 0.4140
Epoch 10/10, Batch 831/883, Training Loss: 0.4340
Epoch 10/10, Batch 832/883, Training Loss: 0.6294
Epoch 10/10, Batch 833/883, Training Loss: 0.2734
Epoch 10/10, Batch 834/883, Training Loss: 0.6268
Epoch 10/10, Batch 835/883, Training Loss: 0.7585
Epoch 10/10, Batch 836/883, Training Loss: 0.4957
Epoch 10/10, Batch 837/883, Training Loss: 0.4423
Epoch 10/10, Batch 838/883, Training Loss: 0.1773
Epoch 10/10, Batch 839/883, Training Loss: 0.4189
Epoch 10/10, Batch 840/883, Training Loss: 0.5931
Epoch 10/10, Batch 841/883, Training Loss: 0.6618
Epoch 10/10, Batch 842/883, Training Loss: 0.6841
Epoch 10/10, Batch 843/883, Training Loss: 0.4270
Epoch 10/10, Batch 844/883, Training Loss: 0.6347
Epoch 10/10, Batch 845/883, Training Loss: 0.2897
Epoch 10/10, Batch 846/883, Training Loss: 0.5276
Epoch 10/10, Batch 847/883, Training Loss: 0.6415
Epoch 10/10, Batch 848/883, Training Loss: 0.4899
Epoch 10/10, Batch 849/883, Training Loss: 0.9345
Epoch 10/10, Batch 850/883, Training Loss: 0.5370
Epoch 10/10, Batch 851/883, Training Loss: 0.4160
Epoch 10/10, Batch 852/883, Training Loss: 0.4917
Epoch 10/10, Batch 853/883, Training Loss: 0.6444
Epoch 10/10, Batch 854/883, Training Loss: 0.5270
Epoch 10/10, Batch 855/883, Training Loss: 0.5684
Epoch 10/10, Batch 856/883, Training Loss: 0.8132
Epoch 10/10, Batch 857/883, Training Loss: 0.7808
Epoch 10/10, Batch 858/883, Training Loss: 0.7803
Epoch 10/10, Batch 859/883, Training Loss: 0.5879
Epoch 10/10, Batch 860/883, Training Loss: 0.3655
Epoch 10/10, Batch 861/883, Training Loss: 0.3949
Epoch 10/10, Batch 862/883, Training Loss: 0.4420
Epoch 10/10, Batch 863/883, Training Loss: 0.2257
Epoch 10/10, Batch 864/883, Training Loss: 0.5689
Epoch 10/10, Batch 865/883, Training Loss: 0.5038
Epoch 10/10, Batch 866/883, Training Loss: 0.8734
Epoch 10/10, Batch 867/883, Training Loss: 0.6745
Epoch 10/10, Batch 868/883, Training Loss: 0.3632
Epoch 10/10, Batch 869/883, Training Loss: 0.2574
Epoch 10/10, Batch 870/883, Training Loss: 1.1791
Epoch 10/10, Batch 871/883, Training Loss: 0.3572
Epoch 10/10, Batch 872/883, Training Loss: 0.6575
Epoch 10/10, Batch 873/883, Training Loss: 0.6054
Epoch 10/10, Batch 874/883, Training Loss: 0.4353
Epoch 10/10, Batch 875/883, Training Loss: 0.5079
Epoch 10/10, Batch 876/883, Training Loss: 0.2797
Epoch 10/10, Batch 877/883, Training Loss: 0.4653
Epoch 10/10, Batch 878/883, Training Loss: 0.3791
Epoch 10/10, Batch 879/883, Training Loss: 0.4489
Epoch 10/10, Batch 880/883, Training Loss: 0.6557
Epoch 10/10, Batch 881/883, Training Loss: 0.4151
Epoch 10/10, Batch 882/883, Training Loss: 0.2304
Epoch 10/10, Batch 883/883, Training Loss: 0.4184
Epoch 10/10, Training Loss: 0.5045, Validation Loss: 0.4842, Validation Accuracy: 0.8045
Test Loss: 0.4861, Test Accuracy: 0.7982
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-04-27 20:44:53,349] Trial 3 finished with value: 0.484209561086207 and parameters: {'batch_size': 16, 'learning_rate': 0.007188326518439874, 'weight_decay': 5.235169847231183e-06}. Best is trial 0 with value: 0.2196991708036512.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/10, Batch 1/111, Training Loss: 1.1284
Epoch 1/10, Batch 2/111, Training Loss: 2.4873
Epoch 1/10, Batch 3/111, Training Loss: 2.0630
Epoch 1/10, Batch 4/111, Training Loss: 1.6952
Epoch 1/10, Batch 5/111, Training Loss: 1.6865
Epoch 1/10, Batch 6/111, Training Loss: 1.0415
Epoch 1/10, Batch 7/111, Training Loss: 1.2857
Epoch 1/10, Batch 8/111, Training Loss: 1.6115
Epoch 1/10, Batch 9/111, Training Loss: 1.1464
Epoch 1/10, Batch 10/111, Training Loss: 1.0288
Epoch 1/10, Batch 11/111, Training Loss: 1.5675
Epoch 1/10, Batch 12/111, Training Loss: 1.1233
Epoch 1/10, Batch 13/111, Training Loss: 1.0533
Epoch 1/10, Batch 14/111, Training Loss: 1.0696
Epoch 1/10, Batch 15/111, Training Loss: 1.0273
Epoch 1/10, Batch 16/111, Training Loss: 1.5196
Epoch 1/10, Batch 17/111, Training Loss: 1.2469
Epoch 1/10, Batch 18/111, Training Loss: 1.3240
Epoch 1/10, Batch 19/111, Training Loss: 1.2535
Epoch 1/10, Batch 20/111, Training Loss: 1.0707
Epoch 1/10, Batch 21/111, Training Loss: 1.1785
Epoch 1/10, Batch 22/111, Training Loss: 1.2585
Epoch 1/10, Batch 23/111, Training Loss: 1.1477
Epoch 1/10, Batch 24/111, Training Loss: 1.0861
Epoch 1/10, Batch 25/111, Training Loss: 1.0766
Epoch 1/10, Batch 26/111, Training Loss: 1.0566
Epoch 1/10, Batch 27/111, Training Loss: 1.0186
Epoch 1/10, Batch 28/111, Training Loss: 1.0037
Epoch 1/10, Batch 29/111, Training Loss: 1.1042
Epoch 1/10, Batch 30/111, Training Loss: 1.0341
Epoch 1/10, Batch 31/111, Training Loss: 1.0197
Epoch 1/10, Batch 32/111, Training Loss: 0.9518
Epoch 1/10, Batch 33/111, Training Loss: 1.0079
Epoch 1/10, Batch 34/111, Training Loss: 0.9915
Epoch 1/10, Batch 35/111, Training Loss: 0.9733
Epoch 1/10, Batch 36/111, Training Loss: 0.9478
Epoch 1/10, Batch 37/111, Training Loss: 0.8914
Epoch 1/10, Batch 38/111, Training Loss: 0.9767
Epoch 1/10, Batch 39/111, Training Loss: 0.9784
Epoch 1/10, Batch 40/111, Training Loss: 0.9224
Epoch 1/10, Batch 41/111, Training Loss: 0.9525
Epoch 1/10, Batch 42/111, Training Loss: 0.9816
Epoch 1/10, Batch 43/111, Training Loss: 1.0113
Epoch 1/10, Batch 44/111, Training Loss: 0.9795
Epoch 1/10, Batch 45/111, Training Loss: 0.9913
Epoch 1/10, Batch 46/111, Training Loss: 0.9843
Epoch 1/10, Batch 47/111, Training Loss: 0.9357
Epoch 1/10, Batch 48/111, Training Loss: 0.9135
Epoch 1/10, Batch 49/111, Training Loss: 1.0514
Epoch 1/10, Batch 50/111, Training Loss: 1.0352
Epoch 1/10, Batch 51/111, Training Loss: 1.1909
Epoch 1/10, Batch 52/111, Training Loss: 1.0669
Epoch 1/10, Batch 53/111, Training Loss: 0.8837
Epoch 1/10, Batch 54/111, Training Loss: 0.9032
Epoch 1/10, Batch 55/111, Training Loss: 0.9136
Epoch 1/10, Batch 56/111, Training Loss: 0.9593
Epoch 1/10, Batch 57/111, Training Loss: 1.1003
Epoch 1/10, Batch 58/111, Training Loss: 1.0731
Epoch 1/10, Batch 59/111, Training Loss: 0.9179
Epoch 1/10, Batch 60/111, Training Loss: 0.9049
Epoch 1/10, Batch 61/111, Training Loss: 0.9510
Epoch 1/10, Batch 62/111, Training Loss: 0.8988
Epoch 1/10, Batch 63/111, Training Loss: 0.8650
Epoch 1/10, Batch 64/111, Training Loss: 1.1661
Epoch 1/10, Batch 65/111, Training Loss: 0.9735
Epoch 1/10, Batch 66/111, Training Loss: 0.8943
Epoch 1/10, Batch 67/111, Training Loss: 1.2585
Epoch 1/10, Batch 68/111, Training Loss: 0.9010
Epoch 1/10, Batch 69/111, Training Loss: 1.0962
Epoch 1/10, Batch 70/111, Training Loss: 0.8950
Epoch 1/10, Batch 71/111, Training Loss: 0.8051
Epoch 1/10, Batch 72/111, Training Loss: 1.0062
Epoch 1/10, Batch 73/111, Training Loss: 0.9519
Epoch 1/10, Batch 74/111, Training Loss: 1.0965
Epoch 1/10, Batch 75/111, Training Loss: 1.0388
Epoch 1/10, Batch 76/111, Training Loss: 1.0660
Epoch 1/10, Batch 77/111, Training Loss: 0.9638
Epoch 1/10, Batch 78/111, Training Loss: 0.9808
Epoch 1/10, Batch 79/111, Training Loss: 0.9367
Epoch 1/10, Batch 80/111, Training Loss: 0.9252
Epoch 1/10, Batch 81/111, Training Loss: 0.8715
Epoch 1/10, Batch 82/111, Training Loss: 0.8571
Epoch 1/10, Batch 83/111, Training Loss: 0.8621
Epoch 1/10, Batch 84/111, Training Loss: 1.2292
Epoch 1/10, Batch 85/111, Training Loss: 0.9787
Epoch 1/10, Batch 86/111, Training Loss: 1.0021
Epoch 1/10, Batch 87/111, Training Loss: 1.1852
Epoch 1/10, Batch 88/111, Training Loss: 0.9357
Epoch 1/10, Batch 89/111, Training Loss: 1.0238
Epoch 1/10, Batch 90/111, Training Loss: 0.9670
Epoch 1/10, Batch 91/111, Training Loss: 0.9199
Epoch 1/10, Batch 92/111, Training Loss: 0.9380
Epoch 1/10, Batch 93/111, Training Loss: 0.9203
Epoch 1/10, Batch 94/111, Training Loss: 1.1162
Epoch 1/10, Batch 95/111, Training Loss: 1.0481
Epoch 1/10, Batch 96/111, Training Loss: 0.8990
Epoch 1/10, Batch 97/111, Training Loss: 1.0190
Epoch 1/10, Batch 98/111, Training Loss: 0.8386
Epoch 1/10, Batch 99/111, Training Loss: 0.9691
Epoch 1/10, Batch 100/111, Training Loss: 0.9094
Epoch 1/10, Batch 101/111, Training Loss: 0.9172
Epoch 1/10, Batch 102/111, Training Loss: 0.8130
Epoch 1/10, Batch 103/111, Training Loss: 0.8289
Epoch 1/10, Batch 104/111, Training Loss: 1.0134
Epoch 1/10, Batch 105/111, Training Loss: 0.9489
Epoch 1/10, Batch 106/111, Training Loss: 0.8031
Epoch 1/10, Batch 107/111, Training Loss: 1.2458
Epoch 1/10, Batch 108/111, Training Loss: 0.8211
Epoch 1/10, Batch 109/111, Training Loss: 0.9628
Epoch 1/10, Batch 110/111, Training Loss: 0.8940
Epoch 1/10, Batch 111/111, Training Loss: 0.9654
Epoch 1/10, Training Loss: 1.0557, Validation Loss: 0.9218, Validation Accuracy: 0.5373
Epoch 2/10, Batch 1/111, Training Loss: 0.8148
Epoch 2/10, Batch 2/111, Training Loss: 0.9183
Epoch 2/10, Batch 3/111, Training Loss: 0.8660
Epoch 2/10, Batch 4/111, Training Loss: 0.8916
Epoch 2/10, Batch 5/111, Training Loss: 0.8746
Epoch 2/10, Batch 6/111, Training Loss: 1.0993
Epoch 2/10, Batch 7/111, Training Loss: 0.9254
Epoch 2/10, Batch 8/111, Training Loss: 0.9596
Epoch 2/10, Batch 9/111, Training Loss: 0.8155
Epoch 2/10, Batch 10/111, Training Loss: 1.1128
Epoch 2/10, Batch 11/111, Training Loss: 0.8278
Epoch 2/10, Batch 12/111, Training Loss: 0.8286
Epoch 2/10, Batch 13/111, Training Loss: 0.8208
Epoch 2/10, Batch 14/111, Training Loss: 0.9167
Epoch 2/10, Batch 15/111, Training Loss: 0.8145
Epoch 2/10, Batch 16/111, Training Loss: 0.8936
Epoch 2/10, Batch 17/111, Training Loss: 0.9176
Epoch 2/10, Batch 18/111, Training Loss: 0.8404
Epoch 2/10, Batch 19/111, Training Loss: 0.9960
Epoch 2/10, Batch 20/111, Training Loss: 0.8111
Epoch 2/10, Batch 21/111, Training Loss: 0.7853
Epoch 2/10, Batch 22/111, Training Loss: 0.8512
Epoch 2/10, Batch 23/111, Training Loss: 0.8852
Epoch 2/10, Batch 24/111, Training Loss: 0.9124
Epoch 2/10, Batch 25/111, Training Loss: 0.8393
Epoch 2/10, Batch 26/111, Training Loss: 0.9404
Epoch 2/10, Batch 27/111, Training Loss: 0.8738
Epoch 2/10, Batch 28/111, Training Loss: 0.9500
Epoch 2/10, Batch 29/111, Training Loss: 0.8301
Epoch 2/10, Batch 30/111, Training Loss: 0.8152
Epoch 2/10, Batch 31/111, Training Loss: 0.8519
Epoch 2/10, Batch 32/111, Training Loss: 0.9320
Epoch 2/10, Batch 33/111, Training Loss: 0.8487
Epoch 2/10, Batch 34/111, Training Loss: 0.8887
Epoch 2/10, Batch 35/111, Training Loss: 0.7919
Epoch 2/10, Batch 36/111, Training Loss: 0.8782
Epoch 2/10, Batch 37/111, Training Loss: 0.7576
Epoch 2/10, Batch 38/111, Training Loss: 0.7774
Epoch 2/10, Batch 39/111, Training Loss: 0.8375
Epoch 2/10, Batch 40/111, Training Loss: 0.7929
Epoch 2/10, Batch 41/111, Training Loss: 0.8304
Epoch 2/10, Batch 42/111, Training Loss: 0.8787
Epoch 2/10, Batch 43/111, Training Loss: 0.8121
Epoch 2/10, Batch 44/111, Training Loss: 0.8160
Epoch 2/10, Batch 45/111, Training Loss: 0.7700
Epoch 2/10, Batch 46/111, Training Loss: 0.7638
Epoch 2/10, Batch 47/111, Training Loss: 0.9294
Epoch 2/10, Batch 48/111, Training Loss: 0.9291
Epoch 2/10, Batch 49/111, Training Loss: 0.8211
Epoch 2/10, Batch 50/111, Training Loss: 0.7722
Epoch 2/10, Batch 51/111, Training Loss: 0.8609
Epoch 2/10, Batch 52/111, Training Loss: 0.7763
Epoch 2/10, Batch 53/111, Training Loss: 0.8216
Epoch 2/10, Batch 54/111, Training Loss: 0.8468
Epoch 2/10, Batch 55/111, Training Loss: 0.9166
Epoch 2/10, Batch 56/111, Training Loss: 0.7465
Epoch 2/10, Batch 57/111, Training Loss: 0.7986
Epoch 2/10, Batch 58/111, Training Loss: 0.8156
Epoch 2/10, Batch 59/111, Training Loss: 0.7974
Epoch 2/10, Batch 60/111, Training Loss: 0.8731
Epoch 2/10, Batch 61/111, Training Loss: 0.8956
Epoch 2/10, Batch 62/111, Training Loss: 0.8076
Epoch 2/10, Batch 63/111, Training Loss: 0.8612
Epoch 2/10, Batch 64/111, Training Loss: 0.8138
Epoch 2/10, Batch 65/111, Training Loss: 0.9044
Epoch 2/10, Batch 66/111, Training Loss: 0.8360
Epoch 2/10, Batch 67/111, Training Loss: 0.8461
Epoch 2/10, Batch 68/111, Training Loss: 0.8776
Epoch 2/10, Batch 69/111, Training Loss: 0.8696
Epoch 2/10, Batch 70/111, Training Loss: 0.8693
Epoch 2/10, Batch 71/111, Training Loss: 0.9308
Epoch 2/10, Batch 72/111, Training Loss: 0.7400
Epoch 2/10, Batch 73/111, Training Loss: 0.8862
Epoch 2/10, Batch 74/111, Training Loss: 0.9431
Epoch 2/10, Batch 75/111, Training Loss: 0.8280
Epoch 2/10, Batch 76/111, Training Loss: 0.8729
Epoch 2/10, Batch 77/111, Training Loss: 0.7905
Epoch 2/10, Batch 78/111, Training Loss: 0.8077
Epoch 2/10, Batch 79/111, Training Loss: 0.8527
Epoch 2/10, Batch 80/111, Training Loss: 0.7073
Epoch 2/10, Batch 81/111, Training Loss: 0.8804
Epoch 2/10, Batch 82/111, Training Loss: 0.9224
Epoch 2/10, Batch 83/111, Training Loss: 0.7645
Epoch 2/10, Batch 84/111, Training Loss: 0.7603
Epoch 2/10, Batch 85/111, Training Loss: 0.7504
Epoch 2/10, Batch 86/111, Training Loss: 0.8643
Epoch 2/10, Batch 87/111, Training Loss: 0.8169
Epoch 2/10, Batch 88/111, Training Loss: 0.8426
Epoch 2/10, Batch 89/111, Training Loss: 0.7374
Epoch 2/10, Batch 90/111, Training Loss: 0.8491
Epoch 2/10, Batch 91/111, Training Loss: 0.7564
Epoch 2/10, Batch 92/111, Training Loss: 0.8044
Epoch 2/10, Batch 93/111, Training Loss: 0.7479
Epoch 2/10, Batch 94/111, Training Loss: 0.7684
Epoch 2/10, Batch 95/111, Training Loss: 0.8853
Epoch 2/10, Batch 96/111, Training Loss: 0.8616
Epoch 2/10, Batch 97/111, Training Loss: 0.7448
Epoch 2/10, Batch 98/111, Training Loss: 0.8408
Epoch 2/10, Batch 99/111, Training Loss: 0.9001
Epoch 2/10, Batch 100/111, Training Loss: 0.7760
Epoch 2/10, Batch 101/111, Training Loss: 0.9841
Epoch 2/10, Batch 102/111, Training Loss: 0.7990
Epoch 2/10, Batch 103/111, Training Loss: 0.9558
Epoch 2/10, Batch 104/111, Training Loss: 0.8840
Epoch 2/10, Batch 105/111, Training Loss: 0.9433
Epoch 2/10, Batch 106/111, Training Loss: 0.9131
Epoch 2/10, Batch 107/111, Training Loss: 0.8252
Epoch 2/10, Batch 108/111, Training Loss: 0.8042
Epoch 2/10, Batch 109/111, Training Loss: 0.7742
Epoch 2/10, Batch 110/111, Training Loss: 0.8716
Epoch 2/10, Batch 111/111, Training Loss: 0.9941
Epoch 2/10, Training Loss: 0.8516, Validation Loss: 0.9871, Validation Accuracy: 0.5566
Epoch 3/10, Batch 1/111, Training Loss: 0.8337
Epoch 3/10, Batch 2/111, Training Loss: 0.8696
Epoch 3/10, Batch 3/111, Training Loss: 0.8176
Epoch 3/10, Batch 4/111, Training Loss: 0.8412
Epoch 3/10, Batch 5/111, Training Loss: 0.7929
Epoch 3/10, Batch 6/111, Training Loss: 0.8665
Epoch 3/10, Batch 7/111, Training Loss: 0.8010
Epoch 3/10, Batch 8/111, Training Loss: 0.7179
Epoch 3/10, Batch 9/111, Training Loss: 0.9144
Epoch 3/10, Batch 10/111, Training Loss: 0.9105
Epoch 3/10, Batch 11/111, Training Loss: 0.8910
Epoch 3/10, Batch 12/111, Training Loss: 0.8027
Epoch 3/10, Batch 13/111, Training Loss: 0.7880
Epoch 3/10, Batch 14/111, Training Loss: 0.8206
Epoch 3/10, Batch 15/111, Training Loss: 0.7976
Epoch 3/10, Batch 16/111, Training Loss: 0.7654
Epoch 3/10, Batch 17/111, Training Loss: 0.9290
Epoch 3/10, Batch 18/111, Training Loss: 0.8446
Epoch 3/10, Batch 19/111, Training Loss: 0.7590
Epoch 3/10, Batch 20/111, Training Loss: 0.7817
Epoch 3/10, Batch 21/111, Training Loss: 0.7741
Epoch 3/10, Batch 22/111, Training Loss: 0.9115
Epoch 3/10, Batch 23/111, Training Loss: 0.7210
Epoch 3/10, Batch 24/111, Training Loss: 0.8259
Epoch 3/10, Batch 25/111, Training Loss: 0.8323
Epoch 3/10, Batch 26/111, Training Loss: 0.8473
Epoch 3/10, Batch 27/111, Training Loss: 0.8152
Epoch 3/10, Batch 28/111, Training Loss: 0.9901
Epoch 3/10, Batch 29/111, Training Loss: 0.7838
Epoch 3/10, Batch 30/111, Training Loss: 0.8342
Epoch 3/10, Batch 31/111, Training Loss: 0.7685
Epoch 3/10, Batch 32/111, Training Loss: 0.8076
Epoch 3/10, Batch 33/111, Training Loss: 0.7933
Epoch 3/10, Batch 34/111, Training Loss: 0.7218
Epoch 3/10, Batch 35/111, Training Loss: 0.9295
Epoch 3/10, Batch 36/111, Training Loss: 0.9377
Epoch 3/10, Batch 37/111, Training Loss: 0.7991
Epoch 3/10, Batch 38/111, Training Loss: 0.9018
Epoch 3/10, Batch 39/111, Training Loss: 0.8359
Epoch 3/10, Batch 40/111, Training Loss: 0.8586
Epoch 3/10, Batch 41/111, Training Loss: 0.8889
Epoch 3/10, Batch 42/111, Training Loss: 0.7480
Epoch 3/10, Batch 43/111, Training Loss: 0.7582
Epoch 3/10, Batch 44/111, Training Loss: 0.8324
Epoch 3/10, Batch 45/111, Training Loss: 0.7191
Epoch 3/10, Batch 46/111, Training Loss: 0.7108
Epoch 3/10, Batch 47/111, Training Loss: 0.8074
Epoch 3/10, Batch 48/111, Training Loss: 0.9034
Epoch 3/10, Batch 49/111, Training Loss: 0.6747
Epoch 3/10, Batch 50/111, Training Loss: 0.7976
Epoch 3/10, Batch 51/111, Training Loss: 0.8700
Epoch 3/10, Batch 52/111, Training Loss: 0.7773
Epoch 3/10, Batch 53/111, Training Loss: 0.8502
Epoch 3/10, Batch 54/111, Training Loss: 0.7687
Epoch 3/10, Batch 55/111, Training Loss: 0.8032
Epoch 3/10, Batch 56/111, Training Loss: 0.7806
Epoch 3/10, Batch 57/111, Training Loss: 0.7674
Epoch 3/10, Batch 58/111, Training Loss: 0.7974
Epoch 3/10, Batch 59/111, Training Loss: 0.8490
Epoch 3/10, Batch 60/111, Training Loss: 0.7833
Epoch 3/10, Batch 61/111, Training Loss: 0.8208
Epoch 3/10, Batch 62/111, Training Loss: 0.7336
Epoch 3/10, Batch 63/111, Training Loss: 0.7475
Epoch 3/10, Batch 64/111, Training Loss: 0.7525
Epoch 3/10, Batch 65/111, Training Loss: 0.8727
Epoch 3/10, Batch 66/111, Training Loss: 0.7778
Epoch 3/10, Batch 67/111, Training Loss: 0.7293
Epoch 3/10, Batch 68/111, Training Loss: 0.8834
Epoch 3/10, Batch 69/111, Training Loss: 0.7402
Epoch 3/10, Batch 70/111, Training Loss: 0.8306
Epoch 3/10, Batch 71/111, Training Loss: 0.8061
Epoch 3/10, Batch 72/111, Training Loss: 0.7443
Epoch 3/10, Batch 73/111, Training Loss: 0.8438
Epoch 3/10, Batch 74/111, Training Loss: 0.7970
Epoch 3/10, Batch 75/111, Training Loss: 0.8315
Epoch 3/10, Batch 76/111, Training Loss: 0.7967
Epoch 3/10, Batch 77/111, Training Loss: 0.7798
Epoch 3/10, Batch 78/111, Training Loss: 0.7841
Epoch 3/10, Batch 79/111, Training Loss: 0.7339
Epoch 3/10, Batch 80/111, Training Loss: 0.8263
Epoch 3/10, Batch 81/111, Training Loss: 0.8011
Epoch 3/10, Batch 82/111, Training Loss: 0.8706
Epoch 3/10, Batch 83/111, Training Loss: 0.7157
Epoch 3/10, Batch 84/111, Training Loss: 0.7878
Epoch 3/10, Batch 85/111, Training Loss: 0.7760
Epoch 3/10, Batch 86/111, Training Loss: 0.7333
Epoch 3/10, Batch 87/111, Training Loss: 0.7460
Epoch 3/10, Batch 88/111, Training Loss: 0.8128
Epoch 3/10, Batch 89/111, Training Loss: 0.6727
Epoch 3/10, Batch 90/111, Training Loss: 0.7350
Epoch 3/10, Batch 91/111, Training Loss: 0.6401
Epoch 3/10, Batch 92/111, Training Loss: 0.7893
Epoch 3/10, Batch 93/111, Training Loss: 0.7387
Epoch 3/10, Batch 94/111, Training Loss: 0.8747
Epoch 3/10, Batch 95/111, Training Loss: 0.7704
Epoch 3/10, Batch 96/111, Training Loss: 0.7889
Epoch 3/10, Batch 97/111, Training Loss: 0.8380
Epoch 3/10, Batch 98/111, Training Loss: 0.7917
Epoch 3/10, Batch 99/111, Training Loss: 0.7689
Epoch 3/10, Batch 100/111, Training Loss: 0.8064
Epoch 3/10, Batch 101/111, Training Loss: 0.8017
Epoch 3/10, Batch 102/111, Training Loss: 0.9031
Epoch 3/10, Batch 103/111, Training Loss: 0.7895
Epoch 3/10, Batch 104/111, Training Loss: 0.7868
Epoch 3/10, Batch 105/111, Training Loss: 0.8045
Epoch 3/10, Batch 106/111, Training Loss: 0.7901
Epoch 3/10, Batch 107/111, Training Loss: 0.8816
Epoch 3/10, Batch 108/111, Training Loss: 0.7937
Epoch 3/10, Batch 109/111, Training Loss: 0.7611
Epoch 3/10, Batch 110/111, Training Loss: 0.8322
Epoch 3/10, Batch 111/111, Training Loss: 0.5525
Epoch 3/10, Training Loss: 0.8028, Validation Loss: 0.9048, Validation Accuracy: 0.5679
Epoch 4/10, Batch 1/111, Training Loss: 0.7650
Epoch 4/10, Batch 2/111, Training Loss: 0.6807
Epoch 4/10, Batch 3/111, Training Loss: 0.7285
Epoch 4/10, Batch 4/111, Training Loss: 0.8721
Epoch 4/10, Batch 5/111, Training Loss: 0.7608
Epoch 4/10, Batch 6/111, Training Loss: 0.7926
Epoch 4/10, Batch 7/111, Training Loss: 0.7504
Epoch 4/10, Batch 8/111, Training Loss: 0.8877
Epoch 4/10, Batch 9/111, Training Loss: 0.8449
Epoch 4/10, Batch 10/111, Training Loss: 0.8801
Epoch 4/10, Batch 11/111, Training Loss: 0.7704
Epoch 4/10, Batch 12/111, Training Loss: 0.7134
Epoch 4/10, Batch 13/111, Training Loss: 0.7329
Epoch 4/10, Batch 14/111, Training Loss: 0.7452
Epoch 4/10, Batch 15/111, Training Loss: 0.8920
Epoch 4/10, Batch 16/111, Training Loss: 0.7742
Epoch 4/10, Batch 17/111, Training Loss: 0.8166
Epoch 4/10, Batch 18/111, Training Loss: 0.7254
Epoch 4/10, Batch 19/111, Training Loss: 0.8440
Epoch 4/10, Batch 20/111, Training Loss: 0.8200
Epoch 4/10, Batch 21/111, Training Loss: 0.7007
Epoch 4/10, Batch 22/111, Training Loss: 0.8493
Epoch 4/10, Batch 23/111, Training Loss: 0.7420
Epoch 4/10, Batch 24/111, Training Loss: 0.7052
Epoch 4/10, Batch 25/111, Training Loss: 0.7306
Epoch 4/10, Batch 26/111, Training Loss: 0.7739
Epoch 4/10, Batch 27/111, Training Loss: 0.8224
Epoch 4/10, Batch 28/111, Training Loss: 0.7586
Epoch 4/10, Batch 29/111, Training Loss: 0.7645
Epoch 4/10, Batch 30/111, Training Loss: 0.7398
Epoch 4/10, Batch 31/111, Training Loss: 0.7349
Epoch 4/10, Batch 32/111, Training Loss: 0.8022
Epoch 4/10, Batch 33/111, Training Loss: 0.7952
Epoch 4/10, Batch 34/111, Training Loss: 0.7016
Epoch 4/10, Batch 35/111, Training Loss: 0.6824
Epoch 4/10, Batch 36/111, Training Loss: 0.6537
Epoch 4/10, Batch 37/111, Training Loss: 0.6984
Epoch 4/10, Batch 38/111, Training Loss: 0.8130
Epoch 4/10, Batch 39/111, Training Loss: 0.7937
Epoch 4/10, Batch 40/111, Training Loss: 0.6885
Epoch 4/10, Batch 41/111, Training Loss: 0.6938
Epoch 4/10, Batch 42/111, Training Loss: 0.7678
Epoch 4/10, Batch 43/111, Training Loss: 0.7113
Epoch 4/10, Batch 44/111, Training Loss: 0.7239
Epoch 4/10, Batch 45/111, Training Loss: 0.7140
Epoch 4/10, Batch 46/111, Training Loss: 0.6662
Epoch 4/10, Batch 47/111, Training Loss: 0.7390
Epoch 4/10, Batch 48/111, Training Loss: 0.8280
Epoch 4/10, Batch 49/111, Training Loss: 0.8413
Epoch 4/10, Batch 50/111, Training Loss: 0.7411
Epoch 4/10, Batch 51/111, Training Loss: 0.7570
Epoch 4/10, Batch 52/111, Training Loss: 0.7651
Epoch 4/10, Batch 53/111, Training Loss: 0.7292
Epoch 4/10, Batch 54/111, Training Loss: 0.7643
Epoch 4/10, Batch 55/111, Training Loss: 0.7012
Epoch 4/10, Batch 56/111, Training Loss: 0.9205
Epoch 4/10, Batch 57/111, Training Loss: 0.7475
Epoch 4/10, Batch 58/111, Training Loss: 0.8304
Epoch 4/10, Batch 59/111, Training Loss: 0.7552
Epoch 4/10, Batch 60/111, Training Loss: 0.8018
Epoch 4/10, Batch 61/111, Training Loss: 0.7519
Epoch 4/10, Batch 62/111, Training Loss: 0.7979
Epoch 4/10, Batch 63/111, Training Loss: 0.7898
Epoch 4/10, Batch 64/111, Training Loss: 0.7430
Epoch 4/10, Batch 65/111, Training Loss: 0.7095
Epoch 4/10, Batch 66/111, Training Loss: 0.7405
Epoch 4/10, Batch 67/111, Training Loss: 0.6854
Epoch 4/10, Batch 68/111, Training Loss: 0.6524
Epoch 4/10, Batch 69/111, Training Loss: 0.7186
Epoch 4/10, Batch 70/111, Training Loss: 0.6407
Epoch 4/10, Batch 71/111, Training Loss: 0.7165
Epoch 4/10, Batch 72/111, Training Loss: 0.7869
Epoch 4/10, Batch 73/111, Training Loss: 0.7176
Epoch 4/10, Batch 74/111, Training Loss: 0.7060
Epoch 4/10, Batch 75/111, Training Loss: 0.7990
Epoch 4/10, Batch 76/111, Training Loss: 0.7262
Epoch 4/10, Batch 77/111, Training Loss: 0.7376
Epoch 4/10, Batch 78/111, Training Loss: 0.6830
Epoch 4/10, Batch 79/111, Training Loss: 0.6569
Epoch 4/10, Batch 80/111, Training Loss: 0.6433
Epoch 4/10, Batch 81/111, Training Loss: 0.6986
Epoch 4/10, Batch 82/111, Training Loss: 0.6778
Epoch 4/10, Batch 83/111, Training Loss: 0.9006
Epoch 4/10, Batch 84/111, Training Loss: 0.7843
Epoch 4/10, Batch 85/111, Training Loss: 0.7238
Epoch 4/10, Batch 86/111, Training Loss: 0.7009
Epoch 4/10, Batch 87/111, Training Loss: 0.7822
Epoch 4/10, Batch 88/111, Training Loss: 0.8021
Epoch 4/10, Batch 89/111, Training Loss: 0.7942
Epoch 4/10, Batch 90/111, Training Loss: 0.6693
Epoch 4/10, Batch 91/111, Training Loss: 0.7093
Epoch 4/10, Batch 92/111, Training Loss: 0.6642
Epoch 4/10, Batch 93/111, Training Loss: 0.8384
Epoch 4/10, Batch 94/111, Training Loss: 0.7924
Epoch 4/10, Batch 95/111, Training Loss: 0.7441
Epoch 4/10, Batch 96/111, Training Loss: 0.6261
Epoch 4/10, Batch 97/111, Training Loss: 0.7622
Epoch 4/10, Batch 98/111, Training Loss: 0.8311
Epoch 4/10, Batch 99/111, Training Loss: 0.6831
Epoch 4/10, Batch 100/111, Training Loss: 0.7020
Epoch 4/10, Batch 101/111, Training Loss: 0.7556
Epoch 4/10, Batch 102/111, Training Loss: 0.6528
Epoch 4/10, Batch 103/111, Training Loss: 0.6525
Epoch 4/10, Batch 104/111, Training Loss: 0.7446
Epoch 4/10, Batch 105/111, Training Loss: 0.6435
Epoch 4/10, Batch 106/111, Training Loss: 0.7381
Epoch 4/10, Batch 107/111, Training Loss: 0.7126
Epoch 4/10, Batch 108/111, Training Loss: 0.7998
Epoch 4/10, Batch 109/111, Training Loss: 0.7913
Epoch 4/10, Batch 110/111, Training Loss: 0.6624
Epoch 4/10, Batch 111/111, Training Loss: 0.9809
Epoch 4/10, Training Loss: 0.7511, Validation Loss: 1.0812, Validation Accuracy: 0.5218
Epoch 5/10, Batch 1/111, Training Loss: 0.7317
Epoch 5/10, Batch 2/111, Training Loss: 0.7261
Epoch 5/10, Batch 3/111, Training Loss: 0.7401
Epoch 5/10, Batch 4/111, Training Loss: 0.7844
Epoch 5/10, Batch 5/111, Training Loss: 0.7690
Epoch 5/10, Batch 6/111, Training Loss: 0.6872
Epoch 5/10, Batch 7/111, Training Loss: 0.6402
Epoch 5/10, Batch 8/111, Training Loss: 0.7479
Epoch 5/10, Batch 9/111, Training Loss: 0.7217
Epoch 5/10, Batch 10/111, Training Loss: 0.7627
Epoch 5/10, Batch 11/111, Training Loss: 0.7066
Epoch 5/10, Batch 12/111, Training Loss: 0.6727
Epoch 5/10, Batch 13/111, Training Loss: 0.7970
Epoch 5/10, Batch 14/111, Training Loss: 0.6755
Epoch 5/10, Batch 15/111, Training Loss: 0.7049
Epoch 5/10, Batch 16/111, Training Loss: 0.7824
Epoch 5/10, Batch 17/111, Training Loss: 0.8671
Epoch 5/10, Batch 18/111, Training Loss: 0.6328
Epoch 5/10, Batch 19/111, Training Loss: 0.7640
Epoch 5/10, Batch 20/111, Training Loss: 0.6538
Epoch 5/10, Batch 21/111, Training Loss: 0.6671
Epoch 5/10, Batch 22/111, Training Loss: 0.7940
Epoch 5/10, Batch 23/111, Training Loss: 0.6714
Epoch 5/10, Batch 24/111, Training Loss: 0.8170
Epoch 5/10, Batch 25/111, Training Loss: 0.7866
Epoch 5/10, Batch 26/111, Training Loss: 0.7024
Epoch 5/10, Batch 27/111, Training Loss: 0.6721
Epoch 5/10, Batch 28/111, Training Loss: 0.7202
Epoch 5/10, Batch 29/111, Training Loss: 0.7072
Epoch 5/10, Batch 30/111, Training Loss: 0.7017
Epoch 5/10, Batch 31/111, Training Loss: 0.7598
Epoch 5/10, Batch 32/111, Training Loss: 0.7605
Epoch 5/10, Batch 33/111, Training Loss: 0.8010
Epoch 5/10, Batch 34/111, Training Loss: 0.7676
Epoch 5/10, Batch 35/111, Training Loss: 0.6776
Epoch 5/10, Batch 36/111, Training Loss: 0.6210
Epoch 5/10, Batch 37/111, Training Loss: 0.7566
Epoch 5/10, Batch 38/111, Training Loss: 0.6699
Epoch 5/10, Batch 39/111, Training Loss: 0.6871
Epoch 5/10, Batch 40/111, Training Loss: 0.7064
Epoch 5/10, Batch 41/111, Training Loss: 0.6675
Epoch 5/10, Batch 42/111, Training Loss: 0.7374
Epoch 5/10, Batch 43/111, Training Loss: 0.6613
Epoch 5/10, Batch 44/111, Training Loss: 0.6346
Epoch 5/10, Batch 45/111, Training Loss: 0.6714
Epoch 5/10, Batch 46/111, Training Loss: 0.7379
Epoch 5/10, Batch 47/111, Training Loss: 0.7822
Epoch 5/10, Batch 48/111, Training Loss: 0.7285
Epoch 5/10, Batch 49/111, Training Loss: 0.6050
Epoch 5/10, Batch 50/111, Training Loss: 0.6227
Epoch 5/10, Batch 51/111, Training Loss: 0.7659
Epoch 5/10, Batch 52/111, Training Loss: 0.5745
Epoch 5/10, Batch 53/111, Training Loss: 0.7203
Epoch 5/10, Batch 54/111, Training Loss: 0.7080
Epoch 5/10, Batch 55/111, Training Loss: 0.7178
Epoch 5/10, Batch 56/111, Training Loss: 0.6629
Epoch 5/10, Batch 57/111, Training Loss: 0.7104
Epoch 5/10, Batch 58/111, Training Loss: 0.7052
Epoch 5/10, Batch 59/111, Training Loss: 0.7431
Epoch 5/10, Batch 60/111, Training Loss: 0.7311
Epoch 5/10, Batch 61/111, Training Loss: 0.7487
Epoch 5/10, Batch 62/111, Training Loss: 0.6678
Epoch 5/10, Batch 63/111, Training Loss: 0.7377
Epoch 5/10, Batch 64/111, Training Loss: 0.8444
Epoch 5/10, Batch 65/111, Training Loss: 0.7934
Epoch 5/10, Batch 66/111, Training Loss: 0.7855
Epoch 5/10, Batch 67/111, Training Loss: 0.7638
Epoch 5/10, Batch 68/111, Training Loss: 0.7978
Epoch 5/10, Batch 69/111, Training Loss: 0.8593
Epoch 5/10, Batch 70/111, Training Loss: 0.8053
Epoch 5/10, Batch 71/111, Training Loss: 0.7301
Epoch 5/10, Batch 72/111, Training Loss: 0.7546
Epoch 5/10, Batch 73/111, Training Loss: 0.6524
Epoch 5/10, Batch 74/111, Training Loss: 0.7601
Epoch 5/10, Batch 75/111, Training Loss: 0.7240
Epoch 5/10, Batch 76/111, Training Loss: 0.7815
Epoch 5/10, Batch 77/111, Training Loss: 0.7671
Epoch 5/10, Batch 78/111, Training Loss: 0.6346
Epoch 5/10, Batch 79/111, Training Loss: 0.6539
Epoch 5/10, Batch 80/111, Training Loss: 0.8194
Epoch 5/10, Batch 81/111, Training Loss: 0.7153
Epoch 5/10, Batch 82/111, Training Loss: 0.6398
Epoch 5/10, Batch 83/111, Training Loss: 0.8826
Epoch 5/10, Batch 84/111, Training Loss: 0.7432
Epoch 5/10, Batch 85/111, Training Loss: 0.7639
Epoch 5/10, Batch 86/111, Training Loss: 0.6370
Epoch 5/10, Batch 87/111, Training Loss: 0.6948
Epoch 5/10, Batch 88/111, Training Loss: 0.6386
Epoch 5/10, Batch 89/111, Training Loss: 0.6963
Epoch 5/10, Batch 90/111, Training Loss: 0.7495
Epoch 5/10, Batch 91/111, Training Loss: 0.7048
Epoch 5/10, Batch 92/111, Training Loss: 0.5617
Epoch 5/10, Batch 93/111, Training Loss: 0.6789
Epoch 5/10, Batch 94/111, Training Loss: 0.8275
Epoch 5/10, Batch 95/111, Training Loss: 0.6576
Epoch 5/10, Batch 96/111, Training Loss: 0.6782
Epoch 5/10, Batch 97/111, Training Loss: 0.6885
Epoch 5/10, Batch 98/111, Training Loss: 0.6116
Epoch 5/10, Batch 99/111, Training Loss: 0.7351
Epoch 5/10, Batch 100/111, Training Loss: 0.6818
Epoch 5/10, Batch 101/111, Training Loss: 0.6340
Epoch 5/10, Batch 102/111, Training Loss: 0.6745
Epoch 5/10, Batch 103/111, Training Loss: 0.7862
Epoch 5/10, Batch 104/111, Training Loss: 0.6652
Epoch 5/10, Batch 105/111, Training Loss: 0.6607
Epoch 5/10, Batch 106/111, Training Loss: 0.6667
Epoch 5/10, Batch 107/111, Training Loss: 0.7497
Epoch 5/10, Batch 108/111, Training Loss: 0.7052
Epoch 5/10, Batch 109/111, Training Loss: 0.7317
Epoch 5/10, Batch 110/111, Training Loss: 0.6455
Epoch 5/10, Batch 111/111, Training Loss: 0.6429
Epoch 5/10, Training Loss: 0.7165, Validation Loss: 1.1240, Validation Accuracy: 0.5521
Epoch 6/10, Batch 1/111, Training Loss: 0.7350
Epoch 6/10, Batch 2/111, Training Loss: 0.7184
Epoch 6/10, Batch 3/111, Training Loss: 0.6929
Epoch 6/10, Batch 4/111, Training Loss: 0.7205
Epoch 6/10, Batch 5/111, Training Loss: 0.6902
Epoch 6/10, Batch 6/111, Training Loss: 0.7276
Epoch 6/10, Batch 7/111, Training Loss: 0.7509
Epoch 6/10, Batch 8/111, Training Loss: 0.6105
Epoch 6/10, Batch 9/111, Training Loss: 0.6635
Epoch 6/10, Batch 10/111, Training Loss: 0.6426
Epoch 6/10, Batch 11/111, Training Loss: 0.6711
Epoch 6/10, Batch 12/111, Training Loss: 0.6463
Epoch 6/10, Batch 13/111, Training Loss: 0.5968
Epoch 6/10, Batch 14/111, Training Loss: 0.5699
Epoch 6/10, Batch 15/111, Training Loss: 0.6158
Epoch 6/10, Batch 16/111, Training Loss: 0.6867
Epoch 6/10, Batch 17/111, Training Loss: 0.6932
Epoch 6/10, Batch 18/111, Training Loss: 0.7739
Epoch 6/10, Batch 19/111, Training Loss: 0.6167
Epoch 6/10, Batch 20/111, Training Loss: 0.6582
Epoch 6/10, Batch 21/111, Training Loss: 0.5958
Epoch 6/10, Batch 22/111, Training Loss: 0.5946
Epoch 6/10, Batch 23/111, Training Loss: 0.6035
Epoch 6/10, Batch 24/111, Training Loss: 0.7145
Epoch 6/10, Batch 25/111, Training Loss: 0.6081
Epoch 6/10, Batch 26/111, Training Loss: 0.6212
Epoch 6/10, Batch 27/111, Training Loss: 0.6040
Epoch 6/10, Batch 28/111, Training Loss: 0.6270
Epoch 6/10, Batch 29/111, Training Loss: 0.7674
Epoch 6/10, Batch 30/111, Training Loss: 0.6327
Epoch 6/10, Batch 31/111, Training Loss: 0.6729
Epoch 6/10, Batch 32/111, Training Loss: 0.6611
Epoch 6/10, Batch 33/111, Training Loss: 0.5566
Epoch 6/10, Batch 34/111, Training Loss: 0.5789
Epoch 6/10, Batch 35/111, Training Loss: 0.5798
Epoch 6/10, Batch 36/111, Training Loss: 0.5937
Epoch 6/10, Batch 37/111, Training Loss: 0.6059
Epoch 6/10, Batch 38/111, Training Loss: 0.6582
Epoch 6/10, Batch 39/111, Training Loss: 0.5921
Epoch 6/10, Batch 40/111, Training Loss: 0.7393
Epoch 6/10, Batch 41/111, Training Loss: 0.6052
Epoch 6/10, Batch 42/111, Training Loss: 0.5073
Epoch 6/10, Batch 43/111, Training Loss: 0.5813
Epoch 6/10, Batch 44/111, Training Loss: 0.6111
Epoch 6/10, Batch 45/111, Training Loss: 0.7658
Epoch 6/10, Batch 46/111, Training Loss: 0.6003
Epoch 6/10, Batch 47/111, Training Loss: 0.6206
Epoch 6/10, Batch 48/111, Training Loss: 0.6520
Epoch 6/10, Batch 49/111, Training Loss: 0.6495
Epoch 6/10, Batch 50/111, Training Loss: 0.6221
Epoch 6/10, Batch 51/111, Training Loss: 0.6498
Epoch 6/10, Batch 52/111, Training Loss: 0.6702
Epoch 6/10, Batch 53/111, Training Loss: 0.7137
Epoch 6/10, Batch 54/111, Training Loss: 0.6701
Epoch 6/10, Batch 55/111, Training Loss: 0.6190
Epoch 6/10, Batch 56/111, Training Loss: 0.5362
Epoch 6/10, Batch 57/111, Training Loss: 0.5825
Epoch 6/10, Batch 58/111, Training Loss: 0.5042
Epoch 6/10, Batch 59/111, Training Loss: 0.6778
Epoch 6/10, Batch 60/111, Training Loss: 0.5966
Epoch 6/10, Batch 61/111, Training Loss: 0.6497
Epoch 6/10, Batch 62/111, Training Loss: 0.5926
Epoch 6/10, Batch 63/111, Training Loss: 0.6591
Epoch 6/10, Batch 64/111, Training Loss: 0.6190
Epoch 6/10, Batch 65/111, Training Loss: 0.5952
Epoch 6/10, Batch 66/111, Training Loss: 0.5966
Epoch 6/10, Batch 67/111, Training Loss: 0.5744
Epoch 6/10, Batch 68/111, Training Loss: 0.6450
Epoch 6/10, Batch 69/111, Training Loss: 0.6608
Epoch 6/10, Batch 70/111, Training Loss: 0.6428
Epoch 6/10, Batch 71/111, Training Loss: 0.6269
Epoch 6/10, Batch 72/111, Training Loss: 0.7190
Epoch 6/10, Batch 73/111, Training Loss: 0.5849
Epoch 6/10, Batch 74/111, Training Loss: 0.6787
Epoch 6/10, Batch 75/111, Training Loss: 0.6203
Epoch 6/10, Batch 76/111, Training Loss: 0.6379
Epoch 6/10, Batch 77/111, Training Loss: 0.5250
Epoch 6/10, Batch 78/111, Training Loss: 0.6588
Epoch 6/10, Batch 79/111, Training Loss: 0.5650
Epoch 6/10, Batch 80/111, Training Loss: 0.6361
Epoch 6/10, Batch 81/111, Training Loss: 0.6520
Epoch 6/10, Batch 82/111, Training Loss: 0.5617
Epoch 6/10, Batch 83/111, Training Loss: 0.6107
Epoch 6/10, Batch 84/111, Training Loss: 0.6447
Epoch 6/10, Batch 85/111, Training Loss: 0.5944
Epoch 6/10, Batch 86/111, Training Loss: 0.5723
Epoch 6/10, Batch 87/111, Training Loss: 0.6482
Epoch 6/10, Batch 88/111, Training Loss: 0.5862
Epoch 6/10, Batch 89/111, Training Loss: 0.6503
Epoch 6/10, Batch 90/111, Training Loss: 0.5131
Epoch 6/10, Batch 91/111, Training Loss: 0.6234
Epoch 6/10, Batch 92/111, Training Loss: 0.5938
Epoch 6/10, Batch 93/111, Training Loss: 0.6935
Epoch 6/10, Batch 94/111, Training Loss: 0.5943
Epoch 6/10, Batch 95/111, Training Loss: 0.6818
Epoch 6/10, Batch 96/111, Training Loss: 0.5438
Epoch 6/10, Batch 97/111, Training Loss: 0.5959
Epoch 6/10, Batch 98/111, Training Loss: 0.5772
Epoch 6/10, Batch 99/111, Training Loss: 0.5145
Epoch 6/10, Batch 100/111, Training Loss: 0.6296
Epoch 6/10, Batch 101/111, Training Loss: 0.7015
Epoch 6/10, Batch 102/111, Training Loss: 0.6799
Epoch 6/10, Batch 103/111, Training Loss: 0.6807
Epoch 6/10, Batch 104/111, Training Loss: 0.5288
Epoch 6/10, Batch 105/111, Training Loss: 0.6978
Epoch 6/10, Batch 106/111, Training Loss: 0.6948
Epoch 6/10, Batch 107/111, Training Loss: 0.5976
Epoch 6/10, Batch 108/111, Training Loss: 0.5951
Epoch 6/10, Batch 109/111, Training Loss: 0.5368
Epoch 6/10, Batch 110/111, Training Loss: 0.5270
Epoch 6/10, Batch 111/111, Training Loss: 0.5788
Epoch 6/10, Training Loss: 0.6298, Validation Loss: 0.6271, Validation Accuracy: 0.7125
Epoch 7/10, Batch 1/111, Training Loss: 0.7829
Epoch 7/10, Batch 2/111, Training Loss: 0.5792
Epoch 7/10, Batch 3/111, Training Loss: 0.5982
Epoch 7/10, Batch 4/111, Training Loss: 0.5064
Epoch 7/10, Batch 5/111, Training Loss: 0.6559
Epoch 7/10, Batch 6/111, Training Loss: 0.6081
Epoch 7/10, Batch 7/111, Training Loss: 0.5436
Epoch 7/10, Batch 8/111, Training Loss: 0.4958
Epoch 7/10, Batch 9/111, Training Loss: 0.6318
Epoch 7/10, Batch 10/111, Training Loss: 0.5730
Epoch 7/10, Batch 11/111, Training Loss: 0.6036
Epoch 7/10, Batch 12/111, Training Loss: 0.6094
Epoch 7/10, Batch 13/111, Training Loss: 0.6061
Epoch 7/10, Batch 14/111, Training Loss: 0.5646
Epoch 7/10, Batch 15/111, Training Loss: 0.6111
Epoch 7/10, Batch 16/111, Training Loss: 0.4736
Epoch 7/10, Batch 17/111, Training Loss: 0.5725
Epoch 7/10, Batch 18/111, Training Loss: 0.6353
Epoch 7/10, Batch 19/111, Training Loss: 0.5811
Epoch 7/10, Batch 20/111, Training Loss: 0.5394
Epoch 7/10, Batch 21/111, Training Loss: 0.5346
Epoch 7/10, Batch 22/111, Training Loss: 0.6231
Epoch 7/10, Batch 23/111, Training Loss: 0.6146
Epoch 7/10, Batch 24/111, Training Loss: 0.6260
Epoch 7/10, Batch 25/111, Training Loss: 0.6410
Epoch 7/10, Batch 26/111, Training Loss: 0.6701
Epoch 7/10, Batch 27/111, Training Loss: 0.5296
Epoch 7/10, Batch 28/111, Training Loss: 0.5956
Epoch 7/10, Batch 29/111, Training Loss: 0.5115
Epoch 7/10, Batch 30/111, Training Loss: 0.7288
Epoch 7/10, Batch 31/111, Training Loss: 0.5114
Epoch 7/10, Batch 32/111, Training Loss: 0.5722
Epoch 7/10, Batch 33/111, Training Loss: 0.7829
Epoch 7/10, Batch 34/111, Training Loss: 0.5561
Epoch 7/10, Batch 35/111, Training Loss: 0.7121
Epoch 7/10, Batch 36/111, Training Loss: 0.5177
Epoch 7/10, Batch 37/111, Training Loss: 0.5354
Epoch 7/10, Batch 38/111, Training Loss: 0.4955
Epoch 7/10, Batch 39/111, Training Loss: 0.6118
Epoch 7/10, Batch 40/111, Training Loss: 0.6293
Epoch 7/10, Batch 41/111, Training Loss: 0.6614
Epoch 7/10, Batch 42/111, Training Loss: 0.5196
Epoch 7/10, Batch 43/111, Training Loss: 0.5891
Epoch 7/10, Batch 44/111, Training Loss: 0.5675
Epoch 7/10, Batch 45/111, Training Loss: 0.5011
Epoch 7/10, Batch 46/111, Training Loss: 0.6238
Epoch 7/10, Batch 47/111, Training Loss: 0.6065
Epoch 7/10, Batch 48/111, Training Loss: 0.6876
Epoch 7/10, Batch 49/111, Training Loss: 0.4943
Epoch 7/10, Batch 50/111, Training Loss: 0.5150
Epoch 7/10, Batch 51/111, Training Loss: 0.6151
Epoch 7/10, Batch 52/111, Training Loss: 0.6117
Epoch 7/10, Batch 53/111, Training Loss: 0.5286
Epoch 7/10, Batch 54/111, Training Loss: 0.5147
Epoch 7/10, Batch 55/111, Training Loss: 0.6180
Epoch 7/10, Batch 56/111, Training Loss: 0.5687
Epoch 7/10, Batch 57/111, Training Loss: 0.6090
Epoch 7/10, Batch 58/111, Training Loss: 0.6453
Epoch 7/10, Batch 59/111, Training Loss: 0.5842
Epoch 7/10, Batch 60/111, Training Loss: 0.5665
Epoch 7/10, Batch 61/111, Training Loss: 0.5719
Epoch 7/10, Batch 62/111, Training Loss: 0.5925
Epoch 7/10, Batch 63/111, Training Loss: 0.6078
Epoch 7/10, Batch 64/111, Training Loss: 0.5579
Epoch 7/10, Batch 65/111, Training Loss: 0.5893
Epoch 7/10, Batch 66/111, Training Loss: 0.4935
Epoch 7/10, Batch 67/111, Training Loss: 0.5452
Epoch 7/10, Batch 68/111, Training Loss: 0.6023
Epoch 7/10, Batch 69/111, Training Loss: 0.7443
Epoch 7/10, Batch 70/111, Training Loss: 0.5434
Epoch 7/10, Batch 71/111, Training Loss: 0.4782
Epoch 7/10, Batch 72/111, Training Loss: 0.6197
Epoch 7/10, Batch 73/111, Training Loss: 0.4842
Epoch 7/10, Batch 74/111, Training Loss: 0.6012
Epoch 7/10, Batch 75/111, Training Loss: 0.6175
Epoch 7/10, Batch 76/111, Training Loss: 0.5663
Epoch 7/10, Batch 77/111, Training Loss: 0.5268
Epoch 7/10, Batch 78/111, Training Loss: 0.5693
Epoch 7/10, Batch 79/111, Training Loss: 0.5747
Epoch 7/10, Batch 80/111, Training Loss: 0.5770
Epoch 7/10, Batch 81/111, Training Loss: 0.5291
Epoch 7/10, Batch 82/111, Training Loss: 0.5624
Epoch 7/10, Batch 83/111, Training Loss: 0.5109
Epoch 7/10, Batch 84/111, Training Loss: 0.5474
Epoch 7/10, Batch 85/111, Training Loss: 0.6893
Epoch 7/10, Batch 86/111, Training Loss: 0.5463
Epoch 7/10, Batch 87/111, Training Loss: 0.5451
Epoch 7/10, Batch 88/111, Training Loss: 0.5992
Epoch 7/10, Batch 89/111, Training Loss: 0.5840
Epoch 7/10, Batch 90/111, Training Loss: 0.7361
Epoch 7/10, Batch 91/111, Training Loss: 0.5596
Epoch 7/10, Batch 92/111, Training Loss: 0.5768
Epoch 7/10, Batch 93/111, Training Loss: 0.6553
Epoch 7/10, Batch 94/111, Training Loss: 0.5996
Epoch 7/10, Batch 95/111, Training Loss: 0.6709
Epoch 7/10, Batch 96/111, Training Loss: 0.6109
Epoch 7/10, Batch 97/111, Training Loss: 0.4578
Epoch 7/10, Batch 98/111, Training Loss: 0.6301
Epoch 7/10, Batch 99/111, Training Loss: 0.5289
Epoch 7/10, Batch 100/111, Training Loss: 0.5835
Epoch 7/10, Batch 101/111, Training Loss: 0.5638
Epoch 7/10, Batch 102/111, Training Loss: 0.5935
Epoch 7/10, Batch 103/111, Training Loss: 0.4431
Epoch 7/10, Batch 104/111, Training Loss: 0.5797
Epoch 7/10, Batch 105/111, Training Loss: 0.5996
Epoch 7/10, Batch 106/111, Training Loss: 0.5702
Epoch 7/10, Batch 107/111, Training Loss: 0.5433
Epoch 7/10, Batch 108/111, Training Loss: 0.6330
Epoch 7/10, Batch 109/111, Training Loss: 0.5076
Epoch 7/10, Batch 110/111, Training Loss: 0.5255
Epoch 7/10, Batch 111/111, Training Loss: 0.6145
Epoch 7/10, Training Loss: 0.5834, Validation Loss: 0.5985, Validation Accuracy: 0.7351
Epoch 8/10, Batch 1/111, Training Loss: 0.5137
Epoch 8/10, Batch 2/111, Training Loss: 0.5267
Epoch 8/10, Batch 3/111, Training Loss: 0.4727
Epoch 8/10, Batch 4/111, Training Loss: 0.5008
Epoch 8/10, Batch 5/111, Training Loss: 0.5098
Epoch 8/10, Batch 6/111, Training Loss: 0.5336
Epoch 8/10, Batch 7/111, Training Loss: 0.5662
Epoch 8/10, Batch 8/111, Training Loss: 0.4843
Epoch 8/10, Batch 9/111, Training Loss: 0.5892
Epoch 8/10, Batch 10/111, Training Loss: 0.4634
Epoch 8/10, Batch 11/111, Training Loss: 0.5048
Epoch 8/10, Batch 12/111, Training Loss: 0.5640
Epoch 8/10, Batch 13/111, Training Loss: 0.6607
Epoch 8/10, Batch 14/111, Training Loss: 0.6435
Epoch 8/10, Batch 15/111, Training Loss: 0.5782
Epoch 8/10, Batch 16/111, Training Loss: 0.6266
Epoch 8/10, Batch 17/111, Training Loss: 0.5480
Epoch 8/10, Batch 18/111, Training Loss: 0.5352
Epoch 8/10, Batch 19/111, Training Loss: 0.6043
Epoch 8/10, Batch 20/111, Training Loss: 0.5101
Epoch 8/10, Batch 21/111, Training Loss: 0.5830
Epoch 8/10, Batch 22/111, Training Loss: 0.5916
Epoch 8/10, Batch 23/111, Training Loss: 0.6598
Epoch 8/10, Batch 24/111, Training Loss: 0.6025
Epoch 8/10, Batch 25/111, Training Loss: 0.5458
Epoch 8/10, Batch 26/111, Training Loss: 0.5595
Epoch 8/10, Batch 27/111, Training Loss: 0.5456
Epoch 8/10, Batch 28/111, Training Loss: 0.4805
Epoch 8/10, Batch 29/111, Training Loss: 0.7247
Epoch 8/10, Batch 30/111, Training Loss: 0.6314
Epoch 8/10, Batch 31/111, Training Loss: 0.6357
Epoch 8/10, Batch 32/111, Training Loss: 0.6646
Epoch 8/10, Batch 33/111, Training Loss: 0.5217
Epoch 8/10, Batch 34/111, Training Loss: 0.5974
Epoch 8/10, Batch 35/111, Training Loss: 0.5836
Epoch 8/10, Batch 36/111, Training Loss: 0.5341
Epoch 8/10, Batch 37/111, Training Loss: 0.5533
Epoch 8/10, Batch 38/111, Training Loss: 0.6043
Epoch 8/10, Batch 39/111, Training Loss: 0.5972
Epoch 8/10, Batch 40/111, Training Loss: 0.5602
Epoch 8/10, Batch 41/111, Training Loss: 0.5859
Epoch 8/10, Batch 42/111, Training Loss: 0.6229
Epoch 8/10, Batch 43/111, Training Loss: 0.5575
Epoch 8/10, Batch 44/111, Training Loss: 0.5921
Epoch 8/10, Batch 45/111, Training Loss: 0.6104
Epoch 8/10, Batch 46/111, Training Loss: 0.6873
Epoch 8/10, Batch 47/111, Training Loss: 0.4850
Epoch 8/10, Batch 48/111, Training Loss: 0.5451
Epoch 8/10, Batch 49/111, Training Loss: 0.6234
Epoch 8/10, Batch 50/111, Training Loss: 0.5142
Epoch 8/10, Batch 51/111, Training Loss: 0.5671
Epoch 8/10, Batch 52/111, Training Loss: 0.4933
Epoch 8/10, Batch 53/111, Training Loss: 0.4845
Epoch 8/10, Batch 54/111, Training Loss: 0.5513
Epoch 8/10, Batch 55/111, Training Loss: 0.5959
Epoch 8/10, Batch 56/111, Training Loss: 0.5574
Epoch 8/10, Batch 57/111, Training Loss: 0.4495
Epoch 8/10, Batch 58/111, Training Loss: 0.5148
Epoch 8/10, Batch 59/111, Training Loss: 0.5542
Epoch 8/10, Batch 60/111, Training Loss: 0.4264
Epoch 8/10, Batch 61/111, Training Loss: 0.6598
Epoch 8/10, Batch 62/111, Training Loss: 0.5510
Epoch 8/10, Batch 63/111, Training Loss: 0.5185
Epoch 8/10, Batch 64/111, Training Loss: 0.4453
Epoch 8/10, Batch 65/111, Training Loss: 0.5696
Epoch 8/10, Batch 66/111, Training Loss: 0.4825
Epoch 8/10, Batch 67/111, Training Loss: 0.5290
Epoch 8/10, Batch 68/111, Training Loss: 0.5136
Epoch 8/10, Batch 69/111, Training Loss: 0.6707
Epoch 8/10, Batch 70/111, Training Loss: 0.5499
Epoch 8/10, Batch 71/111, Training Loss: 0.4892
Epoch 8/10, Batch 72/111, Training Loss: 0.4774
Epoch 8/10, Batch 73/111, Training Loss: 0.5236
Epoch 8/10, Batch 74/111, Training Loss: 0.4763
Epoch 8/10, Batch 75/111, Training Loss: 0.5102
Epoch 8/10, Batch 76/111, Training Loss: 0.4803
Epoch 8/10, Batch 77/111, Training Loss: 0.5219
Epoch 8/10, Batch 78/111, Training Loss: 0.5127
Epoch 8/10, Batch 79/111, Training Loss: 0.6435
Epoch 8/10, Batch 80/111, Training Loss: 0.5972
Epoch 8/10, Batch 81/111, Training Loss: 0.6231
Epoch 8/10, Batch 82/111, Training Loss: 0.5493
Epoch 8/10, Batch 83/111, Training Loss: 0.6200
Epoch 8/10, Batch 84/111, Training Loss: 0.6152
Epoch 8/10, Batch 85/111, Training Loss: 0.5831
Epoch 8/10, Batch 86/111, Training Loss: 0.5041
Epoch 8/10, Batch 87/111, Training Loss: 0.5669
Epoch 8/10, Batch 88/111, Training Loss: 0.4809
Epoch 8/10, Batch 89/111, Training Loss: 0.5481
Epoch 8/10, Batch 90/111, Training Loss: 0.5457
Epoch 8/10, Batch 91/111, Training Loss: 0.6565
Epoch 8/10, Batch 92/111, Training Loss: 0.5066
Epoch 8/10, Batch 93/111, Training Loss: 0.5128
Epoch 8/10, Batch 94/111, Training Loss: 0.4628
Epoch 8/10, Batch 95/111, Training Loss: 0.5488
Epoch 8/10, Batch 96/111, Training Loss: 0.6168
Epoch 8/10, Batch 97/111, Training Loss: 0.5546
Epoch 8/10, Batch 98/111, Training Loss: 0.5720
Epoch 8/10, Batch 99/111, Training Loss: 0.5960
Epoch 8/10, Batch 100/111, Training Loss: 0.5461
Epoch 8/10, Batch 101/111, Training Loss: 0.5238
Epoch 8/10, Batch 102/111, Training Loss: 0.4691
Epoch 8/10, Batch 103/111, Training Loss: 0.5708
Epoch 8/10, Batch 104/111, Training Loss: 0.5414
Epoch 8/10, Batch 105/111, Training Loss: 0.5793
Epoch 8/10, Batch 106/111, Training Loss: 0.5235
Epoch 8/10, Batch 107/111, Training Loss: 0.5570
Epoch 8/10, Batch 108/111, Training Loss: 0.5036
Epoch 8/10, Batch 109/111, Training Loss: 0.6468
Epoch 8/10, Batch 110/111, Training Loss: 0.5074
Epoch 8/10, Batch 111/111, Training Loss: 0.7074
Epoch 8/10, Training Loss: 0.5567, Validation Loss: 0.5784, Validation Accuracy: 0.7486
Epoch 9/10, Batch 1/111, Training Loss: 0.5634
Epoch 9/10, Batch 2/111, Training Loss: 0.5236
Epoch 9/10, Batch 3/111, Training Loss: 0.5616
Epoch 9/10, Batch 4/111, Training Loss: 0.6063
Epoch 9/10, Batch 5/111, Training Loss: 0.4329
Epoch 9/10, Batch 6/111, Training Loss: 0.5333
Epoch 9/10, Batch 7/111, Training Loss: 0.4992
Epoch 9/10, Batch 8/111, Training Loss: 0.5806
Epoch 9/10, Batch 9/111, Training Loss: 0.5575
Epoch 9/10, Batch 10/111, Training Loss: 0.4443
Epoch 9/10, Batch 11/111, Training Loss: 0.5137
Epoch 9/10, Batch 12/111, Training Loss: 0.5167
Epoch 9/10, Batch 13/111, Training Loss: 0.5113
Epoch 9/10, Batch 14/111, Training Loss: 0.4998
Epoch 9/10, Batch 15/111, Training Loss: 0.5123
Epoch 9/10, Batch 16/111, Training Loss: 0.4892
Epoch 9/10, Batch 17/111, Training Loss: 0.5312
Epoch 9/10, Batch 18/111, Training Loss: 0.4932
Epoch 9/10, Batch 19/111, Training Loss: 0.5373
Epoch 9/10, Batch 20/111, Training Loss: 0.4601
Epoch 9/10, Batch 21/111, Training Loss: 0.3980
Epoch 9/10, Batch 22/111, Training Loss: 0.5164
Epoch 9/10, Batch 23/111, Training Loss: 0.6405
Epoch 9/10, Batch 24/111, Training Loss: 0.5375
Epoch 9/10, Batch 25/111, Training Loss: 0.4496
Epoch 9/10, Batch 26/111, Training Loss: 0.5361
Epoch 9/10, Batch 27/111, Training Loss: 0.4796
Epoch 9/10, Batch 28/111, Training Loss: 0.4271
Epoch 9/10, Batch 29/111, Training Loss: 0.4660
Epoch 9/10, Batch 30/111, Training Loss: 0.4259
Epoch 9/10, Batch 31/111, Training Loss: 0.4872
Epoch 9/10, Batch 32/111, Training Loss: 0.4333
Epoch 9/10, Batch 33/111, Training Loss: 0.6786
Epoch 9/10, Batch 34/111, Training Loss: 0.5850
Epoch 9/10, Batch 35/111, Training Loss: 0.5899
Epoch 9/10, Batch 36/111, Training Loss: 0.5587
Epoch 9/10, Batch 37/111, Training Loss: 0.5837
Epoch 9/10, Batch 38/111, Training Loss: 0.5201
Epoch 9/10, Batch 39/111, Training Loss: 0.5433
Epoch 9/10, Batch 40/111, Training Loss: 0.6176
Epoch 9/10, Batch 41/111, Training Loss: 0.6465
Epoch 9/10, Batch 42/111, Training Loss: 0.5061
Epoch 9/10, Batch 43/111, Training Loss: 0.5067
Epoch 9/10, Batch 44/111, Training Loss: 0.5442
Epoch 9/10, Batch 45/111, Training Loss: 0.5223
Epoch 9/10, Batch 46/111, Training Loss: 0.5894
Epoch 9/10, Batch 47/111, Training Loss: 0.6622
Epoch 9/10, Batch 48/111, Training Loss: 0.6077
Epoch 9/10, Batch 49/111, Training Loss: 0.5240
Epoch 9/10, Batch 50/111, Training Loss: 0.4417
Epoch 9/10, Batch 51/111, Training Loss: 0.5881
Epoch 9/10, Batch 52/111, Training Loss: 0.5815
Epoch 9/10, Batch 53/111, Training Loss: 0.5947
Epoch 9/10, Batch 54/111, Training Loss: 0.5489
Epoch 9/10, Batch 55/111, Training Loss: 0.5555
Epoch 9/10, Batch 56/111, Training Loss: 0.4493
Epoch 9/10, Batch 57/111, Training Loss: 0.5341
Epoch 9/10, Batch 58/111, Training Loss: 0.5321
Epoch 9/10, Batch 59/111, Training Loss: 0.6624
Epoch 9/10, Batch 60/111, Training Loss: 0.4626
Epoch 9/10, Batch 61/111, Training Loss: 0.4645
Epoch 9/10, Batch 62/111, Training Loss: 0.5773
Epoch 9/10, Batch 63/111, Training Loss: 0.5581
Epoch 9/10, Batch 64/111, Training Loss: 0.4491
Epoch 9/10, Batch 65/111, Training Loss: 0.4808
Epoch 9/10, Batch 66/111, Training Loss: 0.6263
Epoch 9/10, Batch 67/111, Training Loss: 0.4597
Epoch 9/10, Batch 68/111, Training Loss: 0.4570
Epoch 9/10, Batch 69/111, Training Loss: 0.5014
Epoch 9/10, Batch 70/111, Training Loss: 0.5268
Epoch 9/10, Batch 71/111, Training Loss: 0.4914
Epoch 9/10, Batch 72/111, Training Loss: 0.4327
Epoch 9/10, Batch 73/111, Training Loss: 0.4571
Epoch 9/10, Batch 74/111, Training Loss: 0.4834
Epoch 9/10, Batch 75/111, Training Loss: 0.4986
Epoch 9/10, Batch 76/111, Training Loss: 0.4954
Epoch 9/10, Batch 77/111, Training Loss: 0.4286
Epoch 9/10, Batch 78/111, Training Loss: 0.6375
Epoch 9/10, Batch 79/111, Training Loss: 0.5642
Epoch 9/10, Batch 80/111, Training Loss: 0.4806
Epoch 9/10, Batch 81/111, Training Loss: 0.4849
Epoch 9/10, Batch 82/111, Training Loss: 0.5794
Epoch 9/10, Batch 83/111, Training Loss: 0.4239
Epoch 9/10, Batch 84/111, Training Loss: 0.6137
Epoch 9/10, Batch 85/111, Training Loss: 0.5387
Epoch 9/10, Batch 86/111, Training Loss: 0.5394
Epoch 9/10, Batch 87/111, Training Loss: 0.4022
Epoch 9/10, Batch 88/111, Training Loss: 0.5304
Epoch 9/10, Batch 89/111, Training Loss: 0.5766
Epoch 9/10, Batch 90/111, Training Loss: 0.4484
Epoch 9/10, Batch 91/111, Training Loss: 0.4370
Epoch 9/10, Batch 92/111, Training Loss: 0.4682
Epoch 9/10, Batch 93/111, Training Loss: 0.4538
Epoch 9/10, Batch 94/111, Training Loss: 0.4446
Epoch 9/10, Batch 95/111, Training Loss: 0.4587
Epoch 9/10, Batch 96/111, Training Loss: 0.5015
Epoch 9/10, Batch 97/111, Training Loss: 0.5956
Epoch 9/10, Batch 98/111, Training Loss: 0.4809
Epoch 9/10, Batch 99/111, Training Loss: 0.4605
Epoch 9/10, Batch 100/111, Training Loss: 0.4313
Epoch 9/10, Batch 101/111, Training Loss: 0.5333
Epoch 9/10, Batch 102/111, Training Loss: 0.5652
Epoch 9/10, Batch 103/111, Training Loss: 0.5348
Epoch 9/10, Batch 104/111, Training Loss: 0.4495
Epoch 9/10, Batch 105/111, Training Loss: 0.4944
Epoch 9/10, Batch 106/111, Training Loss: 0.5299
Epoch 9/10, Batch 107/111, Training Loss: 0.5556
Epoch 9/10, Batch 108/111, Training Loss: 0.3928
Epoch 9/10, Batch 109/111, Training Loss: 0.3776
Epoch 9/10, Batch 110/111, Training Loss: 0.4321
Epoch 9/10, Batch 111/111, Training Loss: 0.5517
Epoch 9/10, Training Loss: 0.5158, Validation Loss: 0.5688, Validation Accuracy: 0.7648
Epoch 10/10, Batch 1/111, Training Loss: 0.5497
Epoch 10/10, Batch 2/111, Training Loss: 0.4870
Epoch 10/10, Batch 3/111, Training Loss: 0.4569
Epoch 10/10, Batch 4/111, Training Loss: 0.3657
Epoch 10/10, Batch 5/111, Training Loss: 0.3995
Epoch 10/10, Batch 6/111, Training Loss: 0.5642
Epoch 10/10, Batch 7/111, Training Loss: 0.4927
Epoch 10/10, Batch 8/111, Training Loss: 0.5563
Epoch 10/10, Batch 9/111, Training Loss: 0.4968
Epoch 10/10, Batch 10/111, Training Loss: 0.4634
Epoch 10/10, Batch 11/111, Training Loss: 0.5340
Epoch 10/10, Batch 12/111, Training Loss: 0.5181
Epoch 10/10, Batch 13/111, Training Loss: 0.4888
Epoch 10/10, Batch 14/111, Training Loss: 0.4598
Epoch 10/10, Batch 15/111, Training Loss: 0.4407
Epoch 10/10, Batch 16/111, Training Loss: 0.4563
Epoch 10/10, Batch 17/111, Training Loss: 0.4004
Epoch 10/10, Batch 18/111, Training Loss: 0.4586
Epoch 10/10, Batch 19/111, Training Loss: 0.4545
Epoch 10/10, Batch 20/111, Training Loss: 0.4997
Epoch 10/10, Batch 21/111, Training Loss: 0.4802
Epoch 10/10, Batch 22/111, Training Loss: 0.5157
Epoch 10/10, Batch 23/111, Training Loss: 0.4885
Epoch 10/10, Batch 24/111, Training Loss: 0.7627
Epoch 10/10, Batch 25/111, Training Loss: 0.5210
Epoch 10/10, Batch 26/111, Training Loss: 0.4303
Epoch 10/10, Batch 27/111, Training Loss: 0.4597
Epoch 10/10, Batch 28/111, Training Loss: 0.6066
Epoch 10/10, Batch 29/111, Training Loss: 0.4209
Epoch 10/10, Batch 30/111, Training Loss: 0.5035
Epoch 10/10, Batch 31/111, Training Loss: 0.5284
Epoch 10/10, Batch 32/111, Training Loss: 0.4176
Epoch 10/10, Batch 33/111, Training Loss: 0.4438
Epoch 10/10, Batch 34/111, Training Loss: 0.4514
Epoch 10/10, Batch 35/111, Training Loss: 0.5370
Epoch 10/10, Batch 36/111, Training Loss: 0.5742
Epoch 10/10, Batch 37/111, Training Loss: 0.5352
Epoch 10/10, Batch 38/111, Training Loss: 0.5225
Epoch 10/10, Batch 39/111, Training Loss: 0.5206
Epoch 10/10, Batch 40/111, Training Loss: 0.5006
Epoch 10/10, Batch 41/111, Training Loss: 0.5097
Epoch 10/10, Batch 42/111, Training Loss: 0.4256
Epoch 10/10, Batch 43/111, Training Loss: 0.4724
Epoch 10/10, Batch 44/111, Training Loss: 0.5052
Epoch 10/10, Batch 45/111, Training Loss: 0.5333
Epoch 10/10, Batch 46/111, Training Loss: 0.4040
Epoch 10/10, Batch 47/111, Training Loss: 0.4926
Epoch 10/10, Batch 48/111, Training Loss: 0.4458
Epoch 10/10, Batch 49/111, Training Loss: 0.5220
Epoch 10/10, Batch 50/111, Training Loss: 0.4424
Epoch 10/10, Batch 51/111, Training Loss: 0.4911
Epoch 10/10, Batch 52/111, Training Loss: 0.5253
Epoch 10/10, Batch 53/111, Training Loss: 0.4601
Epoch 10/10, Batch 54/111, Training Loss: 0.5515
Epoch 10/10, Batch 55/111, Training Loss: 0.5394
Epoch 10/10, Batch 56/111, Training Loss: 0.5424
Epoch 10/10, Batch 57/111, Training Loss: 0.4091
Epoch 10/10, Batch 58/111, Training Loss: 0.3774
Epoch 10/10, Batch 59/111, Training Loss: 0.5205
Epoch 10/10, Batch 60/111, Training Loss: 0.3979
Epoch 10/10, Batch 61/111, Training Loss: 0.4158
Epoch 10/10, Batch 62/111, Training Loss: 0.5290
Epoch 10/10, Batch 63/111, Training Loss: 0.4104
Epoch 10/10, Batch 64/111, Training Loss: 0.3692
Epoch 10/10, Batch 65/111, Training Loss: 0.5282
Epoch 10/10, Batch 66/111, Training Loss: 0.4848
Epoch 10/10, Batch 67/111, Training Loss: 0.5311
Epoch 10/10, Batch 68/111, Training Loss: 0.5061
Epoch 10/10, Batch 69/111, Training Loss: 0.3496
Epoch 10/10, Batch 70/111, Training Loss: 0.5168
Epoch 10/10, Batch 71/111, Training Loss: 0.4581
Epoch 10/10, Batch 72/111, Training Loss: 0.4857
Epoch 10/10, Batch 73/111, Training Loss: 0.4839
Epoch 10/10, Batch 74/111, Training Loss: 0.4122
Epoch 10/10, Batch 75/111, Training Loss: 0.4853
Epoch 10/10, Batch 76/111, Training Loss: 0.5135
Epoch 10/10, Batch 77/111, Training Loss: 0.5454
Epoch 10/10, Batch 78/111, Training Loss: 0.4316
Epoch 10/10, Batch 79/111, Training Loss: 0.3862
Epoch 10/10, Batch 80/111, Training Loss: 0.5260
Epoch 10/10, Batch 81/111, Training Loss: 0.4756
Epoch 10/10, Batch 82/111, Training Loss: 0.5904
Epoch 10/10, Batch 83/111, Training Loss: 0.5659
Epoch 10/10, Batch 84/111, Training Loss: 0.5525
Epoch 10/10, Batch 85/111, Training Loss: 0.5032
Epoch 10/10, Batch 86/111, Training Loss: 0.4209
Epoch 10/10, Batch 87/111, Training Loss: 0.4095
Epoch 10/10, Batch 88/111, Training Loss: 0.5741
Epoch 10/10, Batch 89/111, Training Loss: 0.5651
Epoch 10/10, Batch 90/111, Training Loss: 0.5176
Epoch 10/10, Batch 91/111, Training Loss: 0.4925
Epoch 10/10, Batch 92/111, Training Loss: 0.5080
Epoch 10/10, Batch 93/111, Training Loss: 0.4554
Epoch 10/10, Batch 94/111, Training Loss: 0.5682
Epoch 10/10, Batch 95/111, Training Loss: 0.4487
Epoch 10/10, Batch 96/111, Training Loss: 0.4231
Epoch 10/10, Batch 97/111, Training Loss: 0.4950
Epoch 10/10, Batch 98/111, Training Loss: 0.4526
Epoch 10/10, Batch 99/111, Training Loss: 0.5349
Epoch 10/10, Batch 100/111, Training Loss: 0.4788
Epoch 10/10, Batch 101/111, Training Loss: 0.4924
Epoch 10/10, Batch 102/111, Training Loss: 0.4732
Epoch 10/10, Batch 103/111, Training Loss: 0.4143
Epoch 10/10, Batch 104/111, Training Loss: 0.4446
Epoch 10/10, Batch 105/111, Training Loss: 0.5093
Epoch 10/10, Batch 106/111, Training Loss: 0.4583
Epoch 10/10, Batch 107/111, Training Loss: 0.6429
Epoch 10/10, Batch 108/111, Training Loss: 0.4592
Epoch 10/10, Batch 109/111, Training Loss: 0.5036
Epoch 10/10, Batch 110/111, Training Loss: 0.4255
Epoch 10/10, Batch 111/111, Training Loss: 0.3224
Epoch 10/10, Training Loss: 0.4854, Validation Loss: 0.5119, Validation Accuracy: 0.7861
Test Loss: 0.5027, Test Accuracy: 0.7884
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>[I 2025-04-27 21:06:00,586] Trial 4 finished with value: 0.511853081882 and parameters: {'batch_size': 128, 'learning_rate': 0.007663800832372427, 'weight_decay': 9.880842173137917e-05}. Best is trial 0 with value: 0.2196991708036512.
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Best trial:
  Value (best validation loss): 0.2196991708036512
  Params: 
    batch_size: 128
    learning_rate: 0.0005523846634426587
    weight_decay: 7.827086112748106e-05
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
