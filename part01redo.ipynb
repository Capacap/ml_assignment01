{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) üßÆ Optional: En enda neuron\n",
    "\n",
    "- A1: Neuron-implementation: for-loopar och Python-listor (som p√• tavlan lektion 1), ***alternativt***\n",
    "- A2: Neuron-implementation: NumPy vektor-multiplikation internt i varje Neuron-objekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(inputs, weights, bias):\n",
    "    # Initialize output\n",
    "    output = 0\n",
    "\n",
    "    # Calculate output\n",
    "    for i, w in zip(inputs, weights):\n",
    "        output += i * w\n",
    "\n",
    "    # Add bias\n",
    "    output += bias\n",
    "\n",
    "    # Apply activation function (ReLU)\n",
    "    output = max(0, output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def neuron(inputs, weights, bias):\n",
    "    # Initialize output\n",
    "    output = 0\n",
    "\n",
    "    # Calculate output\n",
    "    output = np.dot(inputs, weights) + bias\n",
    "\n",
    "    # Apply activation function (ReLU)\n",
    "    output = np.maximum(0, output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) ‚úÖ¬†ANN-lager: NumPy version\n",
    "\n",
    "Det betyder att vi nu inte l√§ngre beh√∂ver n√•gon klass Neuron, eftersom vi kommer ber√§kna ett helt lager som en enda stor matris-multiplikation:\n",
    "\n",
    "- Alla input till ett lager = NumPy-vektor\n",
    "- Alla vikter f√∂r alla neuroner i ett lager = en NumPy-matris\n",
    "- Observera att vi inte kommer att tr√§na n√§tverket som √§r implementerat som en NumPy-ber√§kning - eftersom det blir mycket enklare i (C) n√§r vi √∂verg√•r till PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def layer(inputs, weights, bias):\n",
    "    # Calculate neuron outputs\n",
    "    outputs = np.dot(weights, inputs) + bias\n",
    "\n",
    "    # Apply ReLU to outputs\n",
    "    outputs = np.maximum(0, outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) ‚úÖ ANN-lager: PyTorch version:\n",
    "\n",
    "- Anv√§nd PyTorch 2.1 (eller b√§ttre). Anv√§nd helst Python 3.10 (eller b√§ttre).\n",
    "- Kopplas f√∂rst ihop alla lager i perceptronen s√• att du f√•r en PyTorch-modell (a.k.a. module). Denna definierar i detalj compute-grafen f√∂r din perceptron.\n",
    "- Anv√§nd d√§refter din perceptron via PyTorch. Googla sj√§lv f√∂r att f√• information om hur detta g√•r till rent praktiskt. Det finns gott om information p√• webben kring PyTorch!\n",
    "- I denna version ska √§ven tr√§ning av n√§tverket ske, d.v.s. vi ska loopa √∂ver epochs, och applicera back-prop. En vidareutveckling av back-prop som kallas ADAM brukar anv√§ndas eftersom den √§r b√•de snabb och inte lika ofta fastnar i d√•liga lokala minima, j√§mf√∂rt med ren back-prop.\n",
    "- Se avsnittet ‚ÄúTips f√∂r (C)‚Äù nedan.\n",
    "\n",
    "## (D) ‚úÖ Samma som (C), men exekverad p√• en CUDA GPU\n",
    "\n",
    "- GPU:n beh√∂ver st√∂da CUDA v11.6 eller h√∂gre, vilket motsvarar en GPU fr√•n NVIDIA‚Äôs Pascal-generation eller senare (Exempel p√• Pascal-kort: GeForce GTX-1080, Quadro P5000, Tesla P100). (Senare generationer: Volta, Turing, Amp√®re, Ada, Hopper, Blackwell).\n",
    "- Google Colab har billiga/gratis notebook-instanser med NVIDIA T4 GPU, vilket √§r en enkel type av Turing-GPU. Denna fungerar utm√§rkt f√∂r uppgiften, men har du en modern NVIDIA-GPU i din dator √§r den troligen snabbare √§n en T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 17:16:13,418 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:13,418 - INFO - Run ID: 20250423_171613\n",
      "2025-04-23 17:16:13,419 - INFO - Training configuration:\n",
      "2025-04-23 17:16:13,419 - INFO - Learning rate: 0.001\n",
      "2025-04-23 17:16:13,419 - INFO - Batch size: 64\n",
      "2025-04-23 17:16:13,420 - INFO - Epochs: 10\n",
      "2025-04-23 17:16:13,420 - INFO - Optimizer: Adam\n",
      "2025-04-23 17:16:13,420 - INFO - Loss function: CrossEntropyLoss\n",
      "2025-04-23 17:16:17,435 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:17,435 - INFO - Epoch [1/10]\n",
      "2025-04-23 17:16:17,436 - INFO - Training Loss: 0.2505\n",
      "2025-04-23 17:16:18,299 - INFO - Validation Loss: 0.1451\n",
      "2025-04-23 17:16:18,299 - INFO - Validation Accuracy: 95.80%\n",
      "2025-04-23 17:16:22,316 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:22,317 - INFO - Epoch [2/10]\n",
      "2025-04-23 17:16:22,317 - INFO - Training Loss: 0.0942\n",
      "2025-04-23 17:16:23,173 - INFO - Validation Loss: 0.1044\n",
      "2025-04-23 17:16:23,174 - INFO - Validation Accuracy: 96.70%\n",
      "2025-04-23 17:16:27,259 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:27,260 - INFO - Epoch [3/10]\n",
      "2025-04-23 17:16:27,260 - INFO - Training Loss: 0.0606\n",
      "2025-04-23 17:16:28,116 - INFO - Validation Loss: 0.0982\n",
      "2025-04-23 17:16:28,116 - INFO - Validation Accuracy: 97.21%\n",
      "2025-04-23 17:16:32,121 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:32,121 - INFO - Epoch [4/10]\n",
      "2025-04-23 17:16:32,122 - INFO - Training Loss: 0.0466\n",
      "2025-04-23 17:16:32,988 - INFO - Validation Loss: 0.0913\n",
      "2025-04-23 17:16:32,989 - INFO - Validation Accuracy: 97.33%\n",
      "2025-04-23 17:16:37,022 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:37,023 - INFO - Epoch [5/10]\n",
      "2025-04-23 17:16:37,023 - INFO - Training Loss: 0.0325\n",
      "2025-04-23 17:16:37,879 - INFO - Validation Loss: 0.0863\n",
      "2025-04-23 17:16:37,879 - INFO - Validation Accuracy: 97.57%\n",
      "2025-04-23 17:16:41,875 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:41,876 - INFO - Epoch [6/10]\n",
      "2025-04-23 17:16:41,876 - INFO - Training Loss: 0.0293\n",
      "2025-04-23 17:16:42,754 - INFO - Validation Loss: 0.1105\n",
      "2025-04-23 17:16:42,754 - INFO - Validation Accuracy: 97.28%\n",
      "2025-04-23 17:16:46,778 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:46,778 - INFO - Epoch [7/10]\n",
      "2025-04-23 17:16:46,779 - INFO - Training Loss: 0.0221\n",
      "2025-04-23 17:16:47,667 - INFO - Validation Loss: 0.0917\n",
      "2025-04-23 17:16:47,668 - INFO - Validation Accuracy: 97.83%\n",
      "2025-04-23 17:16:51,608 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:51,609 - INFO - Epoch [8/10]\n",
      "2025-04-23 17:16:51,609 - INFO - Training Loss: 0.0209\n",
      "2025-04-23 17:16:52,458 - INFO - Validation Loss: 0.1058\n",
      "2025-04-23 17:16:52,459 - INFO - Validation Accuracy: 97.48%\n",
      "2025-04-23 17:16:56,391 - INFO - ====================================================================================================\n",
      "2025-04-23 17:16:56,392 - INFO - Epoch [9/10]\n",
      "2025-04-23 17:16:56,392 - INFO - Training Loss: 0.0176\n",
      "2025-04-23 17:16:57,252 - INFO - Validation Loss: 0.1024\n",
      "2025-04-23 17:16:57,252 - INFO - Validation Accuracy: 97.84%\n",
      "2025-04-23 17:17:01,196 - INFO - ====================================================================================================\n",
      "2025-04-23 17:17:01,196 - INFO - Epoch [10/10]\n",
      "2025-04-23 17:17:01,197 - INFO - Training Loss: 0.0155\n",
      "2025-04-23 17:17:02,056 - INFO - Validation Loss: 0.0911\n",
      "2025-04-23 17:17:02,057 - INFO - Validation Accuracy: 97.94%\n",
      "2025-04-23 17:17:02,775 - INFO - ====================================================================================================\n",
      "2025-04-23 17:17:02,776 - INFO - Best Model: models/run_20250423_171613/checkpoints/checkpoint_epoch_5.pth\n",
      "2025-04-23 17:17:02,776 - INFO - Test Loss: 0.0832\n",
      "2025-04-23 17:17:02,776 - INFO - Test Accuracy: 97.66%\n",
      "2025-04-23 17:17:02,777 - INFO - ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the perceptron neural-network model\n",
    "class Perceptron(nn.Module):\n",
    "    # Define the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Flatten the input\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the layers with ReLU activation function\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # Input layer   \n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Hidden layer\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Output layer\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass through the layers\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Select device to run on\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize the model\n",
    "model = Perceptron().to(device)\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "training_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testing_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Split training data into train and validation subsets\n",
    "training_subset_size = int(0.8 * len(training_dataset))\n",
    "validation_subset_size = len(training_dataset) - training_subset_size\n",
    "training_subset, validation_subset = random_split(training_dataset, [training_subset_size, validation_subset_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(training_subset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
    "testing_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create a unique id and directory for the run\n",
    "checkpoint_filename_prefix = 'checkpoint'\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = os.path.join('models', f'run_{run_id}')\n",
    "checkpoints_dir = os.path.join(run_dir, 'checkpoints')\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "log_file = os.path.join(run_dir, f'run_{run_id}_training.log')\n",
    "fhandler = logging.FileHandler(filename=log_file, mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "\n",
    "# Log hyperparameters\n",
    "logger.info(\"=\" * 100)\n",
    "logger.info(f\"Run ID: {run_id}\")\n",
    "logger.info(f\"Training configuration:\")\n",
    "logger.info(f\"Learning rate: {learning_rate}\")\n",
    "logger.info(f\"Batch size: {batch_size}\")\n",
    "logger.info(f\"Epochs: {num_epochs}\")\n",
    "logger.info(f\"Optimizer: Adam\")\n",
    "logger.info(f\"Loss function: CrossEntropyLoss\")\n",
    "\n",
    "# Training and validation loop\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = None\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        # Move data to device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    # Print training loss\n",
    "    logger.info(\"=\"*100)\n",
    "    logger.info(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    logger.info(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in validation_loader:\n",
    "            # Move data to device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_val_loss = running_val_loss / len(validation_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    # Print validation loss and accuracy\n",
    "    logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    logger.info(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save the checkpoint\n",
    "    checkpoint_filename = f'{checkpoint_filename_prefix}_epoch_{epoch+1}.pth'\n",
    "    checkpoint_path = os.path.join(checkpoints_dir, checkpoint_filename)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    # Update the best model if the current model has a lower validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_model_path = checkpoint_path # Cache the path to the best model for later testing\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "# Get the best model for testing\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "running_test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in testing_loader:\n",
    "        # Move data to device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        running_test_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "avg_test_loss = running_test_loss / len(testing_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "# Print test loss and accuracy\n",
    "logger.info(\"=\"*100)\n",
    "logger.info(f\"Best Model: {best_model_path}\")\n",
    "logger.info(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "logger.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "logger.info(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
