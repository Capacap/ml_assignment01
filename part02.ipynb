{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kravspecifikation f√∂r del 2\n",
    "\n",
    "1. ‚úÖ¬†B√∂rja anv√§nda basala **MLOps best-practices:**\n",
    "    1. üìä¬†Ordna s√• att du har ett bra s√§tt att √∂vervaka tr√§ningen, samt att spara (och h√•lla ordning p√•!) versioner av modellerna du tr√§nar upp. (Tips: Foldrar, smart valda filnamn, etc).\n",
    "    2. üíæ Kom ih√•g att det troligen inte √§r den sista epokens checkpoint (sparad version) som √§r den b√§sta, p.g.a. overfitting om du k√∂r m√•nga epochs. Du beh√∂ver sj√§lv se till att spara checkpoints lite d√• och d√• under tr√§ningen.\n",
    "    3. üóÑÔ∏è¬†Versionshantering av parametrar, kod, och checkpoints (modellens parametrar) blir snabbt v√§ldigt viktigt n√§r man b√∂rjar jobba seri√∂st med machine learning. Om man inte h√•ller reda p√• vilken k√∂rning som gjorts med vilka parametrar, s√• blir det sv√•rt att j√§mf√∂ra prestanda, vidareutveckla modellen senare, etc. Ingen vill ha en modell levererad d√§r programmeraren inte minns hur den gjordes.\n",
    "    4. üöÄ **F√∂rdjupning/√∂verkurs:** Vilka verktyg finns som kan hj√§lpa till att h√•lla ordning p√• versioner, k√∂rningar, resultat, hyper parameters, etc?\n",
    "2. üöÄ **F√∂rdjupning/√∂verkurs:** Addera **performance metrics**, s√• att du kan se hur l√•ng tid respektive tr√§nings-k√∂rning tar. (En k√∂rning = Alla epochs av tr√§ning f√∂r en given modell med specificerad upps√§ttning hyper parameters).\n",
    "3. ‚úÖ¬†Applicera l√§mpliga **data augmentation** methods f√∂r att artificiellt variera, och till och med ‚Äúf√∂rstora‚Äù datam√§ngden lite. Anv√§nd exempelvis skalning, rotation, f√§rgvariation, brus, (spegelv√§ndning?), etc. H√§r √§r lite repetition om data augmentation:\n",
    "    1. Se avsnittet ‚Äú[Uppgift 1 Perceptron foÃàr OCR](https://www.notion.so/Uppgift-1-Perceptron-fo-r-OCR-d9e945f1551341e88665775acacc0bb6?pvs=21)‚Äù l√§ngre ner i denna uppgift (del 3).\n",
    "    2. Bra 10-minuters video om √§mnet: [Pytorch-Data-Augmentation-using-Torchvision](https://youtu.be/Zvd276j9sZ8?si=9fGaNnoRzsHrvxB6)\n",
    "    3. Observera att det inte √§r s√§kert att data augmentation leder till b√§ttre resultat i alla l√§gen, eftersom det beror p√• detaljerna i vad man bygger, och vilken data man har.\n",
    "4. ‚úÖ¬†Det √§r nu dags att testa CNN ist√§llet f√∂r FFN. Byt ut de f√∂rsta lagren i modellen till **convolutional layers** f√∂r att ta upp translationsinvarians i bilden. S√∂k sj√§lv upp vad man brukar ha ***direkt efter varje convolutional layer*** f√∂r att h√•lla ordning p√• antalet dimensioner till n√§sta lager .\n",
    "    \n",
    "    > (Sv√•rt ord ‚Äútranslations-invarians‚Äù: translation=f√∂rflyttning, invarians=‚Äùsamma oavsett‚Äù ‚áí translationsinvarians = ‚Äùspelar ingen roll var i bilden‚Äù)\n",
    "    > \n",
    "5. ‚úÖ¬†Experimentera d√§refter med att addera ett till (eller flera) convolutional layer(s). Tanken √§r att testa **olika n√§t-arkitekturer**. Anv√§nd Google/Chat f√∂r initial gissning f√∂r modell-arkitektur, men sedan √§r det trial-and-error som g√§ller. Att fundera p√•:\n",
    "    1. ‚öñÔ∏è Hur p√•verkas resultatet av olika modell-arkitektur? Var noga med att inte j√§mf√∂ra modeller med stor skillnad i antal parameters, om aspekter som tex lagertyp ska utv√§rderas.\n",
    "    2. ü§î Vilken typ av features detekteras typiskt av de senare convolutional-lagren j√§mf√∂rt med det f√∂rsta convolutional-lagret?\n",
    "        1. Hint: Videon fr√•n lektion om convolutional layers: [Convolutional-Neural-Networks-Explained](https://youtu.be/pj9-rr1wDhM?si=cjWmR5ets048WjfZ). (Sj√§lvfallet kan du ocks√• plotta weight-matrices fr√•n din egen modell ocks√• om du vill!)\n",
    "6. ‚úÖ¬†Applicera l√§mpliga **regularization methods** f√∂r att minimera risken f√∂r overfitting och g√∂ra cost-function-landskapet f√∂rdelaktigt f√∂r back-prop.\n",
    "    1. Exempel p√• s√•dana metoder √§r: drop-out, weight-decay, noise injection, batch normalization, etc.\n",
    "    2. Mer info om regularization: [understanding-regularization-with-pytorch](https://medium.com/analytics-vidhya/understanding-regularization-with-pytorch-26a838d94058)\n",
    "7. üöÄ **F√∂rdjupning:** Skapa en lista av kombinationer av hyper parameters, och l√•t datorn tr√§na din modell p√• nytt f√∂r samtliga hyperparameter settings i din lista. Detta kallas f√∂r **hyper-parameter tuning**, d.v.s. att via trial-and-error hitta inst√§llningar som funkar.\n",
    "    - Enklast m√∂jliga hyper-param-tuning: G√∂r en lista med t.ex. 10 rader (en per k√∂rning), d√§r varje rad specificerar alla hyper-parameters som kan vara sv√•rt att gissa optimalt v√§rde p√•:\n",
    "        - Antal neuroner f√∂r respektive lager\n",
    "        - Typ av activation function\n",
    "        - Vilka lager som ska vara convolutional, storlek p√• convolutional kernel, antal convolutional kernels per lager, etc.\n",
    "        - Learning rate / hyper-params till optimizer ADAM\n",
    "        - Drop-out level\n",
    "        - Settings f√∂r olika typer av data augmentation (rotation, skalning, noise, etc)\n",
    "    - Det finns √§ven en m√§ngd automatiserade typer av hyper-param-tuning. Se exempelvis: https://en.wikipedia.org/wiki/Hyperparameter_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:09:47,943 - INFO - ====================================================================================================\n",
      "2025-04-27 21:09:47,944 - INFO - Starting new experiment with run ID: 20250427_210947\n",
      "2025-04-27 21:09:47,944 - INFO - ====================================================================================================\n",
      "2025-04-27 21:09:47,945 - INFO - Using device: cuda\n",
      "2025-04-27 21:09:48,009 - INFO - Loaded datasets - Training: 60000 samples, Testing: 10000 samples\n",
      "2025-04-27 21:09:48,010 - INFO - ====================================================================================================\n",
      "2025-04-27 21:09:48,010 - INFO - Starting training for model variant_1\n",
      "2025-04-27 21:09:48,010 - INFO - Hyperparameters:\n",
      "2025-04-27 21:09:48,011 - INFO - Learning rate: 0.0001\n",
      "2025-04-27 21:09:48,011 - INFO - Batch size: 128\n",
      "2025-04-27 21:09:48,011 - INFO - L1 lambda: 1e-06\n",
      "2025-04-27 21:09:48,012 - INFO - L2 lambda: 1e-05\n",
      "2025-04-27 21:09:48,012 - INFO - Dropout rate: 0.5\n",
      "2025-04-27 21:09:48,012 - INFO - Early stopping patience: 5\n",
      "2025-04-27 21:09:48,020 - INFO - ====================================================================================================\n",
      "2025-04-27 21:09:48,021 - INFO - Model ID: variant_1\n",
      "2025-04-27 21:09:48,021 - INFO - Run ID: 20250427_210947\n",
      "2025-04-27 21:09:48,022 - INFO - Training configuration:\n",
      "2025-04-27 21:09:48,022 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:09:48,023 - INFO -   Batch size: 128\n",
      "2025-04-27 21:09:48,023 - INFO -   L1 regularization: 1e-06\n",
      "2025-04-27 21:09:48,023 - INFO -   L2 regularization: 1e-05\n",
      "2025-04-27 21:09:48,024 - INFO -   Dropout rate: 0.5\n",
      "2025-04-27 21:09:48,024 - INFO -   Epochs: 10\n",
      "2025-04-27 21:09:48,025 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:09:48,025 - INFO -   Optimizer: Adam\n",
      "2025-04-27 21:09:48,025 - INFO -   Loss function: CrossEntropyLoss\n",
      "2025-04-27 21:09:58,802 - INFO - ====================================================================================================\n",
      "2025-04-27 21:09:58,803 - INFO - Epoch [1/10]\n",
      "2025-04-27 21:09:58,803 - INFO - Training Loss: 0.9644\n",
      "2025-04-27 21:10:01,291 - INFO - Validation Loss: 0.5133\n",
      "2025-04-27 21:10:01,292 - INFO - Validation Accuracy: 90.58%\n",
      "2025-04-27 21:10:01,297 - INFO - New best validation loss: 0.5133\n",
      "2025-04-27 21:10:01,297 - INFO - Epoch 1 duration: 13.27 seconds\n",
      "2025-04-27 21:10:12,016 - INFO - ====================================================================================================\n",
      "2025-04-27 21:10:12,017 - INFO - Epoch [2/10]\n",
      "2025-04-27 21:10:12,017 - INFO - Training Loss: 0.5255\n",
      "2025-04-27 21:10:14,517 - INFO - Validation Loss: 0.3081\n",
      "2025-04-27 21:10:14,518 - INFO - Validation Accuracy: 93.79%\n",
      "2025-04-27 21:10:14,524 - INFO - New best validation loss: 0.3081\n",
      "2025-04-27 21:10:14,524 - INFO - Epoch 2 duration: 13.23 seconds\n",
      "2025-04-27 21:10:25,222 - INFO - ====================================================================================================\n",
      "2025-04-27 21:10:25,222 - INFO - Epoch [3/10]\n",
      "2025-04-27 21:10:25,223 - INFO - Training Loss: 0.3868\n",
      "2025-04-27 21:10:27,683 - INFO - Validation Loss: 0.2268\n",
      "2025-04-27 21:10:27,683 - INFO - Validation Accuracy: 95.03%\n",
      "2025-04-27 21:10:27,688 - INFO - New best validation loss: 0.2268\n",
      "2025-04-27 21:10:27,689 - INFO - Epoch 3 duration: 13.16 seconds\n",
      "2025-04-27 21:10:38,553 - INFO - ====================================================================================================\n",
      "2025-04-27 21:10:38,554 - INFO - Epoch [4/10]\n",
      "2025-04-27 21:10:38,554 - INFO - Training Loss: 0.3232\n",
      "2025-04-27 21:10:41,125 - INFO - Validation Loss: 0.1859\n",
      "2025-04-27 21:10:41,126 - INFO - Validation Accuracy: 95.59%\n",
      "2025-04-27 21:10:41,131 - INFO - New best validation loss: 0.1859\n",
      "2025-04-27 21:10:41,131 - INFO - Epoch 4 duration: 13.44 seconds\n",
      "2025-04-27 21:10:52,297 - INFO - ====================================================================================================\n",
      "2025-04-27 21:10:52,298 - INFO - Epoch [5/10]\n",
      "2025-04-27 21:10:52,298 - INFO - Training Loss: 0.2828\n",
      "2025-04-27 21:10:54,808 - INFO - Validation Loss: 0.1516\n",
      "2025-04-27 21:10:54,809 - INFO - Validation Accuracy: 96.21%\n",
      "2025-04-27 21:10:54,814 - INFO - New best validation loss: 0.1516\n",
      "2025-04-27 21:10:54,814 - INFO - Epoch 5 duration: 13.68 seconds\n",
      "2025-04-27 21:11:05,659 - INFO - ====================================================================================================\n",
      "2025-04-27 21:11:05,659 - INFO - Epoch [6/10]\n",
      "2025-04-27 21:11:05,660 - INFO - Training Loss: 0.2563\n",
      "2025-04-27 21:11:08,181 - INFO - Validation Loss: 0.1393\n",
      "2025-04-27 21:11:08,181 - INFO - Validation Accuracy: 96.53%\n",
      "2025-04-27 21:11:08,186 - INFO - New best validation loss: 0.1393\n",
      "2025-04-27 21:11:08,187 - INFO - Epoch 6 duration: 13.37 seconds\n",
      "2025-04-27 21:11:18,980 - INFO - ====================================================================================================\n",
      "2025-04-27 21:11:18,981 - INFO - Epoch [7/10]\n",
      "2025-04-27 21:11:18,982 - INFO - Training Loss: 0.2350\n",
      "2025-04-27 21:11:21,499 - INFO - Validation Loss: 0.1234\n",
      "2025-04-27 21:11:21,500 - INFO - Validation Accuracy: 96.66%\n",
      "2025-04-27 21:11:21,505 - INFO - New best validation loss: 0.1234\n",
      "2025-04-27 21:11:21,505 - INFO - Epoch 7 duration: 13.32 seconds\n",
      "2025-04-27 21:11:32,599 - INFO - ====================================================================================================\n",
      "2025-04-27 21:11:32,599 - INFO - Epoch [8/10]\n",
      "2025-04-27 21:11:32,600 - INFO - Training Loss: 0.2281\n",
      "2025-04-27 21:11:35,128 - INFO - Validation Loss: 0.1130\n",
      "2025-04-27 21:11:35,129 - INFO - Validation Accuracy: 96.91%\n",
      "2025-04-27 21:11:35,136 - INFO - New best validation loss: 0.1130\n",
      "2025-04-27 21:11:35,137 - INFO - Epoch 8 duration: 13.63 seconds\n",
      "2025-04-27 21:11:45,922 - INFO - ====================================================================================================\n",
      "2025-04-27 21:11:45,923 - INFO - Epoch [9/10]\n",
      "2025-04-27 21:11:45,924 - INFO - Training Loss: 0.2183\n",
      "2025-04-27 21:11:48,411 - INFO - Validation Loss: 0.1101\n",
      "2025-04-27 21:11:48,412 - INFO - Validation Accuracy: 97.00%\n",
      "2025-04-27 21:11:48,417 - INFO - New best validation loss: 0.1101\n",
      "2025-04-27 21:11:48,418 - INFO - Epoch 9 duration: 13.28 seconds\n",
      "2025-04-27 21:11:59,186 - INFO - ====================================================================================================\n",
      "2025-04-27 21:11:59,187 - INFO - Epoch [10/10]\n",
      "2025-04-27 21:11:59,187 - INFO - Training Loss: 0.2123\n",
      "2025-04-27 21:12:01,765 - INFO - Validation Loss: 0.1047\n",
      "2025-04-27 21:12:01,765 - INFO - Validation Accuracy: 97.02%\n",
      "2025-04-27 21:12:01,770 - INFO - New best validation loss: 0.1047\n",
      "2025-04-27 21:12:01,771 - INFO - Epoch 10 duration: 13.35 seconds\n",
      "2025-04-27 21:12:01,771 - INFO - Training duration: 133.75 seconds\n",
      "2025-04-27 21:12:01,772 - INFO - Completed all 10 epochs\n",
      "2025-04-27 21:12:01,772 - INFO - Loading best model from: models/run_20250427_210947/model_variant_1/checkpoints/epoch_10.pth\n",
      "2025-04-27 21:12:02,488 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:02,489 - INFO - Testing duration: 0.71 seconds\n",
      "2025-04-27 21:12:02,489 - INFO - Total duration: 134.46 seconds\n",
      "2025-04-27 21:12:02,490 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:02,490 - INFO - Best Model: models/run_20250427_210947/model_variant_1/checkpoints/epoch_10.pth\n",
      "2025-04-27 21:12:02,491 - INFO - Best Validation Loss: 0.1047\n",
      "2025-04-27 21:12:02,491 - INFO - Test Loss: 0.7590\n",
      "2025-04-27 21:12:02,492 - INFO - Test Accuracy: 72.61%\n",
      "2025-04-27 21:12:02,492 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:02,493 - INFO - Completed training for model variant_1\n",
      "2025-04-27 21:12:02,493 - INFO - Test accuracy: 72.61%\n",
      "2025-04-27 21:12:02,494 - INFO - New best model: variant_1 with accuracy 72.61%\n",
      "2025-04-27 21:12:02,494 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:02,495 - INFO - Starting training for model variant_2\n",
      "2025-04-27 21:12:02,495 - INFO - Hyperparameters:\n",
      "2025-04-27 21:12:02,496 - INFO - Learning rate: 0.0001\n",
      "2025-04-27 21:12:02,496 - INFO - Batch size: 64\n",
      "2025-04-27 21:12:02,496 - INFO - L1 lambda: 1e-06\n",
      "2025-04-27 21:12:02,497 - INFO - L2 lambda: 1e-05\n",
      "2025-04-27 21:12:02,497 - INFO - Dropout rate: 0.25\n",
      "2025-04-27 21:12:02,497 - INFO - Early stopping patience: 5\n",
      "2025-04-27 21:12:02,505 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:02,506 - INFO - Model ID: variant_2\n",
      "2025-04-27 21:12:02,506 - INFO - Run ID: 20250427_210947\n",
      "2025-04-27 21:12:02,507 - INFO - Training configuration:\n",
      "2025-04-27 21:12:02,507 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:12:02,508 - INFO -   Batch size: 64\n",
      "2025-04-27 21:12:02,508 - INFO -   L1 regularization: 1e-06\n",
      "2025-04-27 21:12:02,508 - INFO -   L2 regularization: 1e-05\n",
      "2025-04-27 21:12:02,509 - INFO -   Dropout rate: 0.25\n",
      "2025-04-27 21:12:02,509 - INFO -   Epochs: 10\n",
      "2025-04-27 21:12:02,510 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:12:02,510 - INFO -   Optimizer: Adam\n",
      "2025-04-27 21:12:02,510 - INFO -   Loss function: CrossEntropyLoss\n",
      "2025-04-27 21:12:14,092 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:14,093 - INFO - Epoch [1/10]\n",
      "2025-04-27 21:12:14,093 - INFO - Training Loss: 0.6118\n",
      "2025-04-27 21:12:16,632 - INFO - Validation Loss: 0.2700\n",
      "2025-04-27 21:12:16,633 - INFO - Validation Accuracy: 95.02%\n",
      "2025-04-27 21:12:16,637 - INFO - New best validation loss: 0.2700\n",
      "2025-04-27 21:12:16,638 - INFO - Epoch 1 duration: 14.13 seconds\n",
      "2025-04-27 21:12:28,173 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:28,174 - INFO - Epoch [2/10]\n",
      "2025-04-27 21:12:28,174 - INFO - Training Loss: 0.2741\n",
      "2025-04-27 21:12:30,713 - INFO - Validation Loss: 0.1626\n",
      "2025-04-27 21:12:30,714 - INFO - Validation Accuracy: 96.41%\n",
      "2025-04-27 21:12:30,719 - INFO - New best validation loss: 0.1626\n",
      "2025-04-27 21:12:30,720 - INFO - Epoch 2 duration: 14.08 seconds\n",
      "2025-04-27 21:12:42,410 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:42,410 - INFO - Epoch [3/10]\n",
      "2025-04-27 21:12:42,411 - INFO - Training Loss: 0.2018\n",
      "2025-04-27 21:12:44,989 - INFO - Validation Loss: 0.1217\n",
      "2025-04-27 21:12:44,990 - INFO - Validation Accuracy: 96.96%\n",
      "2025-04-27 21:12:44,994 - INFO - New best validation loss: 0.1217\n",
      "2025-04-27 21:12:44,995 - INFO - Epoch 3 duration: 14.27 seconds\n",
      "2025-04-27 21:12:56,704 - INFO - ====================================================================================================\n",
      "2025-04-27 21:12:56,704 - INFO - Epoch [4/10]\n",
      "2025-04-27 21:12:56,705 - INFO - Training Loss: 0.1718\n",
      "2025-04-27 21:12:59,243 - INFO - Validation Loss: 0.1069\n",
      "2025-04-27 21:12:59,244 - INFO - Validation Accuracy: 97.31%\n",
      "2025-04-27 21:12:59,249 - INFO - New best validation loss: 0.1069\n",
      "2025-04-27 21:12:59,249 - INFO - Epoch 4 duration: 14.25 seconds\n",
      "2025-04-27 21:13:10,742 - INFO - ====================================================================================================\n",
      "2025-04-27 21:13:10,743 - INFO - Epoch [5/10]\n",
      "2025-04-27 21:13:10,743 - INFO - Training Loss: 0.1530\n",
      "2025-04-27 21:13:13,264 - INFO - Validation Loss: 0.0955\n",
      "2025-04-27 21:13:13,264 - INFO - Validation Accuracy: 97.39%\n",
      "2025-04-27 21:13:13,269 - INFO - New best validation loss: 0.0955\n",
      "2025-04-27 21:13:13,270 - INFO - Epoch 5 duration: 14.02 seconds\n",
      "2025-04-27 21:13:24,701 - INFO - ====================================================================================================\n",
      "2025-04-27 21:13:24,702 - INFO - Epoch [6/10]\n",
      "2025-04-27 21:13:24,703 - INFO - Training Loss: 0.1426\n",
      "2025-04-27 21:13:27,230 - INFO - Validation Loss: 0.0873\n",
      "2025-04-27 21:13:27,231 - INFO - Validation Accuracy: 97.51%\n",
      "2025-04-27 21:13:27,236 - INFO - New best validation loss: 0.0873\n",
      "2025-04-27 21:13:27,236 - INFO - Epoch 6 duration: 13.97 seconds\n",
      "2025-04-27 21:13:38,784 - INFO - ====================================================================================================\n",
      "2025-04-27 21:13:38,785 - INFO - Epoch [7/10]\n",
      "2025-04-27 21:13:38,786 - INFO - Training Loss: 0.1314\n",
      "2025-04-27 21:13:41,308 - INFO - Validation Loss: 0.0784\n",
      "2025-04-27 21:13:41,309 - INFO - Validation Accuracy: 97.90%\n",
      "2025-04-27 21:13:41,313 - INFO - New best validation loss: 0.0784\n",
      "2025-04-27 21:13:41,314 - INFO - Epoch 7 duration: 14.08 seconds\n",
      "2025-04-27 21:13:52,809 - INFO - ====================================================================================================\n",
      "2025-04-27 21:13:52,809 - INFO - Epoch [8/10]\n",
      "2025-04-27 21:13:52,810 - INFO - Training Loss: 0.1263\n",
      "2025-04-27 21:13:55,375 - INFO - Validation Loss: 0.0768\n",
      "2025-04-27 21:13:55,375 - INFO - Validation Accuracy: 97.78%\n",
      "2025-04-27 21:13:55,380 - INFO - New best validation loss: 0.0768\n",
      "2025-04-27 21:13:55,380 - INFO - Epoch 8 duration: 14.07 seconds\n",
      "2025-04-27 21:14:06,971 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:06,971 - INFO - Epoch [9/10]\n",
      "2025-04-27 21:14:06,972 - INFO - Training Loss: 0.1191\n",
      "2025-04-27 21:14:09,554 - INFO - Validation Loss: 0.0690\n",
      "2025-04-27 21:14:09,555 - INFO - Validation Accuracy: 98.03%\n",
      "2025-04-27 21:14:09,560 - INFO - New best validation loss: 0.0690\n",
      "2025-04-27 21:14:09,560 - INFO - Epoch 9 duration: 14.18 seconds\n",
      "2025-04-27 21:14:21,036 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:21,037 - INFO - Epoch [10/10]\n",
      "2025-04-27 21:14:21,037 - INFO - Training Loss: 0.1159\n",
      "2025-04-27 21:14:23,567 - INFO - Validation Loss: 0.0719\n",
      "2025-04-27 21:14:23,568 - INFO - Validation Accuracy: 97.90%\n",
      "2025-04-27 21:14:23,573 - INFO - No improvement for 1 epochs (patience: 5)\n",
      "2025-04-27 21:14:23,573 - INFO - Epoch 10 duration: 14.01 seconds\n",
      "2025-04-27 21:14:23,574 - INFO - Training duration: 141.06 seconds\n",
      "2025-04-27 21:14:23,574 - INFO - Completed all 10 epochs\n",
      "2025-04-27 21:14:23,574 - INFO - Loading best model from: models/run_20250427_210947/model_variant_2/checkpoints/epoch_9.pth\n",
      "2025-04-27 21:14:24,328 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:24,329 - INFO - Testing duration: 0.75 seconds\n",
      "2025-04-27 21:14:24,329 - INFO - Total duration: 141.82 seconds\n",
      "2025-04-27 21:14:24,330 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:24,330 - INFO - Best Model: models/run_20250427_210947/model_variant_2/checkpoints/epoch_9.pth\n",
      "2025-04-27 21:14:24,331 - INFO - Best Validation Loss: 0.0690\n",
      "2025-04-27 21:14:24,331 - INFO - Test Loss: 0.5928\n",
      "2025-04-27 21:14:24,332 - INFO - Test Accuracy: 81.12%\n",
      "2025-04-27 21:14:24,332 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:24,333 - INFO - Completed training for model variant_2\n",
      "2025-04-27 21:14:24,334 - INFO - Test accuracy: 81.12%\n",
      "2025-04-27 21:14:24,334 - INFO - New best model: variant_2 with accuracy 81.12%\n",
      "2025-04-27 21:14:24,334 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:24,335 - INFO - Starting training for model variant_3\n",
      "2025-04-27 21:14:24,335 - INFO - Hyperparameters:\n",
      "2025-04-27 21:14:24,336 - INFO - Learning rate: 0.0001\n",
      "2025-04-27 21:14:24,336 - INFO - Batch size: 64\n",
      "2025-04-27 21:14:24,337 - INFO - L1 lambda: 0.0001\n",
      "2025-04-27 21:14:24,337 - INFO - L2 lambda: 1e-05\n",
      "2025-04-27 21:14:24,337 - INFO - Dropout rate: 0.5\n",
      "2025-04-27 21:14:24,338 - INFO - Early stopping patience: 5\n",
      "2025-04-27 21:14:24,345 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:24,345 - INFO - Model ID: variant_3\n",
      "2025-04-27 21:14:24,346 - INFO - Run ID: 20250427_210947\n",
      "2025-04-27 21:14:24,346 - INFO - Training configuration:\n",
      "2025-04-27 21:14:24,347 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:14:24,347 - INFO -   Batch size: 64\n",
      "2025-04-27 21:14:24,347 - INFO -   L1 regularization: 0.0001\n",
      "2025-04-27 21:14:24,348 - INFO -   L2 regularization: 1e-05\n",
      "2025-04-27 21:14:24,348 - INFO -   Dropout rate: 0.5\n",
      "2025-04-27 21:14:24,349 - INFO -   Epochs: 10\n",
      "2025-04-27 21:14:24,349 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:14:24,350 - INFO -   Optimizer: Adam\n",
      "2025-04-27 21:14:24,350 - INFO -   Loss function: CrossEntropyLoss\n",
      "2025-04-27 21:14:35,903 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:35,904 - INFO - Epoch [1/10]\n",
      "2025-04-27 21:14:35,905 - INFO - Training Loss: 1.2636\n",
      "2025-04-27 21:14:38,448 - INFO - Validation Loss: 0.3768\n",
      "2025-04-27 21:14:38,449 - INFO - Validation Accuracy: 92.55%\n",
      "2025-04-27 21:14:38,453 - INFO - New best validation loss: 0.3768\n",
      "2025-04-27 21:14:38,454 - INFO - Epoch 1 duration: 14.10 seconds\n",
      "2025-04-27 21:14:50,191 - INFO - ====================================================================================================\n",
      "2025-04-27 21:14:50,192 - INFO - Epoch [2/10]\n",
      "2025-04-27 21:14:50,192 - INFO - Training Loss: 0.7511\n",
      "2025-04-27 21:14:52,723 - INFO - Validation Loss: 0.2383\n",
      "2025-04-27 21:14:52,724 - INFO - Validation Accuracy: 94.62%\n",
      "2025-04-27 21:14:52,729 - INFO - New best validation loss: 0.2383\n",
      "2025-04-27 21:14:52,729 - INFO - Epoch 2 duration: 14.27 seconds\n",
      "2025-04-27 21:15:04,190 - INFO - ====================================================================================================\n",
      "2025-04-27 21:15:04,190 - INFO - Epoch [3/10]\n",
      "2025-04-27 21:15:04,191 - INFO - Training Loss: 0.6004\n",
      "2025-04-27 21:15:06,729 - INFO - Validation Loss: 0.1803\n",
      "2025-04-27 21:15:06,730 - INFO - Validation Accuracy: 95.60%\n",
      "2025-04-27 21:15:06,734 - INFO - New best validation loss: 0.1803\n",
      "2025-04-27 21:15:06,735 - INFO - Epoch 3 duration: 14.01 seconds\n",
      "2025-04-27 21:15:18,262 - INFO - ====================================================================================================\n",
      "2025-04-27 21:15:18,262 - INFO - Epoch [4/10]\n",
      "2025-04-27 21:15:18,263 - INFO - Training Loss: 0.5409\n",
      "2025-04-27 21:15:20,832 - INFO - Validation Loss: 0.1600\n",
      "2025-04-27 21:15:20,833 - INFO - Validation Accuracy: 95.82%\n",
      "2025-04-27 21:15:20,838 - INFO - New best validation loss: 0.1600\n",
      "2025-04-27 21:15:20,838 - INFO - Epoch 4 duration: 14.10 seconds\n",
      "2025-04-27 21:15:32,333 - INFO - ====================================================================================================\n",
      "2025-04-27 21:15:32,334 - INFO - Epoch [5/10]\n",
      "2025-04-27 21:15:32,334 - INFO - Training Loss: 0.5071\n",
      "2025-04-27 21:15:34,885 - INFO - Validation Loss: 0.1445\n",
      "2025-04-27 21:15:34,886 - INFO - Validation Accuracy: 96.00%\n",
      "2025-04-27 21:15:34,890 - INFO - New best validation loss: 0.1445\n",
      "2025-04-27 21:15:34,891 - INFO - Epoch 5 duration: 14.05 seconds\n",
      "2025-04-27 21:15:46,507 - INFO - ====================================================================================================\n",
      "2025-04-27 21:15:46,508 - INFO - Epoch [6/10]\n",
      "2025-04-27 21:15:46,508 - INFO - Training Loss: 0.4833\n",
      "2025-04-27 21:15:49,095 - INFO - Validation Loss: 0.1327\n",
      "2025-04-27 21:15:49,096 - INFO - Validation Accuracy: 96.26%\n",
      "2025-04-27 21:15:49,101 - INFO - New best validation loss: 0.1327\n",
      "2025-04-27 21:15:49,101 - INFO - Epoch 6 duration: 14.21 seconds\n",
      "2025-04-27 21:16:00,554 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:00,555 - INFO - Epoch [7/10]\n",
      "2025-04-27 21:16:00,555 - INFO - Training Loss: 0.4685\n",
      "2025-04-27 21:16:03,103 - INFO - Validation Loss: 0.1229\n",
      "2025-04-27 21:16:03,103 - INFO - Validation Accuracy: 96.67%\n",
      "2025-04-27 21:16:03,108 - INFO - New best validation loss: 0.1229\n",
      "2025-04-27 21:16:03,109 - INFO - Epoch 7 duration: 14.01 seconds\n",
      "2025-04-27 21:16:14,654 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:14,654 - INFO - Epoch [8/10]\n",
      "2025-04-27 21:16:14,655 - INFO - Training Loss: 0.4614\n",
      "2025-04-27 21:16:17,200 - INFO - Validation Loss: 0.1232\n",
      "2025-04-27 21:16:17,200 - INFO - Validation Accuracy: 96.53%\n",
      "2025-04-27 21:16:17,205 - INFO - No improvement for 1 epochs (patience: 5)\n",
      "2025-04-27 21:16:17,206 - INFO - Epoch 8 duration: 14.10 seconds\n",
      "2025-04-27 21:16:28,722 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:28,723 - INFO - Epoch [9/10]\n",
      "2025-04-27 21:16:28,724 - INFO - Training Loss: 0.4503\n",
      "2025-04-27 21:16:31,257 - INFO - Validation Loss: 0.1147\n",
      "2025-04-27 21:16:31,257 - INFO - Validation Accuracy: 96.73%\n",
      "2025-04-27 21:16:31,262 - INFO - New best validation loss: 0.1147\n",
      "2025-04-27 21:16:31,263 - INFO - Epoch 9 duration: 14.06 seconds\n",
      "2025-04-27 21:16:42,802 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:42,802 - INFO - Epoch [10/10]\n",
      "2025-04-27 21:16:42,803 - INFO - Training Loss: 0.4405\n",
      "2025-04-27 21:16:45,350 - INFO - Validation Loss: 0.1101\n",
      "2025-04-27 21:16:45,350 - INFO - Validation Accuracy: 96.83%\n",
      "2025-04-27 21:16:45,355 - INFO - New best validation loss: 0.1101\n",
      "2025-04-27 21:16:45,356 - INFO - Epoch 10 duration: 14.09 seconds\n",
      "2025-04-27 21:16:45,356 - INFO - Training duration: 141.01 seconds\n",
      "2025-04-27 21:16:45,356 - INFO - Completed all 10 epochs\n",
      "2025-04-27 21:16:45,357 - INFO - Loading best model from: models/run_20250427_210947/model_variant_3/checkpoints/epoch_10.pth\n",
      "2025-04-27 21:16:46,110 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:46,110 - INFO - Testing duration: 0.75 seconds\n",
      "2025-04-27 21:16:46,111 - INFO - Total duration: 141.76 seconds\n",
      "2025-04-27 21:16:46,111 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:46,112 - INFO - Best Model: models/run_20250427_210947/model_variant_3/checkpoints/epoch_10.pth\n",
      "2025-04-27 21:16:46,112 - INFO - Best Validation Loss: 0.1101\n",
      "2025-04-27 21:16:46,112 - INFO - Test Loss: 0.4928\n",
      "2025-04-27 21:16:46,113 - INFO - Test Accuracy: 96.58%\n",
      "2025-04-27 21:16:46,113 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:46,114 - INFO - Completed training for model variant_3\n",
      "2025-04-27 21:16:46,114 - INFO - Test accuracy: 96.58%\n",
      "2025-04-27 21:16:46,114 - INFO - New best model: variant_3 with accuracy 96.58%\n",
      "2025-04-27 21:16:46,115 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:46,115 - INFO - Starting training for model variant_4\n",
      "2025-04-27 21:16:46,115 - INFO - Hyperparameters:\n",
      "2025-04-27 21:16:46,116 - INFO - Learning rate: 0.0001\n",
      "2025-04-27 21:16:46,116 - INFO - Batch size: 32\n",
      "2025-04-27 21:16:46,116 - INFO - L1 lambda: 1e-06\n",
      "2025-04-27 21:16:46,117 - INFO - L2 lambda: 0.0001\n",
      "2025-04-27 21:16:46,117 - INFO - Dropout rate: 0.5\n",
      "2025-04-27 21:16:46,117 - INFO - Early stopping patience: 5\n",
      "2025-04-27 21:16:46,124 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:46,125 - INFO - Model ID: variant_4\n",
      "2025-04-27 21:16:46,125 - INFO - Run ID: 20250427_210947\n",
      "2025-04-27 21:16:46,125 - INFO - Training configuration:\n",
      "2025-04-27 21:16:46,126 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:16:46,127 - INFO -   Batch size: 32\n",
      "2025-04-27 21:16:46,127 - INFO -   L1 regularization: 1e-06\n",
      "2025-04-27 21:16:46,127 - INFO -   L2 regularization: 0.0001\n",
      "2025-04-27 21:16:46,128 - INFO -   Dropout rate: 0.5\n",
      "2025-04-27 21:16:46,128 - INFO -   Epochs: 10\n",
      "2025-04-27 21:16:46,129 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:16:46,129 - INFO -   Optimizer: Adam\n",
      "2025-04-27 21:16:46,129 - INFO -   Loss function: CrossEntropyLoss\n",
      "2025-04-27 21:16:59,404 - INFO - ====================================================================================================\n",
      "2025-04-27 21:16:59,405 - INFO - Epoch [1/10]\n",
      "2025-04-27 21:16:59,406 - INFO - Training Loss: 0.7898\n",
      "2025-04-27 21:17:02,092 - INFO - Validation Loss: 0.3410\n",
      "2025-04-27 21:17:02,093 - INFO - Validation Accuracy: 92.63%\n",
      "2025-04-27 21:17:02,098 - INFO - New best validation loss: 0.3410\n",
      "2025-04-27 21:17:02,098 - INFO - Epoch 1 duration: 15.97 seconds\n",
      "2025-04-27 21:17:15,312 - INFO - ====================================================================================================\n",
      "2025-04-27 21:17:15,313 - INFO - Epoch [2/10]\n",
      "2025-04-27 21:17:15,313 - INFO - Training Loss: 0.4159\n",
      "2025-04-27 21:17:17,950 - INFO - Validation Loss: 0.2074\n",
      "2025-04-27 21:17:17,951 - INFO - Validation Accuracy: 94.55%\n",
      "2025-04-27 21:17:17,956 - INFO - New best validation loss: 0.2074\n",
      "2025-04-27 21:17:17,956 - INFO - Epoch 2 duration: 15.86 seconds\n",
      "2025-04-27 21:17:31,162 - INFO - ====================================================================================================\n",
      "2025-04-27 21:17:31,163 - INFO - Epoch [3/10]\n",
      "2025-04-27 21:17:31,163 - INFO - Training Loss: 0.3297\n",
      "2025-04-27 21:17:33,817 - INFO - Validation Loss: 0.1599\n",
      "2025-04-27 21:17:33,818 - INFO - Validation Accuracy: 95.72%\n",
      "2025-04-27 21:17:33,823 - INFO - New best validation loss: 0.1599\n",
      "2025-04-27 21:17:33,823 - INFO - Epoch 3 duration: 15.87 seconds\n",
      "2025-04-27 21:17:46,925 - INFO - ====================================================================================================\n",
      "2025-04-27 21:17:46,925 - INFO - Epoch [4/10]\n",
      "2025-04-27 21:17:46,926 - INFO - Training Loss: 0.2942\n",
      "2025-04-27 21:17:49,624 - INFO - Validation Loss: 0.1410\n",
      "2025-04-27 21:17:49,625 - INFO - Validation Accuracy: 96.16%\n",
      "2025-04-27 21:17:49,631 - INFO - New best validation loss: 0.1410\n",
      "2025-04-27 21:17:49,632 - INFO - Epoch 4 duration: 15.81 seconds\n",
      "2025-04-27 21:18:03,071 - INFO - ====================================================================================================\n",
      "2025-04-27 21:18:03,072 - INFO - Epoch [5/10]\n",
      "2025-04-27 21:18:03,072 - INFO - Training Loss: 0.2730\n",
      "2025-04-27 21:18:05,717 - INFO - Validation Loss: 0.1268\n",
      "2025-04-27 21:18:05,718 - INFO - Validation Accuracy: 96.47%\n",
      "2025-04-27 21:18:05,724 - INFO - New best validation loss: 0.1268\n",
      "2025-04-27 21:18:05,724 - INFO - Epoch 5 duration: 16.09 seconds\n",
      "2025-04-27 21:18:19,046 - INFO - ====================================================================================================\n",
      "2025-04-27 21:18:19,047 - INFO - Epoch [6/10]\n",
      "2025-04-27 21:18:19,047 - INFO - Training Loss: 0.2562\n",
      "2025-04-27 21:18:21,699 - INFO - Validation Loss: 0.1150\n",
      "2025-04-27 21:18:21,699 - INFO - Validation Accuracy: 96.78%\n",
      "2025-04-27 21:18:21,704 - INFO - New best validation loss: 0.1150\n",
      "2025-04-27 21:18:21,705 - INFO - Epoch 6 duration: 15.98 seconds\n",
      "2025-04-27 21:18:34,869 - INFO - ====================================================================================================\n",
      "2025-04-27 21:18:34,870 - INFO - Epoch [7/10]\n",
      "2025-04-27 21:18:34,871 - INFO - Training Loss: 0.2418\n",
      "2025-04-27 21:18:37,510 - INFO - Validation Loss: 0.1126\n",
      "2025-04-27 21:18:37,511 - INFO - Validation Accuracy: 96.71%\n",
      "2025-04-27 21:18:37,516 - INFO - New best validation loss: 0.1126\n",
      "2025-04-27 21:18:37,516 - INFO - Epoch 7 duration: 15.81 seconds\n",
      "2025-04-27 21:18:50,728 - INFO - ====================================================================================================\n",
      "2025-04-27 21:18:50,729 - INFO - Epoch [8/10]\n",
      "2025-04-27 21:18:50,730 - INFO - Training Loss: 0.2330\n",
      "2025-04-27 21:18:53,378 - INFO - Validation Loss: 0.1097\n",
      "2025-04-27 21:18:53,379 - INFO - Validation Accuracy: 96.84%\n",
      "2025-04-27 21:18:53,384 - INFO - New best validation loss: 0.1097\n",
      "2025-04-27 21:18:53,385 - INFO - Epoch 8 duration: 15.87 seconds\n",
      "2025-04-27 21:19:06,585 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:06,586 - INFO - Epoch [9/10]\n",
      "2025-04-27 21:19:06,586 - INFO - Training Loss: 0.2306\n",
      "2025-04-27 21:19:09,242 - INFO - Validation Loss: 0.1018\n",
      "2025-04-27 21:19:09,242 - INFO - Validation Accuracy: 96.91%\n",
      "2025-04-27 21:19:09,247 - INFO - New best validation loss: 0.1018\n",
      "2025-04-27 21:19:09,248 - INFO - Epoch 9 duration: 15.86 seconds\n",
      "2025-04-27 21:19:22,462 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:22,463 - INFO - Epoch [10/10]\n",
      "2025-04-27 21:19:22,464 - INFO - Training Loss: 0.2199\n",
      "2025-04-27 21:19:25,131 - INFO - Validation Loss: 0.0949\n",
      "2025-04-27 21:19:25,132 - INFO - Validation Accuracy: 97.24%\n",
      "2025-04-27 21:19:25,137 - INFO - New best validation loss: 0.0949\n",
      "2025-04-27 21:19:25,137 - INFO - Epoch 10 duration: 15.89 seconds\n",
      "2025-04-27 21:19:25,138 - INFO - Training duration: 159.01 seconds\n",
      "2025-04-27 21:19:25,138 - INFO - Completed all 10 epochs\n",
      "2025-04-27 21:19:25,139 - INFO - Loading best model from: models/run_20250427_210947/model_variant_4/checkpoints/epoch_10.pth\n",
      "2025-04-27 21:19:25,982 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:25,983 - INFO - Testing duration: 0.84 seconds\n",
      "2025-04-27 21:19:25,984 - INFO - Total duration: 159.85 seconds\n",
      "2025-04-27 21:19:25,984 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:25,985 - INFO - Best Model: models/run_20250427_210947/model_variant_4/checkpoints/epoch_10.pth\n",
      "2025-04-27 21:19:25,985 - INFO - Best Validation Loss: 0.0949\n",
      "2025-04-27 21:19:25,986 - INFO - Test Loss: 0.6466\n",
      "2025-04-27 21:19:25,986 - INFO - Test Accuracy: 81.94%\n",
      "2025-04-27 21:19:25,986 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:25,987 - INFO - Completed training for model variant_4\n",
      "2025-04-27 21:19:25,988 - INFO - Test accuracy: 81.94%\n",
      "2025-04-27 21:19:25,988 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:25,989 - INFO - Starting training for model variant_5\n",
      "2025-04-27 21:19:25,989 - INFO - Hyperparameters:\n",
      "2025-04-27 21:19:25,990 - INFO - Learning rate: 0.0001\n",
      "2025-04-27 21:19:25,990 - INFO - Batch size: 32\n",
      "2025-04-27 21:19:25,990 - INFO - L1 lambda: 1e-05\n",
      "2025-04-27 21:19:25,991 - INFO - L2 lambda: 0.0001\n",
      "2025-04-27 21:19:25,991 - INFO - Dropout rate: 0.25\n",
      "2025-04-27 21:19:25,992 - INFO - Early stopping patience: 5\n",
      "2025-04-27 21:19:25,998 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:25,999 - INFO - Model ID: variant_5\n",
      "2025-04-27 21:19:25,999 - INFO - Run ID: 20250427_210947\n",
      "2025-04-27 21:19:26,000 - INFO - Training configuration:\n",
      "2025-04-27 21:19:26,000 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:19:26,001 - INFO -   Batch size: 32\n",
      "2025-04-27 21:19:26,001 - INFO -   L1 regularization: 1e-05\n",
      "2025-04-27 21:19:26,002 - INFO -   L2 regularization: 0.0001\n",
      "2025-04-27 21:19:26,002 - INFO -   Dropout rate: 0.25\n",
      "2025-04-27 21:19:26,002 - INFO -   Epochs: 10\n",
      "2025-04-27 21:19:26,003 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:19:26,004 - INFO -   Optimizer: Adam\n",
      "2025-04-27 21:19:26,004 - INFO -   Loss function: CrossEntropyLoss\n",
      "2025-04-27 21:19:39,321 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:39,322 - INFO - Epoch [1/10]\n",
      "2025-04-27 21:19:39,322 - INFO - Training Loss: 0.5892\n",
      "2025-04-27 21:19:41,974 - INFO - Validation Loss: 0.2142\n",
      "2025-04-27 21:19:41,975 - INFO - Validation Accuracy: 95.28%\n",
      "2025-04-27 21:19:41,979 - INFO - New best validation loss: 0.2142\n",
      "2025-04-27 21:19:41,980 - INFO - Epoch 1 duration: 15.98 seconds\n",
      "2025-04-27 21:19:55,153 - INFO - ====================================================================================================\n",
      "2025-04-27 21:19:55,154 - INFO - Epoch [2/10]\n",
      "2025-04-27 21:19:55,154 - INFO - Training Loss: 0.2850\n",
      "2025-04-27 21:19:57,797 - INFO - Validation Loss: 0.1352\n",
      "2025-04-27 21:19:57,797 - INFO - Validation Accuracy: 96.44%\n",
      "2025-04-27 21:19:57,802 - INFO - New best validation loss: 0.1352\n",
      "2025-04-27 21:19:57,803 - INFO - Epoch 2 duration: 15.82 seconds\n",
      "2025-04-27 21:20:11,008 - INFO - ====================================================================================================\n",
      "2025-04-27 21:20:11,009 - INFO - Epoch [3/10]\n",
      "2025-04-27 21:20:11,009 - INFO - Training Loss: 0.2319\n",
      "2025-04-27 21:20:13,644 - INFO - Validation Loss: 0.1120\n",
      "2025-04-27 21:20:13,644 - INFO - Validation Accuracy: 96.85%\n",
      "2025-04-27 21:20:13,649 - INFO - New best validation loss: 0.1120\n",
      "2025-04-27 21:20:13,650 - INFO - Epoch 3 duration: 15.85 seconds\n",
      "2025-04-27 21:20:26,897 - INFO - ====================================================================================================\n",
      "2025-04-27 21:20:26,898 - INFO - Epoch [4/10]\n",
      "2025-04-27 21:20:26,899 - INFO - Training Loss: 0.2070\n",
      "2025-04-27 21:20:29,563 - INFO - Validation Loss: 0.0950\n",
      "2025-04-27 21:20:29,564 - INFO - Validation Accuracy: 97.59%\n",
      "2025-04-27 21:20:29,569 - INFO - New best validation loss: 0.0950\n",
      "2025-04-27 21:20:29,570 - INFO - Epoch 4 duration: 15.92 seconds\n",
      "2025-04-27 21:20:42,766 - INFO - ====================================================================================================\n",
      "2025-04-27 21:20:42,767 - INFO - Epoch [5/10]\n",
      "2025-04-27 21:20:42,767 - INFO - Training Loss: 0.1925\n",
      "2025-04-27 21:20:45,424 - INFO - Validation Loss: 0.0864\n",
      "2025-04-27 21:20:45,425 - INFO - Validation Accuracy: 97.49%\n",
      "2025-04-27 21:20:45,429 - INFO - New best validation loss: 0.0864\n",
      "2025-04-27 21:20:45,430 - INFO - Epoch 5 duration: 15.86 seconds\n",
      "2025-04-27 21:20:58,675 - INFO - ====================================================================================================\n",
      "2025-04-27 21:20:58,675 - INFO - Epoch [6/10]\n",
      "2025-04-27 21:20:58,676 - INFO - Training Loss: 0.1898\n",
      "2025-04-27 21:21:01,330 - INFO - Validation Loss: 0.0789\n",
      "2025-04-27 21:21:01,331 - INFO - Validation Accuracy: 97.85%\n",
      "2025-04-27 21:21:01,335 - INFO - New best validation loss: 0.0789\n",
      "2025-04-27 21:21:01,336 - INFO - Epoch 6 duration: 15.91 seconds\n",
      "2025-04-27 21:21:14,555 - INFO - ====================================================================================================\n",
      "2025-04-27 21:21:14,556 - INFO - Epoch [7/10]\n",
      "2025-04-27 21:21:14,557 - INFO - Training Loss: 0.1771\n",
      "2025-04-27 21:21:17,202 - INFO - Validation Loss: 0.0707\n",
      "2025-04-27 21:21:17,202 - INFO - Validation Accuracy: 98.07%\n",
      "2025-04-27 21:21:17,207 - INFO - New best validation loss: 0.0707\n",
      "2025-04-27 21:21:17,208 - INFO - Epoch 7 duration: 15.87 seconds\n",
      "2025-04-27 21:21:30,433 - INFO - ====================================================================================================\n",
      "2025-04-27 21:21:30,434 - INFO - Epoch [8/10]\n",
      "2025-04-27 21:21:30,434 - INFO - Training Loss: 0.1711\n",
      "2025-04-27 21:21:33,101 - INFO - Validation Loss: 0.0707\n",
      "2025-04-27 21:21:33,101 - INFO - Validation Accuracy: 97.92%\n",
      "2025-04-27 21:21:33,106 - INFO - New best validation loss: 0.0707\n",
      "2025-04-27 21:21:33,107 - INFO - Epoch 8 duration: 15.90 seconds\n",
      "2025-04-27 21:21:46,335 - INFO - ====================================================================================================\n",
      "2025-04-27 21:21:46,336 - INFO - Epoch [9/10]\n",
      "2025-04-27 21:21:46,336 - INFO - Training Loss: 0.1665\n",
      "2025-04-27 21:21:48,976 - INFO - Validation Loss: 0.0713\n",
      "2025-04-27 21:21:48,977 - INFO - Validation Accuracy: 97.91%\n",
      "2025-04-27 21:21:48,982 - INFO - No improvement for 1 epochs (patience: 5)\n",
      "2025-04-27 21:21:48,982 - INFO - Epoch 9 duration: 15.88 seconds\n",
      "2025-04-27 21:22:02,207 - INFO - ====================================================================================================\n",
      "2025-04-27 21:22:02,208 - INFO - Epoch [10/10]\n",
      "2025-04-27 21:22:02,209 - INFO - Training Loss: 0.1618\n",
      "2025-04-27 21:22:04,873 - INFO - Validation Loss: 0.0668\n",
      "2025-04-27 21:22:04,874 - INFO - Validation Accuracy: 98.03%\n",
      "2025-04-27 21:22:04,879 - INFO - New best validation loss: 0.0668\n",
      "2025-04-27 21:22:04,879 - INFO - Epoch 10 duration: 15.90 seconds\n",
      "2025-04-27 21:22:04,880 - INFO - Training duration: 158.88 seconds\n",
      "2025-04-27 21:22:04,880 - INFO - Completed all 10 epochs\n",
      "2025-04-27 21:22:04,881 - INFO - Loading best model from: models/run_20250427_210947/model_variant_5/checkpoints/epoch_10.pth\n",
      "2025-04-27 21:22:05,722 - INFO - ====================================================================================================\n",
      "2025-04-27 21:22:05,723 - INFO - Testing duration: 0.84 seconds\n",
      "2025-04-27 21:22:05,723 - INFO - Total duration: 159.72 seconds\n",
      "2025-04-27 21:22:05,724 - INFO - ====================================================================================================\n",
      "2025-04-27 21:22:05,724 - INFO - Best Model: models/run_20250427_210947/model_variant_5/checkpoints/epoch_10.pth\n",
      "2025-04-27 21:22:05,724 - INFO - Best Validation Loss: 0.0668\n",
      "2025-04-27 21:22:05,725 - INFO - Test Loss: 0.5400\n",
      "2025-04-27 21:22:05,725 - INFO - Test Accuracy: 91.01%\n",
      "2025-04-27 21:22:05,725 - INFO - ====================================================================================================\n",
      "2025-04-27 21:22:05,726 - INFO - Completed training for model variant_5\n",
      "2025-04-27 21:22:05,727 - INFO - Test accuracy: 91.01%\n",
      "2025-04-27 21:22:05,727 - INFO - ====================================================================================================\n",
      "2025-04-27 21:22:05,727 - INFO - Training completed for all models\n",
      "2025-04-27 21:22:05,728 - INFO - Summary of model performances:\n",
      "2025-04-27 21:22:05,728 - INFO - Model variant_3: Accuracy 96.58%\n",
      "2025-04-27 21:22:05,729 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:22:05,729 - INFO -   Batch size: 64\n",
      "2025-04-27 21:22:05,730 - INFO -   L1 lambda: 0.0001\n",
      "2025-04-27 21:22:05,730 - INFO -   L2 lambda: 1e-05\n",
      "2025-04-27 21:22:05,730 - INFO -   Dropout rate: 0.5\n",
      "2025-04-27 21:22:05,731 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:22:05,731 - INFO - --------------------------------------------------\n",
      "2025-04-27 21:22:05,732 - INFO - Model variant_5: Accuracy 91.01%\n",
      "2025-04-27 21:22:05,732 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:22:05,733 - INFO -   Batch size: 32\n",
      "2025-04-27 21:22:05,733 - INFO -   L1 lambda: 1e-05\n",
      "2025-04-27 21:22:05,734 - INFO -   L2 lambda: 0.0001\n",
      "2025-04-27 21:22:05,734 - INFO -   Dropout rate: 0.25\n",
      "2025-04-27 21:22:05,734 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:22:05,735 - INFO - --------------------------------------------------\n",
      "2025-04-27 21:22:05,735 - INFO - Model variant_4: Accuracy 81.94%\n",
      "2025-04-27 21:22:05,736 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:22:05,736 - INFO -   Batch size: 32\n",
      "2025-04-27 21:22:05,737 - INFO -   L1 lambda: 1e-06\n",
      "2025-04-27 21:22:05,737 - INFO -   L2 lambda: 0.0001\n",
      "2025-04-27 21:22:05,738 - INFO -   Dropout rate: 0.5\n",
      "2025-04-27 21:22:05,738 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:22:05,739 - INFO - --------------------------------------------------\n",
      "2025-04-27 21:22:05,739 - INFO - Model variant_2: Accuracy 81.12%\n",
      "2025-04-27 21:22:05,739 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:22:05,740 - INFO -   Batch size: 64\n",
      "2025-04-27 21:22:05,740 - INFO -   L1 lambda: 1e-06\n",
      "2025-04-27 21:22:05,740 - INFO -   L2 lambda: 1e-05\n",
      "2025-04-27 21:22:05,741 - INFO -   Dropout rate: 0.25\n",
      "2025-04-27 21:22:05,741 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:22:05,741 - INFO - --------------------------------------------------\n",
      "2025-04-27 21:22:05,742 - INFO - Model variant_1: Accuracy 72.61%\n",
      "2025-04-27 21:22:05,742 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:22:05,743 - INFO -   Batch size: 128\n",
      "2025-04-27 21:22:05,744 - INFO -   L1 lambda: 1e-06\n",
      "2025-04-27 21:22:05,744 - INFO -   L2 lambda: 1e-05\n",
      "2025-04-27 21:22:05,744 - INFO -   Dropout rate: 0.5\n",
      "2025-04-27 21:22:05,745 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:22:05,745 - INFO - --------------------------------------------------\n",
      "2025-04-27 21:22:05,745 - INFO - ====================================================================================================\n",
      "2025-04-27 21:22:05,746 - INFO - Best model: variant_3 with accuracy: 96.58%\n",
      "2025-04-27 21:22:05,746 - INFO - Best model hyperparameters:\n",
      "2025-04-27 21:22:05,746 - INFO -   Learning rate: 0.0001\n",
      "2025-04-27 21:22:05,747 - INFO -   Batch size: 64\n",
      "2025-04-27 21:22:05,747 - INFO -   L1 lambda: 0.0001\n",
      "2025-04-27 21:22:05,747 - INFO -   L2 lambda: 1e-05\n",
      "2025-04-27 21:22:05,748 - INFO -   Dropout rate: 0.5\n",
      "2025-04-27 21:22:05,748 - INFO -   Early stopping patience: 5\n",
      "2025-04-27 21:22:05,749 - INFO - ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the perceptron neural-network model\n",
    "class Perceptron(nn.Module):\n",
    "    # Define the constructor\n",
    "    def __init__(self, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), # 32x32 with 1 padding for 28x28 input dimensions\n",
    "            nn.BatchNorm2d(32), # Add batch normalization after convolution\n",
    "            nn.ReLU(), # Apply ReLU activation function\n",
    "            nn.MaxPool2d(kernel_size=2), # 14x14 output dimensions\n",
    "            nn.Dropout2d(dropout_rate) # Add dropout to convolutional features\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * 14 * 14, 128), # 128 neurons in the hidden layer\n",
    "            nn.BatchNorm1d(128), # Add batch normalization after linear layer\n",
    "            nn.ReLU(), # Apply ReLU activation function\n",
    "            nn.Dropout(dropout_rate), # Add dropout to hidden layer\n",
    "            nn.Linear(128, 10) # 10 neurons in the output layer\n",
    "        )\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        # Pass through the convolutional layers\n",
    "        logits = self.conv_layers(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        logits = logits.view(logits.size(0), -1)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        logits = self.fc_layers(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def train_model(model, training_dataset, testing_dataset, hyperparameters, model_id, run_id):\n",
    "    # Extract hyperparameters\n",
    "    num_epochs = hyperparameters['num_epochs']\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    l1_lambda = hyperparameters['l1_lambda']\n",
    "    l2_lambda = hyperparameters['l2_lambda']\n",
    "    dropout_rate = hyperparameters['dropout_rate']\n",
    "    early_stopping_patience = hyperparameters['early_stopping_patience']\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "    \n",
    "    # Split training data into train and validation subsets\n",
    "    training_subset_size = int(0.8 * len(training_dataset))\n",
    "    validation_subset_size = len(training_dataset) - training_subset_size\n",
    "    training_subset, validation_subset = random_split(training_dataset, [training_subset_size, validation_subset_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(training_subset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
    "    testing_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create model directory\n",
    "    model_dir = os.path.join('models', f'run_{run_id}', f'model_{model_id}')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Create checkpoints directory\n",
    "    checkpoints_dir = os.path.join(model_dir, 'checkpoints')\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    log_file = os.path.join(model_dir, f'model_{model_id}.log')\n",
    "    fhandler = logging.FileHandler(filename=log_file, mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    logger.info(\"=\" * 100)\n",
    "    logger.info(f\"Model ID: {model_id}\")\n",
    "    logger.info(f\"Run ID: {run_id}\")\n",
    "    logger.info(f\"Training configuration:\")\n",
    "    logger.info(f\"  Learning rate: {learning_rate}\")\n",
    "    logger.info(f\"  Batch size: {batch_size}\")\n",
    "    logger.info(f\"  L1 regularization: {l1_lambda}\")\n",
    "    logger.info(f\"  L2 regularization: {l2_lambda}\")\n",
    "    logger.info(f\"  Dropout rate: {dropout_rate}\")\n",
    "    logger.info(f\"  Epochs: {num_epochs}\")\n",
    "    logger.info(f\"  Early stopping patience: {early_stopping_patience}\")\n",
    "    logger.info(f\"  Optimizer: Adam\")\n",
    "    logger.info(f\"  Loss function: CrossEntropyLoss\")\n",
    "\n",
    "    # Training and validation loop\n",
    "    training_start_time = time.time()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = None\n",
    "    early_stopping_counter = 0\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Cache the start time of the epoch\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            # Move data to device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "\n",
    "            # Calculate base loss\n",
    "            base_loss = criterion(outputs, y)\n",
    "            \n",
    "            # L1 regularization (L2 is handled by weight_decay)\n",
    "            l1_reg = torch.tensor(0., device=device)\n",
    "            for param in model.parameters():\n",
    "                l1_reg += torch.norm(param, 1)\n",
    "                \n",
    "            # Total loss with L1 regularization\n",
    "            loss = base_loss + l1_lambda * l1_reg\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "        # Print training loss\n",
    "        logger.info(\"=\"*100)\n",
    "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        logger.info(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_loader:\n",
    "                # Move data to device\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                running_val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "        \n",
    "        # Calculate average loss and accuracy\n",
    "        avg_val_loss = running_val_loss / len(validation_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        # Print validation loss and accuracy\n",
    "        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        logger.info(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Save the checkpoint\n",
    "        checkpoint_filename = f'epoch_{epoch+1}.pth'\n",
    "        checkpoint_path = os.path.join(checkpoints_dir, checkpoint_filename)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # Update the best model if the current model has a lower validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_model_path = checkpoint_path # Cache the path to the best model for later testing\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            logger.info(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            logger.info(f\"No improvement for {epochs_without_improvement} epochs (patience: {early_stopping_patience})\")\n",
    "            \n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "        # Log the duration of the epoch\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        logger.info(f\"Epoch {epoch+1} duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    logger.info(f\"Training duration: {training_duration:.2f} seconds\")\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        logger.info(f\"Training stopped early at epoch {epoch+1}/{num_epochs}\")\n",
    "    else:\n",
    "        logger.info(f\"Completed all {num_epochs} epochs\")\n",
    "\n",
    "    # Get the best model for testing\n",
    "    logger.info(f\"Loading best model from: {best_model_path}\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    # Testing loop\n",
    "    testing_start_time = time.time()\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in testing_loader:\n",
    "            # Move data to device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_test_loss = running_test_loss / len(testing_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    # Log the results\n",
    "    testing_end_time = time.time()\n",
    "    testing_duration = testing_end_time - testing_start_time\n",
    "    total_run_time = testing_end_time - training_start_time\n",
    "    logger.info(\"=\"*100)\n",
    "    logger.info(f\"Testing duration: {testing_duration:.2f} seconds\")\n",
    "    logger.info(f\"Total duration: {total_run_time:.2f} seconds\")\n",
    "    logger.info(\"=\"*100)\n",
    "    logger.info(f\"Best Model: {best_model_path}\")\n",
    "    logger.info(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    logger.info(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    logger.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    logger.info(\"=\"*100)\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a unique ID for the run\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create directory structure\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    run_dir = os.path.join('models', f'run_{run_id}')\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    log_file = os.path.join(run_dir, f\"run_{run_id}.log\")\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    fhandler = logging.FileHandler(filename=log_file, mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "\n",
    "    # Log start of experiment\n",
    "    logger.info(\"=\"*100)\n",
    "    logger.info(f\"Starting new experiment with run ID: {run_id}\")\n",
    "    logger.info(\"=\"*100)\n",
    "\n",
    "    # Select device to run on\n",
    "    device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Transformations for training data with data augmentation\n",
    "    training_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),  # Rotate by up to 10 degrees\n",
    "        transforms.RandomAffine(0, scale=(0.8, 1.2)),  # Random scaling\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),  # Normalize with MNIST mean/std (pre-computed)\n",
    "    ])\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    training_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=training_transform) # Apply transformations to the training data\n",
    "    testing_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "    logger.info(f\"Loaded datasets - Training: {len(training_dataset)} samples, Testing: {len(testing_dataset)} samples\")\n",
    "\n",
    "    # Hyperparameters options\n",
    "    num_variants = 5\n",
    "    num_epochs = 10\n",
    "    batch_size_learning_rate_options = [(32, 0.0001), (64, 0.0001), (128, 0.0001)]\n",
    "    l1_lambda_options = [0.0001, 0.00001, 0.000001]\n",
    "    l2_lambda_options = [0.001, 0.0001, 0.00001]\n",
    "    dropout_rate_options = [0.1, 0.25, 0.5]\n",
    "    early_stopping_patience = 5\n",
    "\n",
    "    hyperparameters_list = []\n",
    "    for _ in range(num_variants):\n",
    "        # Randomly select hyperparameters\n",
    "        batch_size, learning_rate = random.choice(batch_size_learning_rate_options)\n",
    "        l1_lambda = random.choice(l1_lambda_options)\n",
    "        l2_lambda = random.choice(l2_lambda_options)\n",
    "        dropout_rate = random.choice(dropout_rate_options)\n",
    "\n",
    "        # Construct the hyperparameters dictionary\n",
    "        hyperparameters = {\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'l1_lambda': l1_lambda,\n",
    "            'l2_lambda': l2_lambda,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'early_stopping_patience': early_stopping_patience\n",
    "        }\n",
    "\n",
    "        # Ensure hyperparameters are unique\n",
    "        if hyperparameters not in hyperparameters_list:\n",
    "            hyperparameters_list.append(hyperparameters)\n",
    "\n",
    "    # Track best model and accuracy\n",
    "    best_model_accuracy = 0\n",
    "    best_model_id = None\n",
    "    \n",
    "    # Track all model results\n",
    "    model_results = []\n",
    "    \n",
    "    # Train a model for each set of hyperparameters\n",
    "    for i, hyperparameters in enumerate(hyperparameters_list):\n",
    "        # Create a unique ID for this model\n",
    "        model_id = f\"variant_{i+1}\"\n",
    "        \n",
    "        # Log start of training for this model\n",
    "        logger.info(\"=\"*100)\n",
    "        logger.info(f\"Starting training for model {model_id}\")\n",
    "        logger.info(f\"Hyperparameters:\")\n",
    "        logger.info(f\"Learning rate: {hyperparameters['learning_rate']}\")\n",
    "        logger.info(f\"Batch size: {hyperparameters['batch_size']}\")\n",
    "        logger.info(f\"L1 lambda: {hyperparameters['l1_lambda']}\")\n",
    "        logger.info(f\"L2 lambda: {hyperparameters['l2_lambda']}\")\n",
    "        logger.info(f\"Dropout rate: {hyperparameters['dropout_rate']}\")\n",
    "        logger.info(f\"Early stopping patience: {hyperparameters['early_stopping_patience']}\")\n",
    "        \n",
    "        # Initialize a fresh model for each set of hyperparameters with the specified dropout rate\n",
    "        model = Perceptron(dropout_rate=hyperparameters['dropout_rate']).to(device)\n",
    "        \n",
    "        # Train the model and get test accuracy\n",
    "        accuracy = train_model(model, training_dataset, testing_dataset, hyperparameters, model_id, run_id)\n",
    "        \n",
    "        # Log the results for this model\n",
    "        logger.info(f\"Completed training for model {model_id}\")\n",
    "        logger.info(f\"Test accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Store model results\n",
    "        model_results.append({\n",
    "            'model_id': model_id,\n",
    "            'accuracy': accuracy,\n",
    "            'hyperparameters': hyperparameters\n",
    "        })\n",
    "        \n",
    "        # Track the best model\n",
    "        if accuracy > best_model_accuracy:\n",
    "            best_model_accuracy = accuracy\n",
    "            best_model_id = model_id\n",
    "            logger.info(f\"New best model: {model_id} with accuracy {accuracy:.2f}%\")\n",
    "    \n",
    "    # Log summary of all models\n",
    "    logger.info(\"=\"*100)\n",
    "    logger.info(\"Training completed for all models\")\n",
    "    logger.info(\"Summary of model performances:\")\n",
    "    \n",
    "    for result in sorted(model_results, key=lambda x: x['accuracy'], reverse=True):\n",
    "        logger.info(f\"Model {result['model_id']}: Accuracy {result['accuracy']:.2f}%\")\n",
    "        logger.info(f\"  Learning rate: {result['hyperparameters']['learning_rate']}\")\n",
    "        logger.info(f\"  Batch size: {result['hyperparameters']['batch_size']}\")\n",
    "        logger.info(f\"  L1 lambda: {result['hyperparameters']['l1_lambda']}\")\n",
    "        logger.info(f\"  L2 lambda: {result['hyperparameters']['l2_lambda']}\")\n",
    "        logger.info(f\"  Dropout rate: {result['hyperparameters']['dropout_rate']}\")\n",
    "        logger.info(f\"  Early stopping patience: {result['hyperparameters']['early_stopping_patience']}\")\n",
    "        logger.info(\"-\" * 50)\n",
    "    \n",
    "    # Log best model details\n",
    "    logger.info(\"=\"*100)\n",
    "    logger.info(f\"Best model: {best_model_id} with accuracy: {best_model_accuracy:.2f}%\")\n",
    "    best_model_hp = next(result['hyperparameters'] for result in model_results if result['model_id'] == best_model_id)\n",
    "    logger.info(f\"Best model hyperparameters:\")\n",
    "    logger.info(f\"  Learning rate: {best_model_hp['learning_rate']}\")\n",
    "    logger.info(f\"  Batch size: {best_model_hp['batch_size']}\")\n",
    "    logger.info(f\"  L1 lambda: {best_model_hp['l1_lambda']}\")\n",
    "    logger.info(f\"  L2 lambda: {best_model_hp['l2_lambda']}\")\n",
    "    logger.info(f\"  Dropout rate: {best_model_hp['dropout_rate']}\")\n",
    "    logger.info(f\"  Early stopping patience: {best_model_hp['early_stopping_patience']}\")\n",
    "    logger.info(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
