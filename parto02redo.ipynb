{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kravspecifikation f√∂r del 2\n",
    "\n",
    "1. ‚úÖ¬†B√∂rja anv√§nda basala **MLOps best-practices:**\n",
    "    1. üìä¬†Ordna s√• att du har ett bra s√§tt att √∂vervaka tr√§ningen, samt att spara (och h√•lla ordning p√•!) versioner av modellerna du tr√§nar upp. (Tips: Foldrar, smart valda filnamn, etc).\n",
    "    2. üíæ Kom ih√•g att det troligen inte √§r den sista epokens checkpoint (sparad version) som √§r den b√§sta, p.g.a. overfitting om du k√∂r m√•nga epochs. Du beh√∂ver sj√§lv se till att spara checkpoints lite d√• och d√• under tr√§ningen.\n",
    "    3. üóÑÔ∏è¬†Versionshantering av parametrar, kod, och checkpoints (modellens parametrar) blir snabbt v√§ldigt viktigt n√§r man b√∂rjar jobba seri√∂st med machine learning. Om man inte h√•ller reda p√• vilken k√∂rning som gjorts med vilka parametrar, s√• blir det sv√•rt att j√§mf√∂ra prestanda, vidareutveckla modellen senare, etc. Ingen vill ha en modell levererad d√§r programmeraren inte minns hur den gjordes.\n",
    "    4. üöÄ **F√∂rdjupning/√∂verkurs:** Vilka verktyg finns som kan hj√§lpa till att h√•lla ordning p√• versioner, k√∂rningar, resultat, hyper parameters, etc?\n",
    "2. üöÄ **F√∂rdjupning/√∂verkurs:** Addera **performance metrics**, s√• att du kan se hur l√•ng tid respektive tr√§nings-k√∂rning tar. (En k√∂rning = Alla epochs av tr√§ning f√∂r en given modell med specificerad upps√§ttning hyper parameters).\n",
    "3. ‚úÖ¬†Applicera l√§mpliga **data augmentation** methods f√∂r att artificiellt variera, och till och med ‚Äúf√∂rstora‚Äù datam√§ngden lite. Anv√§nd exempelvis skalning, rotation, f√§rgvariation, brus, (spegelv√§ndning?), etc. H√§r √§r lite repetition om data augmentation:\n",
    "    1. Se avsnittet ‚Äú[Uppgift 1 Perceptron foÃàr OCR](https://www.notion.so/Uppgift-1-Perceptron-fo-r-OCR-d9e945f1551341e88665775acacc0bb6?pvs=21)‚Äù l√§ngre ner i denna uppgift (del 3).\n",
    "    2. Bra 10-minuters video om √§mnet: [Pytorch-Data-Augmentation-using-Torchvision](https://youtu.be/Zvd276j9sZ8?si=9fGaNnoRzsHrvxB6)\n",
    "    3. Observera att det inte √§r s√§kert att data augmentation leder till b√§ttre resultat i alla l√§gen, eftersom det beror p√• detaljerna i vad man bygger, och vilken data man har.\n",
    "4. ‚úÖ¬†Det √§r nu dags att testa CNN ist√§llet f√∂r FFN. Byt ut de f√∂rsta lagren i modellen till **convolutional layers** f√∂r att ta upp translationsinvarians i bilden. S√∂k sj√§lv upp vad man brukar ha ***direkt efter varje convolutional layer*** f√∂r att h√•lla ordning p√• antalet dimensioner till n√§sta lager .\n",
    "    \n",
    "    > (Sv√•rt ord ‚Äútranslations-invarians‚Äù: translation=f√∂rflyttning, invarians=‚Äùsamma oavsett‚Äù ‚áí translationsinvarians = ‚Äùspelar ingen roll var i bilden‚Äù)\n",
    "    > \n",
    "5. ‚úÖ¬†Experimentera d√§refter med att addera ett till (eller flera) convolutional layer(s). Tanken √§r att testa **olika n√§t-arkitekturer**. Anv√§nd Google/Chat f√∂r initial gissning f√∂r modell-arkitektur, men sedan √§r det trial-and-error som g√§ller. Att fundera p√•:\n",
    "    1. ‚öñÔ∏è Hur p√•verkas resultatet av olika modell-arkitektur? Var noga med att inte j√§mf√∂ra modeller med stor skillnad i antal parameters, om aspekter som tex lagertyp ska utv√§rderas.\n",
    "    2. ü§î Vilken typ av features detekteras typiskt av de senare convolutional-lagren j√§mf√∂rt med det f√∂rsta convolutional-lagret?\n",
    "        1. Hint: Videon fr√•n lektion om convolutional layers: [Convolutional-Neural-Networks-Explained](https://youtu.be/pj9-rr1wDhM?si=cjWmR5ets048WjfZ). (Sj√§lvfallet kan du ocks√• plotta weight-matrices fr√•n din egen modell ocks√• om du vill!)\n",
    "6. ‚úÖ¬†Applicera l√§mpliga **regularization methods** f√∂r att minimera risken f√∂r overfitting och g√∂ra cost-function-landskapet f√∂rdelaktigt f√∂r back-prop.\n",
    "    1. Exempel p√• s√•dana metoder √§r: drop-out, weight-decay, noise injection, batch normalization, etc.\n",
    "    2. Mer info om regularization: [understanding-regularization-with-pytorch](https://medium.com/analytics-vidhya/understanding-regularization-with-pytorch-26a838d94058)\n",
    "7. üöÄ **F√∂rdjupning:** Skapa en lista av kombinationer av hyper parameters, och l√•t datorn tr√§na din modell p√• nytt f√∂r samtliga hyperparameter settings i din lista. Detta kallas f√∂r **hyper-parameter tuning**, d.v.s. att via trial-and-error hitta inst√§llningar som funkar.\n",
    "    - Enklast m√∂jliga hyper-param-tuning: G√∂r en lista med t.ex. 10 rader (en per k√∂rning), d√§r varje rad specificerar alla hyper-parameters som kan vara sv√•rt att gissa optimalt v√§rde p√•:\n",
    "        - Antal neuroner f√∂r respektive lager\n",
    "        - Typ av activation function\n",
    "        - Vilka lager som ska vara convolutional, storlek p√• convolutional kernel, antal convolutional kernels per lager, etc.\n",
    "        - Learning rate / hyper-params till optimizer ADAM\n",
    "        - Drop-out level\n",
    "        - Settings f√∂r olika typer av data augmentation (rotation, skalning, noise, etc)\n",
    "    - Det finns √§ven en m√§ngd automatiserade typer av hyper-param-tuning. Se exempelvis: https://en.wikipedia.org/wiki/Hyperparameter_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 18:58:19,597 - INFO - ====================================================================================================\n",
      "2025-04-23 18:58:19,598 - INFO - Run ID: 20250423_185819\n",
      "2025-04-23 18:58:19,598 - INFO - Training configuration:\n",
      "2025-04-23 18:58:19,599 - INFO - Learning rate: 0.0001\n",
      "2025-04-23 18:58:19,599 - INFO - Batch size: 64\n",
      "2025-04-23 18:58:19,599 - INFO - L1 regularization: 0.0001\n",
      "2025-04-23 18:58:19,600 - INFO - L2 regularization: 0.001\n",
      "2025-04-23 18:58:19,600 - INFO - Epochs: 10\n",
      "2025-04-23 18:58:19,601 - INFO - Optimizer: Adam\n",
      "2025-04-23 18:58:19,601 - INFO - Loss function: CrossEntropyLoss\n",
      "2025-04-23 18:58:30,657 - INFO - ====================================================================================================\n",
      "2025-04-23 18:58:30,658 - INFO - Epoch [1/10]\n",
      "2025-04-23 18:58:30,659 - INFO - Training Loss: 0.8305\n",
      "2025-04-23 18:58:33,147 - INFO - Validation Loss: 0.3006\n",
      "2025-04-23 18:58:33,148 - INFO - Validation Accuracy: 91.67%\n",
      "2025-04-23 18:58:33,152 - INFO - Epoch 1 duration: 13.55 seconds\n",
      "2025-04-23 18:58:44,116 - INFO - ====================================================================================================\n",
      "2025-04-23 18:58:44,117 - INFO - Epoch [2/10]\n",
      "2025-04-23 18:58:44,117 - INFO - Training Loss: 0.4708\n",
      "2025-04-23 18:58:46,676 - INFO - Validation Loss: 0.2228\n",
      "2025-04-23 18:58:46,676 - INFO - Validation Accuracy: 94.10%\n",
      "2025-04-23 18:58:46,681 - INFO - Epoch 2 duration: 13.53 seconds\n",
      "2025-04-23 18:58:57,638 - INFO - ====================================================================================================\n",
      "2025-04-23 18:58:57,639 - INFO - Epoch [3/10]\n",
      "2025-04-23 18:58:57,639 - INFO - Training Loss: 0.3743\n",
      "2025-04-23 18:59:00,107 - INFO - Validation Loss: 0.1785\n",
      "2025-04-23 18:59:00,108 - INFO - Validation Accuracy: 95.12%\n",
      "2025-04-23 18:59:00,112 - INFO - Epoch 3 duration: 13.43 seconds\n",
      "2025-04-23 18:59:11,022 - INFO - ====================================================================================================\n",
      "2025-04-23 18:59:11,023 - INFO - Epoch [4/10]\n",
      "2025-04-23 18:59:11,023 - INFO - Training Loss: 0.3266\n",
      "2025-04-23 18:59:13,532 - INFO - Validation Loss: 0.1513\n",
      "2025-04-23 18:59:13,532 - INFO - Validation Accuracy: 96.01%\n",
      "2025-04-23 18:59:13,536 - INFO - Epoch 4 duration: 13.42 seconds\n",
      "2025-04-23 18:59:24,447 - INFO - ====================================================================================================\n",
      "2025-04-23 18:59:24,448 - INFO - Epoch [5/10]\n",
      "2025-04-23 18:59:24,449 - INFO - Training Loss: 0.3001\n",
      "2025-04-23 18:59:26,897 - INFO - Validation Loss: 0.1504\n",
      "2025-04-23 18:59:26,898 - INFO - Validation Accuracy: 95.92%\n",
      "2025-04-23 18:59:26,902 - INFO - Epoch 5 duration: 13.36 seconds\n",
      "2025-04-23 18:59:37,806 - INFO - ====================================================================================================\n",
      "2025-04-23 18:59:37,807 - INFO - Epoch [6/10]\n",
      "2025-04-23 18:59:37,807 - INFO - Training Loss: 0.2808\n",
      "2025-04-23 18:59:40,269 - INFO - Validation Loss: 0.1360\n",
      "2025-04-23 18:59:40,270 - INFO - Validation Accuracy: 96.13%\n",
      "2025-04-23 18:59:40,274 - INFO - Epoch 6 duration: 13.37 seconds\n",
      "2025-04-23 18:59:51,082 - INFO - ====================================================================================================\n",
      "2025-04-23 18:59:51,083 - INFO - Epoch [7/10]\n",
      "2025-04-23 18:59:51,083 - INFO - Training Loss: 0.2651\n",
      "2025-04-23 18:59:53,534 - INFO - Validation Loss: 0.1477\n",
      "2025-04-23 18:59:53,535 - INFO - Validation Accuracy: 95.88%\n",
      "2025-04-23 18:59:53,539 - INFO - Epoch 7 duration: 13.26 seconds\n",
      "2025-04-23 19:00:04,394 - INFO - ====================================================================================================\n",
      "2025-04-23 19:00:04,395 - INFO - Epoch [8/10]\n",
      "2025-04-23 19:00:04,395 - INFO - Training Loss: 0.2515\n",
      "2025-04-23 19:00:06,868 - INFO - Validation Loss: 0.1385\n",
      "2025-04-23 19:00:06,869 - INFO - Validation Accuracy: 95.96%\n",
      "2025-04-23 19:00:06,873 - INFO - Epoch 8 duration: 13.33 seconds\n",
      "2025-04-23 19:00:17,735 - INFO - ====================================================================================================\n",
      "2025-04-23 19:00:17,735 - INFO - Epoch [9/10]\n",
      "2025-04-23 19:00:17,736 - INFO - Training Loss: 0.2456\n",
      "2025-04-23 19:00:20,180 - INFO - Validation Loss: 0.1171\n",
      "2025-04-23 19:00:20,180 - INFO - Validation Accuracy: 96.58%\n",
      "2025-04-23 19:00:20,185 - INFO - Epoch 9 duration: 13.31 seconds\n",
      "2025-04-23 19:00:31,038 - INFO - ====================================================================================================\n",
      "2025-04-23 19:00:31,038 - INFO - Epoch [10/10]\n",
      "2025-04-23 19:00:31,039 - INFO - Training Loss: 0.2356\n",
      "2025-04-23 19:00:33,480 - INFO - Validation Loss: 0.1104\n",
      "2025-04-23 19:00:33,481 - INFO - Validation Accuracy: 96.95%\n",
      "2025-04-23 19:00:33,485 - INFO - Epoch 10 duration: 13.30 seconds\n",
      "2025-04-23 19:00:33,486 - INFO - Training duration: 133.88 seconds\n",
      "2025-04-23 19:00:34,240 - INFO - ====================================================================================================\n",
      "2025-04-23 19:00:34,240 - INFO - Testing duration: 0.75 seconds\n",
      "2025-04-23 19:00:34,241 - INFO - Total duration: 134.64 seconds\n",
      "2025-04-23 19:00:34,241 - INFO - ====================================================================================================\n",
      "2025-04-23 19:00:34,241 - INFO - Best Model: models/run_20250423_185819/checkpoints/checkpoint_epoch_10.pth\n",
      "2025-04-23 19:00:34,242 - INFO - Test Loss: 0.5036\n",
      "2025-04-23 19:00:34,242 - INFO - Test Accuracy: 96.77%\n",
      "2025-04-23 19:00:34,243 - INFO - ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the perceptron neural-network model\n",
    "class Perceptron(nn.Module):\n",
    "    # Define the constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), # 32x32 with 1 padding for 28x28 input dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2) # 14x14 output dimensions\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * 14 * 14, 128), # 128 neurons in the hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10) # 10 neurons in the output layer\n",
    "        )\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        # Pass through the convolutional layers\n",
    "        logits = self.conv_layers(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        logits = logits.view(logits.size(0), -1)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        logits = self.fc_layers(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Select device to run on\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize the model\n",
    "model = Perceptron().to(device)\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "l1_lambda = 0.0001\n",
    "l2_lambda = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "# Transformations for training data with data augmentation\n",
    "training_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),  # Rotate by up to 10 degrees\n",
    "    transforms.RandomAffine(0, scale=(0.8, 1.2)),  # Random scaling\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),  # Normalize with MNIST mean/std (pre-computed)\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "training_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=training_transform) # Apply transformations to the training data\n",
    "testing_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Split training data into train and validation subsets\n",
    "training_subset_size = int(0.8 * len(training_dataset))\n",
    "validation_subset_size = len(training_dataset) - training_subset_size\n",
    "training_subset, validation_subset = random_split(training_dataset, [training_subset_size, validation_subset_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(training_subset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
    "testing_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create a unique id and directory for the run\n",
    "checkpoint_filename_prefix = 'checkpoint'\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = os.path.join('models', f'run_{run_id}')\n",
    "checkpoints_dir = os.path.join(run_dir, 'checkpoints')\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "log_file = os.path.join(run_dir, f'run_{run_id}_training.log')\n",
    "fhandler = logging.FileHandler(filename=log_file, mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "\n",
    "# Log hyperparameters\n",
    "logger.info(\"=\" * 100)\n",
    "logger.info(f\"Run ID: {run_id}\")\n",
    "logger.info(f\"Training configuration:\")\n",
    "logger.info(f\"Learning rate: {learning_rate}\")\n",
    "logger.info(f\"Batch size: {batch_size}\")\n",
    "logger.info(f\"L1 regularization: {l1_lambda}\")\n",
    "logger.info(f\"L2 regularization: {l2_lambda}\")\n",
    "logger.info(f\"Epochs: {num_epochs}\")\n",
    "logger.info(f\"Optimizer: Adam\")\n",
    "logger.info(f\"Loss function: CrossEntropyLoss\")\n",
    "\n",
    "# Training and validation loop\n",
    "training_start_time = time.time()\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = None\n",
    "for epoch in range(num_epochs):\n",
    "    # Cache the start time of the epoch\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        # Move data to device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "\n",
    "        # Calculate base loss\n",
    "        base_loss = criterion(outputs, y)\n",
    "        \n",
    "        # L1 regularization (L2 is handled by weight_decay)\n",
    "        l1_reg = torch.tensor(0., device=device)\n",
    "        for param in model.parameters():\n",
    "            l1_reg += torch.norm(param, 1)\n",
    "            \n",
    "        # Total loss with L1 regularization\n",
    "        loss = base_loss + l1_lambda * l1_reg\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    # Print training loss\n",
    "    logger.info(\"=\"*100)\n",
    "    logger.info(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    logger.info(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in validation_loader:\n",
    "            # Move data to device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_val_loss = running_val_loss / len(validation_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    # Print validation loss and accuracy\n",
    "    logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    logger.info(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save the checkpoint\n",
    "    checkpoint_filename = f'{checkpoint_filename_prefix}_epoch_{epoch+1}.pth'\n",
    "    checkpoint_path = os.path.join(checkpoints_dir, checkpoint_filename)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    # Update the best model if the current model has a lower validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_model_path = checkpoint_path # Cache the path to the best model for later testing\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "    # Log the duration of the epoch\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    logger.info(f\"Epoch {epoch+1} duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "training_duration = training_end_time - training_start_time\n",
    "logger.info(f\"Training duration: {training_duration:.2f} seconds\")\n",
    "\n",
    "# Get the best model for testing\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Testing loop\n",
    "testing_start_time = time.time()\n",
    "model.eval()\n",
    "running_test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in testing_loader:\n",
    "        # Move data to device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        running_test_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "avg_test_loss = running_test_loss / len(testing_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "# Log the results\n",
    "testing_end_time = time.time()\n",
    "testing_duration = testing_end_time - testing_start_time\n",
    "total_run_time = testing_end_time - training_start_time\n",
    "logger.info(\"=\"*100)\n",
    "logger.info(f\"Testing duration: {testing_duration:.2f} seconds\")\n",
    "logger.info(f\"Total duration: {total_run_time:.2f} seconds\")\n",
    "logger.info(\"=\"*100)\n",
    "logger.info(f\"Best Model: {best_model_path}\")\n",
    "logger.info(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "logger.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "logger.info(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
